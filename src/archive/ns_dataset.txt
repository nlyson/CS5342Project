Random and Pseudorandom Numbers
When to use random numbers?•Generation of a stream key for symmetric stream cipher •Generation of keys for public-key algorithms•RSA public-key encryption algorithm (described in Chapter 3)•Generation of a symmetric key for use as a temporary session key•used in a number of networking applications, such as Transport Layer Security (Chapter 5), Wi-Fi (Chapter 6), e-mail security (Chapter 7), and IP security (Chapter 8) •In a number of key distribution scenarios•Kerberos (Chapter 4)
Two types of random numbers•True random numbers:•generated in non-deterministic ways.
They are not predictable and repeatable•Pseudorandom numbers:•appear random, but are obtained in a deterministic, repeatable, and predictable manner
Properties of Random Numbers•Randomness•Uniformity•distribution of bits in the sequence should be uniform •Independence•no one subsequence in the sequence can be inferred from the others •Unpredictable•satisfies the "next-bit test“
Entropy•A measure of uncertainty•In other words, a measure of how unpredictable the outcomes are•High entropy= unpredictable outcomes = desirable in cryptography•The uniform distribution has the highest entropy (every outcome equally likely, e.g. fair coin toss)•Usually measured in bits (so 3 bits of entropy = uniform, random distribution over 8 values) Entropy of an information source
Know Your Threat Model•Threat model:A model of who your attacker is and what resources they have•One of the best ways to counter an attacker is to attack their reasons
Story…•The bear race•Takeaway:Even if a defense is not perfect, it is important to always stay on top of best security measures I don’t have to outrun the bear.
I just have to outrun you
Human Factors•The users•Users like convenience (ease of use)•If a security system is unusable, it will be unused•Users will find way to subvert security systems if it makes their lives easier•The programmers•Programmers make mistakes•Programmers use tools that allow them to make mistakes (e.g.C and C++)•Everyone else•Social engineering attacks exploit other people’s trust and access for personal gain
Design in security from the start•When building a new system, include security as part of the design considerations rather than patching it after the fact•A lot of systems today were not designed with security from the start, resulting in patches that don’t fully fix the problem!•Keep these security principles in mind whenever you write code!
Security Services and Mechanisms
Supplementary materials•Internet Security Glossary, v2 –produced by Internet Society (ISOC) https://datatracker.ietf.org/doc/html/rfc4949•X.800 –OSI network securityhttps://www.itu.int/rec/dologin_pub.asp?lang=f&id=T-REC-X.800-199103-I!!PDF-E&type=items
Summary for Chapter 1•Have learned:•Security requirements•Attack models•X.800 secure architecture, security services, mechanisms
Review Questions•William Stallings (WS), “Network Security Essentials”, 6thGlobal Edition•RQ 1.1 -1.3•Prob 1.5
AES Specification•symmetric block cipher •128-bit data, 128/192/256-bit keys •stronger & faster than Triple-DES •provide full specification & design details •both C & Java implementations•NIST have released all submissions & unclassified analyseshttps://csrc.nist.gov/CSRC/media/Projects/Cryptographic-Standards-and-Guidelines/documents/aes-development/Rijndael-ammended.pdf
The AES Cipher -Rijndael •an iterative rather than feistelcipher•treats data in 4 groups of 4 bytes•operates an entire block in every round•designed to be:•resistant against known-plaintext attacks•speed and code compactness on many CPUs•design simplicity
Rijndael•processes data as 4 groups of 4 bytes (state) = 128 bits•has 10/12/14 rounds in which state undergoes: •byte substitution (1 S-box used on every byte) •shift rows (permute bytes row by row) •mix columns (alter each byte in a column as a function of all of the bytes in the column) •add round key (XOR state with key material) •128-bit keys –10 rounds, 192-bit keys –12 rounds, 256-bit keys –14 rounds
AES Encryption and Decryption
AES encryption round
AES pros•Most operations can be combined into XOR and table lookups -hence very fast & efficient
Take-home Exercises•Find an AES API to encrypt a text (A), then decrypt it and check whether the original text (A) equals the decrypted text (B).
Whether A = B?•Compare the decryption time with different key lengths, and with DES and 3DES.
•Suggestions: find a large A file.
Run decryption a couple of times and take the average.
Reading materials•FIPS 197, Advanced Encryption Standard (AES) (nist.gov)
WPEC 2024: NIST Workshop on Privacy-Enhancing Cryptography•Time: September 24–26, 2024•Free to register•Virtual conference via Zoom•https://csrc.nist.gov/events/2024/wpec2024
A strong encryption algorithm attackerencryption algorithmplaintext / enquirycyphertext / response
Secure Encryption Scheme•Unconditional security•no matter how much computer power is available, the cipher cannot be broken since the ciphertext provides insufficient information to uniquely determine the corresponding plaintext•Computational security•the cost of breaking the cipher exceeds the value of the encrypted information;•or the time required to break the cipher exceeds the useful lifetime of the information
Desired characteristics•Cipher needs to completely obscure statistical properties of original message•more practically Shannon suggested combining elements to obtain:•Confusion –how does changing a bit of the key affect the ciphertext?•Diffusion –how does changing one bit of the plaintext affect the ciphertext?
confusion ciphertext plaintextdiffusion
Ways to achieve•Symmetric Encryption: •substitution / transposition / hybrid•Asymmetric Encryption: •Mathematical hardness -problems that are efficient to compute in one direction, but inefficient to reverse by the attacker•Examples: Modular arithmetic, factoring, discrete logarithm problem, Elliptic Logs over Elliptic Curves
Two basic types•Block Ciphers •Typically64, 128 bitblocks•A k-bit plaintext block maps to a k-bit ciphertext block •Usually employ Feistel structure •Stream Ciphers•A key is used to generate a stream of pseudo-random bits –key stream•Just XOR plaintext bits with the key stream for encryption •For decryption generate the key stream and XOR with the ciphertext!
Symmetric Block Encryption
Block cipher•the most commonly usedsymmetric encryption algorithms•input: fixed-size blocks (Typically 64, 128 bitblocks), output: equal size blocks•provide secrecy and/or authentication services•Data Encryption Standard (DES), triple DES (3DES), and the Advanced Encryption Standard (AES)s•Usually employ Feistel structure
Feistel Cipher Structure
Feistel Cipher Structure•most symmetric block ciphers are based on a Feistel Cipher Structure•based on the two primitive cryptographic operations•substitution(S-box)•permutation (P-box)•provide confusionand diffusionof message
Feistel Cipher Structure•Horst Feistel devised the feistelcipher in the 1973•based on concept of invertible product cipher•partitions input block into two halves•process through multiple rounds which•perform a substitution on left data half•based on round function of right half & subkey•then have permutation swapping halves•implements Shannon’s substitution-permutation network concept
Feistel Encryption and Decryption
1 INTRODU CTION  It was a dark and stormy night.
Somewhere in the distance a dog howled.
A shiny object caught  Alice’s eye.
A diamond cufflink!
Only one person in the household could afford diamond cufflinks!
 So it was the butler, after all!
Alice had to warn Bob.
But how could she get a message to him with - out alerting the butler?
If she phoned Bob, the butler might listen on an extension.
If she sent a car - rier pigeon out the window with the message taped to its foot, how would Bob know it was Alice  that was sending the message and not Trudy attempting to frame the butler because he spurned her  advances?
 That’s what this book is about.
Not much character development for Alice and Bob, we’re  afraid; nor do we really get to know the butler.
But we do discuss how to communicate securely  over an insecure medium.
 What do we mean by “communicating securely”?
Alice should be able to send a message to  Bob that only Bob can understand, even though Alice can’t avoid having others see what she sends.
 When Bob receives a message, he should be able to know for certain that it was Alice who sent the  message, and that nobody tampered with the contents of the message in the time between when  Alice launched the message and Bob received it.
 What do we mean by an “insecure medium”?
Well, in some dictionary or another, under the  definition of “insecure medium” should be a picture of the Internet.
The world is evolving towards  interconnecting every computer, household appliance, automobile, child’s toy, and embedded med - ical device, all into some wonderful global internetwork.
How wonderful!
You’d be able to control your nuclear power plant with simple commands sent across the network while you were vacation - ing in Fiji.
Or sunny Havana.
Or historic Pyongyang.
Inside the network the world is scary.
There  are links that eavesdroppers can listen in on.
Information needs to be forwarded through packet  switches, and these switches can be reprogrammed to listen to or modify data in transit.
 The situation might seem hopeless, but we may yet be saved by the magic of cryptography,  which can take a message and transform it into a bunch of numbers known as ciphertext.
The  ciphertext is unintelligible gibberish except to someone who knows the secret to reversing the trans - formation.
Cryptography allows us to disguise our data so that eavesdroppers gain no information  from listening to the information as transmitted.
Cryptography also allows us to create an unforge - able message and detect if it has been modified in transit.
One method of accomplishing this is with  a digital signature , a number associated with a message and its sender that can be verified as  1
2 I NTRODUC TION 1.1  authentic by others, but can only be generated by the sender.
This should seem astonishing.
How  can there be a number you can verify but not generate?
A person’s handwritten signature can (more  or less) only be generated by that person, though it can (more or less) be verified by others.
But it would seem as if a number shouldn’t be hard to generate, especially if it can be verified.
Theoreti - cally, you could generate someone’s digital signature by trying lots of numbers and testing each one  until one passed the verification test.
But with the size of the numbers used, it would take too much  compute time (for instance, several universe lifetimes) to generate the signature that way.
So a digi- tal signature has the same property as a handwritten signature (theoretically) has, in that it can only  be generated by one person but can be verified by lots of people.
But a digital signature does more  than a handwritten signature.
Since the digital signature depends on the contents of the message, if  someone alters the message the signature will no longer be correct, and the tampering will be  detected.
This will all become clear if you read Chapter 2 Introduction to Cryptography .
 Cryptography is a major theme in this book, not because cryptography is intrinsically inter - esting (which it is), but because many of the security features people want in a computer network  can best be provided through cryptography.
 1.1 OPINIO NS, PRODUC TS  Opinions expressed are those of the authors alone (and possibly not even agreed upon by all the authors).
Opinions do not necessarily represent the views of any of the authors’ past, current, or  future employers.
Any mention of commercial products or reference to commercial organizations is  for information only.
It does not imply recommendation or endorsement by NIST or any of the cur - rent, future, or prior organizations employing any of the authors.
 1.2 ROADM AP TO THE BOOK  We aim to make this book comprehensible to engineers, giving intuition about designs.
But read - ability doesn’t mean lack of technical depth.
We try to go beyond the information one might find in specifications to give insight into the designs.
Given that specifications are easily available on the  web today, we do not give exact packet formats.
 This book should be usable as a textbook at either the undergraduate or graduate level.
Most  of the chapters have homework problems at the end.
And to make life easier for professors who  want to use the book, we will provide slides and an answer manual (to the professors using the
1.2 ROADM AP TO THE BOOK 3  book) for many of the chapters.
Even if you are not taking a class, you might want to do the home - work problems.
This book should be understandable to anyone with technical curiosity, a sense of  humor*, and a good night’s sleep in the recent past.
The chapters are:  • Chapter 1 Introduction : This gives an overview of the chapters and some network basics.
 • Chapter 2 Introduction to Cryptography : This explains the cryptographic principles which  will be covered in more detail in later chapters.
 • Chapter 3 Secret Key Cryptography : This chapter explains the uses of secret key crypto - graphic algorithms and describes how cryptographers create these algorithm.
 • Chapter 4 Modes of Operation : Since most secret key algorithms encrypt a fairly small ( e.g.,  128 bits) block, this chapter explains various algorithms to efficiently and securely encrypt arbitrarily large amounts of data.
 • Chapter 5 Cryptographic Hashes : This chapter explains what hashes are used for and intu - ition into the methods by which cryptographers create secure and efficient hashes.
 • Chapter 6 First-Generation Public Key Algorithms : This chapter describes the designs of  the current widely deployed public key algorithms.
Unfortunately, these would be insecure if  the world were able to create a sufficiently large quantum computer.
So the world will  migrate to different public key algorithms that we’ll describe in Chapter 8 Post-Quantum  Cryptography .
 • Chapter 7 Quantum Computing : This chapter gives an intuitive understanding of how a  quantum computer differs from a classical computer, as well as explaining the intuition  behind the two major cryptography-relevant quantum algorithms (Grover’s and Shor’s).
 • Chapter 8 Post-Quantum Cryptography : This describes the types of math problems that  would remain difficult to solve even if there were quantum computers, how to turn them into  public key algorithms, and various optimizations that can make them efficient.
 • Chapter 9 Authentication of People : This chapter describes the challenges involved in  authenticating humans, and various types of technology that are or could be deployed.
 • Chapter 10 Trusted Intermediaries : This chapter describes technologies for distributing  cryptographic keys.
It also talks about trust model issues in today’s deployed designs.
 • Chapter 11 Communication Session Establishment : This chapter describes conceptual  issues in doing mutual authentication handshakes and establishing secure sessions.
 • Chapter 12 IPsec : This chapter goes into detail about the design of IPsec.
 • Chapter 13 SSL/TLS and SSH : This chapter goes into detail about the design of SSL/TLS  and SSH.
 *Although a sense of humor is not strictly necessary for understanding the book, it is an important characteristic to have in  general.
4 I NTRODUC TION 1.3  • Chapter 14 Electronic Mail Security : This chapter describes various issues and solutions  involved in electronic mail.
 • Chapter 15 Electronic Money : This chapter describes various goals of electronic money and  various technologies to address them.
It covers cryptocurrencies and anonymous cash.
 • Chapter 16 Cryptographic Tricks : This describes various exotic technologies such as secure  multiparty computation and homomorphic encryption, as well as widely used technologies  such as secret sharing.
 • Chapter 17 Folklore : This chapter gives a summary of some of the design lessons discussed  in the rest of the book and also describes some common misconceptions.
 • Glossary : We define many of the terms we use in the book.
 • Math : This provides more in-depth coverage of the mathematics used in the rest of the book.
 The appendix is written solely by Mike Speciner.
It’s not essential for appreciating the rest of  the book.
It’s a sample of the content in Mike’s github repositories https://github.com/ms0 /.  1.3 TERMINOLOG Y  Computer science is filled with ill-defined terminology used by different authors in conflicting  ways.
Some people take terminology very seriously, and once they start to use a certain word in a  certain way, are extremely offended if the rest of the world does not follow.
 When I use a word, it means just what I choose it to mean—neither more  nor less.
 —Humpty Dumpty (in Through the Looking Glass )  Some terminology we feel fairly strongly about.
We do not use the term hacker to describe the van - dals that break into computer systems.
These criminals call themselves hackers, and that is how they got the name.
But they do not deserve the name.
True hackers are master programmers, incor - ruptibly honest, unmotivated by money, and careful not to harm anyone.
The criminals termed  “hackers” are not brilliant and accomplished.
It is really too bad that they not only steal money,  people’s time, and worse, but they’ve also stolen a beautiful word that had been used to describe  some remarkable and wonderful people.
We instead use words like intruder , bad guy , and impostor .
 We grappled with the terms secret key and public key cryptography.
Often in the security liter - ature the terms symmetric and asymmetric are used instead of secret and public .
When we say  secret key , we mean a key that is used both for encryption and decryption.
When we say public key ,  we are referring to a key pair consisting of a public key (used for encryption or signature verifica -
1.3 TERMINOLOGY 5  tion) and a private key (used for decryption or signing).
Using the terms public key and private key  is occasionally regrettable because both the words public and private start with “p”.
 We use the term privacy when referring to the desire to keep communication from being seen  by anyone other than the intended recipients.
Some people in the security community avoid the  term privacy because they feel its meaning has been corrupted to mean the right to know , because in  some countries there are laws known as privacy laws that state that citizens have the right to see  records kept about them.
Privacy also tends to be used when referring to keeping personal informa - tion about people from being collected and misused.
The security community also avoids the use of  the word secrecy , because secret has special meaning within the military context, and they feel it  would be confusing to talk about the secrecy of a message that was not actually labeled top secret  or secret .
The term most commonly used in the security community for keeping communication  from being seen is confidentiality .
We find that strange because confidential , like secret , is a secu- rity label, and the security community should have scorned use of confidential , too.
In the first edi - tion, we chose not to use confidentiality because we felt it had too many syllables, and saw no  reason not to use privacy .
For the second edition we reconsidered this decision, and were about to  change all use of privacy to confidentiality until one of us pointed out we’d have to change the book  title to something like Network Security: Confidential Communication in a Non-Confidential  World , at which point we decided to stick with privacy .
 Speaker: Isn’t it terrifying that on the Internet we have no privacy?
 Heckler1: You mean confidentiality .
Get your terms straight.
 Heckler2: Why do security types insist on inventing their own language?
 Heckler 3: It’s a denial-of-service attack.
 —Overheard at gathering of security types  We often refer to things involved in a conversation by name; for instance, Alice and Bob, whether  the things are people or computers.
This is a convenient way of making descriptions unambiguous  with relatively few words, since the pronoun she can be used for Alice, and he can be used for Bob.
 It also avoids lengthy inter-author arguments about whether to use the politically incorrect he, a  confusing she, an awkward he/she or (s)he , an ungrammatical they, an impersonal it, or an awkward  rewriting to avoid the problem.
We remain slightly worried that people will assume when we’ve  named things with human names that we are always referring to people.
Assume Alice, Bob, and the rest of the gang may be computers unless we specifically say something like the user Alice , in  which case we’re talking about a human.
 When we need a name for a bad guy, we usually choose Trudy (since it sounds like intruder )  or Eve (since it sounds like eavesdropper ) or Mallory (since it sounds like malice ).
Everyone would  assume Alice, Eve, and Trudy are she, and Bob is he.
For inclusivity, we wanted at least one of the evil characters to be male, and we chose Mallory as the name of a male evildoer.
Mallory can be
6 I NTRODUC TION 1.4  used for either gender, and is gaining more popularity as a female name, but when we use Mallory  we will assume Mallory is male and use the pronoun he.
 With a name like yours, you might be any shape, almost .
 —Humpty Dumpty to Alice (in Through the Looking Glass)  Occasionally, one of the four of us authors will want to make a personal comment.
In that case we  use I or me with a subscript.
When it’s a comment that we all agree with, or that we managed to slip  past me 3 (the rest of us are wimpier), we use the term we.
 1.4 NOTA TION  We use the symbol ⊕ (pronounced ex-or ) for bitwise-exclusive-or.
We use the symbol | for concate - nation.
We denote encryption with curly brackets followed by the key with which something was encrypted, as in { message }K, which means message is encrypted with K. We denote a signature  with square brackets followed by the key, as in [message ] Bob.
Sometimes the key is a subscript, and  sometimes not.
There is no deep meaning to that.
Honestly, it’s that sometimes we are using a key  that has subscripts, such as KAlice, and the formatting tool we are using makes it very difficult to  have a subscripted subscript.
 1.5 CRYPTOGRAP HICA LLY PROTECTED SESSI ONS  When Alice and Bob use modern cryptography and protocols, such as IPsec (Chapter 12) or TLS (Chapter 13), they first exchange a few messages in which they establish session secrets.
These ses - sion secrets allow them to encrypt and integrity-protect their conversation.
Although their physical  connectivity is a path across the Internet, once Alice and Bob create the protected session, data that  they send to each other is as trustworthy as if they had a private physically protected link.
 There are various terms for this type of protected session.
We will usually refer to it as a  secure session .
It is considered good security practice to use several cryptographic keys in a secure  session between Alice and Bob.
For example, there might be different session keys for  • encryption of Alice to Bob traffic,  • encryption of Bob to Alice traffic,
1.6 ACTIVE A ND PASSI VE ATTACKS 7  • integrity protection of Alice to Bob traffic, and  • integrity protection of Bob to Alice traffic.
 Alice and Bob will each have a database describing their current secure sessions.
The information  in the database will include information such as how the session will be identified on incoming  data, who is on the other end of the session, which cryptographic algorithms are to be used, and the  sequence numbers for data to be sent or received on the session.
 1.6 ACTIVE AND PASSIVE ATTACK S  A passive attack is one in which the intruder eavesdrops but does not modify the message stream  in any way.
An active attack is one in which the intruder may transmit messages, replay old mes - sages, modify messages in transit, or delete or delay selected messages in transit.
Passive attacks are less risky for the attacker, because it is tricky to detect or prove someone has eavesdropped.
If  the attacker is not on the path between Alice and Bob, the passive attack can be done by having an  accomplice router make copies of the traffic and send them to the attacker, who can then analyze  the data later, in private.
 A typical active attack is one in which an intruder impersonates one end of the conversation,  or acts as a meddler-in-the-middle (MITM ). (
Note that the acronym MITM used to be expanded  to be man-in-the-middle, but the industry is trying to move to more inclusive language.
In this case,  it is acknowledging that being annoying is not gender-specific.)
A MITM attack is where an active attacker, say, Trudy, acts as a relay between two parties (Alice and Bob), and rather than simply for - warding messages between Alice and Bob, modifies, deletes, or inserts messages.
If Trudy were faithfully forwarding messages, she could be acting as a passive eavesdropper, or she could be a  correctly functioning router.
 If Alice communicates with Bob using a secure session protocol with strong cryptographic  protection, Trudy would gain no information by eavesdropping and would not be able to modify messages without being detected.
 However, Trudy might be able to impersonate Bob’s IP address to Alice, tricking Alice into  establishing a secure session between Alice and Trudy.
Then Trudy can simultaneously imperson - ate Alice to Bob, and establish a secure session between Trudy and Bob. (
See Figure 1-1.)
 shared key KA-T shared key KT-B  Figure 1-1.
MITM Attack Alice Trudy Bob
8 I NTRODUC TION 1.7  Alice and Bob will think they are talking to each other, but in fact they are each talking to Trudy.
 Data sent by Alice to Bob will be decrypted by Trudy using the session secret for the Alice-Trudy  secure session, and encrypted for Bob with the session secret for the Trudy-Bob secure session.
It is  difficult for Alice and Bob to know that they have a MITM.
Alice could attempt to make sure she’s really talking to Bob by asking questions such as “What did I order when we first met for dinner?”,
 but Trudy can forward the questions and answers.
We will explain in §11.6 Detecting MITM how  Alice and Bob can detect a MITM.
And as we will explain in later chapters, if Alice has credentials  for Bob that Trudy cannot impersonate, Alice and Bob can prevent a MITM.
 1.7 LEGAL ISSUES  The legal aspects of cryptography are fascinating, but the picture changes quickly, and we are cer - tainly not experts in law.
Although it pains us to say it, if you’re going to build anything involving  cryptography, talk to a lawyer.
The combination of patents and export controls slowed down  deployment of cryptographically secure networking, and caused strange technical choices.
 1.7.1 Patents  One legal issue that affects the choice of security mechanisms is patents.
Most cryptographic tech - niques were covered by patents and historically this has slowed their deployment.
One of the important criteria for NIST’s selection of algorithms (such as AES [§3.7 Advanced Encryption  Standard (AES) ], SHA-3 [§5.6.2 Construction of SHA-3 ], and post-quantum algorithms [Chapter 8  Post-Quantum Cryptography ]) is whether they are royalty-free.
 The widely deployed RSA algorithm (see §6.3) was developed at MIT, and under the terms of  MIT’s funding at the time, there were no license fees for U.S. government use.
It was only patented  in the U.S., and licensing was controlled by one company, which claimed that the Hellman-Merkle  patent also covered RSA, and that patent is international.
Interpretation of patent rights varies by country, so the legal issues were complex.
At any rate, the last patent on RSA ran out on 20 Septem - ber 2000.
There were many parties on that day.
 “I don’t know what you mean by your way ,” said the Queen: “all the ways  about here belong to me…”  —Through the Looking Glass
1.7.2 SOME NETWORK BASICS 9  To avoid large licensing fees, many protocol standards used DSA (see §6.5) instead of RSA.
 Although in most respects DSA is technically inferior to RSA, when first announced it was adver - tised that DSA would be freely licensable so it would not be necessary to reach agreement with the  RSA-licensing company.
But the company claimed Hellman-Merkle covered all public key cryp - tography, and strengthened its position by acquiring rights to a patent by Schnorr that was closely  related to DSA.
Until the patents expired (and luckily the relevant patents have expired), the situa - tion was murky.
 1.7.2 Government Regulations  Mary had a little key (It’s all she could export)  And all the email that she sent  Was opened at the Fort.
 —Ron Rivest  The U.S. government (as well as other governments) used to impose severe restrictions on export of encryption.
This caused much bitterness in the computer industry and led to some fascinating tech - nical designs so that domestic products, which were legally allowed to use strong encryption, could  use strong encryption where possible, and yet interoperate with exportable products that were not  allowed to use strong encryption.
Although U.S. companies still need permission from the Depart - ment of Commerce to export products containing cryptography, since around the year 2000 it has  been easy to get products approved.
 Additionally, even today, some countries have usage controls, so even if it were legal to  export a product, it might not be legally usable inside some other country.
For instance, some coun - tries have developed their own cryptographic algorithms, and they want all their citizens to use those.
Most of the reason for these sorts of rules is so that a government can’t be prevented from  accessing data, for instance, for law enforcement purposes.
 1.8 SOME NETWOR K BASICS  Although I 2 get a bit frustrated with teaching network concepts as if TCP/IP is the only way, or the  best way to design a network, this is what today’s Internet is built with, so we need to understand  some of the details.
Here is a brief introduction.
10 I NTRODUC TION 1.8.1  1.8.1 Network Layers  A good way of thinking about networking concepts is with layers .
The concept of a layer is that  inside a node, there are interfaces to adjacent layers (the layer above or the layer below).
Between  nodes, there are protocols for talking to peer layers.
The actual protocol inside a layer can in theory  be replaced by a layer that gives similar functionality to the adjacent layers.
Although layers are a  good way to learn about networks, deployed networks do not cleanly follow a layering model.
Lay - ers often use data associated with layers other than peer layers or adjacent layers.
Layers are often subdivided into more layers, and an implementation might merge layers.
ISO (International Organi - zation for Standardization) defined a model with seven layers.
The bottom layers look like this:  • Layer 1, physical layer .
Defines how to send a stream of bits to a neighbor node (neighbors  reside on the same link).
 • Layer 2, data link layer .
Defines how to structure a string of bits (provided by layer 1) into  packets between neighbor nodes.
This requires using the stream of bits to signal information  such as “this is the beginning of a packet”, “this is the end of a packet”, and an integrity  check.
 • Layer 3, network layer .
This allows a source node to send a packet of information across  many links.
The source adds header information to a packet to let the network know where to  deliver the packet.
This is analogous to putting a postal message inside an envelope, and writ - ing the destination on the envelope.
A network will consist of many links.
Nodes known as  routers or switches forward between links.
Such nodes are connected to two or more links.
 They have a table known as a forwarding table that tells them which link to forward on, to get  closer to the destination.
Usually, network addresses are assigned hierarchically, so that a  bunch of addresses can be summarized in one forwarding entry.
This is analogous to the post  office only needing to look at the destination country, and then once inside that country, for - warding towards the state, and once inside the state, forwarding to the destination city, etc.
 The usual protocol deployed in the Internet today for layer 3 is IP (Internet Protocol), which  basically consists of adding a header to a packet identifying the source and destination, a hop  count (so the network can discard packets that are looping), and other information.
There are two versions of IP.
IPv4 has 32-bit addresses.
IPv6 has 128-bit addresses.
One extra piece of  information in the IP header is the 16-bit “protocol type”, which indicates which layer 4 pro - tocol is sending the data.
 • Layer 4, transport layer .
This is information that is put in by the source, and interpreted at  the destination.
The service provided by TCP (Transmission Control Protocol, RFC 793) to  the layer above it consists of accepting a stream of bytes at the source, and delivering the  stream of bytes to the layer above TCP at the destination, without loss or duplication.
To  accomplish this, TCP at the sender numbers bytes; TCP at the destination uses the sequence  numbers to acknowledge receipt of data, reorder data that has arrived out of sequence, and
1.8.2 SOME NETWORK BASICS 11  ask for retransmission of lost data.
UDP (User Datagram Protocol, RFC 768) is another layer  4 protocol that does not worry about lost or reordered data.
Many processes in the layer above  TCP or UDP will be reachable at the same IP address, so both UDP and TCP headers include  ports (one for source, and one for destination), which tell the destination which process  should receive the data.
 1.8.2 TCP and UDP Ports  There are two 16-bit fields in TCP and UDP—a source port and a destination port .
Typically an  application on a server will be reachable at a “well-known port”, meaning that the port is specified  in the protocol.
If a client wants to reach that application at a server, the protocol type field in the IP  header will be either TCP (6) or UDP (17) and the destination port field in the layer 4 header (TCP or UDP in this case) will be the well-known port for that application.
For example, HTTP is at port  80, and HTTPS is at port 443.
The source port will usually be a dynamically assigned port (49152  through 65535).
 1.8.3 DNS (Domain Name System)  Another aspect of Internet networking we will be discussing is DNS.
It is basically a distributed  directory that maps DNS names ( e.g., example.com ) to IP addresses.
DNS names are hierarchical.
 A simple way to think of DNS is that for each level in the DNS name ( e.g., root, .org, .com ,  example.com ) there is a server that keeps a directory associated with names in that level.
The root  would have a directory that allows looking up servers for each of the top-level domains (TLDs)  (e.g., .org, .com , .gov, .tv).
There are currently over a thousand TLDs, so the root would have  information associated with each of those TLDs in its database.
In general, to find a DNS name, a  node starts at the root, finds the server that holds the directory for the next level down, and keeps  going until it gets to the server that stores information about the actual name.
There are several  advantages to DNS being hierarchical.
 • Someone that wishes to purchase a DNS name has a choice of organizations from which to  purchase a name.
If a name is purchased from the organization managing names in the TLD  .org, the purchased name will be of the form example.org .
If you purchase the name exam - ple.org , you can then name anything that would be below that name in the DNS hierarchy,  such as xyz.example.com or labs.xyz.example.com .
 • The DNS database will not become unmanageably large, because no organization needs to keep the entire DNS database.
In fact, nobody knows how many names are in the DNS data - base.
12 I NTRODUC TION 1.8.4  • It is fine to have the same lower level name in multiple databases.
For instance, there is no  problem with there being DNS names example.com and example.org .
 1.8.4 HTTP and URLs  When we access things on the web, we use a protocol known as HTTP (hypertext transfer proto - col).
HTTP allows specifying more than a DNS name; it allows specifying a particular web page at  the service with a DNS name.
The URL (uniform resource locator) is the address of the web page.
 The URL contains a DNS name of the service, followed by additional information that is inter - preted solely by the server that receives the request.
The additional information might be, for instance, the directory path at the destination server that finds the information to construct the page  being requested.
 Sometimes humans type URLs, but usually URLs are displayed as links in a webpage that  can be clicked on.
It is common for people to do an Internet search ( e.g., using Google or Bing) for  something, and then click on choices.
URLs can be very long and ugly, and people usually don’t  look at the URL they click on.
Often the web page that displays a link does not display the actual  URL.
Mousing over the link will sometimes show the human a URL.
Unfortunately, the web page  can choose what to display on the page as the link, and what to display the mouse-over link as.
 These can be different from the actual URL that will be followed if the link is clicked on.
For exam - ple, on a web page, a clickable link (which is usually displayed in a different color), might display  as “click here for information”, and if a suspicious user moused-over the link, it might display  “http://www.example.com/information ”, but if the user clicks on the link, the malicious webpage  could send them to any URL, e.g., http://www.rentahitman.com .
 The two main HTTP request types are GET and POST .
GET is for reading a web page and  POST is for sending information to a web server.
The response contains information such as the  content requested and status information (such as “OK” or “not found” or “unauthorized”).
One  status that might be included in a response is a redirect.
This informs the browser that it should go  to a different URL.
The browser will then go to the new URL, as if the user had clicked on a link.
 1.8.5 Web Cookies  If a client is browsing content that requires authentication and access control, or is accumulating  information such as items in a virtual shopping basket to be purchased when the user is finished  browsing the on-line catalog, the information for that session needs to be kept somewhere.
But HTTP is stateless.
Each request/response interaction is allowed to take place over a fresh TCP con - nection.
The cookie mechanism enables the server to maintain context across many  request/response interactions.
A cookie is a piece of data sent to the client by the server in response
1.9 NAMES FOR HUMA NS 13  to an HTTP request.
The cookie need not be interpreted by the client.
Instead, the client keeps a list  of DNS names and cookies it has received from a server with that DNS name.
If the client made a  request at example.com , and example.com sent a cookie, the client will remember ( example.com :  cookie) in its cookie list.
When the client next makes an HTTP request to example.com , it searches  its cookie database for any cookies received from example.com , and includes those cookies in its  HTTP request.
 The cookie might contain all the relevant information about a user, or the server might keep a  database with this information.
In that case, the cookie only needs to contain the user’s identity (so  the server can locate that user in its database), along with proof that the user has already authenti - cated to the server.
For example, if Alice has authenticated to Bob, Bob could send Alice a cookie  consisting of some function of the name “Alice” and a secret that only Bob knows.
A cookie will be  cryptographically protected by the server in various ways.
It might be encrypted with a key that  only the server knows.
It might contain information that only allows the cookie to be used from a  specific machine.
And it is almost always protected when transmitted across the network because  the client and server will be communicating over a secure session.
 1.9 NAMES FOR HUMANS  It is sometimes important that an identifier for a human be unique, but it is not important for a  human to have a single unique name.
Humans have many unique identifiers.
For example, an email  address, a telephone number, or a username specific to a website.
In theory, nobody but the human  needs to know that the various identities refer to the same human, but, unfortunately, it has become  easy for organizations to correlate various identities.
Also, it is not uncommon for family members or close friends to share an account, so an email address or username at a website might actually be  multiple humans, but we will ignore that issue.
 Human names are problematic.
Consider email addresses.
Typically, companies let the first  John Smith use the name John@companyname for his email address, and then perhaps the next  one will be Smith@companyname , and the next one JSmith@companyname , and the next one  has to start using middle initials.
Then, to send to your colleague John Smith, you have to do the  best you can to figure out which email address in the company directory is the one you want, based  on various attributes (such as their location or their job title, if you are lucky enough to have this  information included in the directory).
There will be lots of confusion when one John Smith gets messages intended for a different John Smith.
This is a problem for both the John Smith that is mis - takenly sent the email, as well as the John Smith who was the intended recipient of the email.
Usu - ally, a person can quickly delete spam, but with a name like John Smith, irrelevant-looking email  might actually be important email for a different John Smith in the company, so must be carefully
14 I NTRODUC TION 1.10  read, and forwarded just in case.
And the unfortunate John Smith who received the email has the  problem of figuring out which John Smith he should forward the email to.
 One way of solving this problem is that once a company hired someone with a particular  name, they just wouldn’t hire another.
I2 (with the name Radia Perlman, which is probably unique  in the entire world) think that’s reasonable, but someone with a name like John Smith might start  having problems finding a company that could hire him.
 Now why did you name your baby John ?
Every Tom, Dick, and Harry is  named John.
 —Sam Goldwyn  1.10 AUTHE NTICA TION AND AUTHOR IZATION  Authentication is when Alice proves to Bob that she is Alice.
Authorization is having something decide what a requester is allowed to do at service Bob.
 1.10.1 ACL (Access Control List)  Typically the way a server decides whether a user should have access to a resource is by first  authenticating the user, and then consulting a database associated with the resource that indicates  who is allowed to do what with that resource.
For instance, the database associated with a file might  say that Alice can read it, and George and Carol are allowed to read and write it.
This database is often referred to as an ACL (access control list ).
 1.10.2 Central Administration/Capabilities  Another model of authorization is that instead of listing, with each resource, the set of authorized users and their rights ( e.g., read, write , execute ), service Bob might have a database that listed, for  each user, everything she was allowed to do.
If everything were a single application, then the ACL model and the central administration model would be basically the same, since in both cases there would be a database that listed all the authorized users and what rights each had.
But in a world in  which there are many resources, not all under control of the same organization, it would be difficult  to have a central database listing what each user was allowed to do.
This model would have scaling  problems if there were many resources each user was allowed to access, especially if resources
1.10.3 15 AUTHENTI CATION AND AUTHORI ZATION  were created and deleted at a high rate.
And if resources are under control of different organiza - tions, there wouldn’t be a single organization trusted to manage the authorization information.
 Some people worry that ACLs don’t scale well if there are many users allowed access to each  resource.
But the concept of groups helps the scaling issue.
 1.10.3 Groups  Suppose there were a file that should be accessible to, say, any Dell employee.
It would be tedious  to type all the employee names into that file’s ACL.
And for a set of employees such as “all Dell  employees”, there would likely be many resources with the same set of authorized users.
And it  would take a lot of storage to have such huge ACLs on that many resources, and whenever anyone  joined or left the company, all those ACLs would have to be modified.
 The concept of a group was invented to make ACLs more scalable.
It is possible to include a  group name on an ACL, which means that any member of the group is allowed access to the  resource.
Then, group membership can be managed in one place, rather than needing to update  ACLs at every resource when the membership changes.
 Traditionally, a server that protected a resource with a group named on the ACL needed to  know all the members of the group, but if there are many servers that store resources for that group,  this would be inefficient and inconvenient.
Also, that model would preclude more flexible group  mechanisms; for example:  • cross-organizational groups, where no one server is allowed to know all the members;  • anonymous groups, where someone can prove membership in the group without having to  divulge their identity.
 Traditionally, groups were centrally administered, so it was easy to know all the groups to which a  user belonged, and the user would not belong to many groups.
But in many situations, it is useful  for any user to be able to create a group (such as Alice’s friends , or students who have already  turned in their exams in my course ), and have anyone be able to name such a group on an ACL.
 Scaling up this simple concept of users, groups, and ACLs to a distributed environment has  not been solved in practice.
This section describes various ways that it might be done and chal - lenges with truly general approaches.
 1.10.4 Cross-Organizational and Nested Groups  It would be desirable for an ACL to allow any Boolean combination of groups and individuals.
For  instance, the ACL might be the union of six named individuals and two named groups.
If someone  is one of the named individuals, or in one of the groups, the ACL would grant them permission.
It
16 I NTRODUC TION 1.10.4  would also be desirable to have the ACL be something like Group A and NOT group B, e.g., U.S.  citizen and not a felon.
 Likewise, it would be desirable for group membership to be any Boolean combination of  groups and individuals, e.g., the members of Alliance-executives might be CompanyA-execs ,  CompanyB-execs , and John Smith .
Each of the groups Alliance-executives , CompanyA - execs , and CompanyB-execs is likely to be managed by a different organization, and the mem - bership is likely to be stored on different servers.
How, then, can a server (Bob) that protects a  resource that has the group Alliance-executives on the ACL know whether to allow Alice access?
 If she’s not explicitly listed on the ACL, she might be a member of one of the groups on the ACL.
But Bob does not necessarily know all the members of the group.
Let’s assume that the group name (Alliance-executives ) can be looked up in a directory to find out information such as its network  address and its public key.
Or perhaps the group name would contain the DNS name of the server that manages that group, so the group name might be  example.com/Alliance-executives .
 • Bob could periodically find every group on any ACL on any resource it protects, and attempt  to collect the complete membership.
This means looking up all the members of all subgroups,  and subgroups of subgroups.
This has scaling problems (the group memberships might be  very large), performance problems (there might be a lot of traffic with servers querying group  membership servers for membership lists), and cache staleness problems.
How often would  this be done?
Once a day is a lot of traffic, but a day is a lot of time to elapse for Alice’s group  membership to take effect, and for revocations to take effect.
 • When Alice requests access, Bob could then ask the on-line group server associated with the  group whether Alice is a member of the group.
This could also be a performance nightmare  with many queries to the group server, especially in the case of unauthorized users creating a  denial of service attack by requesting access to services.
At the least, once Alice is discovered  to either belong or not belong, Bob could cache this information.
But again, if the cache is  held for a long time, it means that membership can take a long time to take effect, and revoca - tion can also take a long time to take effect.
 • When Alice requests access to a resource, Bob could reply, “You are not on the ACL, but here  are a bunch of groups that are on the ACL, so if you could prove you are a member of one of  those groups, you can access it.”
Alice could then contact the relevant group servers and  attempt to get certification of group membership.
Then, she can reply to Bob with some sort of proof that she is a member of one of the groups on the ACL.
Again, this could have a cache  staleness problem, and a performance problem if many users (including unauthorized trou - blemakers) contact group servers asking for proof of group membership.
1.10.5 17 AUTHENTI CATION AND AUTHORI ZATION  1.10.5 Roles  The term role is used in many different ways.
Authorization based on roles is referred to as RBAC  (role-based access control ).
In most usage today, a role is the same as what we described as a  group.
In some cases, Alice will need to log in as the role rather than as the individual.
For example,  she might log in as admin .
However, many users might need to log in as admin , and for auditing  purposes, it is important to know which user was invoking the admin role.
Therefore, for auditing  purposes, a user might be simultaneously logged in as the role (admin) and as the individual  (Alice).
In many environments, a role is given a name such as “admin”, and being able to invoke the  admin role on your own machine should not authorize you to invoke the admin role on controlling a  nuclear power plant.
So, both groups and roles should have names that specify the domain, e.g., the  DNS name of the service that manages the group or role membership.
 Although a lot of systems implement roles as the same thing as groups, it is confusing to have  two words that mean the same thing.
We recommend that the difference between a group and a role  is that a role needs to be consciously invoked by a user, often requiring additional authentication  such as typing a different password.
In contrast, we recommend that with a group, all members  automatically have all rights of the group, without even needing to know which groups they are a member of.
With roles, users may or may not be allowed to simultaneously act in multiple roles,  and perhaps multiple users may or may not be allowed to simultaneously act in a particular role  (like President of the United States ).
Again, we are discussing various ways things could work.
 Deployed systems make different choices.
Some things people would like to see roles solve:  • When a user is acting in a particular role, the application presents a different user interface.
 For instance, when a user is acting as manager , the expense reporting utility might present  commands for approving expense reports, whereas when the user is acting as employee , the  application might present commands for reporting expenses.
 • Having roles enables a user to be granted a subset of all the permissions they might have.
If  the user has to explicitly invoke the privileged role, and only keeps themselves authenticated as that role for as long as necessary, it is less likely that a typo will cause them to inadvert - ently do an undesirable privileged operation.
 • Allowing a user to be able to run with a subset of her rights (not invoking her most privileged  role except when necessary) gives some protection from malicious code.
While running untrusted code, the user should be careful to run in an unprivileged role.
 • Sometimes there are complex policies, such as that you are allowed to read either file A or  file B but not both.
Somehow, proponents of roles claim roles will solve this problem.
This  sort of policy is called a Chinese wall .
18 I NTRODUC TION 1.11  1.11 MALW ARE: VIRUSES , WORMS, TROJAN HORSES  Lions and tigers and bears, oh my !
 —Dorothy (in the movie The Wizard of Oz )  People like to categorize different types of malicious software and assign them cute biological  terms (if one is inclined to think of worms as cute).
We don’t think it’s terribly important to distin - guish between these things, so in the book we’ll refer to all kinds of malicious software generically as malware .
However, here are some of the terms that seem to be infecting the literature.
 • Trojan horse —instructions hidden inside an otherwise useful program that do bad things.
 Usually, the term Trojan horse is used when the malicious instructions are installed at the  time the program is written (and the term virus is used if the instructions get added to the pro - gram later).
 • virus —a set of instructions that, when executed, inserts copies of itself into other programs.
 • worm —a program that replicates itself by installing copies of itself on other machines across  a network.
 • trapdoor —an undocumented entry point intentionally written into a program, often for  debugging purposes, which can be exploited as a security flaw.
Often people forget to take  these out when the product ships.
However, sometimes these undocumented “features” are intentionally put into software by an employee who thinks he might at some point become disgruntled.
 • bot—a machine that has been infected with malicious code that can be activated to do some  malicious task by some controller machine across the Internet.
The controller is often referred  to as a bot herder .
The intention of the disease-infected rodents (apologies if we offend  actual disease-infected rodents who might be reading this book) who turn a computer into a  bot is that the owner of the machine should not be aware that their machine is infected.
To the  owner of the machine, it continues operating as usual.
However, the controller can at some  point rally all the bots under its control to do some sort of mischief like sending out spam or flooding some service with nuisance messages.
The bots under its control are often referred  to as a bot army .
There are price lists on the dark web for renting a bot army, priced accord - ing to the size of the army and the amount of time you would like to rent them. (
The dark  web is a subset of the Internet that is not visible to search engines, and requires special access  mechanisms.
It is an ideal place for purchasing illegal goods.)
 • logic bomb —malicious instructions that trigger on some event in the future, such as a partic - ular time occurring.
The delay might advantage criminals that create a logic bomb, because
1.11.1 MALWARE : VIRUSES , WORMS , TROJAN HORSE S 19  they can first deploy the logic bomb in many places.
Or, they can make the bad event occur  long after they have left the company, so they won’t be under suspicion.
 • ransomware —malicious code that encrypts the user’s data, and then the helpful criminal  offers to help get the data back, for a small fee (such as several million dollars).
 Most of the items above exploit bugs in operating systems or applications.
There are also vulnera - bilities that exploit bugs in humans.
One example is phishing , the act of sending email to a huge  unstructured list of email addresses, that will trick some small percentage of the recipients into  infecting their own machines, or divulging information such as a credit card number.
Spear phish - ing means sending custom messages to a more focused group of people.
 1.11.1 Where Does Malware Come From?
 Where do these nasties come from?
Originally, it was hobbyists just experimenting with what they  could do.
Today, big money can be made with malware.
A bot army can be rented to someone that  wants to damage a competitor, or a political organization they disagree with.
And malware can be  used for demanding ransoms.
It is also used by sophisticated spy organizations.
A notable example  is Stuxnet.
This was malware aimed specifically at damaging a certain type of equipment (centri - fuges) used in Iran, to slow Iran’s ability to produce nuclear weapons.
 How could an implementer get away with intentionally writing malware into a program?
 Wouldn’t someone notice by looking at the program?
A good example of how hard it can be to  decipher what a program is doing, even given the source code, is the following nice short program,  written by Ian Phillipps (see Figure 1-2 ).
 It was a winner of the 1988 International Obfuscated C Code Contest.
It is delightful as a  Christmas card.
It does nothing other than its intended purpose (I 1 have analyzed the thing carefully  and I 2 have complete faith in me 1), but we doubt many people would take the time to understand  this program before running it  But also, nobody looks.
Often when you buy a program, you do not have access to the source  code, and even if you did, you probably wouldn’t bother reading it all, or reading it very carefully.
 Many programs that run have never been reviewed by anybody.
A claimed advantage of the “open  source” movement (where all software is made available in source code format) is that even if you  don’t review it carefully, there is a better chance that someone else will.
 What does a virus look like?
A virus can be installed in just about any program by doing the  following:  • replace any instruction, say the instruction at location x, by a jump to some free place in  memory, say location y; then  • write the virus program starting at location y; then
20 I NTRODUC TION 1.11.2  /* Have yourself an obfuscated Christmas! */
 #include <stdio.h>  main(t,_,a)  char *a; {  return!0<t?t<3?main(-79,-13,a+main(-87,1-_,main(-86,0,a+1)+a)):  1,t<_?main(t+1,_,a):3,main(-94,-27+t,a)&&t==2?_<13?
 main(2,_+1,"%s %d %d\n"):9:16:t<0?t<-72?main(_,t,  "@n'+,#'/*{}w+/w#cdnr/+,{}r/*de}+,/*{*+,/w{%+,/w#q#n+,/#{l,+,/n{n+,/+#n+,/#\  ;#q#n+,/+k#;*+,/'r :'d*'3,}{w+K w'K:'+}e#';dq#'l \  q#'+d'K#!/+k#;q#'r}eKK#}w'r}eKK{nl]'/#;#q#n'){)#}w'){){nl]'/+#n';d}rw' i;# \  ){nl]!/n{n#'; r{#w'r nc{nl]'/#{l,+'K {rw' iK{;[{nl]'/w#q#n'wk nw' \  iwk{KK{nl]!/w{%'l##w#' i; :{nl]'/*{q#'ld;r'}{nlwb!/*de}'c \  ;;{nl'-{}rw]'/+,}##'*}#nc,',#nw]'/+kd'+e}+;#'rdq#w!
nr'/ ') }+}{rl#'{n' ')# \  }'+}##(!!/")
 :t<-50?_==*a?putchar(31[a]):main(-65,_,a+1):main((*a=='/')+t,_,a+1)  :0<t?main(2,2,"%s"):*a=='/'||main(0,main(-61,*a,  "!
ek;dc i@bK'(q)-[w]*%n+r3#l,{}:\nuwloca-O;m .vpbks,fxntdCeghiry"),a+1);  }  Figure 1-2.
Christmas Card?
 • place the instruction that was originally at location x at the end of the virus program, followed  by a jump to x+1.
 Besides doing whatever damage the virus program does, it might replicate itself by looking for any  executable files in any directory and infecting them.
Once an infected program is run, the virus is  executed again, to do more damage and to replicate itself to more programs.
Most viruses spread  silently until some triggering event causes them to wake up and do their dastardly deeds.
If they did  their dastardly deeds all the time, they wouldn’t spread as far.
 1.11.2 Virus Checkers  How can a program check for viruses?
There’s rather a race between the brave and gallant people  who analyze the viruses and write clever programs to detect and eliminate them, and the foul- smelling scum who devise new types of viruses that will escape detection by all the current virus  checkers.
 The oldest form of virus checker knows instruction sequences that have appeared in known  viruses, but are believed not to occur in benign code.
It checks all the files on disk and instructions  in memory for those patterns of commands, and raises a warning if it finds a match hidden some - where inside some file.
Once you own such a virus checker, you need to periodically get updates of  the patterns file that include the newest viruses.
1.12 SECURITY GATEWAY 21  To evade detection of their viruses, virus creators have devised what are known as a polymor - phic viruses .
When they copy themselves, they change the order of their instructions or change  instructions to functionally similar instructions.
A polymorphic virus may still be detectable, but it  takes more work, and not just a new pattern file.
Modern virus checkers don’t just periodically scan  the disk.
They actually hook into the operating system and inspect files before they are written to disk.
 Another type of virus checker takes a snapshot of disk storage by recording the information in  the directories, such as file lengths.
It might even take message digests of the files.
It is designed to  run, store the information, and then run again at a future time.
It will warn you if there are suspi - cious changes.
One virus, wary of changing the length of a file by adding itself to the program,  compressed the program so that the infected program would wind up being the same length as the  original.
When the program was executed, the uncompressed portion containing the virus decom - pressed the rest of the program, so (other than the virus portion) the program could run normally.
 Some viruses attack the virus checkers rather than just trying to elude them.
If an attacker can  penetrate a company that disseminates code, such as virus signatures or program patches, they can  spread their malware to all the customers of that company.
 1.12 SECURITY GATEW AY  A security gateway (see Figure 1-3) sits between your internal network and the rest of the network  and provides various services.
Such a box usually provides many functions, such as firewall  (§1.12.1), web proxy (§1.12.2), and network address translation (§1.14).
We will describe these  features in the next few sections.
 your net security  gateway Internet  Figure 1-3.
Security Gateway  1.12.1 Firewall  There was a time when people assumed that their internal network and all the users and systems on  the internal network were trustworthy.
The only challenge was connectivity to the Internet.
Users
22 I NTRODUC TION 1.12.1  need to communicate with publicly available services.
The company needs to make some of its own  services available to customers located outside the corporate network.
So the thought was to install  a box between your network and the scary Internet that can keep things secure, somehow.
 The belief that a firewall between your internal network and the scary Internet is all you need  to protect you has long been discredited.
Even if you had the most secure firewall, i.e., a box that  blocked any traffic between your network and the Internet, malware inside your network could do  arbitrary damage.
And even carefully configured firewalls often break legitimate usage.
 A recent buzzword is zero trust .
It basically means the opposite of the previous thinking.
 Although firewalls can add some amount of extra security, all applications must protect themselves.
 They must authenticate everything they are talking to and enforce access control rules.
The  buzzword defense-in-depth means having multiple stages of security, so if one fails, hopefully  another will protect you.
Although people pretty much agree that the old firewall-will-protect-you  model is no longer the right way to think about security, a lot of the mechanisms supporting that  model are still deployed.
 Firewalls centrally manage access to services in ways that individual systems should, but  often don’t.
Firewalls can enforce policies such as systems outside the firewall can’t access file ser - vices on any systems inside the firewall .
With such a restriction, even if the internal systems are  more open than they should be, or have bugs, they can’t be attacked directly from systems outside  the firewall.
 Note that a firewall need not be a physical box that the company buys.
It could instead be  software on each machine that does the same sorts of filtering that a firewall box would do.
This  concept is sometimes called a distributed firewall .
Both “firewall” and “distributed firewall” are  buzzwords, and, as with most buzzwords, the definitions evolve and are used by different vendors  in different ways.
Usually distributed firewall implies that all the components doing “firewall stuff”  are centrally managed.
Otherwise, it would just be multiple firewalls.
 The simplest form of firewall selectively discards packets based on configurable criteria, such  as addresses in the IP header, and does not keep state about ongoing connections.
For example, it  might be configured to only allow some systems on your network to communicate outside, or some  addresses outside your network to communicate into your network.
For each direction, the firewall  might be configured with a set of legal source and destination addresses, and it drops any packets  that don’t conform.
This is known as address filtering .
 Packet filters usually look at more than the addresses.
A typical security policy is that for cer - tain types of traffic ( e.g., email, web surfing), the rewards outweigh the risks, so those types of traf - fic should be allowed through the firewall, whereas other types of traffic (say, remote terminal  access), should not be allowed through.
 To allow certain types of traffic between host A and B while disallowing others, a firewall can  look at the protocol type in the IP header, at the ports in the layer 4 (TCP or UDP) header, and at  anything at any fixed offset in the packet.
For web traffic, either the source or destination port will likely be 80 (http) or 443 (https).
For email, either the source or destination port will likely be 25.
23 1.12.2 SECURITY GATEWAY  Firewalls can be even fancier.
Perhaps the policy is to allow connections initiated by  machines inside the firewall, but disallow connections initiated by machines outside the firewall.
 Suppose machine A inside the firewall initiates a connection to machine B outside the firewall.
 During the conversation, the firewall will see packets from both directions (A to B as well as B to  A), so it can’t simply disallow packets from B to A. The way it manages to enforce only connec - tions initiated by A, is to look at the TCP header.
TCP has a flag (called ACK) that is set on all but  the first packet, the one that establishes the connection.
So if the firewall disallows packets from B without  ACK set in the TCP header, then it will usually have the desired effect.
 Another approach is a stateful packet filter , i.e., a packet filter that remembers what has hap- pened in the recent past and changes its filtering rules dynamically as a result.
A stateful packet fil - ter could, for instance, note that a connection was initiated from inside using IP address s, to IP  address d, and then allow (for some period of time) connections from IP address d to IP address s.  1.12.2 Application-Level Gateway/Proxy  An application-level gateway, otherwise known as a proxy , acts as an intermediary between a client  and the server providing a service.
The gateway could have two network adaptors and act as a  router, but more often it is placed between two packet-filtering firewalls, using three boxes (see Fig- ure 1-4).
The two firewalls are routers that refuse to forward anything unless it is to or from the  gateway.
Firewall F 2 refuses to forward anything from the global net unless the destination address  is the gateway and refuses to forward anything to the global net unless the source is the gateway.
 Firewall F1 refuses to forward anything from your network unless the destination address is the  gateway, and refuses to forward anything to your network unless the source address is the gateway.
To transfer a file from your network to the global network, you could have someone from inside  transfer the file to the gateway machine, and then the file is accessible to be read by the outside  world.
Similarly, to read a file into your network, a user can arrange for it to first get copied to the  gateway machine.
To log into a machine in the global network, you could first log into the gateway  machine, and from there you could access machines in the remote network.
An application-level  gateway is sometimes known as a bastion host .
It must be implemented and configured to be very  your  net firewall  F1 firewall  F2 Internet gateway  Figure 1-4.
Application-Level Gateway
24 I NTRODUC TION 1.12.3  secure.
The portion of the network between the two firewalls is known as the DMZ (demilitarized  zone).
 The gateway need not support every possible application.
If the gateway does not support an  application, either the firewalls on either side of the gateway should block that application (based  on layer 4 port), or both firewalls would need to allow that application, depending on whether you  want that application to work through the firewall.
An example strategy is to allow only electronic  mail to pass between your corporate network and the outside world.
The intention is to specifically disallow other applications, such as file transfer and remote login.
However, electronic mail can  certainly be used to transfer files.
Sometimes a firewall might specifically disallow very large elec - tronic mail messages, on the theory that this will limit the ability to transfer files.
But often, large  electronic mail messages are perfectly legitimate, and any file can be broken down into small  pieces.
 Sometimes an application might specifically be aware of proxies running on an application  gateway.
For example, browsers can be configured with the address of a proxy, and then all requests  will be directed to the proxy.
The proxy might be configured with which connections are allowed  and which are disallowed, and could even inspect the data passing through to check for malware.
 1.12.3 Secure Tunnels  A tunnel (see Figure 1-5) is a point-to-point connection created between nodes A and B, where the  connection is a path across a network.
A and B can treat the tunnel as a direct link between each  other.
An e ncrypted tunnel is when A and B establish a secure session.
An encrypted tunnel is  sometimes called a VPN (virtual private network).
We think that’s a really bad term, and a more  accurate term would be VPL (virtual private link).
Whoever decided to call it a VPN imagined that  the encrypted tunnel becomes an extra link in your private network, so a network consisting of a  combination of private links and encrypted tunnels across the Internet becomes your private net - work.
If we just use the acronym VPN for the encrypted tunnel (like the industry seems to) and  don’t think about what VPN expands to, I2 guess we can live with the term.
 Suppose the only reason you’ve hooked into the Internet is to connect disconnected pieces of  your own network to each other.
Instead of the configuration in Figure 1 -5, you could have bought  dedicated links between G 1, G2, and G 3, and trusted those links as part of your corporate network  because you owned them.
But it’s likely to be cheaper to have the Gs pass data across the Internet.
 How can you trust your corporate data crossing over the Internet?
You do this by configuring G 1,  G2, and G3 with security information about each other (such as cryptographic keys), and creating  secure tunnels between them.
 The mechanics of the tunnel between G1 and G2, from the IP (network layer 3) point of view,  is that when A sends a packet to C, A will launch it with an IP header that has source =A,  destination =C. When G 1 sends it across the tunnel, it puts it into another envelope, i.e., it adds an
25 1.12.4 SECURITY GATEWAY  Internet G1 G2  G3 A C  B  Figure 1-5.
Connecting a Private Network over a Public Internet  additional IP header, treating the inner header as data.
The outer IP header will contain source = G1  and destination = G2.
And all the contents (including the inner IP header) will be encrypted and  integrity protected, so it is safe to traverse the Internet.
 1.12.4 Why Firewalls Don’t Work  Firewalls alone (without also doing end-to-end security) assume that all the bad guys are on the out - side, and everyone inside can be completely trusted.
This is, of course, an unwarranted assumption.
 Should employees have access (read and write) to the salary database, for instance?
 Even if the company is so careful about hiring that no employee would ever do anything bad  intentionally, firewalls can be defeated if an attacker can inject malicious code into a machine on  the corporate network.
This can be done by tricking someone into downloading something from the Internet or launching an executable from an email message.
It is quite common for an attacker to  break into one system inside your firewall, and then use that system as a platform for attacking  other systems.
Someone once described firewall-protected networks as “hard and crunchy on the  outside; soft and chewy on the inside.”
 Firewalls often make it difficult for legitimate users to get their work done.
The firewall might  be configured incorrectly or might not recognize a new legitimate application.
And if the firewall  allows one application through (say email or http), people figure out how to do what they need to do  by disguising it as traffic that the firewall is configured to allow.
The ironic term for disguising traf - fic in order to fool a firewall is to carry your traffic in a firewall-friendly protocol.
Since firewalls  commonly allow http traffic (since it’s the protocol used for browsing the web), there are many  proposals for doing things over http.
The most extreme example is to carry IP over http, which would allow any traffic through!
Firewall-friendly?
The whole point is to defeat the best efforts of the firewall administrator to disallow what you are doing!
It isn’t somehow “easier” for the firewall
26 I NTRODUC TION 1.13  to carry http traffic than any other.
The easiest thing for the firewall would be to allow everything or  nothing through!
 Just as breaking a large file into lots of pieces to be individually carried in separate emails is  inefficient, having protocols run on top of http rather than simply on top of IP is also inefficient in  terms of bandwidth and computation.
 1.13 DENIAL-OF-SERVIC E (DOS) A TTAC KS  A denial-of-service attack (DoS) is one in which an attacker prevents good guys from accessing a  service, but does not enable unauthorized access to any services.
In the naive old days, security peo - ple dismissed the prospect of denial-of-service attacks as unlikely, since the attacker had nothing to  gain.
Of course, that turned out to be faulty reasoning.
There are terrorists, disgruntled employees,  and people who delight in causing mischief for no good reason.
 In the earliest types of denial-of-service attacks, the attacker repeatedly sent messages to the  victim machine.
Most machines at the time were vulnerable to this sort of attack since they had resources that could easily be depleted.
For instance, the storage area for keeping track of pending  TCP connections tended to be very limited, on the order of, say, ten connections.
The probability of  ten legitimate users connecting during a single network round trip time was sufficiently small that  ten was a reasonable number.
But it was easy for the attacking machine to fill up this table on a  server, even if the attacking machine was attached to the Internet with a low-speed link.
 To avoid being caught at this mischief, it was common for the attacker to send these mali - cious packets from forged source addresses.
This made it difficult to find (and prosecute) the  attacker, and it made it difficult to recognize packets from the malicious machine and filter them at  a firewall.
 As a defense, people advocated having routers have the capability of doing sanity checks on  the source address.
These routers could be configured to drop packets with a source address that  could not have legitimately come from the direction from which the packet was received.
Routers  might be configured with which source addresses to expect on each of their ports, or they might infer the expected direction from their forwarding tables.
This concept was not deployed because it  would cause problems.
If sanity checks are based on configured information, topological changes  in the Internet (such as links going down and alternative routes being used) could cause the routers  to make incorrect assumptions.
And Mobile IP (RFC 5944) allows a node to move around in the  Internet and keep its IP address, which would confuse routers attempting sanity checks.
 A deployed defense against a single malicious node attempting to swamp the resources of a  server was to increase resources at the server so that a single attacker, at the speeds at which such  attackers were typically connected to the Internet, could not fill the pending TCP connection table.
1.14 NAT (N ETW ORK ADDRESS TRANSLATION) 27  Another level of DoS escalation was to send a single packet that caused a lot of legitimate  machines to send messages to the victim machine.
An example of such a packet is a packet trans - mitted to the broadcast address, with the packet’s source address forged to the address of the vic - tim’s machine, asking for all receivers to respond.
All the machines that receive the broadcast will  send a response to the victim’s machine.
Such a mechanism magnifies the effect the attacker can  have from his single machine, since each packet he creates turns into n packets directed at the vic - tim machine.
 As a defense again machines sending packets from forged IP addresses, protocols such as  TCP, IPsec, and TLS have been designed to avoid requiring a receiver, Bob, to keep state or do sig - nificant computation if requests are arriving from forged IP source addresses.
Unless the requester  can receive packets at the IP address they claim to be coming from, Bob will not need to keep state  about the request.
Only when the requester returns something that Bob sent to its claimed IP  address will Bob pay attention to this request.
 But then came the next level of escalation, which is known as a distributed-denial-of - service attack (DDoS ).
In this form of attack, the attacker breaks into a lot of innocent machines,  and installs software on them to have them all attack the victim machine.
These innocent machines are called zombies or drones or bots.
With enough bots attacking it, any machine can be made  inaccessible, since even if the machine itself can process packets as fast as they can possibly arrive,  the links or routers in front of that machine can be overwhelmed.
The defenses in TCP, IPsec, and  TLS will not help, since the bot machines are using their own IP addresses in the requests.
Since requests are coming from hundreds or thousands of innocent machines, it is hard to distinguish  these packets from packets coming from legitimate users.
 1.14 NAT (N ETWO RK ADDRESS TRANSL ATIO N)  NAT was designed, out of necessity, because IPv4 addresses were too small (four bytes) to give  unique addresses to every node on the Internet.
With NAT, a piece of the Internet (say a corporate  network) can use IP addresses that are not globally unique, and, in fact, these addresses are reused in many other networks.
This means that a node inside such a network cannot be contacted from  outside that network.
However, if the security gateway provides NAT functionality, it will have a  pool of globally unique IP addresses that can be assigned as needed.
 The NAT box will almost certainly not have a large enough pool of IP addresses to give a glo- bally reachable IP address to every internal node communicating to outside nodes.
So the NAT box  also translates the TCP or UDP port as well.
So a NAT implementation might have a mapping of  〈internal IP, internal port 〉 maps to 〈external IP, external port 〉.
When internal node Alice sends a  packet to external destination Bob, the NAT box will replace the source IP and port to the external
28 I NTRODUC TION 1.14.1  IP and port in the NAT box’s table.
Likewise, when packets arrive from Bob for Alice’s assigned  external 〈IP, port〉, the NAT box replaces these fields in the destination fields in Bob’s packet before  forwarding the packet on the internal network.
 There is somewhat of a security problem with this approach if the NAT box simply translates  tuples of 〈IP address, port 〉.
Suppose Alice starts a connection to Bob, and the NAT box then creates  an entry to translate Alice’s internal address and port to, for instance, globally reachable tuple  〈 IPAlice, Port Alice〉.
If the NAT box will forward anything to Alice that is addressed to Alice’s tem - porarilty assigned global address and port, then any node on the Internet could send a packet to  Alice by addressing it to 〈IPAlice, PortAlice〉.
Sometimes this is the desired behavior.
For example,  assume there is a conferencing system.
Alice, George, and Carol (all behind NATs) join the confer - ence by contacting the central server, but it is not desirable for all of the conference communication  to go through the central server.
If the NAT box allows anyone that knows Alice’s temporary global  address to contact Alice, then the conference coordinator can tell all of the members the other mem - bers’ global addresses, and they can then directly communicate with each other.
 To create the behavior that only the node that Alice has initiated a connection to, to be able to  reach Alice, then the NAT box will keep a mapping of 4-tuples to 〈external IP, port 〉 pairs.
For  example, if Alice, at internal IP= a, internal port= p initiates a connection to external Bob, at IP  address= B, port=P B, the NAT box might assign Alice, for this connection, the external address and  port 〈IPA, Port A〉.
The NAT table would include Bob’s address in the mapping, and only allow pack - ets from Bob’s address and port to be forwarded to Alice.
So the NAT entry would have a six-tuple  〈B, PB, IPA, PortA, a, p〉, meaning that only packets from Bob (at 〈B, PB〉) would be translated and  forwarded to Alice.
 Another fortuitous use of NATs is for all the devices inside a home to have the same IP  address.
This is useful because some ISPs (Internet Service Providers) charged for Internet connec - tivity per device, and the NAT box made it appear to the ISP as if there were only a single device in  the house.
The ISP’s answer to this threat to their pricing model was to include in the 74-page EULA (end user license agreement) that everyone has to click on (but nobody reads), an agreement  by the user not to use a NAT box.
Now, if an actual human read the 74-page agreement, they would most likely think “What’s a NAT box?”
 1.14.1 Summary  These are the main concepts in the Internet.
We will give more details about these as they come up in the book.
 The Internet has evolved from the original design, and the design was never the only or best  way to design a network, but the industry has made it work.
An analogy is the English language.
It  might be overly complicated, with all the spelling and grammar exceptions, but it does the job.
And
1.14.1 29 NAT (N ETW ORK ADDRESS TRANSLATION)  each year some mysterious panel of people decide which new words should officially be added to  English, and how the grammar rules should change.
 Similarly, for the Internet.
If there is anything it can’t do, at least so far, the world has figured  out how to evolve the Internet to do what is needed.
2 INTRODU CTION TO  CRYPTOGRAPHY  2.1 INTRODUC TION  The word cryptography comes from the Greek words κρυπτο (hidden or secret ) and γραφη (writ- ing).
So, cryptography is the art of secret writing.
More generally, people think of cryptography as  the art of mangling information into apparent unintelligibility in a manner allowing a secret method  of unmangling.
The basic service provided by cryptography is the ability to send information between participants in a way that prevents others from reading it.
In this book, we will concentrate  on the kind of cryptography that is based on representing information as numbers and mathemati - cally manipulating those numbers.
This kind of cryptography can provide other services, such as  • integrity checking—reassuring the recipient of a message that the message has not been  altered since it was generated by a legitimate source,  • authentication—verifying someone’s (or something’s) identity.
 There are three basic kinds of cryptographic functions: hash functions, secret key functions, and  public key functions.
In this chapter, we’ll introduce what each kind is and what it’s useful for.
In  later chapters we will go into more detail.
Public key cryptography (see §2.3 Public Key Cryptogra - phy) involves the use of two keys.
Secret key cryptography (see §2.2 Secret Key Cryptography )  involves the use of one key.
Hash functions (see §2.4 Hash Algorithms ) involve the use of zero  keys!
The hash algorithms are not secret, don’t involve keys, and yet they are vital to cryptographic systems.
 But back to the traditional use of cryptography.
A message in its original form is known as  plaintext or cleartext .
The mangled information is known as ciphertext .
The process for produc - ing ciphertext from plaintext is known as encryption .
The reverse of encryption is called decryp - tion.
 encryption decryption  plaintext ciphertext plaintext  31
32 I NTRODUC TION TO CRYPTOGRAP HY 2.1.1  While cryptographers invent clever secret codes, cryptanalysts attempt to break these codes.
 These two disciplines constantly try to keep ahead of each other.
 2.1.1 The Fundamental Tenet of Cryptography  Ultimately, the security of cryptography depends on what we call the  Fundamental Tenet of Cryptography  If lots of smart people have failed to solve a problem,  then it probably won’t be solved (soon).
 2.1.2 Keys  Cryptographic systems tend to involve both an algorithm and secret information.
The secret is  known as the key.
The reason for having a key in addition to an algorithm is that it is difficult to  keep devising new algorithms that will allow reversible scrambling of information, and it is difficult  to quickly explain a newly devised algorithm to the person with whom you’d like to start communi - cating securely.
With a good cryptographic scheme, it is perfectly okay to have everyone, including  the bad guys (and the cryptanalysts), know the algorithm, because knowledge of the algorithm  without the key does not help unmangle the information.
 The concept of a key is analogous to the combination of a combination lock.
Although the  concept of a combination lock is well known (you dial in the secret numbers in the correct sequence and the lock opens), you can’t open a combination lock easily without knowing the combination.
 2.1.3 Computational Difficulty  It is important for cryptographic algorithms to be reasonably efficient for the good guys to com - pute.
The good guys are the ones with knowledge of the keys.*
Cryptographic algorithms are not  impossible to break without the key.
A bad guy can simply try all possible keys until one works  (this assumes the bad guy will be able to recognize plausible plaintext).
The security of a crypto - graphic scheme depends on how much work it is for the bad guy to break it.
If the best possible  scheme will take ten million years to break using all the computers in the world, then it can be con - sidered reasonably secure.
 *We’re using the terms good guys for the cryptographers and bad guys for the cryptanalysts.
This is a convenient shorthand  and not a moral judgment—in any given situation, which side you consider good or bad depends on your point of view.
2.1.3 INTRODUC TION 33  Going back to the combination lock example, a typical combination might consist of three  numbers, each a number between 1 and 40.
Let’s say it takes ten seconds to dial in a combination.
 That’s reasonably convenient for the good guy.
How much work is it for the bad guy?
There are 403  possible combinations, which is 64000.
At ten seconds per try, it would take a week to try all com - binations, though on average it would only take half that long (even though the right number is  always the last one you try!).
 Often a scheme can be made more secure by making the key longer.
In the combination lock  analogy, making the key longer would consist of requiring four numbers to be dialed in.
This would  make a little more work for the good guy.
It might now take thirteen seconds to dial in the combina - tion.
But the bad guy has forty times as many combinations to try, at thirteen seconds each, so it  would take a year to try all combinations.
And if it took that long, he might want to stop to eat or  sleep.
 With cryptography, computers can be used to exhaustively try keys.
Computers are a lot faster  than people, and they don’t get tired, so millions of keys can be tried per second in software on a  single computer, and dramatically more with specialized hardware.
Also, lots of keys can be tried in  parallel if you have multiple computers, so time can be saved by spending money on more  computers.
 The cryptographic strength of a cryptographic algorithm is often referred to as the work fac - tor required to break it, defined as the number of operations required on classical computers.
Peo - ple are usually vague about what an “operation” is.
An ideal encryption algorithm with n-bit keys  should require a work factor of 2n to break it.
It can’t get better than that, because a brute-force  attack (where an attacker tries all possible keys) would be 2n .
Another way of expressing work fac - tor is security strength , usually expressed as the size of key in an ideal secret key encryption algo - rithm (one in which there is no better attack than brute force).
 Sometimes a cryptographic algorithm has a variable-length key.
It can be made more secure  by increasing the length of the key.
Increasing the length of the key by one bit makes the good guy’s  job just a little bit harder but makes the bad guy’s job up to twice as hard (because the number of  possible keys doubles).
Some cryptographic algorithms have a fixed-length key, but a similar algo - rithm with a longer key can be devised if necessary.
 Assume a perfect algorithm, meaning that the work for the good guys is proportional to the  length of the key (say n bits), and the work for the bad guys is exponential in the length of the key  (e.g., work factor 2n).
Let’s say you were using 128-bit keys.
If computers got twice as fast, the  good guys can use a 256-bit key and have the same performance as before, but the bad guys now  have 2128 times as many keys to try, and the fact that their computer is also twice as fast as before  doesn’t help them much.
So faster computers mean that the computation gap between good guys  and bad guys can be made much bigger, while maintaining the same performance for the good  guys.
34 I NTRODUC TION TO CRYPTOGRAP HY 2.1.4  Keep in mind that breaking the cryptographic scheme is often only one way of getting what  you want.
For instance, a bolt cutter works no matter how many digits are in the combination.
For  another example, see xkcd.com/538 .
 You can get further with a kind word and a gun than you can with a kind  word alone.
 —Willy Sutton, bank robber  2.1.4 To Publish or Not to Publish  It might seem as though keeping a cryptographic algorithm secret will enhance its security.
Cryp - tanalysts would not only need to break the algorithm, but first figure out what the algorithm is.
 These days the consensus is that publishing the algorithm and having it get as much scrutiny  as possible makes it more secure.
Bad guys will probably find out what the algorithm is eventually  (through reverse engineering whatever implementation is distributed), so it’s better to tell a lot of  nonmalicious people about the algorithm so that in case there are weaknesses, a good guy will dis - cover them rather than a bad guy.
A good guy who discovers a weakness will want to gain fame by  letting the world know they have found a weakness (after first warning the developers of the system so they can fix the problem).
Making your cryptographic algorithm publicly known provides free  consulting from the academic community as cryptanalysts look for weaknesses so they can publish  papers about them.
A bad guy who discovers a weakness will exploit it for doing bad-guy things  like embezzling money or stealing trade secrets.
 2.1.5 Earliest Encryption  We use the terms secret code and cipher interchangeably to mean any method of encrypting data.
 The earliest documented cipher is attributed to Julius Caesar.
The way the Caesar cipher would  work if the message were in English is as follows.
Substitute for each letter of the message the letter that is 3 letters later in the alphabet (and wrap around to A from Z).
Thus an A would become a D,  and so forth.
For instance, DOZEN would become GRCHQ .
Once you figure out what’s going on,  it’s very easy to read messages encrypted this way (unless, of course, the original message was in Latin).
 A slight enhancement to the Caesar cipher was distributed as a premium with Ovaltine in the  1940s as Captain Midnight secret decoder rings . (
There were times when this might have been a  violation of export controls for distributing cryptographic hardware!)
The variant is to pick a secret  number n between 1 and 25, instead of always using 3.
Substitute for each letter of the message the  letter that is n later (and wrap around to A from Z, of course).
Thus, if the secret number was 1, an
2.1.6 INTRODUC TION 35  A would become a B, and so forth.
For instance, HAL would become IBM.
If the secret number was  25, then IBM would become HAL .
Regardless of the value of n, since there are only 26 possible val - ues of n to try, it is still very easy to break this cipher if you know it’s being used and you can rec - ognize a message once it’s decrypted.
 The next type of cryptographic system developed is known as a monoalphabetic cipher ,  which consists of an arbitrary mapping of one letter to another letter.
There are 26!
possible pair - ings of letters, which is approximately 4 ×1026. [
Remember, n!,
which reads “ n factorial”, means  n ×(n−1)× (n −2)×…×1.]
This might seem secure, because to try all possibilities, if it took a micro - second to try each one, would take about ten trillion years.
However, by statistical analysis of lan - guage (knowing that certain letters and letter combinations are more common than others), it turns  out to be fairly easy to break.
For instance, many daily newspapers have a daily cryptogram, which  is a monoalphabetic cipher, and can be broken by people who enjoy that sort of thing during their  subway ride to work.
An example is  Cf lqr’xs xsnyctm n eqxxqgsy iqul qf wdcp eqqh, erl lqrx qgt iqul!
 Computers have made much more complex cryptographic schemes both necessary and possible.
Necessary because computers can try keys at a rate that would exhaust an army of clerks, and pos - sible because computers can execute the complex algorithms quickly and without errors.
 2.1.6 One-Time Pad (OTP)  There is an easy-to-understand, fast-to-execute, and perfectly secure encryption scheme called a  one-time pad .
While it is rarely practical to implement, other more practical but less secure  schemes are often compared to it.
A one-time pad consists of a long stream of random bits known  to the communicating parties but not to anyone else.
We’ll use the symbol ⊕ for the operation XOR  (bitwise exclusive or).
Encryption consists of ⊕ing the plaintext with the random bits of a one-time  pad, and decryption consists of ⊕ing the ciphertext with those same random bits.
 An eavesdropper who only sees the ciphertext sees something that is indistinguishable from  random numbers.
One can think of the one-time pad as being the secret key, and while an eaves - dropper could go through all possible keys to learn the message, they would also see all other possi - ble messages of the same length and have no way to tell which one was actually sent.
Because no  amount of computation would allow an eavesdropper to do better than guessing, the scheme is  called information-theoretically secure .
 One-time pads are generally impractical because they require the two parties to agree in  advance on a random bitstream that is as long as all the messages they will ever want to send to one  another.
One-time pads are insecure if the same random numbers are used to encrypt two different  messages.
That’s because an eavesdropper who knows the communicating parties are doing that can learn the ⊕ of the two messages.
This might not seem like a big deal, but in practice it can leak
36 I NTRODUC TION TO CRYPTOGRAP HY 2.2  a lot of information.
For example, assume two images encrypted by ⊕ing with the same one-time  pad.
If the two encrypted images are ⊕’d, you’ll get the ⊕ of the two images.
Even a human can  often see what both images are if the result is displayed.
The more times a one-time pad is reused,  the easier it becomes to determine what the one-time pad is.
This is why cryptographers call this  method a one-time pad, to remind people not to use it more than once (see Homework Problem 11).
 Because it tends to be impractical for Alice and Bob to agree upon a truly random one-time  pad with as many bits as the total amount of information they might want to send to each other,  some cryptographic schemes use a fixed-length secret number as a seed to generate a  pseudorandom bitstream.
The pseudorandom stream generated from the seed is ⊕’d with plaintext  as a means of encrypting the plaintext (and the same stream is ⊕’d by the receiver to decrypt).
This  sort of scheme, usually referred to as a stream cipher , looks like a one-time pad scheme.
However,  if the stream is generated from a seed, it doesn’t have information-theoretic security because an  attacker could go through all possible seeds to find the one that results in an intelligible message.
 Some stream ciphers we discuss in the book include RC4 (§3.8), and CTR mode ( §4.2.3).
 2.2 SECR ET KEY CRYP TOG RAPHY  Secret key cryptography involves the use of a single key.
Given a message (the plaintext ) and the  key, encryption produces unintelligible data (called an IRS Publication—no!
no!
that was just a fin - ger slip, we meant to say ciphertext ), which is about the same length as the plaintext was.
Decryp - tion is the reverse of encryption, and uses the same key as encryption.
 encrypt key  plaintext ciphertext decrypt plaintext  Secret key cryptography is sometimes referred to as conventional cryptography or symmetric  cryptography .
The Captain Midnight code and the monoalphabetic cipher are both examples of  secret key algorithms, though both are easy to break.
In this chapter we describe the functionality of  cryptographic algorithms, but not the details of particular algorithms.
In Chapter 3 we describe the  details of some popular secret key cryptographic algorithms.
The next few sections describe the  types of things one might do with secret key cryptography.
2.2.1 SECRE T KEY CRYPTOGRAPHY 37  2.2.1 Transmitting Over an Insecure Channel  It is often impossible to prevent eavesdropping when transmitting information.
For instance, a tele- phone conversation can be tapped, a letter can be intercepted, and a message transmitted on the  Internet can be observed by routers along the path.
 If you and I agree on a shared secret (a key), then by using secret key cryptography we can  send messages to one another on a medium that can be tapped, without worrying about eavesdrop - pers.
All we need to do is have the sender encrypt the messages and the receiver decrypt them using  the shared secret.
An eavesdropper will only see unintelligible data.
This is the classic use of secret  key cryptography.
 2.2.2 Secure Storage on Insecure Media  Since it is difficult to assume any stored data can be kept from prying eyes, it is best to store it  encrypted.
This must be done carefully, because forgetting the key makes the data irretrievable.
 2.2.3 Authentication  In spy movies, when two agents who don’t know each other must rendezvous, they are each given a  password or pass phrase that they can use to recognize one another.
For example, Alice’s secret  phrase might be “The moon is bright tonight.”
Bob’s response might be “Not as bright as the sun.”
 If Alice were not talking to the real Bob, she will have divulged the secret phrase to the imposter.
 Even if she is talking to Bob, she may have also divulged the secret phrase to an eavesdropper.
 The term strong authentication means that someone can prove knowledge of a secret with - out revealing it.
Strong authentication is possible with cryptography.
Strong authentication is par - ticularly useful when two computers are trying to communicate over an insecure network (since few people can execute cryptographic algorithms in their heads).
Suppose Alice wants to make sure  she is talking to Bob, and they share a key K AB.
Alice picks a random number rA, encrypts it with  KAB, and sends that to Bob.
The quantity { rA}KAB is known as a challenge .
Bob decrypts the chal - lenge and sends rA to Alice.
This is known as Bob’s response to the challenge { rA}KAB.
Alice  knows that she is speaking to someone who knows KAB because the response matches rA. See Fig- ure 2-1.
 Alice Bob  Figure 2-1.
Challenge–Response Authentication with Shared Secret {rA}KAB  rA
38 I NTRODUC TION TO CRYPTOGRAP HY 2.2.4  If someone, say, Fred, were impersonating Alice, he could get Bob to decrypt a value for him  (though Fred wouldn’t be able to tell if the person he was talking to was really Bob because Fred  doesn’t know KAB), but Bob’s response to Fred’s challenge would not be useful later in impersonat - ing Bob to the real Alice because the real Alice would pick a different challenge.
 Note that in this particular protocol, there is the opportunity for Fred to obtain some 〈chosen  ciphertext , plaintext〉 pairs, since he can claim to be Alice and ask Bob to decrypt a challenge for  him.
For this reason, it is essential that challenges be chosen from a large enough space, say 264 val- ues, so that there is no significant chance of using the same challenge twice.
We discuss all sorts of  authentication tricks and pitfalls in Chapter 11 Communication Session Establishment .
 2.2.4 Integrity Check  Another use of secret key cryptography is to generate a fixed-length cryptographic checksum asso - ciated with a message.
 What is a checksum ?
An ordinary (noncryptographic) checksum protects against accidental  corruption of a message.
The original derivation of the term checksum comes from the operation of  breaking a message into fixed-length blocks (for instance, 32-bit words) and adding them up.
The  sum is sent along with the message.
The receiver similarly breaks up the message, repeats the addi - tion, and checks the sum .
If the message had been garbled en route, the sum will hopefully not  match the sum sent and the message will be rejected.
Unfortunately, if there were two or more  errors in the transmission that canceled each other, the error would not be detected.
It turns out this  is not terribly unlikely, given that if flaky hardware flips a bit somewhere, it is likely to flip another  somewhere else.
To protect against such “regular” flaws in hardware, more complex checksums  called cyclic redundancy checks (CRCs) were devised.
But these still only protect against faulty  hardware and not an intelligent attacker.
Since CRC algorithms are published, an attacker who  wanted to change a message could do so, compute the CRC on the new message, and send that  along.
 To provide protection against malicious changes to a message, a secret integrity check algo - rithm is required, i.e., an attacker that does not know the algorithm should not be able to compute  the correct integrity check for a message.
As with encryption algorithms, it’s better to have a com - mon (known) algorithm and a secret key.
This is what a cryptographic checksum does.
Given a key  and a message, the algorithm produces a fixed-length message authentication code (MAC ) that  can be sent with the message.
MACs used to be called MIC s (message integrity code s) in some  older standards, but the term MAC seems to have become more popular.
 If anyone were to modify the message without knowing the key, they would have to guess a  MAC, and the chance of picking a correct MAC depends on the length of the MAC.
A typical MAC  is at least 48 bits long, so the chance of getting away with a forged message is only one in  280 trillion.
2.3 PUBLIC KEY CRYPTOGRAPHY 39  Such message authentication codes have been in use to protect the integrity of large interbank  electronic funds transfers for quite some time.
The messages are not kept secret from an eavesdrop - per, but the integrity check assures that only someone with knowledge of the key could have created  or modified the message.
 2.3 PUBLIC KEY CRYPTOGRAP HY  Public key cryptography is sometimes also referred to as asymmetric cryptography .
The first pub - lication to present the concept of public key cryptography was in 1975 [DIFF76b], though it is now  known that scientists from GCHQ, the British equivalent to the U.S. NSA, had discovered this tech - nology a few years earlier.
Clifford Cocks is now known to have invented the RSA technology in  1973, and Malcom J. Williamson invented the Diffie-Hellman technology in 1974.
 Unlike secret key cryptography, each individual has two keys: a private key that should only  be known to the key owner, and a public key that can safely be told to the entire world.
The keys are  related to each other, in that Alice’s public key could be used so that anyone with knowledge of her  public key can encrypt something for Alice, and only Alice, knowing her private key, will be able to  decrypt it.
There are other uses as we will describe.
 Note that we call the private key a private key and not a secret key .
This convention is an  attempt to make it clear in any context whether public key cryptography or secret key cryptography  is being used.
Some people use the term secret key for the private key in public key cryptography or  use the term private key for the secret key in secret key technology.
We hope to convince people to  use the term secret key only as the single secret number used in secret key cryptography.
The term  private key should refer to the key in public key cryptography that must not be made public.
 Unfortunately, both words public and private begin with p. We will sometimes want a single  letter to refer to one of the keys.
The letter p won’t do.
We will use the letter e to refer to the public  key, since the public key is used when encrypting a message.
We’ll use the letter d to refer to the  private key, because the private key is used to decrypt a message.
 public key private key  plaintext encrypt ciphertext decrypt plaintext  There is an additional thing one can do with public key technology, which is to generate a  digital signature on a message.
A digital signature is a number associated with a message, like a  checksum or the MAC described in §2.2.4 Integrity Check .
However, unlike a checksum, which can  be generated by anyone, a digital signature can only be generated by someone knowing the private
40 I NTRODUC TION TO CRYPTOGRAP HY 2.3.1  private key public key  plaintext sign signature verify yes/no  plaintext  key.
A public key signature differs from a secret key MAC because verification of a MAC requires  knowledge of the same secret as was used to create it.
Therefore, anyone who can verify a MAC  can also generate one, and so be able to substitute a different message and corresponding MAC.
 In contrast, verification of the signature only requires knowledge of the public key.
So Alice  can sign a message by generating a signature that only she can generate (using her private key).
 Other people can verify that it is Alice’s signature (because they know her public key) but cannot  forge her signature.
This is called a signature because it shares with handwritten signatures the  property that it is possible to recognize a signature as authentic without being able to forge it.
 Public key cryptography can do anything secret key cryptography can do, but to give the  same security level, the known public key cryptographic algorithms are orders of magnitude slower  than the best known secret key cryptographic algorithms.
So public key algorithms are usually used  in combination with secret key algorithms.
Public key cryptography is very useful because network security based on public key technology tends to be more easily configurable.
Public key cryptogra - phy might be used in the beginning of communication for authentication and to establish a tempo - rary shared secret key, then the secret key is used to encrypt the remainder of the conversation using  secret key technology.
 For instance, suppose Alice wants to talk to Bob.
A typical technique is that Alice uses Bob’s  public key to encrypt a secret key, then uses that secret key to encrypt whatever else she wants to  send to Bob.
Since the secret key is much smaller than the message, using the slow public key cryp - tography to encrypt the secret key is not that much of a performance hit.
Notice that given this pro - tocol, Bob does not know that it was Alice who sent the message.
This could be fixed by having  Alice digitally sign the encrypted secret key using her private key.
 Now we’ll describe the types of things one might do with public key cryptography.
 2.3.1 Transmitting Over an Insecure Channel  Suppose Alice’s 〈public key , private key 〉 pair is 〈eA, dA〉.
Suppose Bob’s key pair is 〈eB, dB〉.
 Assume Alice knows Bob’s public key, and Bob knows Alice’s public key, and they each know their  own private key.
Actually, accurately learning other people’s public keys is one of the biggest
2.3.2 PUBLIC KEY CRYPTOGRAPHY 41  challenges in using public key cryptography and will be discussed in detail in §10.4 PKI.
But for  now, don’t worry about it.
 Alice  encrypt mA using eB  decrypt to mB using dA Bob  decrypt to mA using dB  encrypt mB using eA  2.3.2 Secure Storage on Insecure Media  For performance reasons, you would encrypt the data with a secret key, but then you can encrypt the  secret key with the public key of whoever is authorized to read the data, and include that with the  encrypted data.
An advantage of public key cryptography is that Alice can encrypt something for  Bob without knowing his decryption key.
If there are multiple authorized readers, Alice can encrypt  the secret key with each of the authorized readers, and include those encrypted quantities as well, with the encrypted data.
 2.3.3 Authentication  With secret key cryptography, if Alice and Bob want to communicate, they have to share a secret.
If  Bob wants to be able to prove his identity to lots of entities, then with secret key technology he will  need to remember lots of secret keys, one for each entity to which he would like to prove his iden - tity.
Possibly he could use the same shared secret with Alice as with Carol, but that has the disad - vantage that then Carol and Alice could impersonate Bob to each other.
 Public key technology is much more convenient.
Bob only needs to remember a single secret,  his own private key.
It is true that if Bob wants to be able to verify the identity of thousands of enti - ties, then he will need to know (or be able to obtain when necessary) thousands of public keys.
In  §10.4 PKI we discuss how this might be done.
 Here’s an example of how Alice can use public key cryptography for verifying Bob’s identity  (assuming Alice knows Bob’s public key).
Alice chooses a random number r, encrypts it using  Bob’s public key eB, and sends the result to Bob.
Bob proves he knows dB by decrypting the mes - sage and sending r back to Alice.
 Alice Bob  encrypt r using eB decrypt to r using dB  r
42 I NTRODUC TION TO CRYPTOGRAP HY 2.3.4  2.3.4 Digital Signatures  Forged in USA  engraved on a screwdriver claiming to be of brand Craftsman ™  It is often useful to prove that a message was generated by a particular individual.
This is easy with  public key technology.
Bob’s signature for a message m can only be generated by someone with  knowledge of Bob’s private key.
And the signature depends on the contents of m. If m is modified in  any way, the signature no longer matches.
So digital signatures provide two important functions.
 They prove who generated the information (in this case, Bob), and they prove that the information has not been modified (by anyone other than Bob) since the message and matching signature were  generated.
 Digital signatures offer an important advantage over secret key–based cryptographic  MACs— non-repudiation .
Suppose Bob sells widgets and Alice routinely buys them.
Alice and  Bob might agree that, rather than placing orders through the mail with signed purchase orders,  Alice will send electronic mail messages to order widgets.
To protect against someone forging orders and causing Bob to manufacture more widgets than Alice actually needs, Alice will include  an integrity check on her messages.
This could be either a secret key–based MAC or a public key–  based signature.
But suppose sometime after Alice places a big order, she changes her mind (the  bottom fell out of the widget market).
Since there’s a big penalty for canceling an order, she doesn’t confess that she’s canceling, but instead denies that she ever placed the order.
Bob sues.
If Alice  authenticated the message by computing a MAC based on a key she shares with Bob, then Bob  knows Alice did place the order.
That is because nobody other than Bob and Alice knows that key, so if Bob knows he didn’t create the message, he knows it must have been Alice.
But he can’t prove  it to anyone!
Since he knows the same secret key that Alice used to sign the order, he could have  forged the signature on the message himself, and he can’t prove to the judge that he didn’t!
If it  were a public key signature, he could show the signed message to the judge and the judge could  verify that it was signed with Alice’s key.
Alice could still claim, of course, that someone had stolen  and misused her key (it might even be true!),
but the contract between Alice and Bob could reason - ably hold her responsible for damages caused by her inadequately protecting her key.
Unlike secret  key cryptography, where the keys are shared, you can always tell who’s responsible for a signature  generated with a private key.
2.4 HASH ALGORITHM S 43  2.4 HASH ALGOR ITHM S  Hash algorithms are also known as message digest algorithms .
A cryptographic hash function is a  mathematical transformation that inputs a message of arbitrary length (transformed into a string of  bits) and outputs a fixed-length (short) number.
We’ll call the hash of a message m, hash( m).
It has  the following properties:  • For any message m, it is relatively easy to compute hash( m).
This just means that in order to  be practical it can’t take a lot of processing time to compute the hash.
 • Given a hash value h, it is computationally infeasible to find an m that hashes to h.  • Even though many different values of m will hash to the same value (because m is of arbitrary  length, and hash( m) is fixed length), it is computationally infeasible to find two messages that  hash to the same thing.
 We’ll give examples of secure hash functions in Chapter 5 Cryptographic Hashes .
 2.4.1 Password Hashing  When a user types a password, the system has to be able to determine whether the user got it right.
 If the system stores the passwords unencrypted, then anyone with access to the system storage or  backup tapes can steal the passwords.
Luckily, it is not necessary for the system to know a pass- word in order to verify its correctness.
Instead of storing the password, the system can store a hash  of the password.
When a password is supplied, the system computes the password’s hash and com - pares it with the stored value.
If they match, the password is deemed correct.
If the hashed password  file is obtained by an attacker, it is not immediately useful because the passwords can’t be derived  from the hashes.
 Historically, some systems made the password file publicly readable, an expression of confi - dence in the security of the hash.
Even if there are no cryptographic flaws in the hash, it is possible  to guess passwords and hash them to see if they match.
If a user is careless and chooses a password  that is guessable (say, a word that would appear in a 50000-word dictionary of potential pass - words), an exhaustive search would “crack” the password even if the encryption were sound.
For  this reason, many systems hide the hashed password list (and those that don’t, should).
 2.4.2 Message Integrity  Cryptographic hash functions can be used to generate a MAC to protect the integrity of messages  transmitted over insecure media in much the same way as secret key cryptography.
44 I NTRODUC TION TO CRYPTOGRAP HY 2.4.3  If we merely sent the message and used the hash of the message as a MAC, this would not be  secure, since the hash function is well-known.
The bad guy can modify the message, compute a  new hash for the new message, and transmit that.
 However, if Alice and Bob have agreed on a secret, Alice can use a hash to generate a MAC  for a message to Bob by taking the message, concatenating the secret, and computing the hash of  message|secret .
This is called a keyed hash .
Alice then sends the hash and the message (without  the secret) to Bob.
Bob concatenates the secret to the received message and computes the hash of the result.
If that matches the received hash, Bob can have confidence the message was sent by  someone knowing the secret. (
Note: There are some cryptographic subtleties to making this actu - ally secure.
See §5.4.10 Computing a MAC with a Hash .)
 Alice Bob secret secret  message keyed  hash MAC keyed  hash MAC  =?
yes/no  message  2.4.3 Message Fingerprint  If you want to know whether some large data structure ( e.g., a program) has been modified from  one day to the next, you could keep a copy of the data on some tamper-proof backup store and peri - odically compare it to the active version.
With a hash function, you can save storage.
You simply  save the hash of the data on the tamper-proof backup store (which, because the hash is small, could  be a piece of paper in a filing cabinet).
If the hash value hasn’t changed, you can be confident none  of the data has.
 A note to would-be users—if it hasn’t already occurred to you, it has occurred to the bad  guys—the program that computes the hash must also be independently protected for this to be secure.
Otherwise, the bad guys can change the file but also change the hashing program to report  the hash as though the file were unchanged!
 2.4.4 Efficient Digital Signatures  Public key algorithms are much slower than hash algorithms.
Therefore, to sign a message, which  might be arbitrarily large, Alice does not usually sign the actual message.
Instead, she computes a  hash of the message and uses her private key to sign the hash of the message.
2.5 BREAKING AN ENCRY PTION SCHEME 45  2.5 BREA KIN G AN ENCRY PTION SCHEME  What do we mean when we speak of a bad guy Fred breaking an encryption scheme?
Examples are  Fred being able to decrypt something without knowing the key, or figuring out what the key is.
Var - ious attacks are classified as ciphertext only , known plaintext , chosen plaintext , chosen cipher - text and side-channel attacks .
 Note that cryptographers are well aware of all these attacks, so most modern systems are  designed to be resilient against these attacks.
 2.5.1 Ciphertext Only  In a ciphertext-only attack, Fred has seen (and presumably stored) some ciphertext that he can ana - lyze at leisure.
Typically, it is not difficult for a bad guy to obtain ciphertext. (
If a bad guy can’t  access the encrypted data, then there would have been no need to encrypt the data in the first place!)
 How can Fred figure out the plaintext if all he can see is the ciphertext?
One possible strategy  is to search through all the keys.
Fred tries the decrypt operation with each key in turn.
It is essential  for this attack that Fred be able to recognize when he has succeeded.
For instance, if the message  was English text, then it is highly unlikely that a decryption operation with an incorrect key could  produce something that looked like intelligible text.
Because it is important for Fred to be able to differentiate plaintext from gibberish, this attack might be better called a recognizable-plaintext  attack.
 It is also essential that Fred have enough ciphertext.
For instance, using the example of a  monoalphabetic cipher, if the only ciphertext available to Fred were XYZ , then there is not enough  information.
There are many possible letter substitutions that would lead to a legal three-letter  English word.
There is no way for Fred to know whether the plaintext corresponding to XYZ is  THE or CAT or HAT.
As a matter of fact, in the following sentence, any of the words could be the  plaintext for XYZ :  The hot cat was sad but you may now sit and use her big red pen.
 [Don’t worry—we’ve found a lovely sanatorium for the coauthor who wrote that.
 —the other coauthors ]  Often it isn’t necessary to search through a lot of keys.
For instance, the authentication  scheme Kerberos (see §10.3 Kerberos ) assigns to user Alice a secret key derived from Alice’s pass - word according to a straightforward, published algorithm.
The secret key Kerberos uses today is  typically 256 bits.
If Alice chooses her password unwisely ( e.g., a word in the dictionary), then Fred  does not need to search through all 2256 possible keys—instead he only needs to try the derived  keys of the 50000 or so passwords in a dictionary of potential passwords.
46 I NTRODUC TION TO CRYPTOGRAP HY 2.5.2  Another form of information leakage from ciphertext alone is known as traffic analysis , in  which information is inferred based on seeing transmitted ciphertext.
For instance, it might be use - ful to know that a lot of traffic is suddenly being transmitted between two companies, because it  might signal a potential merger.
To avoid leaking information based on which parties are communi - cating, traffic can be sent through an intermediary, which could send lots of dummy traffic to all its  customers at all times, and just replace dummy traffic with real traffic when there is real traffic to  send.
See §14.18 Message Flow Confidentiality .
 The length of a message might leak information.
For instance, when a teenager gets a letter  from a college they applied to, they know before opening the envelope (based on whether it is thick or thin), whether the letter is a rejection or an acceptance.
To avoid leaking information based on  how long a message is, messages can be padded with extra bits so they are all the same size.
 A cryptographic algorithm has to be secure against a ciphertext-only attack because of the  accessibility of the ciphertext to cryptanalysts.
But, in many cases, cryptanalysts can obtain addi - tional information, so it is important to design cryptographic systems to withstand the next two  attacks as well.
 2.5.2 Known Plaintext  Sometimes life is easier for the attacker.
Suppose Fred has somehow obtained some 〈plaintext,  ciphertext 〉 pairs.
How might he have obtained these?
One possibility is that secret data might not  remain secret forever.
For instance, the data might consist of specifying the next city to be attacked.
Once the attack occurs, the plaintext to the previous day’s ciphertext is now known.
Another exam - ple is where all messages start with the same plaintext, for instance, the date.
 With a monoalphabetic cipher, a small amount of known plaintext would be a bonanza.
From  it, the attacker would learn the mappings of a substantial fraction of the most common letters (every  letter that was used in the plaintext Fred obtained).
Some cryptographic schemes might be good  enough to be secure against ciphertext-only attacks but not good enough against known plaintext attacks.
In these cases, it becomes important to design the systems that use such a cryptographic  algorithm to minimize the possibility that a bad guy will ever be able to obtain 〈plaintext, cipher - text〉 pairs.
 2.5.3 Chosen Plaintext  Sometimes, life may be easier still for the attacker.
In a chosen plaintext attack, Fred can choose any plaintext he wants and get the system to tell him what the corresponding ciphertext is.
How  could such a thing be possible?
2.5.4 BREAKING AN ENCRY PTION SCHEME 47  Suppose a telegraph company offered a service in which they encrypt and transmit messages  for you.
Suppose Fred had eavesdropped on Alice’s encrypted message.
Now he’d like to break the  telegraph company’s encryption scheme so that he can decrypt Alice’s message.
 He can obtain the corresponding ciphertext to any message he chooses by paying the tele - graph company to send the message for him, encrypted.
For instance, if Fred knew they were using  a monoalphabetic cipher, he might send the message  The quick brown fox jumps over the lazy dog.
 knowing that he would thereby get all the letters of the alphabet encrypted and then be able to decrypt with certainty any encrypted message.
Even if the telegraph company is using a strong  cipher, if the same message sent twice encrypts to the same ciphertext, then if the attacker can guess the plaintext, he can verify his guess by sending that message and seeing whether the ciphertext matches.
As we will see in Chapter 4 Modes of Operation , modern cryptographic systems encrypt  in a way that encrypting the same plaintext twice will result in two different ciphertexts.
 2.5.4 Chosen Ciphertext  If an attacker (Trudy) makes up a message or modifies a real message from Alice to Bob, and sends it to Bob, claiming to be Alice, Trudy might learn something by observing how Bob reacts.
Even if  Bob detects that the message is not formatted as a message from Alice should be, Trudy might learn  something based on the particular error message Bob sends when rejecting the message, or even  based on how long it takes Bob to respond with an error.
 One example of a chosen ciphertext attack was a vulnerability in a widely deployed system  (SSL), found by Bleichenbacher [BLEI98 ].
The attack works as follows: First, Trudy eavesdrops  and records a session between Alice and Bob.
In particular, Trudy will find the part of the session  where Alice sent a secret key encrypted with Bob’s public key.
That secret key was then used to  cryptographically protect the rest of the session.
Trudy can then pretend to try to initiate a connec - tion to Bob sending modified versions of the encrypted secret key that Alice sent.
These connec - tions will fail, because Trudy doesn’t know the secret key Alice used, but this attack takes advantage of the fact that in this implementation, Bob was super helpful in letting Trudy know what  was specifically wrong with each modified message.
With this attack, Trudy would eventually ( e.g.,  after on the order of million connection attempts) discover the session key for the recorded  Alice-Bob session, and, therefore, Trudy would be able to decrypt that entire Alice-Bob conversa - tion.
These sorts of attacks can generally be protected against by simple precautions, such as always  protecting data with both encryption and integrity protection, and by giving the least information  possible about why communication attempts fail.
 There are more subtle chosen ciphertext attacks and defenses, which we discuss in ( §8.1.3  Defense Against Dishonest Ciphertext ).
These generally force the attacker to prove they have
48 I NTRODUC TION TO CRYPTOGRAP HY 2.5.5  generated their ciphertext in a way that does not deviate from the specification, for example, by  including in the ciphertext an encryption of the random seed used to create the ciphertext.
 2.5.5 Side-Channel Attacks  A side-channel attack , introduced by Paul Kocher [KOCH96], isn’t an attack on an algorithm  itself, but rather on an implementation of an algorithm in a particular environment.
By observing  side effects of an implementation while it is executing, an attacker might discover information such  as some or all the bits of the plaintext or the key being used.
Usually, what’s observed is how long it takes something to execute, though some side channels are based on careful measurement of power  consumption or even the sound the computer makes.
The most powerful attacks are possible when  Trudy, the attacker, can run a program on a system very close to the implementation she would like  to observe.
For example, she might run a process on the same computer.
Trudy can measure how  long her own operations take while the victim process is running on another thread.
By carefully measuring instruction times as it accesses memory, Trudy can get information about which cache lines the victim process is using.
Another example where Trudy might powerfully gain information  is if she installs malicious software or hardware on a smart card reader.
Again, she is close to the  implementation running on the smart card and can observe how much power it uses, and other  information.
Even an attacker with only network access may be able to determine things by care - fully measuring how long it takes a server to respond to messages.
 One way of defending against side-channel attacks is for an implementation to make sure that  it always behaves the same way, no matter what its inputs are.
For instance, it could perform extra  computation so that timing always matches that of the worst-case input.
Another type of mitigation  is to randomize the inputs using a random function of the inputs and then convert the result to what  it would be if the implementation computed based on the actual input.
 2.6 RAND OM NUMBERS  The generation of random numbers is too important to be left to chance.
 —Robert Coveyou, Oak Ridge National Laboratory  In this section, we will discuss some of the considerations that go into the generation and use of random numbers.
For more reading on the subject of random number generators, see NIST  SP800-90 parts A, B, and C.
2.6.1 RANDOM NUMBERS 49  You can have perfect cryptographic algorithms, and perfectly designed protocols, but if you  can’t select good random numbers, your system can be remarkably insecure.
Random numbers may  be required in choosing cryptographic keys, challenges, or other inputs to a cryptographic system.
 Although I3’d love to rigorously define “random”, I2 won’t let me3 because a really rigorous  definition is beyond the scope and spirit of the book.
For instance, in [KNUT69 ], fifteen pages are  devoted to coming up with a definition of a random sequence.
For those readers who are not intim - idated by notions such as ( m,k)-distributed, serial correlation coefficients, and  Riemann-integrability (don’t try to find those terms in our glossary), the discussion of randomness in [KNUT69 ] is actually very interesting (though, honestly, not to me 2).
 In the context of cryptography, we measure the quality of random numbers by how much  work it would be for an attacker to guess the chosen value.
If that’s more than the work required to break the cryptographic system in some other way, then the quantity is random enough.
Ideally, it  would require testing about 2 n strings to find a specific n-bit random string (actually, 2n−1 tests on  average, assuming average luck on the part of the attacker).
Random number generators used in  cryptographic systems usually involve three steps:  • Gathering entropy  • Computing a random seed  • Compute a pseudorandom number stream from the seed  Each of these steps is perilous in its own way, in terms of bugs that could make a system insecure in ways that are hard to detect.
 2.6.1 Gathering Entropy  The concept of entropy comes from physics, but in this context entropy is a measure of the diffi - culty of guessing the value of something.
Something with 2n equally likely values has n bits of  entropy.
If the values are not equally likely, there is less entropy.
A perfectly random stream of bits  would have one bit of entropy per bit of data, but even data that is easier than that to guess has  value.
Timing user keystrokes using a high-resolution clock may be fairly predictable but still give  a few bits of entropy per keystroke.
Measuring the seek time of disks is similarly predictable but not  in the low order bits.
If the device has a microphone or camera, a large number of bits can be read  containing a reasonably high amount of entropy.
 One of the problems with mechanical sources is that their quality can degrade over time.
User  keystroke timing may contain a lot of entropy until the user is replaced with an automated agent that feeds the keystrokes from a script.
Disk seek times may become highly predictable when the  disk hardware is replaced with a solid-state drive.
A conservative design will combine lots of  entropy sources in such a way that the system will be secure if any of them is good.
50 I NTRODUC TION TO CRYPTOGRAP HY 2.6.2  Most modern CPUs have a built-in random number generator from which random bits can be  harvested, but it is dangerous to rely on that single source.
That’s because subverting the design of  the random number source on a CPU chip would be a high-value target for intelligence agencies or  well-funded criminal organizations.
It would be very difficult to detect that the output of the chip’s  random number function was not based on truly random input, but instead, guessable by someone  who knows the seed and algorithm that they secretly embedded in the chip design.
 Some systems go to extreme lengths to build hardware that gathers provably good sources of  entropy like counting events of radioactive decay with a Geiger counter or measuring the polariza - tion of photons.
There is no reason to believe that these sources of randomness are any better than  more conventional ones, and like a random number source built into a CPU, it would be difficult to  know whether the high-priced device is really counting Geigers :-) or generating numbers that are  guessable by the agency that secretly embedded an algorithm.
 2.6.2 Generating Random Seeds  The second step is to turn a lot of sources of unguessable quantities into a random seed.
The best  way to do that is to perform a cryptographic hash of the data.
Some people recommend ⊕ing the  different sources together, which is just as good if the different sources are uncorrelated.
But if two sources turned out to be identical, ⊕ing them together would entirely remove their benefit (see  Homework Problem 6), while hashing their concatenation would keep almost all the entropy they  contain so long as the hash fuinction output is large enough to hold it.
 2.6.3 Calculating a Pseudorandom Stream from the Seed  A pseudorandom number generator (PRNG) takes the random seed and produces a long stream  of cryptographically random data.
Since the PRNG generates a stream deterministically from a  seed, the entropy in the stream will not be greater than the size of the seed.
Also, if someone were to  capture the state of the PRNG, they would be able to calculate all the subsequent output.
Some - times parts of the stream will not be secret, such as when the stream is used for choosing a random initialization vector (IV) [see §4.2.2 CBC (Cipher Block Chaining) ].
It is essential, then, that seeing  part of the pseudorandom stream not allow an attacker to compute the rest of the stream.
It is also  desirable for an implementation to throw away state so that previous output of the stream will not  be able to be calculated based on the current state.
For example, if the implementation always kept  the initial seed, someone who stole that seed would be able to calculate the entire stream, past and  future.
If an implementation were to crash, the system might dump the entire state.
 There are many secure ways to construct a pseudorandom number generator using secret key  encryption and hash functions, but it is best to use one of the standardized ones from [NIST15a].
2.6.4 RANDOM NUMBERS 51  There are two reasons for that—first, it is required for some security compliance regimes, but just  as important is that those algorithms come with sample data that allow you to test whether your  implementation is correct.
Bugs in random number generators will produce output that almost cer - tainly looks random to any tester but may be trivially attackable by someone who discovers the bug  and sees some of the output.
 Sometimes, Alice and Bob need to agree on a lot of different keys or other secret values.
It is  often convenient to derive all of them from a seed so that Alice need only send the seed to Bob and  the other secret information can be derived from that seed.
The deterministic function for deriving  information from a seed is either known as a pseudorandom function (PRF ) or a key-derivation  functio n (KDF ).
This function will generally take two inputs: the seed, and an additional input  called a data variable which identifies which of the possible keys or secret values are being gener - ated from the seed.
 2.6.4 Periodic Reseeding  It is good security practice to periodically add randomness to the PRNG.
This involves gathering  entropy as the program runs, and when there is enough, mixing it in with the state.
The reason to do  this is so that if an attacker learns the seed you are using, they will only be able to predict the ran - dom stream for a limited amount of time.
 It is better to wait until a reasonable amount of entropy has been built up, and mix it all in at  once, rather than mixing in new entropy bit-by-bit as it is gathered.
Adding 128 bits of entropy once is worth a lot more than adding eight bits of entropy a thousand times (see Homework Problem 9).
 2.6.5 Types of Random Numbers  Applications that use random numbers have different requirements.
For most applications, for  instance one that generates test cases for debugging a computer program, all that might be required  is that the numbers are spread around with no obvious pattern.
For such applications it might be  perfectly reasonable to use something like the digits of π.
However, for cryptographic applications  such as choosing a cryptographic key, it is essential that the numbers be unguessable.
Consider the  following pseudorandom number generator.
It starts with a truly random seed, say by timing a  human’s keystrokes.
Then it computes a hash of the seed, then at each step it computes a hash of the output of the previous step.
Assuming a good hash function, the output will pass any sort of statisti - cal tests for randomness, but an intruder that captures one of the intermediate quantities will be able to compute the rest.
52 I NTRODUC TION TO CRYPTOGRAP HY 2.6.6  In general, the functions provided in programming languages for random numbers are not  designed to be unguessable in the cryptographic sense.
They are designed merely to pass statistical  tests.
Calling one of these to generate cryptographic keys is an invitation to disaster.
 2.6.6 Noteworthy Mistakes  Random number implementations have made some amusing mistakes.
Typical mistakes are:  • Seeding the generator with a seed that is from too small a space.
Suppose each time a crypto - graphic key needed to be chosen, the application obtained sixteen bits of true randomness  from special purpose hardware and used those sixteen bits to seed a pseudorandom number  generator.
The problem is that there would be only 65536 possible keys it would ever choose,  and that is a very small space for an adversary (equipped with a computer) to search.
Jeff  Schiller found an implementation in a product that computed RSA key pairs randomly, but  from an 8-bit seed!
Jeff attempted to report the bug to one of the lead developers of the prod - uct (let’s call that person Bob, though that was not his name).
Bob did not believe Jeff.
So Jeff  wrote a program to compute all 256 possible key pairs the product could generate, found  Bob’s key, and sent Bob an email signed with Bob’s private key.
 • Using a hash of the current time when an application needs a random value.
The problem is  that some clocks do not have fine granularity, so an intruder that knew approximately when  the program was run would not need to search a very large space to find the exact clock value  for the seed.
For instance, if a clock has granularity of 1/60 second, and the intruder knew the  program chose the user’s key somewhere within a particular hour, there would only be  60×60×60 = 216000 possible values.
A widely deployed product used a microsecond granu - larity concatenated with some other values that were not very secret.
Ian Goldberg and David  Wagner discovered this bug and demonstrated breaking keys in 25 seconds on a slow machine  [GOLD96].
 • Divulging the seed value.
An implementation, again discovered by Jeff Schiller (who warned  the company so that the implementation has been fixed), used the time of day to choose a  per-message encryption key.
The time of day in this case may have had sufficient granularity,  but the problem was that the application included the time of day in the unencrypted header of the message!
 One event too important to leave out involves an intelligence agency that may have tricked the world into deploying a pseudorandom number generator with a back door that allowed them to pre - dict the output (assuming they could see parts of the output).
It was called Dual_EC_DBRG, and in spite of warnings from industry experts and much lower performance than competing algorithms,  they managed to convince NIST to standardize it, and RSA Data Security to make it the default in
2.7 NUMBERS 53  their widely used software.
It was only after evidence of a back door was revealed in the Edward  Snowden papers that the algorithm was discredited and removed from service.
 2.7 NUMBERS  Cryptographic algorithms manipulate messages and keys, both of which are defined in terms of bit  strings.
The algorithms are usually defined in terms of arithmetic operations on numbers, where the  numbers come from some interpretation of groups of bits.
Mathematics has been studied for thou - sands of years, and mathematical properties are well understood, making analysis of algorithms  based on mathematical principles more reliable.
 Mathematics deals with many kinds of numbers: integers, rational numbers, real numbers,  complex numbers, etc.
There are infinitely many of each of these kinds of numbers, which makes  them not representable in any finite number of bits.
Cryptographic algorithms manipulate numbers  with thousands of bits, and require exact, rather than approximate, representations of the quantities they manipulate.
 Let’s call the things in the set that a mathematical system operates on elements .
Elements  might be integers, real numbers, rational numbers, complex numbers, polynomials, matrices, or  other objects.
Various cryptographic algorithms require a mathematical system to have certain  properties.
Familiar properties of numbers in ordinary math are that there are two operations, + and  × (called plus and times , or addition and multiplication ), and:  • commutativity : For any x and y, x+y = y+x and x×y = y× x  • associativity : For any x, y, and z, (x+y)+z = x+(y+z).
Also, ( x×y)×z = x×(y×z)  • distributivity : For any x, y, and z, x× (y+z)= (x× y)+(x× z)  • additive identity : There is a number 0 such that for any x, 0+x=x and x+0= x  • multiplicative identity : There is a number 1 such that for any x, 1×x=x and x×1=x  • additive inverse : For any x, there is a number −x such that x+(−x)=0  −1• multiplicative inverse : For any x other than 0.
there is a number x −1 such that x× x =1  A system that satisfies all these properties is called a field .
A system that has these properties  except that some elements might not have multiplicative inverses, and multiplication might not be  commutative, is called a ring.
A system that only has one operation, and for which every element  has an inverse, is called a group .
 Some cryptographic systems require a system with some or all the above properties and also  require being able to exactly represent each element in a fixed number of bits.
Will integers work?
 Integers have an additive identity, namely 0.
And they have a multiplicative identity, namely 1.
54 I NTRODUC TION TO CRYPTOGRAP HY 2.7.1  However, most integers do not have multiplicative inverses. (
For example, ½ is the multiplicative  inverse of 2 in real numbers, but ½ is not an integer.)
Also, integers can get arbitrarily large, and we  want to be able to represent each element exactly in a fixed number of bits.
 2.7.1 Finite Fields  Luckily, there is a mathematical structure that will do everything we want.
It is called a finite field .
 A finite field of size n has n different elements and has all the properties we need.
One form of  finite field is the integers modulo p, where p is a prime.
Arithmetic mod p uses integers between 0  and p −1.
Addition and multiplication are done just like with regular arithmetic, but if the answer is  not between 0 and p−1, the answer is divided by the modulus p, and the answer is the remainder.
 For example, with mod 7 arithmetic, the elements are {0, 1, 2, 3, 4, 5, 6}.
If you add 2 and 4, you  get 6, which is one of the elements.
However, if you add 5 and 6, the answer is 11, so you need to  divide by the modulus, 7, and take the remainder, which is 4.
We will talk more about modular arithmetic in Chapter 6 First-Generation Public Key Algorithms , when we talk about the RSA algo - rithm, which is only secure if the modulus is not prime.
 Évariste Galois founded the theory of finite fields.
For each prime p, there is exactly one  finite field with p elements, and it will be equivalent to the integers mod p. There is also exactly one  k kfinite field for each higher power of p, for instance, p .
The field with p elements is usually  denoted by GF(pk), where GF stands for Galois field , in honor of Galois.
Arithmetic mod prime p  would be denoted GF( p), although it is sometimes denoted as Z .p  Note that if we use modular arithmetic, say mod 11, it will require four bits to represent each  element, but there will be five values of the bits that will not be elements in the field {11, 12, 13, 14, 15}.
This causes some problems, and it wastes space since we are using four bits to specify only 11  values.
For that reason, cryptographic algorithms often use GF(2 n) arithmetic, because each ele - ment corresponds to a unique n-bit value and vice versa.
 Arithmetic in GF(2n) is very efficient.
Addition is ⊕, which computers love to do.
GF(2n)  multiplication is also efficient (for computers).
Multiplication in GF(2n) can be thought of as mod - ular arithmetic of polynomials with coefficients mod 2.
For instance, the bit string 110001 would  represent the polynomial x 5+x 4+1.
Given that we need to represent GF(2n) elements in n bits, it is  necessary after a multiplication of two n-bit polynomials to reduce the answer (which might be  2n −1 bits) modulo a degree- n polynomial.
The polynomial modulus needs to be irreducible , which  in this case means only divisible by itself and 1.
 There are also finite rings and finite groups, and these are also commonly used in cryptogra - phy.
For instance, RSA, which uses modular arithmetic with a non-prime modulus, does not use a  finite field, but it does use a finite ring.
2.7.2 NUMBERS 55  2.7.2 Exponentiation  We talked about + and × as two operations on the elements in our set.
But cryptographic algorithms  often require exponentiation.
Exponentiation is not another operation on two elements in the set  like + and × are.
Instead, exponentiation takes as input an element a and an integer x and multiplies  x x as together.
This is written a .
 That’s fine if x is small, but what if the exponent x is a number with thousands of digits?
It  would take several universe lifetimes, even for a supercomputer, to do that many multiplications.
 The trick that makes exponentiation by a very large number practical is repeated squaring .
That  means starting with a, squaring it (meaning multiplying it by itself) to get a 2, and squaring a 2 to get  4 a , squaring a 4 to get a 8, and so on.
So, as long as the exponent x is a power of two, e.g., 2k , we can  compute ax with only k multiplies (rather than 2k−1 multiplies).
If the exponent x is not a power of  two, we can still use the trick of repeated squaring.
If x is a power of two, it will look, in binary, like  1 followed by a bunch of zeroes.
If x is not a power of two, the binary representation of x will con - sist of some 0s and some 1s.
To raise a number a to the exponent x, do the following.
Use a pointer  that points to a bit in x and initially points to the leftmost (most significant) bit of x. Have a second  value that we will refer to as the intermediate value, which is initialized to 1.
For each bit in x, do  the following:  1.
If the bit in x pointed to by the pointer is 1, multiply the intermediate value by a.  2.
If the pointer is at the rightmost bit of x, you are done; otherwise, move the pointer to the  right one bit position.
 3.
Square the intermediate value.
 Using this algorithm, the number of multiplies to raise something to the power x depends on the  number of 1s in the binary representation of x. In the worst case, it will take twice the number of  bits in x (if all the bits in x are 1).
In the best case, it will take the number of bits in x (if x is a power  of two).
 We will go deeper into these concepts as they are required for specific algorithms.
 2.7.3 Avoiding a Side-Channel Attack  A straightforward implementation of the algorithm in §2.7.2 Exponentiation is an opportunity for a  side-channel attack, because the implementation would behave differently on each bit of the expo - nent, depending on whether the bit was a 0 or a 1.
An implementation can avoid providing a  side-channel by behaving the same way on each bit.
For instance, it could always implement step 2  (multiplying the intermediate value by a), saving both the result of step 1 and of step 2, and then  discard the result of step 1 if the bit was a 1 or discard the result of step 2 if the bit was a 0.
56 I NTRODUC TION TO CRYPTOGRAP HY 2.7.4  Even this remediation might still allow a side-channel attack because anything that contains  conditional branches might cause observable behavior differences, such as which memory locations  are read and written.
So, an implementation might execute the exponentiation algorithm by com - puting both the result of squaring alone (let’s call that the IF0 value) and of squaring and multiply - ing by a (let’s call that the IF1 value).
Then the implementation can take the relevant bit in the  exponent (which will be 0 or 1), and ⊕ the bit times the IF1 value with the complement of the bit  times the IF0 value.
 Even this might not be enough.
To truly eliminate side channels, you need to know the details  of the particular platform.
 2.7.4 Types of Elements used in Cryptography  Sometimes cryptographic systems use mod p arithmetic, where p is a prime.
RSA, as we will see,  uses mod n arithmetic, where n is definitely not a prime.
Other cryptographic systems, especially  some of the post-quantum algorithms, use polynomials or matrices, but with coefficients that use  either modular arithmetic or GF(2n).
When modular arithmetic is used in post-quantum cryptogra - phy, the modulus is either prime or a power of two.
Note that numbers modulo a power of two do  not form a finite field (because powers of two other than 21 are not prime).
 2.7.5 Euclidean Algorithm  The Euclidean algorithm is an efficient method of finding the greatest common divisor (gcd) of two  numbers a and b. The gcd of a and b is written as gcd( a,b).
It is the largest number that evenly  divides both a and b.  The Euclidean algorithm can also be used to find the multiplicative inverse of b mod a, and  this is what the Euclidean algorithm is mostly used for in cryptography.
Sometimes, using the  Euclidean algorithm for finding multiplicative inverses is called the extended Euclidean algorithm.
 The insight into efficiently finding gcds is that if a number d is a divisor of both a and b, it is  also a divisor of a−b.
It is also a divisor of a minus any multiple of b. Therefore, if you divide a by  b, the remainder is also divisible by d. The Euclidean algorithm starts with a and b, and at each step,  calculates smaller numbers that are divisible by d, until the last step when the remainder is 0.
 At each step, there will be two numbers A and B, initialized to a and b. Divide A by B to get  the remainder R, which will be smaller than B. At the next step, set A equal to B, and B equal to R,  and keep iterating until R is 0.
The final value of B will be the gcd of a and b. For example, consider  finding the gcd of 420 and 308.
 420÷ 308 = 1 remainder 112  308÷ 112 = 2 remainder 84
2.7.6 NUMBERS 57  112÷ 84 = 1 remainder 28  84÷28 = 3 remainder 0  Therefore, 28 is the gcd of 420 and 308.
 When we want to find the multiplicative inverse of b mod a, the goal is to find integers u and  v such that u× a+v×b=1.
Then v will be the multiplicative inverse of b mod a, because v× b will be  1 more than a multiple of a. The integer b will only have an inverse mod a if gcd( a,b) is 1.
So let’s  use two numbers a and b such that gcd( a,b)=1.
We’ll use 109 and 25.
First, let’s apply the Euclid - ean algorithm to find gcd(109,25).
 109÷ 25 = 4 remainder 9  25÷9 = 2 remainder 7  9÷7 = 1 remainder 2  7÷2 = 3 remainder 1  2÷1 = 2 remainder 0  So, gcd(109,25)= 1.
We want to represent each of the remainders as some multiple of a (109) plus  some multiple of b (25).
We know that a=1× a+0×b and that b =0×a+1×b.
After the first step, we  know that a÷ b = 4 remainder 9, or rewriting, we get  9=1×a−4×b.
 The second line tells us that 25 ÷9=2 remainder 7.
That means 7=25 −2×9.
Well, 25= b. And 9 is  1× a−4× b. Substituting for 9, we get 7=25 −2×(1×a− 4×b), Since 25= b, this simplifies to  7=−2×a+9×b.
 The third line tells us that 9 ÷ 7=1 remainder 2.
That means that 2=9 −1× 7.
Substituting for 9  (1×a− 4×b) and 7 (−2×a +9×b), we get  2=3×a −13×b.
 The next line tells us that 7 ÷2=3 remainder 1.
That means that 1=7 −3× 2.
Substituting for 7  (−2× a+9× b) and 2 (3×a−13× b), we get 1=( −2×a +9×b)− 3×(3× a−13×b), which simplifies to  1=−11×a+48× b.  This tells us that 48 times b is one more than a multiple of a. In other words, 48 is b’s inverse mod  a. Going back to the values we chose for a and b (109 and 25), we have calculated that 25’s inverse  mod 109 is 48.
And indeed, 25 ×48=1200 and 1200 ÷ 109= 11 remainder 1.
 2.7.6 Chinese Remainder Theorem  The Chinese Remainder Theorem will be useful for making certain RSA operations more efficient.
 We’ll give a special case definition, since that’s all that we need.
As we will see, a typical RSA  modulus n is the product of two primes p and q. The Chinese Remainder Theorem states that if you  know what a number, say x, equals mod n, then you can easily calculate what x is mod p and what x  is mod q. And likewise, if you know what x is mod p, and what x is mod q, then you can easily cal - culate what x is mod n.
58 I NTRODUC TION TO CRYPTOGRAP HY 2.8  It’s pretty easy to see how knowing what x is mod n enables you to calculate what x is mod p,  and what x is mod q. To find out what x is mod p, just divide x mod n by p and the remainder will be  what x is mod p. Likewise for calculating what x mod n is mod q.  The other direction is a bit trickier.
Suppose you know that a number equals a mod p and b  mod q, and you want to know what it is mod pq.
 The first thing you need to do is calculate p’s multiplicative inverse mod q and q’s multiplica - tive inverse mod p. You can do that with the extended Euclidean algorithm.
 Now you know p −1 mod q. And you know q −1 mod p. Note  p ×(p −1 mod q)=1 mod q and it is equal to 0 mod p.  q ×(q −1 mod p)=1 mod p and 0 mod q.  You are looking for a number that is a mod p and b mod q. Multiply q×(q −1 mod p) by a, but don’t  reduce mod p. You’ll get something that is a mod p and also 0 mod q. (When you started, you knew  that a was equal to a mod p, but a is probably not equal to 0 mod q.)  Likewise, multiply p×(p −1 mod q) by b, but don’t reduce mod q. You now have a number that  is equal to 0 mod p and b mod q.  Add these two quantities together.
You’ll get a× (q ×(q −1 mod p))+b× (p× (p −1 mod q)).
Now  you can reduce mod n, and the result will be the number, mod n, that is also equal to a mod p and b  mod q.  Why is the Chinese Remainder Theorem useful?
As we will see in §6.3.4.5 Optimizing RSA  Private Key Operations , instead of doing calculations mod n, the numbers can be converted into  their representations mod p and mod q, and the operations can be performed mod p and mod q, and  knowing the answer mod p, and knowing the answer mod q, the answer can be converted into the  answer mod n. Since p and q are roughly half the size of n, this will be more efficient than doing the  operations mod n.  Note the Chinese Remainder Theorem holds even if the smaller moduli are not themselves  prime, as long as they are relatively prime (no factors in common).
Since people think of p and q as  primes, we’ll use j and k and restate the Chinese Remainder Theorem as “If j and k are relatively  prime and you know a mod j and a mod k, you can compute a mod jk, and vice versa.”
 2.8 HOMEWO RK  1.
What is the dedication to this book?
 2.
Random J. Protocol-Designer has been told to design a scheme to prevent messages from  being modified by an intruder.
Random J. decides to append to each message a hash of that  message.
Why doesn’t this solve the problem?
2.8 HOMEWORK 59  3.
Suppose Alice, Bob, and Carol want to use secret key technology to authenticate each other.
 If they all used the same secret key K, then Bob could impersonate Carol to Alice (actually  any of the three can impersonate the other to the third).
Suppose instead that each had their own secret key, so Alice uses K A, Bob uses KB, and Carol uses KC.
This means that each of  Alice, Bob, and Carol, to prove his or her identity, responds to a challenge with a function of  his or her secret key and the challenge.
Is this more secure than having them all use the same secret key K? (
Hint: What does Alice need to know in order to verify Carol’s answer to  Alice’s challenge?)
 4.
As described in §2.4.4 Efficient Digital Signatures , it is common, for performance reasons, to  sign a hash of a message rather than the message itself.
Why is it so important that it be diffi - cult to find two messages with the same hash?
 5.
Assume a cryptographic algorithm in which the performance for the good guys (the ones that know the key) grows linearly with the length of the key, and for which the only way to break  it is a brute-force attack of trying all possible keys.
Suppose the performance at a certain key- size is adequate for the good guys ( e.g., encryption and decryption can be done as fast as the  bits can be transmitted over the wire).
Then suppose advances in computer technology make  computers twice as fast.
Given that both the good guys and the bad guys get faster computers,  does this advance in computer speed work to the advantage of the good guys, the bad guys, or does it not make any difference?
 6.
Suppose you had a source of really good randomness, and you copied the random output into  two places and ⊕’d these.
How random would the result be?
What if you concatenated the  two quantities and hashed the result?
Would that be random?
 7.
Suppose the seed for a PRNG (see §2.6.3 Calculating a Pseudorandom Stream from the  Seed ) is n bits long.
How many bits of output of the PRNG would you need to see in order to  verify a guess of a seed that it is using (with high probability)?
 8.
Suppose you could see some, but not all, of the output of a PRNG.
Assuming you know the  algorithm the PRNG is using, would you be able to verify a guess of a seed based solely on  seeing every tenth bit?
Would this require trying more potential seeds than if you were able to see all the bits of the PRNG output (assuming you were able to see enough output bits)?
 9.
Suppose you could see 400 bits of output of a PRNG every second, and you knew that it  started with eight bits of randomness and mixed in eight bits of new randomness every sec - ond.
How difficult would it be, after sixteen seconds, to calculate what the state of the PRNG is, compared to a PRNG that mixed in 128 bits of randomness every sixteen seconds?
 10.
Suppose a process generates about eight bits of randomness every second, and suppose a  PRNG used the 8-bit random output every second, and suppose you could see the output of  the PRNG.
Why is it better to wait until you have, say, 128 bits of randomness, before using  the randomness to reseed your PRNG?
In other words, if the random seed were a function of
60 I NTRODUC TION TO CRYPTOGRAP HY 2.8  sixteen 8-bit random chunks, how many possible seeds would there be?
What if you waited  until there were 128 bits of randomness?
 11.
Suppose Alice wants to send a secret message M to Bob, but has not agreed upon a secret  with Bob.
Furthermore, assume that Alice can be assured that when she sends something to  Bob, it will arrive at Bob without anyone having tampered with the message.
So the only  threat is someone reading the transmissions between Alice and Bob.
Alice chooses a random  secret SA, and sends SA⊕ M to Bob.
Nobody (including Bob) can read this message.
Bob now  creates his own secret SB, ⊕s what he received from Alice with SB, and transmits SA⊕ M⊕SB  to Alice.
Alice now removes her secret by ⊕ing with SA and returns M⊕ SB to Bob.
Bob can  now ⊕ with his secret ( SB) in order to read the message.
Is this secure?
 Alice Bob  12.
Use the Euclidean algorithm to compute gcd(1953,210).
 13.
Use the Euclidean algorithm to compute the multiplicative inverse of 9 mod 31.
 14.
A number is 11 mod 36 and also 7 mod 49.
What is the number mod 1764?
How about if a  number is 11 mod 36 and also 11 mod 49—what would that number be mod 1764?
SA⊕M  SA⊕ M⊕SB  M⊕SB
3 SECRET KEY CRYPTOG RAPHY  3.1 INTRODUC TION  Secret key encryption schemes require that both the party that does the encryption and the party that  does the decryption share a secret key.
We will discuss two types of secret key encryption schemes:  • block cipher .
This takes as input a secret key and a plaintext block of fixed size (older  ciphers used 64-bit blocks, modern ciphers use 128-bit blocks).
It produces a ciphertext block  the same size as the plaintext block.
To encrypt messages larger than the blocksize, the block  cipher is used iteratively with algorithms called modes of operation that are the subject of the  next chapter.
A block cipher also has a decryption operation that does the reverse computa - tion.
 • stream cipher .
This uses the key as a seed for a pseudorandom number generator, produces a  stream of pseudorandom bits, and ⊕s (bitwise exclusive ors) that stream with the data.
Since  ⊕ is its own inverse, the same computation performs both encryption and decryption.
 This chapter describes the block ciphers DES, 3DES, and AES.
The world has mostly converted to  AES.
However, it is useful to see the structure of DES and 3DES because they give insight into the  design of a block cipher.
This chapter also describes the stream cipher RC4.
Many weaknesses have  been found in RC4, so although it has nice performance properties, it is no longer widely used.
We  describe RC4 because it is an example of a stream cipher that isn’t constructed from a block cipher.
 3.2 GENERIC BLOC K CIPHER ISSUES  3.2.1 Blocksize, Keysize  It’s fairly obvious that if the keysize is too small (for instance, four bits), the cryptographic scheme  would not be secure because it would be too easy to search through all possible keys.
There’s a  61
62 S ECRET KEY CRYPTOGRAPHY 3.2.2  similar issue with the size of the block of plaintext to be encrypted.
If the blocksize is too small (say  one octet, as in a monoalphabetic cipher), then if you ever had enough paired 〈plaintext ,ciphertext 〉  blocks, you could construct a table to be used for decryption.
It might be possible to obtain such  pairs because messages might only remain secret for a short time, for example, if a message says  where the army will attack the next day.
 Having a blocksize too large has performance penalties, so you wouldn’t want a blocksize  much larger than you need.
The cryptographer’s guideline is that with a blocksize of n bits, you  shouldn’t encrypt more than 2n/2 blocks with the same key, and, in fact, you should change your key  significantly more frequently than once every 2n/2 blocks.
In the 1970s, when DES was being  designed, 64 bits seemed a reasonable size, since no one expected to be encrypting anywhere close  to 32 gigabytes of data.
Modern standards call for 128-bit blocks, which is reasonably safe as long  as you don’t plan on encrypting petabytes of data without changing the key.
 3.2.2 Completely General Mapping  For ease of explanation (and readability, because we are going to need to write out 64-bit values),  let’s assume for now a scheme that uses 64-bit blocks.
The most general way of encrypting a 64-bit  block is to take each of the 264 input values and map it to a unique one of the 264 output values. (
It  is necessary that the mapping be one-to-one , i.e., only one input value maps to any given output  value, since otherwise decryption would not be possible.)
 Suppose Alice and Bob want to decide upon a mapping that they can use for encrypting their  conversations.
How would they specify it?
To specify a monoalphabetic cipher with English letters  requires mapping each of 26 values to 26 possible values, which can be specified with 26 × 5 bits.
 For instance,  a→q b→d c→w d→x e→a f→f g→z h→b …  But now assume a block cipher that maps 64-bit blocks to 64-bit blocks.
How would you  specify a general mapping?
Well, let’s start:  0000000000000000 →8ad1482703f217ce  0000000000000001 →b33dc8710928d701  0000000000000002 →29e856b28013fa4c  Hmm, we probably don’t want to write this all out.
There are 264 possible input values, and  for each one we have to specify a 64-bit output value.
Constructing such a table would take 270 bits.
 In theory, Alice and Bob could share a 270-bit quantity specifying the full mapping, using it like a
3.2.3 GENER IC BLOCK CIPHER ISSUE S 63  secret key.
But it is doubtful that they could remember a key that large, or even be able to say it to  each other within a lifetime, or store it on anything.
So this is not particularly practical.
 3.2.3 Looking Random  Block ciphers are designed to take a reasonable-size key ( i.e., more like 128 bits rather than the 270  bits as described in the previous section) and generate a one-to-one mapping that looks completely  random to someone who does not know the key.
Looking random means that it should look (to  someone who doesn’t know the key) as if the mapping from an input value to an output value were  generated by using a random number generator.
A block cipher that looks random in this way is  called a pseudorandom permutation (PRP ).
 If the mapping were truly random, any single-bit change to the input would result in a totally  independently chosen random number output.
The two different output numbers should have no  correlation, meaning that about half the bits should be the same and about half the bits should be  different.
For instance, it can’t be the case that the third bit of output always changes if the twelfth  bit of input changes, or even that the probability of the third bit of output changing if the twelfth bit  of input changes is anything significantly different from ½ (averaged over all possible inputs).
So  cryptographic algorithms are designed to spread bits around , in the sense that a single input bit  should have influence on all the bits of the output, and be able to change any one of them with a  probability of about ½ (depending on the values of the other bits of input).
 A (not very practical) way of creating a truly random mapping uses an imaginary box whim - sically called an oracle .
Note that a similar concept exists with hashes, and there it is called the ran- dom oracle model .
The box will answer questions of the form “What is P encrypted with key K ?”
 and “What is C decrypted with key K?”
The oracle will keep a table of entries 〈key K, plaintext P,  ciphertext C〉, to remember all the answers it has ever given.
To answer “What is plaintext P  encrypted with K?”,
the box will check whether it has an entry for 〈K,P,C〉.
If so, it will return the  answer “ C”.
If no such entry exists, the box will generate a random value R, and if R does not  already exist as the answer for that K and some other P, the box will make an entry 〈K,P,R〉, and  reply “ R”.
If R already exists as ciphertext in some entry, then the box chooses a different random  R.  Likewise for decryption.
If the box is asked “What is ciphertext C decrypted with key K?”,
 the box will answer “ P” if there is an entry 〈K,P,C〉.
Otherwise, it will generate a random value R  and check whether the triple 〈K,R,x〉 already exists for any ciphertext x. If so, it generates a differ - ent R and checks again.
If not, it enters 〈K,R,C〉 and returns “ R”.
 This obviously impractical approach of asking an oracle to encrypt and decrypt each block is  used to define the ideal cipher model .
Ideal cipher security is stronger than PRP security, because  if someone knows the key that generates the PRP, they can see that it is not random.
Unlike PRP  security, but like random oracle security, ideal cipher security is so strong that it is provably impos -
64 S ECRET KEY CRYPTOGRAPHY 3.3  sible for any efficiently computable function to have it [CANE98].
Nonetheless, the ideal cipher  model gives block cipher designers something to aspire to.
Both the ideal cipher model and the PRP  model are used in security proofs, where the proof assumes the underlying block cipher conforms  to the PRP (or ideal cipher) model, and then proves properties of the system.
 3.3 CONSTRUC TING A PRACTI CAL BLOC K CIPHE R  A generic block cipher takes as input an n-bit plaintext block and a key, and outputs an n-bit cipher- text block.
The usual construction uses an inexpensive but not-very-secure block cipher multiple  times.
This is analogous to shuffling cards multiple times.
Each time is called a round .
Let’s call  the not-very-secure block cipher a round transformation .
In designing a block cipher, cryptogra - phers try to match the strength of the keysize with the number of rounds.
Once there are enough  rounds so that the most efficient method of breaking the cipher is by doing a brute-force search on  all the potential keys, doing extra rounds is a waste of computation.
If there aren’t enough rounds,  then a larger keysize does not add security.
 3.3.1 Per-Round Keys  The round takes a key as a parameter, so that the round transformation will be different depending  on the key.
Sometimes using the same key for all rounds can lead to attacks that can’t be fixed by  simply increasing the number of rounds, so typically each round uses a different key.
If the block  cipher does r rounds, and each round uses an x-bit key, this would result in a very large key ( x×r  bits).
So, instead of requiring the block cipher to use an x×r-bit key, per-round keys are derived  from the actual block cipher key, say k bits.
DES simply uses different 48-bit subsets of the 56-bit  key for each round, while AES uses a slightly more complicated approach to get per-round keys  that are less obviously related to one another.
Creating per-round keys from the main key is referred  to as the key expansion step or key schedule .
In cases where many plaintext blocks will be  encrypted with the same key, it can be a performance optimization to only perform the key expan - sion step once and cache the key schedule.
 3.3.2 S-boxes and Bit Shuffles  The round transformation (in both DES and AES) contains a component that transforms a set of  input bits into a set of output bits.
This function is known in the literature as an S-box (S stands for
3.3.3 CONSTR UCTING A PRACTI CAL BLOCK CIPHER 65  substitution ).
An S-box specifies, for each of the 2k possible values of the k-bit input, the value of  the output.
To make the round transformation practical to implement, the S-box only maps a small  number of bits, instead of mapping full blocks.
To use the small S-box, the round breaks the input  into pieces and uses an S-box on each piece.
An S-box in DES maps 6-bit inputs to 4-bit outputs.
 The S-box in AES maps 8-bit inputs into 8-bit outputs.
DES uses eight different S-boxes, i.e., each  S-box does a different mapping.
In AES, all the S-boxes are identical.
 Note that since the S-boxes only act on a subset of the bits, if we only did a single round, then  a bit of input can only affect a small number of bits of the output ( e.g., eight bits for AES), since  each input bit goes into only one of the S-boxes. (
To be precise, in AES each bit goes into only one  S-box.
In DES, some of the bits are input into two S-boxes.)
 In each round, after the S-boxes, the bits affected by a particular input bit get spread around  so that after several rounds, each single input bit will affect all the output bits.
DES calls this opera - tion a P-box (the P is for permutation ).
The word “permutation” is sometimes used to describe  functions of n input bits that rearrange the positions of those n input bits. “
Permutation” is also  sometimes used to describe any one-to-one mapping from the 2n possible input values of n bits to  the 2n possible output values.
The P-box in DES is a permutation in the first sense; to avoid confu - sion, we will call the DES P-box a bit shuffle .
A bit shuffle takes as input n bits and moves the bits  around, so that the third bit might become the eleventh bit.
The output will have the same number  of 0s and 1s as the input.
 3.3.3 Feistel Ciphers  Of course, it is important that an encryption algorithm can be reversed so that decryption is possi - ble.
One method (used by AES) of having a cipher that is reversible is to make all the components  reversible.
With DES, the S-boxes are clearly not reversible since they map 6-bit inputs to 4-bit out - puts.
So instead, DES is designed to be reversible using a clever technique known as a Feistel  cipher .
 A Feistel cipher [ FEIS73] builds reversible transformations out of one-way transformations  by only working on half the bits of the input value at a time.
Let’s assume 64-bit blocks.
Figure 3-1  shows both how encryption and decryption work.
In a Feistel cipher there is some irreversible com - ponent that scrambles the input.
We’ll call that component the mangler function .
 In encryption for round n, the 64-bit input to round n is divided into two 32-bit halves called  L and R .
Round n generates as output 32-bit quantities Ln+1 and Rn+1.
The concatenation of Ln n n+1  and Rn+1 is the 64-bit output of round n and, if there’s another round, the input to round n+1.
 Ln+1 is simply R .
To compute Rn+1, do the following.
R and K are input to the mangler n n n  function, which takes as input 32 bits of data plus some bits of key to produce a 32-bit output.
The  32-bit output of the mangler is ⊕’d with Ln to obtain Rn+1.
66 S ECRET KEY CRYPTOGRAPHY 3.3.3  64-bit input 64-bit output  32-bit Ln 32-bit Rn  Mangler  Function  ⊕  32-bit Ln+1 32-bit Rn+1 32-bit Ln 32-bit Rn  Mangler  Function  ⊕  32-bit L 32-bit Rn+1 n+1 Kn Kn  Encryption Decryption  Figure 3-1.
Feistel Cipher  Given the above, suppose you want to run a Feistel cipher ( e.g., DES) backward, i.e., to  decrypt something.
Suppose you know the round key K , and the two 32-bit outputs Ln+1 and Rn n+1.
 How do you get L and R ?
Well, R is just Ln+1.
Now you know R , Ln+1, Rn+1, and K .
You also n n n n n  know that Rn+1 equals L ⊕mangler( R ,K ).
You can compute mangler( R ,K ) since you know Rn n n n n n  and K .
Now ⊕ that with Rn+1.
The result will be L .
Note that the mangler is never run backwards.
n n  The Feistel cipher is elegantly designed to be reversible without constraining the mangler function  to be reversible.
Theoretically, the mangler could map all values to zero, and it would still be possi - ble to run the algorithm backwards, but having the mangler function map all functions to zero  would make the algorithm pretty insecure (see Homework Problem 8).
 If you examine Figure 3-1 carefully, you will see that decryption is identical to encryption  with the 32-bit halves swapped.
In other words, feeding Rn+1|Ln+1 into round n produces R |L as n n  output.
If the algorithm specifies that at the end of a series of Feistel rounds the two halves of the  output are swapped (and DES does this), there is a cute trick one can do in an implementation.
A  single implementation can perform either encryption or decryption depending only on the order in  which the per-round keys are supplied.
So whether you are encrypting or decrypting depends only  on the key schedule supplied, where decryption reverses the order of the per-round keys.
64-bit output 64-bit input
3.4 CHOOSING CONSTA NTS 67  3.4 CHOOSI NG CONSTAN TS  Cryptographers are (justifiably) a suspicious bunch.
It is theoretically possible to design a crypto - graphic function that is breakable if you know the secret of how it was designed.
Such a function  (that is breakable by the designer) is said to have a back door .
For example, the S-boxes in DES  seem chosen arbitrarily, so it is understandable for someone to be suspicious about why that partic - ular design was chosen.
 It is common for cryptographic algorithms to have components that need somewhat arbitrary  parameters.
To avoid suspicion, designers today choose constants in such a way that they are out - side the control of the designer, and yet still have the necessary properties for the security of the  design.
This technique for designing algorithms is sometimes known as the nothing up my sleeve  technique.
Example choices of constants might be some number of bits of π or 2.
 3.5 DATA ENCRYP TION STANDARD (DES)  DES was published in 1977 by NIST (the National Institute of Standards and Technology, at that  time called NBS—the National Bureau of Standards) for use in commercial and unclassified U.S.  Government applications.
It was designed by a team from IBM based on their own Lucifer cipher,  with consultation from NSA.
DES uses a 56-bit key, and maps a 64-bit input block into a 64-bit out - put block.
The key actually looks like a 64-bit quantity, but one bit in each of the eight octets is used  for odd parity on each octet.
Therefore, only seven of the bits in each octet are actually meaningful  as a key.
 DES is efficient to implement in hardware but relatively slow if implemented in software.
 Although making software implementations difficult was not a documented goal of DES, people  have asserted that DES was specifically designed with this in mind, perhaps because this would  limit its use to organizations that could afford hardware-based solutions, or perhaps because it made  it easier to control access to the technology.
At any rate, advances in CPUs have made the cost of  implementing DES in software acceptable.
 Advances in semiconductor technology, and the ability to use technology such as GPUs for  inexpensive massive parallelism, make the keysize issue more critical.
Perhaps a 64-bit key might  have extended the useful lifetime of DES by a few years, but even that would not be secure today.
 NIST today recommends that all crypto be at least as hard to break as a block cipher with a keysize  of 112 bits, and most designs aim for at least 128 bits.
68 S ECRET KEY CRYPTOGRAPHY 3.5.1  Why 56 bits?
 Use of a 56-bit key is one of the most controversial aspects of DES.
Even before DES was  adopted, people outside of the intelligence community complained that 56 bits provided inad - equate security [DENN82, DIFF76a , DIFF77, HELL79 ].
So why were only 56 of the 64 bits  of a DES key used in the algorithm?
The disadvantage of using eight bits of the key for parity  checking is that it makes DES considerably less secure (256 times less secure against exhaus - tive search).
 OK, so what is the advantage of using eight bits of the key for parity?
Well, let’s say  you receive a key electronically, and you want to sanity-check it to see if it could actually be  a key.
If you check the parity of the quantity, and it winds up not having the correct parity,  then you’ll know something went wrong.
 The problem with this reasoning is that there is a one in 256 chance (given the parity  scheme) that even if you were to get 64 bits of garbage, the result will happen to have the cor - rect parity and therefore look like a key.
That is way too large a possibility of error for it to  afford any useful protection to any application.
 The key, at 56 bits, was always pretty much universally acknowledged to be too small  to be secure.
People (not us, surely!)
have suggested that our government consciously  decided to weaken the security of DES just enough so that NSA would be able to break it.
We  would like to think there is an alternative explanation, but we have never heard a plausible  one proposed.
 3.5.1 DES Overview  DES is quite understandable, and has some very elegant tricks.
Let’s start with the basic structure of  DES ( Figure 3 -2).
 The 64-bit input is subjected to an initial bit shuffle (which has no security value) to obtain a  64-bit result.
The 56-bit key is expanded into sixteen 48-bit per-round keys by taking a different  48-bit subset of the 56 bits for each of the keys.
Each round takes as input the 64-bit output of the  previous round and the 48-bit per-round key, and produces a 64-bit output.
After the sixteenth  round, the 64-bit output has its halves swapped and is then subjected to another bit shuffle that hap - pens to be the inverse of the initial bit shuffle.
Swapping the two halves of the 64-bit output after the  final round adds no cryptographic strength to the algorithm, but it does have an interesting side ben - efit.
As noted in §3.3.3 Feistel Ciphers , swapping the two halves allows the encrypt and decrypt  operations to be identical except for the key schedule.
 That is the overview of how encryption works.
Decryption works by essentially running DES  backwards.
To decrypt a block, you’d first run it through the initial bit shuffle to undo the final bit
⋅ ⋅  ⋅ ⋅  ⋅ ⋅ 3.5.2 DATA ENCRYP TION STAND ARD (DES) 69  64-bit input 56-bit key  Initial Bit Shuffle  Round 1  Round 2  ⋅  ⋅  ⋅ Expand to 16  per-round keys  48-bit K1  48-bit K2  Round 16 48-bit K16  swap left and right halves  Final Bit Shuffle  64-bit output  Figure 3-2.
Basic Structure of DES  shuffle. (
The initial and final bit shuffles are inverses of each other.)
You’d do the same key expan - sion, though you’d use the keys in the opposite order (first using K16, the key you produced last).
 Then you run sixteen rounds just like for encryption. (
We explained why this works in §3.3.3 Feis- tel Ciphers .)
After sixteen rounds of decryption, the output has its halves swapped and is then sub - jected to the final bit shuffle (to undo the initial bit shuffle).
 3.5.2 The Mangler Function  The mangler function takes as input the 32-bit R (the right half of the 64-bit input into round n)n  that we’ll simply call R, and the 48-bit K that we’ll call K, and produces a 32-bit output that, when n  ⊕’d with L , produces Rn+1 (the next R).
n  The mangler function first expands R from a 32-bit value to a 48-bit value.
It does this by  breaking R into eight 4-bit chunks and then expanding each of those chunks to 6 bits by taking the
70 S ECRET KEY CRYPTOGRAPHY 3.5.3  adjacent bits and concatenating them to the chunk.
The leftmost and rightmost bits of R are consid - ered adjacent. (
See Figure 3-3.)
 Figure 3 -3.
 Expansion of R to 48 bits  The 48-bit round key K is broken into eight 6-bit chunks.
Chunk i of the expanded R is ⊕’d  with chunk i of K to yield a 6-bit output.
That 6-bit output is fed into an S-box , a substitution that  produces a 4-bit output for each possible 6-bit input.
Since there are 64 possible input values (6  bits) and only sixteen possible output values (4 bits), the S-box clearly maps several input values to  the same output value.
In DES, there are eight S-boxes, each of which does a different mapping of  6-bit inputs to 4-bit outputs.
 3.5.3 Undesirable Symmetries  The combination of the fact that DES key expansion only involves picking subsets of the initial key  bits, the fact that the only use of per-round keys is to ⊕ them with plaintext or intermediate values,  and the use of a Feistel structure, results in unfortunate symmetries that result in weaknesses in  DES.
Nobody really cares at this point, because the small keysize makes DES obsolete, but these  weaknesses do illustrate the kinds of weaknesses that can be avoided with careful design.
The DES  weaknesses covered in this section are:  • There are four keys that are weak , meaning that these keys are their own inverses, i.e.,  encrypting twice with the same key results in the plaintext.
Two obvious weak keys are all 0s  and all 1s, because round keys are just different subsets of the key bits.
So, for example, with  a key of zero, all round keys (in either decrypt or encrypt) will be zero, and since encrypt and  decrypt are the same operation, except for the order of the round keys, encrypt will be the  same operation as decrypt.
 • There are six pairs of keys that are semi-weak , meaning that the keys in the pair are inverses  of each other.
 • For all keys K, if plaintext P encrypted with key K becomes ciphertext C, then ~ P encrypted  with ~ K becomes ~ C. (Note: ~ x is the one’s complement of x, i.e., x with all its bits  inverted— 0s changed to 1s and vice versa.)
What is the implication of this?
Suppose there is
3.5.4 3DES (M ULTIP LE ENCRYP TION DES) 71  a system that encrypts using a key K, and it allows the attacker (say Eve) two chosen plain- texts.
Now assume Eve chooses P and ~ P to be encrypted, so Eve will know the encryption  of P is C1, and the encryption of ~ P is C2.
Now Eve wants to do a brute-force search for key  K. If DES (with a 56-bit key) did not have this weakness, it would take 256 encryptions (worst  case), or 255 encryptions (average case) in the brute-force search to find a key K that mapped  P to C1.
However, with this weakness, the brute-force search for Eve only takes half as many  encryptions.
The reason for this is that Eve need only try half the keys, say, the keys that have  the top bit 0.
For each such key K, Eve encrypts P with K. If the result is C1, then she knows  that K was the key.
If the result is ~ C2, then she knows that the key is ~ K. Because she only  needs to do encryptions with half the keys, it would take 255 encryptions (worst case), or 254  encryptions (average case).
 3.5.4 What’s So Special About DES?
 DES is actually quite simple.
One gets the impression that anyone could design a secret key encryp - tion algorithm.
Just take the input and key, mix them together somehow, do this over and over until  you think it’s enough, and you have an algorithm.
In fact, however, these things are very mysteri - ous.
For example, the DES S-boxes seem totally arbitrary.
However, they must have been carefully  designed for strength.
Biham and Shamir [BIHA91 ] have shown that with an incredibly trivial  change to DES consisting of swapping S-box 3 with S-box 7, DES is about an order of magnitude  less secure in the face of a specific (admittedly not very likely) attack. [
COPP94] is a nice paper  that describes how DES was secretly designed to avoid differential cryptanalysis.
There is, how - ever, another cryptanalysis technique, linear cryptanalysis, that first appears in public literature in  [MATS92].
The DES S-boxes do not appear to be designed to be especially strong against linear  cryptanalysis, suggesting that the DES designers didn’t know about it.
Linear cryptanalysis was  applied to attack DES (still not very practically) in 1993 and 1994 [MATS93, MATS94].
 3.6 3DES (M ULTIP LE ENCRY PTION DES)  Remember that a cryptographic scheme has two functions, known as encrypt and decrypt .
The two  functions are inverses of each other, but in fact each one takes an arbitrary block of data and garbles  it in a way that is reversed by the other function.
So it would be just as secure to perform the decrypt  operation on the plaintext as a method of encrypting it, and then perform encrypt on the result as a  method of getting back to the plaintext again.
It might be confusing to say something like encrypt  with decryption , so we’ll just refer to the two functions as E and D.
72 S ECRET KEY CRYPTOGRAPHY 3.6.1  The generally accepted method of making DES more secure through multiple encryptions is  known as EDE , 3DES , TDEA (Triple Data Encryption Algorithm ), or TDES (Triple Data  Encryption Standard ).
 Actually, any encryption scheme might be made more secure through multiple encryptions.
 EDE could as easily be done with, say, AES.
But AES has already standardized variants with differ - ent keysizes, and it is much more efficient to create versions of AES with larger keysizes by modi - fying the algorithm than by doing multiple encryptions with a small-keysize version of AES.
 DES uses EDE as follows:  1.
Three keys are used: K1, K2, and K3.
 2.
Each block of plaintext is subjected to E with K1, then D with K2, and then E with K3.
The  result is simply a new secret key scheme—a 64-bit block is mapped to another 64-bit block.
 K1 K2 K3  m E D E c  Decryption simply reverses the operation.
 K3 K2 K1  c D E D m  Now we’ll discuss why 3DES is defined this way.
There are various choices that could have  been made:  1.
Three encryptions were chosen.
It could have been two or 714.
Is three the right number?
 2.
Why are the functions EDE rather than, say, EEE or EDD?
 3.6.1 How Many Encryptions?
 Let’s assume that the more times the block is encrypted, the more secure it is.
So encrypting 714  times would be some amount more secure than encrypting three times.
The problem is that it is  expensive to do an encryption.
We don’t want to do any more encryptions than are necessary for the  scheme to be really secure.
There are problems with using only two encryptions (see §3.6.1.2  Encrypting Twice with Two Keys ), and three was chosen because it is the smallest integer bigger  than two.
3.6.1.1 73 3DES (M ULTIP LE ENCRYP TION DES)  3.6.1.1 Encrypting Twice with the Same Key  Suppose we didn’t want to bother with two keys.
Would it make things more secure if we encrypted  twice in a row with the same key?
 K Kplaintext ⎯ ⎯→ ⎯ ⎯→ ciphertext  This turns out not to be much more secure than single encryption with K, since exhaustive  search of the (56-bit) keyspace still requires searching only 256 keys.
Each step of testing a key is  twice as much work, since the attacker needs to do double encryption, but a factor of two for the  attacker is not considered much added security, especially since the good guys have their work dou - bled as well.
 (How about E followed by D using the same key?
That’s double the work for the good guy  and no work for the bad guy, so that’s generally not considered good cryptographic form.)
 3.6.1.2 Encrypting Twice with Two Keys  If encrypting twice using two different keys were as secure as a DES-like scheme with a keysize of  112 bits, then encrypting twice would have sufficed.
However, it isn’t, as we will show.
 We’ll use two DES keys, K1 and K2, and encrypt each block twice, first using key K1 and then  using key K2.
 K K1 2plaintext ⎯⎯→ ⎯⎯→ ciphertext  Is this as cryptographically strong as using a double-length secret key (112 bits)?
The reason  you might think it would be as strong as a 112-bit key is that the straightforward brute-force attack  would have to guess both K1 and K2 in order to determine if a particular plaintext block encrypted  to a particular ciphertext block.
 However, there is a less straightforward attack that breaks double-encryption DES in roughly  twice the time of a brute-force breaking of single-encryption DES.
The attack is not particularly  practical as it requires a somewhat unreasonably large amount of memory.
However, there are vari - ants of this attack that require only a little bit more computation and much less memory.
For this  reason, double encryption is not generally done.
 One attack (called a Meet-in-the-Middle attack ) involves the following steps:  1.
Assume you have a few 〈plaintext,ciphertext 〉 pairs 〈m1,c1〉, 〈m2,c2〉, 〈m3,c3〉 where ci was  derived from doubly encrypting mi with K1 and K2.
You want to find K1 and K2.
 2.
First make Table A with 256 entries, where each entry consists of a DES key K and the result  rK of applying that key to encrypt m1.
Sort the table in numerical order by rK.  3.
Now make Table B with 256 entries, where each entry consists of a DES key K and the result  sK of applying that key to decrypt c1.
Sort the table in numerical order by sK.
3.6.1.3 74 S ECRET KEY CRYPTOGRAPHY  4.
Search through the sorted lists to find matching entries, 〈KA,t〉 from table A and 〈KB,t〉 from  Table B. Each match provides KA as a candidate K1 and KB as a candidate K2 because KA  encrypts m1 to t and KB encrypts t to c1.
So KA|KB is a candidate double-length key that maps  m1 to c1.
 5.
If there are multiple matching entries (pairs of keys 〈KA,KB〉) that map m1 to c1 (which there  almost certainly will be), test whether each candidate pair of keys maps m2 to c2.
If you’ve  tested all the candidate 〈KA,KB〉 pairs and multiple of them map m1 to c1 and also map m2 to  c2, then try a third 〈plaintext,ciphertext 〉 pair 〈m3,c3〉.
The correct key pair 〈K1,K2〉 will  always work, of course, and an incorrect pair of keys will almost certainly fail to work on any  other 〈mi ,ci 〉 pair.
 How many matches should you expect to find after searching the two tables?
Well, there are 264  possible blocks and only 256 table entries in each table (because there are only 256 keys).
Therefore,  each 64-bit block has only a one in 256 chance of appearing in each of the tables.
Of the 256 blocks  that appear in Table A, only 1 /256 of them also appear in Table B. That means that there should be  about 248 entries that appear in both tables.
One of those corresponds to the correct 〈K1,K2〉 pair  and the others are imposters.
We’ll test them against 〈m2,c2〉.
If 〈KA,KB〉 is an imposter, the proba - bility that D( c2,KB) will equal E( m2,KA) is about 1 /264 .
There are about 248 imposters, so the prob - ability of one of them satisfying D( c2,KB) = E(m2,KA) is about 248/264, or roughly one in 216.
Each  test against an additional 〈mi ,ci 〉 reduces the probability by a factor of 264 , so the probability that  there will still be false matches after trying three 〈m, c〉 pairs is about 1 /280 .
 3.6.1.3 Triple Encryption with Only Two Keys  3DES does triple encryption.
A once-popular variation of 3DES was to only use two keys and set  K3=K1.
Here are two reasons for using only two keys with 3DES.
 1.
The attack on the 2DES variant described in §3.6.1.2 Encrypting Twice with Two Keys could  be applied to 3DES with three keys and reduce the work of breaking 3DES to 2112 encryp - tions, and it is generally considered bad form to have a cryptographic system where there are  more efficient attacks than brute force.
 2.
A property of an ideal cipher is that it is difficult to find a key that will map a given plaintext  block to a given ciphertext block.
With either double encryption or with 3DES with three  keys, it is relatively easy to find a key that maps a given plaintext to a given ciphertext.
Fol - lowing the pattern of attack from section §3.6.1.2 Encrypting Twice with Two Keys , if you  encrypted the plaintext with 232 randomly chosen 〈K1,K2〉 pairs and decrypted the ciphertext  with 232 randomly chosen values of K3, there would likely be at least one common value.
 That would give the attacker the needed three keys to encrypt the plaintext to the ciphertext.
 Most uses of a block encryption algorithm could not be exploited just because an attacker  could find some key that mapped from a given plaintext to a given ciphertext.
In any system
3.6.2 ADVANCED ENCRYPTION STANDAR D (AES) 75  where the keys are bigger than the blocksize, there will be lots of keys with that property, but  usually only the one correct key is useful.
An example where such a threat would be impor - tant is the use of encryption in the Unix password hash (see §5.5 Creating a Hash Using a  Block Cipher ).
In such a scheme, a password is used as a cryptographic key to encrypt a con - stant, and the result is stored.
Using EDE with three keys, it is straightforward to find a triple  of keys that maps a given plaintext to a given ciphertext, and the result would be accepted by  the system as a valid password.
It wouldn’t have to be the actual user’s password; any value  that maps the plaintext (the constant) to the stored value would be accepted by the system.
 There is no known practical way of finding such a triple with K1 = K3.
 Subsequent to these arguments, it was discovered that two-key 3DES did not provide 112 bits of  security [ MERK81 ].
So 3DES with two keys was generally abandoned in favor of 3DES with three  keys despite all the theoretical problems with doing so.
Due to the risk of using any block cipher  with a 64-bit blocksize, AES has replaced DES and 3DES in almost all uses except, surprisingly,  banking.
 3.6.2 Why EDE Rather Than EEE?
 Why is K2 used in decrypt mode?
Admittedly, it is no more trouble to run DES in either mode, and  either way gives a mapping.
DES would be just as good always done backwards ( i.e., swap encrypt  and decrypt ).
One reason for the choice of EDE was to allow hardware to be built that could be used  for both EDE encryption and ordinary DES encryption by setting K1 = K2 =K3.
The hardware does  three times the work, but it gets the job done (see Homework Problem 7).
 3.7 ADVANCED ENCRYP TION STANDARD (AES)  3.7.1 Origins of AES  In the 1990s, the world needed a new secret key standard.
DES’s key was too small.
Triple DES  (3DES) was too slow, and the 64-bit blocksize in DES and 3DES was also too small.
 The National Institute of Standards and Technology (NIST) decided it wanted to facilitate  creation of a new standard, but it had at least as difficult a public relations problem as a technical  problem.
After years of some branches of the U.S. government trying everything they could to  hinder deployment of secure cryptography, there was likely to be strong skepticism if a branch of  the government stepped forward and said We’re from the government, and we’re here to help you  develop and deploy strong crypto .
76 S ECRET KEY CRYPTOGRAPHY 3.7.2  NIST really did want to help create an excellent new security standard.
The new standard  should be efficient, flexible, secure, and unencumbered (free to implement).
But how could it help  create one?
Staying out of the picture wouldn’t help, since no similar organizations were volunteer - ing.
Proposing an NSA-designed cipher, designed in secret, wouldn’t work since everyone would  speculate that there were back doors.
So on January 2, 1997, NIST announced a contest to select a  new encryption standard.
Proposals would be accepted from anyone, anywhere in the world.
The  candidate ciphers had to meet a bunch of requirements, including having a documented design  rationale (and not just Here’s a bunch of transforms we do on the data ).
Then there were several  years in which conferences were held for presentation of papers analyzing the candidates.
In addi - tion to NIST, there was a group of highly motivated cryptographers (including the authors of com - peting entries) looking for flaws in the competing proposals, and comparing the candidate  algorithms for characteristics such as performance.
 After lots of analysis, NIST chose Rijndael , named after the two Belgian cryptographers who  developed and submitted it—Joan Daemen and Vincent Rijmen [DAEM99].
On 26 November  2001, AES, a standardization of Rijndael, became a Federal Information Processing Standard  [NIST01].
 Rijndael has twenty-five variants because it provides for five different blocksizes and five dif - ferent keysizes.
These two parameters can be chosen independently from 128, 160, 192, 224, and  256 bits.
AES has only three variants because it mandates a blocksize of 128 bits with a keysize of  128, 192, or 256 bits, but AES is otherwise identical to the submitted Rijndael proposal.
We will  only describe the AES variants, and not refer to Rijndael further.
 3.7.2 Broad Overview  A common structure of an encryption algorithm is to process the input in rounds, where each round  has two sorts of transformations—linear and nonlinear.
A linear transformation is one where each  of the outputs can be independently computed as a weighted sum of the inputs. (
Or more formally,  a transformation F is linear if it satisfies F(a+b) = F(a)+F(b), with a and b members of some addi - tive group.
In most of the cases we will discuss, + is the ⊕ operation.)
If the linear transformation is  based on individual bits, then each output bit is the ⊕ of some subset of the input bits.
If the linear  transformation is computed on octets (as in AES), then each output octet is a weighted sum of the  input octets (with the computation done in a finite field so the output is the appropriate size).
In  DES, the linear transformation is just a bit shuffle, which is very easy to compute in hardware (just  move wires around, and no gates are required).
In AES, the linear function is more complex.
In  DES, the bit shuffle results in the same number of 1s in the output as in the input.
In AES, because  the linear function is not a simple bit shuffle, the input and output might have different numbers of  1s.
3.7.2 ADVANCED ENCRYPTION STANDAR D (AES) 77  Typically, the nonlinear transformation (called an S-box ) is done on a small subset of the bits.
 The input is partitioned into small groups, and each group is transformed with an S-box that trans - lates each input value into an output value, ideally in as nonlinear a manner as possible.
Why is the  S-box implemented on a small set of bits?
In DES, the eight S-boxes are defined as tables of  input→output, each entry mapping a 6-bit input to a 4-bit output (so the table for each S-box in  DES has 64 entries, each with a 4-bit output value).
In AES, there is just one S-box, with 8-bit input  and output.
The AES S-box was originally envisioned to be implemented as a 256-octet lookup  table, although rather than a randomly chosen mapping, there is a formula for computing the S-box  output.
The formula was provided to show there was no nefarious choice of mapping.
Most likely,  AES would be secure with most mappings, although the designers of the AES S-box made sure that  their defined mapping met certain criteria (such as not mapping an input to itself or its complement,  or having two inputs that map to each other), as well as having good quantitative properties for  resistance to differential and linear cryptanalysis.
Note that if a table-lookup S-box had 128 bits of  input, the table would have to have 2128 entries.
Even with a computed S-box, although there would  be no huge lookup table, if the input were a large number of bits, it would take a lot of analysis to  ensure that the chosen mapping didn’t have any weak transformations (such as having an input A  mapping to B and input B mapping to A).
 Why not just make all the transformations linear?
If you do a sequence of linear transforma - tions, you will wind up with another linear transformation.
If an encryption algorithm were a linear  transformation, it would not be secure, because given a small number of 〈plaintext ,ciphertext 〉 pairs,  you could solve linear equations to calculate the key.
 Why not make all the transformations nonlinear?
This actually could be secure, but it is not  usually done.
With small S-boxes, an input bit can only affect the other bits with which it is  grouped, so it is necessary to have another step, after the S-boxes, to move the bits around.
While  it’s possible that this other step (called the mixing layer) could also be nonlinear, designers usually  prefer linear operations to provide mixing between the S-boxes, because it’s easier to prove certain  properties for a linear function than a nonlinear function.
For a deeper analysis see [DAEM01 ].
 There are various terms in the literature for the nonlinear transformation and the linear trans - formation.
Sometimes the nonlinear layer is referred to as the confusion layer , and the linear layer  is referred to as the diffusion layer .
Confusion and diffusion are terms coined by Shannon  [SHAN45 ] that refer to design goals of ciphers—confusion has to do with making sure the mathe - matical relationship between any two bits is complicated, while diffusion has to do with making  sure each bit affects all the other bits.
The implication of using this terminology is that the goal of  the nonlinear layer is to provide confusion, while the goal of the linear layer is to provide diffusion.
 While these concepts don’t necessarily correspond to the real design goals for the layers of a round  in all cipher designs, or even those that can be neatly divided into linear and nonlinear layers, they  are close enough to the design goals in most ciphers that the terminology is popular.
 Another common terminology is a substitution-permutation network (SPN ).
We think this  terminology is incredibly confusing, because in AES (which is considered to be an SPN) both the
78 S ECRET KEY CRYPTOGRAPHY 3.7.3  nonlinear (substitution) and the linear (permutation) transformations are one-to-one, and therefore  according to math terminology, both are permutations of the input values.
So we will not use the  term SPN, and instead use the terms nonlinear layer and linear layer .
 An encryption algorithm needs to be reversible (so that decryption is possible).
DES manages  to be reversible because it uses the Feistel construction, which allows the inverse of a DES round to  be computed even though the mangler function is not invertible.
That gave the DES designers a lot  of flexibility in their choice of S-boxes but at the cost of only being able to scramble half the bits of  state in each round.
AES takes a different approach, in that each operation is reversible, which  implies that all the operations are constrained to be one-to-one.
 In AES, there actually are more input bits than output bits, because the input consists of the  bits in a block plus the bits in a key, and the output consists of just a block.
So, how does the key  affect the computation?
The key is expanded into round keys, and the round key is ⊕’d into the state  between rounds.
 3.7.3 AES Overview  AES is a 128-bit block cipher with a choice of keysizes—128, 192, or 256 bits—with the resulting  variants imaginatively called AES-128, AES-192, and AES-256.
 AES is similar to DES in that there is a key expansion algorithm that takes the key and  expands it into a bunch of round keys, and the algorithm executes a series of rounds that progres - sively mangle a plaintext block into a ciphertext block. (
See Figure 3-4.)
With DES, each round  takes a 64-bit input and a 48-bit round key and produces a 64-bit output that (along with the next  round key) is the input to the next round.
With AES, each round takes a 128-bit input and a 128-bit  round key and produces a 128-bit output that is fed into the next round.
Unlike DES, AES is not a  Feistel cipher.
Instead, each of its steps is itself reversible.
That means that AES can scramble all  the bits on each round, and therefore only needs about half as many rounds to achieve the same  level of security as it would need if it were a Feistel cipher.
In accordance with the principle that  you should only have as many rounds as are needed to make exhaustive search of the key space  cheaper than any other form of cryptanalysis, AES has more rounds when the keys are bigger.
 AES-128 has ten rounds, AES-192 has twelve rounds, and AES-256 has fourteen rounds.
If AES  had a 64-bit version, it would have eight rounds, which would be comparable to the sixteen rounds  in DES. (
Again, because AES is not a Feistel cipher, each AES round is twice as effective as a DES  round.)
 Another difference between DES and AES is that DES operates on bits, whereas AES oper - ates on octets.
This makes AES software implementations more efficient.
 Another difference is that the S-boxes and P-boxes in DES seem somewhat arbitrary and need  to be specified with tables, whereas with AES the design gives a relatively simple mathematical for - mula for all the transformations, and the rationale is openly stated.
3.7.3 ADVANCED ENCRYPTION STANDAR D (AES) 79  16 octet input  a0 a4 a8 a12  a1 a5 a9 a13  a2 a6 a10 a14  a3 a7 a11 a15  ⊕  Round 1  ⊕ 128-bit (16 octet) key K0  128-bit (16 octet) key K1 key (16, 24, or 32 octets)  Key Expansion  ⋅ ⋅ ⋅  ⋅ ⋅ ⋅  ⋅ ⋅ ⋅  Round Nr  128-bit (16 octet) key K  ⊕ Nr  16 octet output  Figure 3 -4.
Basic Structure of AES  In AES, the key (128, 192, or 256 bits) is expanded into a series of 128-bit round keys, where  there is one more round key than there are rounds.
AES encryption or decryption consists of ⊕ing a  round key into the intermediate value at the beginning of the process, at the end of the process, and  between each pair of rounds.
80 S ECRET KEY CRYPTOGRAPHY 3.7.4  Each round operates on a 128-bit block, which can be thought of as a 4 ×4 array of octets.
We  will refer to the block at the beginning of the round as the input state and at the end of the round as  the output state .
 The nonlinear operation in a round (AES’s S-box) is known as SubBytes .
It performs a  one-to-one mapping of input octet value to output octet value for each of the sixteen octets in the  state.
Although that would allow for sixteen different S-boxes (one for each octet of input), all the  AES S-boxes are identical.
 There are two operations in the linear layer of AES:  • ShiftRows .
Each of the state’s four rows in the 4 ×4 array of octets is rotated left by a  row-specific number of octets, with the octets falling off the left rotated into the right posi - tion.
The top (first) row is rotated zero octets, the second row by one octet, the third row by  two octets, and the fourth row by three octets.
 • MixColumns .
This acts on each column of four octets in the 4 ×4 array of octets indepen - dently.
Each column is multiplied by the same, fixed, 4 ×4 matrix (using GF(28) arithmetic).
 Because that matrix has an inverse, this step (as are all the steps) is reversible.
When com - bined with ShiftRows, every octet in the 4 ×4 array affects every other octet within two  rounds.
 3.7.4 Key Expansion  Key expansion with DES is very simple.
In DES, there is a need for sixteen round keys, each 48  bits long, and they are all generated by taking different subsets of the 56-bit key in different orders.
 Each key bit ends up being used in thirteen or fourteen of the sixteen round keys.
 AES key expansion works differently.
It has 128-bit round keys, while the full key can be  128, 192, or 256 bits.
AES starts with the full key and generates additional keysize blocks ( key- blocks ) of key material.
Each new keyblock is a complicated (but one-to-one) function of the previ - ous keyblock, combining a number of simpler functions: applying the S-box to each octet in a  4-octet group, rotating the octets in a 4-octet group, ⊕ing a per-keyblock constant into one octet,  and ⊕ing 4-octet groups together.
Once sufficiently expanded, the round keys get taken from the  keying material 128 bits at a time.
You get one round key from each 128-bit keyblock, two round  keys from each 256-bit keyblock, or three round keys from each pair of 192-bit keyblocks.
 3.7.5 Inverse Rounds  Since each operation is invertible, decryption can be done by performing the inverse of each opera - tion in the opposite order from that for encryption and using the round keys in reverse order.
3.7.6 RC4 81  Just as DES does an additional swap step at the end to enable a single implementation to do  either encryption or decryption just by supplying a different expanded key, AES has some special  features designed to make a combined implementation of encryption and decryption easier.
In par - ticular, in order to make the cipher and its inverse more similar in structure, the MixColumns step is  omitted from the last round.
 3.7.6 Software Implementations of AES  Unlike DES, AES was specifically designed to be efficient to implement in software.
Since GF(28)  arithmetic ( Chapter 2.7.1 Finite Fields ) is not generally fast in software, and octet-oriented opera - tions tend to be time consuming, AES was cleverly designed so that by building some large tables  dependent on the key, each round can be implemented with sixteen 4-octet memory fetches, some  rotates, and some ⊕s.
 This is almost never done, however.
All the major CPU vendors have implemented AES  instructions that combine several steps and run substantially faster than any software implementa - tion could.
Furthermore, on any CPU where multiple processes are running, researchers discovered  that any implementation that does table lookups where the addresses are dependent on secret data is  highly vulnerable to side channel attacks [KOCH96, BERN05 ].
 3.8 RC4  A long random (or pseudorandom) string used to encrypt a message with a simple ⊕ operation is  known as a one-time pad .
A stream cipher generates a one-time pad and applies it to a stream of  plaintext with ⊕.  RC4 is a stream cipher designed by Ron Rivest that for a time was the most widely used  encryption algorithm in the world.
RC4 was initially a trade secret but was “outed” in 1994 by an  anonymous posting on the Cypherpunks mailing list.
It was later incorporated into a number of  standard protocols, including SSL and WEP.
It is noteworthy because of its extreme simplicity and  high performance.
 A number of subtle flaws have been discovered.
One issue is the problem of related keys .
 You must use a different key for each message or stream you want to encrypt.
A design where you  do this by concatenating a secret and a nonce will not be secure because it will produce two streams  with similarities that enable cryptanalysis.
Instead, you should compute the key as a hash of the  secret and the nonce.
82 S ECRET KEY CRYPTOGRAPHY 3.8  There have also been flaws discovered where the initial octets of RC4 output are not equally  likely to have any of the 256 possible values.
For example, the second octet has a probability of  1/128 rather than the expected 1/256 of being zero.
This can be a problem if the same message is  encrypted many times with different keys.
This flaw was practical to exploit in WEP because the  first few octets of plaintext sent by each node is a constant.
That problem (that the initial output of  RC4 is not uniformly distributed) can be avoided by skipping the first 4K (or so) octets of the  stream before using the pad.
Another problem is that RC4 makes key-dependent memory accesses,  making it potentially vulnerable to side channel attacks.
Because of these security issues, and the  fact that most modern CPUs implement AES in specialized hardware that make it even faster than  RC4, RC4 has fallen into disuse.
It has been deprecated in most standards.
 The RC4 algorithm is an extremely simple (and fast) generator of pseudorandom streams of  octets.
So simple that we can show you the code!
The key can be from one to 256 octets long.
Even  with a minimal key (a single null octet), the generated pseudorandom stream passes all the usual  randomness tests (and so makes a good enough pseudorandom number generator for a variety of  non-cryptographic purposes—I3’ve used it for that purpose on numerous occasions).
RC4 keeps  258 octets of state information: 256 octets are a pseudorandom permutation of 0,1,…,255 (called s  in the code below) and two indices into that array called iand j. The index iincrements mod 256.
 The index jis pseudorandom and on each iteration is incremented by the octet indexed by i. Each  iteration of RC4 swaps the octets pointed to by i and j, which maintains s as a permutation of  0,1,…,255 that changes after each iteration.
To initialize RC4 from a key, rc4init iterates through  256 steps swapping octets and incrementing jby both a pseudorandom octet from the permutation  and an octet from the key:  static uint8_t s[256], i, j;  void rc4init(uint8_t *key, int keylen) {  i = j = 0;  do s[i] = i; while (++i);  do { j = (j + s[i] + key[i%keylen]);  uint8_t t = s[i]; s[i] = s[j]; s[j] = t;  } while (++i);  j = 0;  }  Once the state is initialized, for each octet output from RC4, i and j are both updated, the two  octets they point to are swapped, and then the sum of those two octets is used as an index into the  permutation to decide which octet to return.
 uint8_t rc4step() {  j += s[++i];  uint8_t t = s[i]; s[i] = s[j]; s[j] = t;  return (s[t=s[i]+s[j]]);  }
3.9 HOMEWORK 83  3.9 HOMEWO RK  1.
Write a program that implements an oracle that encrypts and decrypts, using 8-bit blocks and  8-bit keys (as described in §3.2.3 Looking Random ).
 2.
If both the keysize and the blocksize were 128 bits, for how many keys, on average, would the  encryption of a block with given value x be a block with given value y?
 3.
If the keysize is 256 bits and the blocksize is 128 bits, for how many keys, on average, would  the encryption of a block with given value x be a block with given value y?
 4.
Keeping in mind that a useful encryption algorithm requires the ability to decrypt, which of  the following are possible?
 a) An encryption algorithm for which there are two tuples, 〈 key1,block1〉 and 〈key2,block2〉,  that map to the same ciphertext block.
 b) An encryption algorithm for which there are two tuples, 〈key1,block 1〉 and 〈key1,block 2〉,  that map to the same ciphertext block.
 c) An encryption algorithm that maps plaintext blocks of size k bits to ciphertext blocks of  size j bits.
Is this possible if k < j?
How about if k > j?
 d) An encryption algorithm that takes as input a triple 〈key, plaintext block of size k, random  number R〉, and outputs 〈R, ciphertext block of size k〉 (where the ciphertext depends on all  three inputs).
 5.
Is it possible for an encryption algorithm, when encrypting with key K, to map plaintext P to  itself?
 6.
Show that if each round transformation in an encryption algorithm is linear, that the encryp - tion algorithm itself will be linear.
 7.
Suppose one had a piece of hardware that did 3DES implemented using EDE.
How could you  use that hardware to implement DES?
 8.
Suppose in a Feistel cipher, the mangler function mapped every 32-bit value to zero, regard - less of the value of its input.
What function would encryption compute if there was only one  round?
What function would encryption compute if there were eight rounds?
 9.
Show that DES encryption and decryption are identical except for the order of the 48-bit  keys.
Hint: Running a round backwards is the same as running it forwards but with the halves  swapped, and DES has a swap after round 16 when run forwards (see §3.5.1 DES Overview ).
4 MODES OF OPERATION  4.1 INTRODUCTION  We’ve covered how to encrypt a 128-bit block with AES, or a 64-bit block with DES, and these  primitives have the nice property that if an attacker changes any part of the ciphertext, the result of  a decryption will be effectively a random number.
Sadly, most useful messages are longer than 128  bits.
Modes of operatio n are techniques for encrypting arbitrary-sized messages using the block  encryption algorithms as primitives to be applied iteratively.
There are additional desirable prope r- ties that such algorithms can have.
If we send the same message multiple times, it would be desi r- able to have the encrypted message be different each time so that an eavesdropper can’t tell that  we’re sending the same message repeatedly.
If an attacker modifies an encrypted message, we  would like to be able to detect that it is no longer a valid encrypted message by computing some  sort of message authentication code (MAC).
If an attacker can cause the system to encrypt a me s- sage of his choosing (a chosen-plaintext attac k), this should not give the attacker the ability to  verify guesses of any other data that was sent.
This chapter will tie up these various loose ends .
 We will discuss three kinds of modes :  • those that encrypt a message of any size, which we describe in §4.2  • those the compute a MAC of message of any size, which we describe in §4.3  • those that do both, which we describe in §4.4  There are a surprising number of modes of operation defined of each type.
Some of them are used  for historical reasons (when better algorithms are defined, it takes a very long time for the world to  convert to them), while others exist because they solve the challenges of some particular scenario  better than the more commonly used ones.
We will present the most commonly used ones along  with the scenarios they were designed for .
 The purpose of this chapter is to give intuition into the algorithms, with the rationale for the  design and potential pitfalls.
For exact formats, refer to the standards .
 Many of these modes use the bitwise-exclusive-or (XOR) operation that we will denote with  the symbol ⊕. We debated whether, when referring to the bitwise-exclusive-or operation in text, we  85
86 MODES OF OPERATION 4.2  should use XOR or ⊕ and decided to use ⊕ because “XOR” looked too similar visually to some of  the mode names such as XEX and XTS.
We will continue using the symbol ⊕ throughout the book .
 4.2 ENCRYPTING A LARGE MESSAGE  Let’s assume we’re using a block cipher with 128-bit blocks.
How do you encrypt a message larger  than 128 bits?
There are several schemes defined in the NIST standard documents (the NIST SP  800-38 series).
These schemes are applicable to any secret key block cipher that encrypts  fixed-length blocks .
 4.2.1 ECB (Electronic Code Book )  This mode consists of doing the obvious thing, and it is usually the worst method.
You break the  message into 128-bit blocks (padding the last one out to a full 128 bits), and encrypt each block  with the secret key (see Figure 4-1).
The other side receives the encrypted blocks and decrypts each  block in turn to recover the original messag e (see Figure 4-2).
 messag e  break into  block s  m1 m2 m3 m4 m5 m6 m7 m8 m9  encrypt with E E E E E E E E E secret ke y  c1 c2 c3 c4 c5 c6 c7 c8 c9  Figure 4-1.
Electronic Code Book Encryptio n  There are a number of problems with this approach.
First, if a message contains two identical  blocks, the corresponding two blocks of ciphertext will be identical.
This will give an eavesdropper  some information.
Whether it is useful or not depends on the context.
We’ll give an example where  ECB would have a problem.
Suppose that the eavesdropper knows that the plaintext is an alphabe t- ically sorted list of employees and salaries, tabularly arranged (see Figure 4-3).
 To create an example that fits nicely on a page, assume a cipher like DES or 3DES with  8-octet blocks.
Further suppose that, as luck would have it, each line is exactly 64 octets long, and  the blocks happen to be divided in the salary field between the thousands’ and the ten-thousands’
4.2.1 ENCRYPTING A LARGE MESSAGE 87  c c c c c c c c c1 2 3 4 5 6 7 8 9  decrypt with D D D D D D D D D secret ke y  m1 m2 m3 m4 m5 m6 m7 m8 m9  reassemble  block s  messag e  Figure 4-2.
Electronic Code Book Decryptio n  digits.
Since identical plaintext blocks produce identical ciphertext blocks, if you could see the  ciphertext, you could tell which employees have identical salaries .
You could also tell which  employees have salaries in the same $10,000 ranges.
Furthermore, if you were one of the emplo y- ees, you could change your salary in the database to match that of any other employee by copying  the ciphertext blocks from that employee to the corresponding blocks of your own entry .
 Nam e Positio n Salar y  Clinton, H.R. President 400,000.00  Kent, Clark Mild-mannered Reporter 18,317.42  Lannister, Tyrion Accounts Payable 68,912.57  Madoff, Bernard Accounting Clerk 623,321.16  Nixon, Richard Audio Engineer 21,489.15  Quixote, Don Green Energy Solutions 14,445.22  Reindeer, R. Lighting Engineer 410.11  Wayne, Bruce Chiropterologist 82,415.22  Weiner, Anthony Social Media Strategist 38,206.51  White, Walter  Cook 94,465.58  | | | | | | | | |  Block boundarie s  Figure 4-3.
Payroll Dat a  So, ECB has two serious flaws.
Patterns in the ciphertext, such as repeated blocks, leak info r- mation, and nothing prevents someone from rearranging, deleting, modifying, or duplicating  blocks .
88 MODES OF OPERATION 4.2.2  4.2.2 CBC (Cipher Block Chaining)  CBC mode avoids some of the problems in ECB.
Using CBC, even if the same block repeats in the  plaintext, it will not cause repeats in the ciphertext.
CBC mode is still commonly used even though  modes discussed later in this chapter are technically superior .
 4.2.2.
1 Randomized EC B  First we’ll give an example of how to avoid some of the issues with ECB with an example mode  (that we’ll call randomized EC B) that is inefficient but helps for understanding CBC.
Note that we  are only introducing this mode (randomized ECB) as a way of explaining CBC mode in the next  section.
Randomized ECB is not a mode that is used or standardized .
 Assuming a 128-bit blocksize, generate a 128-bit random number ri for each plaintext block  mi to be encrypted.
⊕ the plaintext block with the random number, encrypt the result, and transmit  both the unencrypted random number ri and the ciphertext block ci (see Figure 4-4).
To decrypt  this, you’d decrypt each ciphertext block ci, and then ⊕ the result with the random number ri.
 generate  random  number s m1 m2 m3 m4 m5 m6  r1 ⊕ r2 ⊕ r3 ⊕ r4 ⊕ r5 ⊕ r6 ⊕  E E E E E E  r1 c1 r2 c2 r3 c3 r4 c4 r5 c5 r6 c6 encrypt with  secret ke y  Figure 4-4.
Randomized Electronic Code Book Encryptio n  The main problem with this scheme is efficiency.
It causes twice as much information to be  transmitted, since a random number has to be transmitted along with each block of ciphertext.
 Another problem with it is that an attacker can still rearrange the blocks and have a predictable  effect on the resulting plaintext.
For instance, if r2|c2 were removed entirely, it would result in m2  being absent in the decrypted plaintext.
If r |c2 were swapped with r |c7, then m and m would be 2 7 2 7  swapped in the result.
Worse yet, an attacker knowing the value of any block m can change it in an  predictable way by making the corresponding change in r . (
See Homework Problem 4.)
n  4.2.2.
2 CBC  Now we can explain CBC.
CBC generates its own “random numbers” for all but the first block.
It  uses ci as ri+1.
In other words, it takes the previous block of ciphertext and uses that as the “random  number” that will be ⊕’d into the next plaintext block.
To avoid leaking the information that two  messages encrypted with the same key have the same first plaintext blocks, CBC selects one
4.2.2.
2 ENCRYPTING A LARGE MESSAGE 89  random number that gets ⊕’d into the first block of plaintext and transmits it along with the data.
 This initial random number is known as an IV (initialization vecto r). (
See Figure 4-5.)
 m1 m2 m3 m4 m5  IV ⊕ ⊕ ⊕ ⊕ ⊕  encrypt with secret E E E E E key  IV c1 c2 c3 c4 c5  Figure 4-5.
Cipher Block Chaining Encryptio n  Decryption is simple because ⊕ is its own inverse (see Figure 4-6).
 IV c1 c2 c3 c4 c5  decrypt with secret  key D D D D D  ⊕ ⊕ ⊕ ⊕ ⊕  m1 m2 m3 m4 m5  Figure 4-6.
Cipher Block Chaining Decryptio n  CBC encryption requires essentially the same amount of computation as ECB encryption, since the  cost of the ⊕ is trivial compared to the cost of an encryption.
However, CBC has an important pe r- formance disadvantage: Since encrypting each block of plaintext requires knowing the ciphertext  value for the previous block, CBC encryption cannot take advantage of the parallel processing  capabilities of most modern CPUs to encrypt multiple blocks at once.
The cost of generating and  transmitting the IV can also be considered a performance disadvantage .
 Some older implementations of CBC mode omitted the IV (or equivalently, used the value 0  as the IV).
There are cases where this would not adversely affect security: e.g., where each message  is encrypted with a different key or where the first block of each of the messages encrypted with the  same key contains a sequence number.
However, there are enough cases where omitting the IV  would cause problems that the NIST specification disallows it.
For example, suppose the encrypted  file of employees and salaries is transmitted weekly.
If there were no IV, then an eavesdropper  could tell where the ciphertext first differed from the previous week and perhaps determine the first  person whose salary had changed .
90 MODES OF OPERATION 4.2.2.
3  Another example is where a general sends information each day saying continue holding  your positio n. Without the IV, the ciphertext will be the same every day until the general decides to  send something else, like start bombin g. Then the ciphertext would suddenly change, alerting the  enemy .
 A randomly chosen IV guarantees that even if the same message is sent repeatedly, the  ciphertext will be completely different each time.
There remain some other threats, however, that  can and have caused security problems.
We’ll explain those in the next few sections .
 4.2.2.
3 CBC Threat—Modifying Ciphertext Block s  Using CBC (rather than ECB) does not eliminate the problem of someone creating desired plaintext  by modifying the ciphertext.
It just changes the nature of the threat.
Attackers can no longer see  repeated values and simply copy or move ciphertext blocks in order to, say, swap their salary with  the salary of the VP of marketing.
But there are still possible attacks.
What would happen if they  changed a block of the ciphertext, say, the value of ciphertext block c ?
Changing c has a predic t- n n  able effect on mn+1 because c gets ⊕’d with the decrypted cn+1 to yield mn+1.
For instance, chan g- n  ing bit 3 of c changes bit 3 of mn+1.
Modifying c also garbles block m to some unpredictable n n n  value .
 For example, let’s say our attacker Ann knows that the plaintext corresponding to a certain  octet range in the ciphertext is her personnel record.
And to make the diagram simpler, let’s assume  64-bit blocks and that the salary is expressed as ASCII characters :  Tacker, Ann A. System Security Officer 44,122.10  | m1 | m2 |m3 |m4 | m5 |m6 |m7 |m8 |  Let’s say Ann wants to increase her salary by 10K. In this case she knows that the final octet  of m7 is the ten-thousands digit of her salary.
The bottom three bits of the ASCII for 4 is 100.
To  give herself a raise of 10K, she merely has to flip the last bit of c6.
Since c6 gets ⊕’d into the  decrypted c7(c7 has not been modified, so the decrypted c7 will be the same), the result will be the  same as before, i.e., the old m7, but with the last bit flipped, which changes the ASCII 4 into a 5.
 Unfortunately for Ann, as a side-effect of this change, a value she will not be able to predict  will appear in her job-title field (since she cannot predict what the modified c6 will decrypt to),  which will affect m6:  Tacker, Ann A. System Security Offi!
zºŒ(‰9™ 54,122.10  | m1 | m2 |m3 |m4 | m5 |m6 |m7 |m8 |  A human who reads this report and issues checks is likely to suspect something is wrong.
 However, if it’s just a program, it would be perfectly happy with it.
And a bank would most likely  take the check even if some unorthodox information appeared in what to them is a comment field .
4.2.3 ENCRYPTING A LARGE MESSAGE 91  In the above example, Ann made a change she could control in one block at the expense of  getting a value she could neither control nor predict in the preceding block .
 An encryption mode that protects both the confidentiality and the authenticity of a message  with no greater cost than just encrypting the data was the holy grail of cryptographic protocol  design for many years, with many proposals subsequently discredited because security flaws were  found.
In 2007, GCM (see §4.4.
2) was formally standardized as a means to accomplish confident i- ality and integrity [NIST0 7].
It is somewhat more expensive than encryption alone because of the  arithmetic operations it does with the data, but in software these are much less expensive than  encryption.
Most modern CPUs implement encryption in hardware, making this a much less impo r- tant concern .
 4.2.3 CTR (Counter Mode )  Counter mode uses the key and the IV to generate a pseudorandom bit sequence that is then ⊕’d  with the data.
Such a sequence is known as a one-time pad.
CTR creates the first block of the  one-time pad by encrypting an IV.
For block n of the one-time pad, CTR encrypts IV +n−1. (
See  Figure 4-7.)
 IV IV+1 IV+2  K E K E K E  m1 ⊕ m2 ⊕ m3 ⊕  c c c IV 1 2 3  Figure 4-7.
Counter Mode (CTR )  An advantage of counter mode is that if multiple CPUs (or multiple hardware implement a- tions of the block cipher) are being used, different parts of the one-time pad can be computed in  parallel.
With CBC, blocks must be encrypted in sequence.
With CTR, the one-time pad can be  pre-computed, and encryption is simply an ⊕.  CTR is insecure if different data is encrypted with the same key and IV (because then a  one-time pad would be used multiple times).
Even if a different key and IV are used each time, an  attacker can also change the plaintext in predictable ways (if there is no additional MAC), merely  by ⊕ing the ciphertext with the bits the attacker wishes to change .
92 MODES OF OPERATION 4.2.3.
1  4.2.3.
1 Choosing IVs for CTR Mod e  As noted in the previous section, it is crucial that different plaintexts are never encrypted using the  same encrypted counters.
Choosing unique IVs is somewhat challenging in CTR mode because an  IV value must be different for each block rather than just for each message.
So if a hundred-block  message is encrypted with an IV of X, it must be assured that no future messages choose an IV in  the range X through X+99.
The CTR standard does not specify how to assure uniqueness.
It gives  several examples, one being that the initial IV is chosen randomly when the key is chosen, and the  IV is then incremented for each encrypted block.
The other modes that use CTR (CCM [§4.4.
1] and  GCM [§4.4.
2]) are a bit more explicit.
For example, GCM specifies that the 128-bit IV is part i- tioned into the most significant 96 bits, which must be unique for each message encrypted with the  same key (which we’ll call the per-message portion), and the remaining 32 bits count blocks within  a message.
That design limits the length of a single GCM-encrypted message to 232 blocks, i.e.,  about 69 gigabytes, assuming 128-bit blocks.
That is much longer than the maximum message  length in most protocols.
The CCM standard is similar, but it allows an implementation to choose  how many bits to use for the per-message portion and how many to use for the block counter .
 How do you assure that the per-message portion is indeed never used twice?
One approach is  to select it at random.
This is nice in some ways.
For example, it is stateless, so if one of the co m- municating parties reboots and forgets what its last IV was, it is no more likely to repeat an IV than  it would otherwise be.
The downside is, at least with a large number of messages (i.e., a few tri l- lion), there’s a non-negligible chance of repeating a 96-bit per-message value.
The NIST standard  specifies that if you choose the per-message portion of the IV at random, you need to change keys  before you encrypt more than 232 messages .
 Another approach is to start the per-message portion at a random value (when the key is  changed) and increment it for each new message.
This guarantees that the IV will not repeat until  the counters wrap around.
However, this approach introduces the burden of keeping track of where  you are in the sequence, and it’s likely that implementations will have flaws where IVs get accide n- tally reused because of losing counter state .
 In some applications, encryption is done in parallel, using the same key, by multiple proce s- sors that cannot stay tightly synchronized with each other.
A trick to avoid reusing per-message va l- ues in this scenario is to partition the per-message field into two fields—one that each of the  parallel processors assigns in a way that avoids conflicting with values it has used with that key  (e.g., starting at a random value and incrementing for each message) and the other a unique co n- stant assigned to each of the parallel processors .
4.2.4 ENCRYPTING A LARGE MESSAGE 93  4.2.4 XEX (XOR Encrypt XOR )  XEX mode was designed for disk encryption, where the data is encrypted at a low level and the  disk layout is fixed.
As a result, the XEX design had the constraint that the ciphertext could not be  bigger than the plaintext.
All the previously described modes except ECB use a randomly generated  IV for each independently encrypted message, which causes the ciphertext to be longer than the  plaintext.
XEX does use an IV, but it is implicit and predictable—the disk sector number.
XEX was  invented by Rogaway [ROGA0 4].
It’s not a standard, but we explain it here because it makes it ea s- ier to understand XTS (which is a standard and described in the next section). (
See Figure 4-8.)
 IV E2 Bmod1 ⊗2 Bmod2 ⊗2 Bmod3 ⊗2  m1 m2 m3  ⊕ ⊕ ⊕  E1 E1 E1  ⊕ ⊕ ⊕  c1 c2 c3  Figure 4-8.
XEX Mod e  XEX mode acts on 128-bit blocks and encrypts each block independently from all the others.
 This mode is well-suited for applications that read or write individual portions of a data structure.
 Achieving length preservation requires sacrificing integrity protection.
Unlike CTR mode, XEX  and its XTS variant (see §4.2.
5) are designed so that even attackers that know the corresponding  plaintext of an encrypted block cannot change it to a value of their choosing. (
See Homework Pro b- lem 6.)
Any change to the ciphertext will result in an effectively random value in the corresponding  plaintext blocks when the data is decrypted.
On the other hand, unlike with CBC mode, only the  blocks where ciphertext was modified are affected .
 The basic idea of XEX mode is to have a block-specific value that is ⊕’d into each data block  before and after the block is encrypted.
We’ll refer to the block-specific value as the block mod i- fier (Bmo d).
The Bmod is a function of the block’s address and a secret key.
The desired properties  of a function that computes the Bmod is that it be inexpensive to compute (in particular, more eff i- cient than an encryption operation) and that the Bmod value of each block looks like a random  number to anyone who does not know the secret key .
 XEX and XTS modes can be thought of as having two secret keys—one that encrypts the IV  and one that encrypts each block of data.
In Figures 4-8, 4-9, and 4-10, E1 indicates encryption with  the first key, while E2 indicates encryption with the second.
The original published version of XEX  uses the same value for both keys, while the standardized version of XTS uses double-length keys
94 MODES OF OPERATION 4.2.5  (256 bits for AES-128, 384 bits for AES-192, or 512 bits for AES-256), which specify independent  values for the two keys .
 XEX mode assumes there is an implicit unique IV for each message.
If this is used for disk  encryption, the location on disk where the message starts (i.e., its disk sector number) is a reaso n- able implicit IV. (
Note that in the literature, this implicit IV is referred to as a twea k.)  The block modifier Bmod is computed as a function of the secret key, the IV, and the block  number within the message.
For those readers who are not completely comfortable with finite field  arithmetic, the important point is that although Bmo d1 requires an encryption operation to compute,  any other Bmo dj can be computed in software more efficiently than an encryption operation, with  an operation that is a function of Bmo d1 and j.  For those readers comfortable with finite field arithmetic, in the Rogaway paper, Bmo d1 (the  Bmod of block 1, the first block of the message) consists of the IV encrypted with the key.
Bmo dj+1  (the Bmod of block j+1 of the message) is Bmo dj⊗2, where ⊗ 2 means multiplication by  000...010 in the finite field GF( 2128), i.e., multiplication by x mod the primitive polynomial  128 2 x +x 7+x +x+1.
It is important that the modulus is primitive so that x has maximal order, meaning  that Bmo dj won’t repeat before taking on 2128−1 different values .
 There is an important security weakness in this approach.
The IV is supposed to be unique for  each encrypted message, but by choosing it based on the disk sector number, it will be repeated  whenever a new message overwrites an existing one on the disk.
This allows someone who can  observe the ciphertext on the disk on multiple days to figure out which blocks were modified.
If  attackers could also update the disk, they can revert any blocks to previous values.
This weakness is  accepted in order to achieve the goal of not expanding the plaintext during encryption .
 4.2.5 XTS (XEX with Ciphertext Stealing )  This mode is officially called XTS-AES, but it could be used with any encryption algorithm, so  we’ll just call it XTS.
XEX requires the size of a message to be encrypted to be a multiple of the  cryptographic blocksize.
XTS is a clever variant of XEX that encrypts multiblock messages that  need not be a multiple of the cryptographic blocksize while still keeping the ciphertext the same  size as the plaintext.
XTS is standardized in [IEEE0 7 and NIST1 0].
XTS accomplishes this goal (of  being length-preserving despite a message not being a multiple of the cryptographic blocksize)  through a very clever but somewhat complicated trick known as ciphertext stealin g. Note that only  the final block of the message (let’s call that m ) might be undersized, so as we’ll see, all blocks n  other than the last two can be encrypted normally. (
See Figure 4-9 and Figure 4-10.)
 What can you do with the final block, m , if, for example, it is 93 bits instead of the blocksize n  of 128 bits?
You could pad m to be 128 bits, but then the ciphertext would be longer than the plai n-n  text, and furthermore, how would you know which of the bits of the decrypted block m were pa d- n  ding?
4.2.5 ENCRYPTING A LARGE MESSAGE 95  Bmod1 ⊗2 IV E2  m1  ⊕  E1  ⊕  c1 Bmod2  E1  ⊕ Bmod ⊗2 Bmod n−1 n⊗2 … ⊗2  mn−1 mn  ⊕ ⊕  E1 E1  ⊕ ⊕ m2  ⊕  c2 cn−1 cn  Figure 4-9.
XTS Mode Encryptio n  Ciphertext stealing pads the final block m with as many of the bits of the previous block of n  ciphertext (c ) as necessary to make block m be full sized (in our example, 12 8−93=35 bits need n−1 n  to be stolen from c ).
The padded block m is then encrypted .n−1 n  But the ciphertext is now longer than the plaintext, since the last block of ciphertext is now a  full-sized block.
To solve that problem, we swap ciphertext blocks c and cn−1 and truncate the final n  ciphertext block (which was the encryption of block mn−1) to be the size of the original block m .
In n  our example, where the original block m was 93 bits long, the final ciphertext block will be 93 bits.
n  Now the ciphertext is the same size as the message, but how do we decrypt either of the last two  blocks ?
To obtain block m , decrypt c (using implicit IVs and Bmods associated with block m ).
n n−1 n  The result will be plaintext m with appended padding, but you need to know how much of the n  result is message and how much is padding.
The size of plaintext block mn will be the size of the  IV E2  c1  ⊕  ⊕ D1  m1 ⊗2 Bmodn−1 ⊗2 Bmodn  cn−1 cn  ⊕ ⊕  D1 D1  ⊕ ⊕ c2  ⊕  ⊕ D1  m2 mn−1 mn Bmod1 ⊗2 Bmod2 ⊗2 …  Figure 4-10.
XTS Mode Decryptio n
96 MODES OF OPERATION 4.3  final ciphertext block (in our example, 93 bits).
The padding (the remaining 12 8−93=35 bits) is  stolen ciphertext from cn−1.
 Suppose you only needed to read plaintext block n−1.
You would need to do two decryptions.
 First, do what’s in the previous paragraph to obtain the padding (the stolen ciphertext) used to pad  block m .
Then, take the final ciphertext block c , append the stolen ciphertext to it, and decrypt the n n  now-complete ciphertext block as if it were the encryption of mn−1.
 4.3 GENERATING MACS  A secret key system can be used to generate a cryptographic integrity check known as a MAC  (Message Authentication Code).
While the preceding modes, when properly used, offer good pr o- tection against an eavesdropper deciphering a message, none offers good protection against an  attacker modifying the message without being detected.
It is, therefore, good practice whenever  encrypting data to include a MAC with the data.
For some applications encryption is unnecessary,  but MACs are used to protect the data from modification in transit.
Some MACs in common use  are based on secret key encryption functions, some on hashes, and some on public key algorithms.
 MACs using public key algorithms are known as digital signatures.
In this chapter, we’ll focus on  MACs based on secret key encryption functions.
MACs using a secret key cannot be computed or  verified without knowledge of the secret key.
Note that one of the most commonly used MAC  schemes is known as HMAC.
It has the same properties as the schemes in this chapter, but because  it is based on hashes rather than on secret key encryption functions, it is discussed in Chapter 5  Cryptographic Hashe s (§5.8 The Internal Encryption Algorithm s).
 4.3.1 CBC-MA C  CBC-MAC computes a MAC of a message using key K. It encrypts the message in CBC mode,  using key K, and uses the last block (called the residu e, see Figure 4-11) as the MAC for the me s- sage.
If an attacker modifies any portion of a message, the residue will no longer be the correct  value (except with probability 1 in 2128, assuming 128-bit blocks) .
 4.3.1.
1 CBC Forgery Attac k  There are some weaknesses in using CBC residues with the same key to protect many messages of  varying lengths.
Having seen CBC residues on some messages, an attacker might be able to predict  the CBC residues computed over other messages.
For example, suppose someone sent a message
4.3.2 GENERATING MACS 97  m1 m2 m3 m4 m5 m6  ⊕ ⊕ ⊕ ⊕ ⊕  encrypt with secret  key E E E E E E  CBC residu e c1 c2 c3 c4 c5  Figure 4-11.
Cipher Block Chaining Residu e  m1|m2|m3 with CBC residue c3 and later sent a longer message m1|m2|m3|m4|m5|m6 with CBC res i- due c6.
An attacker could then forge a message ( c3⊕ m4)|m5|m6 and know its CBC residue would  also be c6.
See Homework Problem 1 2.
 4.3.2 CMA C  To avoid the above problem with CBC-MAC, a slight variant called CMAC was defined and sta n- dardized in [NIST9 3].
It is very similar to CBC-MAC except that it ⊕s a secret value (derived from  the key K) into the final block of plaintext before encrypting (see Homework Problem 9).
 There are actually two secret values (X1 and X2) derived from the key K. The reason for  deriving two secrets (X1 and X2) is to be able to pad a message to an integral number of blocks  without adding an additional block when the message does not require padding.
Padding a message  to be an integral number of blocks is always an annoying detail.
Since it’s necessary to know what  portion of the plaintext is padding, it can be tricky to distinguish a message that is padded from one  whose plaintext happens to look like the padding format.
The usual solution is to require every  message to contain padding, so plaintext that is already an integral number of blocks will wind up  being padded with an extra block of padding .
 CMAC deals with the padding issue in a different way.
If the message for which the MAC is  being computed is not a multiple of the blocksize, it is padded with a 1 bit followed by as many 0  bits as are required to fill out the block.
But, you wonder, how can you tell the difference between a  padded message and an unpadded message, given that any block (other than all zeros) will have a  final 1 someplace?
CMAC’s solution is to ⊕ the value X1 into the final block if that final block was  not padded and to ⊕ a different value (X2) into the final block if that final block was padded.
The  MAC computed by CMAC is based on the plaintext of the first n−1 blocks and the (possibly) pa d- ded nth block ⊕’d with either X1 or X2 (depending on whether the last block was padded).
Then the  actual message (i.e., with the padding bits removed), along with the MAC, is transmitted (or  stored) .
98 MODES OF OPERATION 4.3.3  The special values X1 and X2 are derived from the key by a means that does not make a lot of  intuitive sense unless you understand finite field arithmetic.
K is used to encrypt a constant block  containing all zeros.
The result is ⊗’d by 2 to get X1.
X1 is then ⊗’d by 2 to get X2.
 4.3.3 GMA C  While MACs based on CBC residues and hash algorithms have been around for years, GMAC  works in a fundamentally different way and is typically faster to compute, although the perfo r- mance gap between GMAC and CBC-based MACs using AES has narrowed significantly since the  introduction of hardware-supported CPU instructions for AES on most platforms.
Note that the G  in GMAC stands for Galois, because these modes use Galois field arithmetic (see §2.7.1 Finite  Field s).
The older MAC functions require a cryptographic pass over the data—hash algorithms and  encryption algorithms are of similar complexity and performance.
GMAC is more like a checksu m- ming algorithm, doing inexpensive calculations based on the contents of the message.
GMAC only  needs to do the expensive encrypt operation on two single-block values regardless of the length of  the message.
The world is slowly migrating to this new kind of MAC algorithm.
Its only downside  is that, unlike earlier MAC algorithms, it requires that a unique IV be used for each message until  the key is changed.
If the same IV and key are ever used with two different messages, an attacker  who sees the results can then compute a MAC with that key for any message.
We won’t explain  how reusing an IV in GMAC allows this, but if you take our word for it, contrast that consequence  with the consequence of reusing an IV in CBC mode (see Homework Problem 7).
 4.3.3.
1 GHAS H  GHASH (Figure 4-12) is a building block for computing a MAC.
GHASH is used by both GMAC  and GCM.
It is much less expensive to compute than CBC residue, because the expensive encry p- tion operation in CBC residue is replaced with an inexpensive finite field multiplication (⊗).
The  inputs to GHASH are a message and a value H. The flow of GHASH is similar to CBC residue, in  that the computed output of block mi is ⊕’d with mi+1, and then (where CBC residue would encrypt  with key K) GHASH does ⊗H.  GHASH itself is not usable as a MAC, because anyone who knows the message and H can  compute the GHASH value of that message.
Furthermore, even if someone does not know H but  sees a message and the corresponding GHASH value, they can derive H. However, GHASH will be  used as a component in GMAC (§4.3.
3), and it will be secure because GMAC will hide H and the  GHASH value .
1 4.3.3.
2 GENERATING MACS 99  m m2 m3 … mn  ⊗H ⊕ ⊗ H ⊕ ⊗ H … ⊕ ⊗ H GHAS H  Figure 4-12.
GHAS H  4.3.3.
2 Transforming GHASH into GMA C  GMAC (Figure 4-13) transforms GHASH into a cryptographically secure but still inexpensive  MAC.
It has been proven that if AES is secure as an encryption algorithm, GMAC will be secure as  a MAC.
GMAC takes as input a message M, a 96-bit IV that should never be used on two different  messages, and a key K. It will output a 128-bit MAC .
 0–127 bits 64 bits 64 bits  Message M 0 (pad ) length of M 0  H 0 0 E GHAS H  96 bits 32 bits  IV 1 E ⊕  IV GMA C  Figure 4-13.
GMA C  GMAC starts by padding the message to be an integral number of 128-bit blocks.
Then,  GMAC gives the padded message and a value H to GHASH (§4.3.3.
1).
The value H is computed  by GMAC as the AES encryption of the value 0 by the key K.  Next, GMAC takes the 128-bit value consisting of the 96-bit IV concatenated with the value  1 expressed in 32 bits, i.e., 31 bits of 0 followed by a 1, encrypts that 128-bit value with K, and ⊕s  that with GHASH.
The result is GMAC.
Note that neither H nor the GHASH value is exposed.
 Since any given IV will only be used once, an eavesdropper will get no information about GHASH  nor know whether two messages have the same GHASH.
H could have been specified to be a sec- ond key as input to the algorithm, but to avoid that, H is defined to be the encryption of the constant  0, and the constant 1 is appended to the IV to prevent the result from being 0 .
 Other than using two blocks requiring AES encryption, one to compute H and one to encrypt  the 128-bit value containing the IV, all the other computations in GMAC (⊗ and ⊕) are inexpe n- sive.
Since H will be the same for any message with the same K, an implementation could save H,  and on subsequent messages, only do a single AES-encrypt operation (to encrypt the 128-bit value  containing the IV).
The security of the scheme depends on the same IV never being used with two
100 MODES OF OPERATION 4.4  different messages processed with the same key.
Techniques for accomplishing that are described  in §4.2.3.1 Choosing IVs for CTR Mod e.  4.4 ENSURING PRIVACY AND INTEGRITY TOGETHER  Commonly, you will want to ensure both the privacy and the integrity of a transmitted message.
 You can do that by applying any of the MAC algorithms and any of the encryption algorithms to the  message (and in either order).
But that assumes that the two keys are independent of one another,  and that you are willing to do twice the work .
 There are two standardized combined modes (CCM and GCM) that manage to use a single  key for encryption and integrity protection but ensure that doing so does not weaken either.
CCM  requires twice the cryptographic work of encryption or integrity protection alone, while GCM is  typically less expensive because it uses GMAC as the MAC, and GMAC is typically less expensive  to compute than an encryption of a message .
 4.4.1 CCM (Counter with CBC-MAC )  CCM mode, standardized in NIST SP 800-38C, combines CTR mode encryption and CBC-MAC  (CBC residue) integrity protection.
CCM allows part of the message (the associated dat a) to be  integrity protected but not encrypted.
The application specifies which part of the message is assoc i- ated data and which part of the message is to be both encrypted and integrity protected.
 CCM mode uses CBC-MAC and not CMAC.
CCM is still designed to resist the CBC forgery  attack (see §4.3.1.
1), but it does so in a different way than CMAC.
The CBC standard specifies that  the MAC is computed and appended to the plaintext before the combined quantity is encrypted.
 The fact that the MAC is encrypted prevents the CBC forgery attack.
Although CCM uses the same  key for integrity and encryption, encryption and integrity protection using CCM requires twice the  cryptographic work as doing encryption or integrity protection alone.
The MAC is computed over  not just the message but also over an additional prefix block.
That prefix block contains the nonce  (from which the first counter value is derived) and plaintext length for CTR mode encryption.
The  fact that the plaintext length needs to be known before the CBC-MAC operation starts is a perfo r- mance drawback for CCM .
4.4.2 ENSURING PRIVACY AND INTEGRITY TOGETHER 101  4.4.2 GCM (Galois/Counter Mode )  GCM (Figure 4-14) combines counter mode encryption (see §4.2.
3) with GMAC (see §4.3.
3) in a  way that allows both modes to use the same cryptographic key .
 In CTR mode, an IV is incremented for each block.
The IV for the CTR encryption has the  same format as the 128-bit value in GMAC (see Figure 4-13) except that the low 32 bits (which in  GMAC are always 0…01) start at 2 (0…010) and increment for each block to be encrypted .
 As with CCM mode, GCM mode has the ability to have part of the message, called assoc i- ated dat a, be integrity protected but not encrypted.
The plaintext to be encrypted is CTR encrypted  to produce the ciphertext.
The MAC computation in GCM covers the associated data, the ciphe r- text, and an extra block containing the 64-bit length of the encrypted message and the 64-bit length  of the associated data, to prevent the kind of attacks mentioned in §4.3.1.1 CBC Forgery Attac k.  plaintex t  CTR Encrypt  64 bits 64 bits  associated data A pad ciphertext C pad length A length C  H0 0 E GHAS H  96 bits 32 bits  IV 1 E ⊕  GMA C  Figure 4-14.
Galois/Counter Mod e  Note in Figure 4-14, the boxes that are shaded (associated data A, ciphertext C, and GMAC)  all need to be transmitted.
Other items, such as the IV and the lengths of A and C, have to be known  by the application or communicated somehow .
 GCM also has another interesting property.
As in all versions of counter mode, subtly bad  things happen to confidentiality if an IV is ever reused with the same key, but in GCM, horrible  things also happen to integrity if an IV is ever reused with the same key.
Upon seeing the MACs  associated with two messages encrypted using the same key and IV, an attacker can compute the  integrity protection secret H and can then forge arbitrary messages.
So it is very important that an  implementation never reuse an IV .
 GMAC takes a 96-bit IV that the caller must ensure is never reused.
The IV is concatenated  with a 32-bit constant 1 and then encrypted.
GMAC made this choice in order to be compatible  with GCM.
Both use the same format of IV, where the top 96 bits must never be reused with the  same key, concatenated with a counter starting at 2 and incremented for each succeeding block.
 Since it would be a problem for two blocks to use the same 128-bit value, GCM limits the amount  of plaintext in a single message to less than 232 blocks .
102 MODES OF OPERATION 4.5  As you can see, GCM requires considerably more care to use correctly than the other modes,  but the payoff is doubling the speed of the implementation (more if parallelism is exploited) co m- pared to previous mechanisms for encrypting with integrity protection, so this approach is beco m- ing much more common .
 4.5 PERFORMANCE ISSUES  Performing encryption and decryption is often computation-intensive, and in uses where data is  going out over a communications line or onto a disk, there is a real-time requirement associated  with it that can be challenging.
One way to improve performance is to perform the block encryption  operations in parallel, either with multiple CPUs or with multiple hardware encryption engines  operating in parallel.
While this can always be done if there are multiple streams of data being pr o- cessed in parallel, the ability to exploit parallelism when encrypting or decrypting a single stream is  influenced by the design of the cryptographic mode .
 For example, with CBC mode the encryption of a block cannot begin until the encryption of  the previous block is completed (because the previous block of ciphertext is ⊕’d into the plaintext  before it is encrypted).
With ECB or the various counter modes, once a message is known, all the  blocks can be encrypted in parallel if resources permit.
Interestingly, this is also possible with CBC  decryption even though it is not possible with CBC encryption (see Homework Problem 5).
With  counter mode, the encryption (or decryption) can begin even before any of the message is known.
 That’s because the values being encrypted are counters, and these are not dependent on the me s- sage.
When the message becomes known, a simple ⊕ with the keystream is all that is necessary,  which minimizes latency .
 4.6 HOMEWORK  1.
In our example mode Randomized EC B (see Figure 4-4), suppose that a bit of one of the  ciphertext blocks, say ci, is flipped (intentionally or unintentionally).
What will that do to the  decrypted message?
Suppose a bit of an ri is flipped?
What would that do to the decrypted  message?
Suppose the ciphertext were being transmitted as a stream of octets, and one octet  was lost (and the receiver was not notified that an octet was lost)?
What would that do to the  decrypted message ?
4.6 HOMEWORK 103  2.
In CBC mode, suppose one bit of ciphertext is flipped.
What will that do to the decrypted  message?
Suppose the ciphertext were being transmitted as a stream of octets, and one octet  was lost (without the receiver being notified that an octet was lost)?
What would that do to  the decrypted message?
Suppose an entire block was lost.
What would that do to the  decrypted message ?
 3.
Consider the following alternative method of encrypting a message.
To encrypt a message,  use the algorithm for CBC decryption with an implicit IV of zero.
To decrypt a message, use  the algorithm for CBC encryption with an implicit IV of zero.
Would this work? (“
Working”  means that decrypt reverses the encrypt operation.)
What are the security implications of this,  if any, as contrasted with the “normal” CBC?
Hints: How many ciphertext blocks change if  you change one block of message in normal CBC compared to our proposed CBC variant?
If  there are repeated blocks of plaintext, are there detectable patterns in the ciphertext?
 4.
Suppose you know that the employee database is encrypted using randomized ECB (see  §4.2.2.
1), and suppose you know that your salary is in plaintext block m7, that you have  access to the ciphertext, and you know the syntax of the plaintext.
How can you modify the  ciphertext to increase your salary ?
 5.
Why is it possible to decrypt, in parallel, blocks of ciphertext that were encrypted using CBC  mode, but it is not possible to encrypt, in parallel, blocks of plaintext with CBC mode ?
 6.
Suppose a message is encrypted with CTR mode, and you know that plaintext block m7 con- tains your salary.
Suppose you have access to the ciphertext.
How can you modify the ciphe r- text to increase your salary ?
 7.
In CBC mode, suppose you reused the same IV for two messages.
What are the security  implications ?
 8.
Suppose you wanted to decrypt only block n of a file that was encrypted with CBC mode  (and you knew the key with which the file was encrypted).
Which blocks of the encrypted file  would you need to read?
What cryptographic operations would you need to do ?
 9.
How does ⊕ing K1 into the final block in CMAC prevent the attack described in section  §4.3.1.1 CBC Forgery Attac k?
 10.
For each scenario, compare the number of AES operations required in XTS versus XEX:  a) Encrypting an n-block plaintext that is an integral number of blocks.
 b) Encrypting an n-block plaintext that is not an integral number of blocks.
 c) Decrypting an n-block ciphertext.
 d) Decrypting only block n. (Two questions: if the ciphertext is an integral number of blocks,  or if it isn’t.)
 e) Decrypting only block n−1. (
Two questions: if the ciphertext is an integral number of  blocks, or if it isn’t.)
104 MODES OF OPERATION 4.6  f) Decrypting only block n−2? (
Two questions: if the ciphertext is an integral number of  blocks, or if it isn’t. )
 11.
How can you do an XTS-like trick with CBC mode encryption (and decryption) so as to be  length-preserving ?
 12.
What message could you forge (see section §4.3.1.1 CBC Forgery Attac k) if you saw two  messages with CBC-MACs—one that consisted of m1|m2 with CBC-MAC x1, and another  that consisted of m1|m2|m3|m4 with CBC-MAC x2?
 13.
With CBC-MAC, it is possible to create a message that has all but one block chosen by the  user and has any chosen CBC-MAC value, provided that one block somewhere in the me s- sage is constrained by the computation rather than chosen by the user.
Show how you can  construct a message that has CBC-MAC equal to 0, where you want particular values for m1,  m2, m3, m4, m6, m7, m8, and you don’t care what value is in m5.
You can do this with any key,  so let’s say you are constrained to use key =K, and let’s say you are using AES-128.
What  does m need to be so that the message m |m |m |m |m |m |m |m will have CBC-MAC equal 5 12345678  to 0?
5 CRYPTOGRAPHIC HASHE S  5.1 INTRODUC TION  A hash function inputs an arbitrary-sized bitstring and outputs a fixed-size bitstring, ideally so that  all output values are equally likely.
A cryptographic hash (also known as a message digest ) has  some extra security properties:  • preimage resistance : It should be computationally infeasible to find a message that has a  given pre-specified hash.
 • collision resistance : It should be computationally infeasible to find two messages that have  the same hash.
 • second preimage resistance : It should be computationally infeasible to find a second mes - sage that has the same hash as a given message.
 The term message digest was originally more popular, but hash is more commonly used today.
As  evidence that the world has largely abandoned use of the term message digest , the NIST hash func - tions [ NIST15b ][NIST15c ] have names beginning with SHA, which stands for secure hash algo - rithm .
Earlier hash algorithms had names beginning with MD ( e.g., MD5), which stands for  message digest.
We will use the terms cryptographic hash , hash, and message digest interchange - ably.
 In §3.2.3 Looking Random we talked about the ideal cipher model for a block cipher, using an  imaginary box known as an oracle.
An almost identical model of an (imaginary) oracle applies to  hash functions.
The hash oracle has a table of 〈input-value ,output-value 〉 pairs, and if asked what  the output is for an input, if that input value is in its table, it responds with the output value as spec - ified.
If the input value is not in its table, the oracle generates a random bitstring for the output  value, adds that 〈input-value , output-value 〉 pair to its table, and gives that output as the answer.
 Often hash functions are used in ways that are only secure if they “look random” in ways that are  not completely captured by the properties of collision resistance, preimage resistance, and second  preimage resistance.
 In proofs, cryptographers would like to say things like “This protocol is secure so long as the  hash function it uses is secure.”
But it’s hard to define exactly what it means for a hash function to  105
106 C RYPTOGRAP HIC HASHES 5.1  be secure, so they instead say it would be secure if the hash function were a random oracle.
This is  often called a proof in the random oracle model.
 Ideally, the difficulty of finding a preimage of a given n-bit hash should be no better than  brute-force search, which would mean testing approximately 2n messages.
As we’ll see, finding a  collision (two messages with the same n-bit hash) is easier.
Even if the hash were a random oracle,  finding a collision will only require testing 2n/2 messages.
Given these properties, a secure hash  function with n bits should be derivable from a hash function with more than n bits merely by tak - ing any particular subset of n bits from the larger hash.
 There certainly will be many messages that yield the same hash, because a message can be of  arbitrary length and the hash will be some fixed length, say 256 bits.
For instance, for 1024-bit mes - sages and a 256-bit hash there are, on average, 2768 messages that map to any one particular hash.
 So, certainly, by trying lots of messages, one would eventually find two that mapped to the same  hash.
The problem is that “lots” is so many that it is essentially impossible.
Assuming a good  256-bit hash function, it would take trying approximately 2256 possible messages before one would  find a message that mapped to a particular hash, or approximately 2128 messages before finding two  that had the same hash (see §5.2 The Birthday Problem ).
 What can an attacker do if he can find a message with a specific hash?
If Alice has signed a  message saying “Alice agrees to pay Bob $10”, she is signing the hash of that message.
If Bob  could find another message that has the same hash, then Alice’s signature works as a signature on  the new message.
For instance, Bob might try different messages until he finds one with the hash  that Alice signed, where the new message says “Alice agrees to pay Bob $9493840298.21”.
 What can an attacker do if he can find two messages with the same hash?
Suppose Alice  wants to fire Fred, and asks her diabolical secretary, Bob, who happens to be Fred’s friend, to com - pose a letter explaining that Fred should be fired, and why.
After Bob writes the letter, Alice will  read it, compute a hash, and cryptographically sign the hash using her private key.
Bob would like  to instead write a letter saying that Fred is wonderful and his salary ought to be doubled.
However,  Bob cannot forge Alice’s signature on a new hash.
If he can find two messages with the same hash,  though, one that Alice will agree to sign because it captures what she’d like to say and one that says  what Bob would like to say, then Bob can substitute his own message after Alice generates the  signed hash, and it will look like Alice signed Bob’s message.
 To make the example easy, suppose the hash function outputs only 64 bits and is a good hash  function in the sense that its output looks random.
Then the only way to find two messages with the  same hash would be by trying enough messages so that, by the birthday problem, two would have  the same hash.
 If Bob started by writing a letter that Alice would approve of, found the hash of that, and then  attempted to find a different message with that hash, he’d have to try 264 different messages.
How - ever, suppose he had a way of generating lots of messages of each type (type 1—those that Alice  would be willing to sign; type 2—those that Bob would like to send).
Then by the birthday problem  he’d only have to try about 232 messages of each type before he found two whose hashes matched.
5.1 INTRODUC TION 107  How can Bob possibly generate that many letters, especially since they’d all have to make  sense to a human?
Well, suppose there are two choices of wording in each of 32 places in the letter.
 Then there are 232 possible messages he can generate.
For example:  Type 1 message  I am writing {this memo |} to {demand | request | inform you } that {Fred | Mr. Fred Jones }  {must |} be {fired | terminated } {at once | immediately }.
As the {July 11 | 11 July } {memorandum |  memo } {from | issued by } {personnel | human resources } states, to meet {our | the corporate }  {quarterly | third quarter } budget {targets | goals }, {we must eliminate all discretionary spending |  all discretionary spending must be eliminated }.
 {Despite | Ignoring } that {memo | memorandum | order }, Fred {ordered | purchased }  {Post-its | nonessential supplies } in a flagrant disregard for the company’s {budgetary crisis |  current financial difficulties }.
 Type 2 message  I am writing {this letter | this memo | this memorandum |} to {officially |} commend Fred  {Jones |} for his {courage and independent thinking | independent thinking and courage }. {
He |  Fred } {clearly |} understands {the need | how} to get {the | his} job {done | accomplished } {at all  costs | by whatever means necessary }, and {knows | can see } when to ignore bureaucratic {non- sense | impediments }.
I {am hereby recommending | hereby recommend } {him | Fred } for {promo - tion | immediate advancement } and {further | } recommend a {hefty | large } {salary |  compensation } increase.
 There are enough computer-generatable variants of the two letters that Bob can compute  hashes on the various variants until he finds a match.
With a standard laptop processor, it’s pretty  easy to generate and test 232 messages in a few minutes.
This shows that a 64-bit hash (our exam - ple) is too small.
As a rule, if you want your hash function to be as secure as your block cipher with  an n-bit key, the hash should have 2 n bits.
 Ideally, the hash function should be easy to compute.
One wonders what the “minimal”  secure hash function might be.
It is safer for a function to be overkill, in the sense of shuffling  beyond what it necessary, but then it is harder to compute than necessary.
The designers would  rather waste computation than discover later that the function was not secure.
Just as with block  ciphers, the hash algorithms tend to be computed in rounds.
Designers typically choose the number  of rounds by finding the smallest number of rounds necessary to protect against known attacks, and  then add a few extra rounds just to be safe, in case attack techniques improve.
 Later in this chapter we’ll describe the hash functions that have been standardized by NIST.
108 C RYPTOGRAP HIC HASHES 5.2  5.2 THE BIRTHDAY PROBLEM  If there are 23 or more people in a room, the odds are better than 50% that two of them will have the  same birthday.
Analyzing this parlor trick can give us some insight into cryptography.
We’ll assume  that a birthday is basically an unpredictable function taking a human to one of 365 values (yeah  yeah, 366 for you nerds).
 Let’s do this in a slightly more general way.
Let’s assume n inputs (which would be humans  in the birthday example) and k possible outputs, and an unpredictable mapping from input to out - put.
With n inputs, there are n(n−1)/2 pairs of inputs.
For each pair there’s a probability of 1 /k of  both inputs producing the same output value, so you’ll need about k/2 pairs in order for the proba - bility to be about ½ that you’ll find a matching pair.
That means that if n is greater than k , there’s  a good chance of finding a matching pair.
 5.3 A B RIEF HISTORY OF HASH FUNC TION S  Surprisingly, the drive for hash algorithms started with public key cryptography.
RSA was invented,  which made it possible to digitally sign messages.
But computing a signature on a long message  with RSA was sufficiently slow that RSA would not have been practical by itself.
A cryptographi - cally secure hash function with high performance would make RSA much more useful.
Instead of  computing a signature over a long message, the RSA signature would be computed on the mes - sage’s hash.
So MD and MD2 (RFC 1319) were created (around 1989).
MD was proprietary and  never published.
It was used in some of RSADSI’s secure mail products.
 Around 1990, Ralph Merkle of Xerox developed a hash algorithm called SNEFRU  [MERK90] that was several times faster than MD2.
This prodded Ron Rivest into developing MD4  (RFC 1320), a hash algorithm that took advantage of the fact that newer processors could do 32-bit  operations and was therefore able to be even faster than SNEFRU.
Then SNEFRU was broken by  Biham and Shamir [BIHA92 ] (the cryptographic community considered it broken because they  were able to find two messages with the same SNEFRU hash).
Independently, den Boer and Bosse - laers [DENB92] found weaknesses in a version of MD4 with two rounds instead of three.
This did  not officially break MD4, but it made Ron Rivest sufficiently nervous that he decided to strengthen  it and create (in 1991) MD5 (RFC 1321), which is a little slower than MD4.
 What does a “weakness” or “being broken” mean for hash algorithms?
In an ideal n-bit hash  algorithm, finding a collision should require 2n/2 operations.
A weakness is an attack that could the - oretically find collisions in less than 2n/2 operations.
Being broken means publishing an actual col - lision.
5.3 A BRIEF HISTORY OF HASH FUNC TIONS 109  MD4 and MD5 were subsequently broken (in the sense that collisions were found), though  they could be used securely in some cases if you’re careful. (
However, if you were careful and used  one of these functions in a way that was secure, you’d have to be prepared to explain to your cus - tomers why you are using a function that they read was “broken”.)
 After MD4 and MD5 were published, but before they were broken, NIST proposed the  NSA-designed SHA (1993).
SHA is very similar in design to MD5, but even more strengthened.
It  is 160 bits instead of 128, and a little slower.
After a never-published flaw in the SHA proposal was  discovered, NIST revised it at the eleventh hour in an effort to make it more secure, and called the  revised version SHA-1 (in 1995, and the previous version was retroactively named SHA-0).
They  were apparently right to be concerned.
After years of cryptanalysis, weaknesses have been found in  both SHA-0 and SHA-1, but as of 2022, the best known SHA-1 attack for finding collisions costs  something like 263operations, while SHA-0 collisions can be found for only about 239 operations.
 Wanting a longer hash length, NIST adopted and standardized SHA-2 in 2004 (which, like  SHA-1, was designed by NSA).
Then SHA-1 was broken (a collision-finding algorithm with an  expected runtime of significantly less than the expected 280 hash operations was found).
Since  SHA-2 uses an algorithm similar to SHA-1, it was feared that SHA-2 might someday be broken  (although it hasn’t been as of 2021).
So NIST standardized the SHA-3 family, which uses a rather  different structure in hopes that any vulnerabilities in SHA-1 would not apply.
Another attraction of  SHA-3 is that, like AES, SHA-3 was the result of a competition rather than being designed by  NSA.
While SHA-1 produces a 160-bit hash, the SHA-2 and SHA-3 families produce 224-, 256-,  384-, and 512-bit hashes.
The various hash sizes in SHA-2 and SHA-3 are intended to match the  security of 3DES, AES-128, AES-192, and AES-256, respectively.
 Note that the standard terminology for the SHA-2 family is to only refer to the hash size, as in  SHA-224, SHA-256, SHA-384, SHA-512, and SHA-512/ n where n is the truncated size of the hash  and can be any value from 1 through 511 except for 384 (for that you’re supposed to use SHA-384).
 We think the terminology SHA2-224, SHA2-256,… is clearer, especially since the SHA-3 family  can produce many of those same hash lengths and are referred to as SHA3-224, SHA3-256,  SHA3-384, and SHA3-512.
 SHA-2 could have specified a single algorithm, say SHA2-512, and then specified each of the  smaller hashes as just truncating the 512-bit hash to the relevant size.
But since SHA2-512 was  designed to have security equal to AES-256, it would be slower than necessary if it just needed the  security of AES-128.
All the SHA-2 family are very similar.
SHA2-384 is essentially the same as  SHA2-512 but truncated to 384 bits.
And SHA2-224 is essentially the same as SHA2-256 but trun - cated to 224 bits.
And if you are in the mood for a hash value other than these sizes (but less than  512), you can truncate SHA2-512 to any size you like.
One difference between the sizes, though, is  that a value we call the IV (see Figure 5-4) is unique to each intended hash length in order to avoid  having the first n bits of a larger hash be the same value as the computed n-bit hash.
SHA2-256 and  SHA2-512 are very similar algorithms, but the word sizes in SHA2-512 are double the sizes in  SHA2-256, there are more rounds in SHA2-512, and they use different constants.
SHA2-512 actu -
110 C RYPTOGRAP HIC HASHES 5.4  ally runs faster on 64-bit CPUs than SHA2-256, so many implementations that only want a 256-bit  hash use SHA2-512 truncated to 256 bits.
 The SHA-3 family has parameters ( r and c; see §5.6.2 Construction of SHA-3 ) that can trade  off security and performance, and the standard specifies values of these parameters for each of the  standardized hash sizes.
Other than the values of these parameters, all the SHA-3 algorithms are  identical.
 There have been many other hash functions that were designed along the way, including per - fectly reasonable ones that were submitted to the SHA-3 hash competition that simply lost out to  KECCAK due to issues such as performance or safety margin.
There were also hash competitions in  Europe and Japan, and we haven’t described the winners of those.
We can’t mention all the hash  functions that have been designed, or we’d have to change the name of this section from A Brief  History of Hash Functions to A Comprehensive History of Hash Functions .
However, we will men - tion the RIPEMD family of various-size hashes (128, 160, 256, and 320) because RIPEMD-160  [DOBB96] is used in some cryptocurrencies, and no weaknesses in RIPEMD-160 have been pub - lished.
Note that MD stands for message digest.
 5.4 NIFTY THINGS TO DO W ITH A HASH  Before we look at the details of several popular hash algorithms, let’s look at some interesting uses  of hashes.
 5.4.1 Digital Signatures  Since a hash is fixed length ( e.g., 256 bits), and messages can be arbitrarily long, there will be many  messages with the same 256-bit hash.
However, hashes are designed so that it is very unlikely to  ever find two messages with the same hash.
Therefore, the hash can be used as a shorthand of a  message.
Since public key signatures are slow, and computing hashes are fast, digital signatures  sign the hash of a message instead of the actual message.
 In fact, hashing the message before signing is also more secure.
Without hashing, many digi - tal signature algorithms (including RSA) have the property that an attacker who sees a signed mes - sage can forge other 〈message ,signature 〉 pairs that will successfully verify.
If, instead, the public  key is used to sign a hash of the message, then although the attacker might still be able to construct  valid signatures for certain constructed hash values, these signatures would only be useful if the  attacker could find a message that hashed to the constructed hash value.
5.4.2 NIFTY THINGS TO DO WITH A HASH 111  5.4.2 Password Database  Let’s assume a human at a client machine wishes to authenticate to a server.
In the simplest form of  password authentication, the server keeps a list of 〈username , password 〉 pairs.
The human types  their username and password, and the client machine sends these to the server (perhaps over an  encrypted connection to the server).
The server verifies that the 〈username , password 〉 pair is in the  list.
This design has the problem that if someone stole the database at the server, they would know  all the usernames and passwords.
 It is more secure to have the server store a database of 〈username , hash( password )〉 pairs.
 There are various ways the hashed password can be used for authentication.
The client might send  the 〈username , password 〉 to the server, which would then hash the received password and verify  that the result matches the 〈username , hash( password )〉 in the database.
Or the client might hash  the password and use that as a shared secret with the server, for instance, by having the server send  a challenge and having the client machine send back hash( challenge |hash( password )).
We discuss  storage of passwords more fully in §9.8 Off-Line Password Guessing .
 5.4.3 Secure Shorthand of Larger Piece of Data  A typical storage device stores blocks of data, typically 8K octets, though the blocks might be vari - able length.
In some cases, such as backing up the contents of all the laptops at a company, a lot of  the data will be the same on all the laptops ( e.g., the software of the operating system).
To save stor - age, a storage system might perform what is known as deduplication , where the storage system  only keeps a single copy of a block and has all references to that block point to the one copy of the  stored block.
 A typical method of performing deduplication is for the storage system to keep a table of  hashes of all the blocks that are stored.
When the storage system receives a block to be stored, it  hashes the block, checks whether that hash is in the deduplication table, and, if so, it does not need to store the block again, although it might need to store a new pointer to that block.
 To save network bandwidth as well as storage, the client might send the hash of a block to the  storage system, and the storage system could respond “already have that” or “send me the block”.
 5.4.4 Hash Chains  A hash chain takes a piece of data D and hashes it many times, e.g., hash500(D), i.e.,  hash(hash( …hash( D)…)).
There are many applications of this concept.
 • Hash chains can make brute-force password guessing slower.
Suppose someone stole the  database of hashed user passwords.
If the server stores hash500(pwd) rather than hash( pwd)
112 C RYPTOGRAP HIC HASHES 5.4.5  for each user, then an attacker that stole the server database and is attempting through brute  force to find potential passwords in the database would have 500 times as much work for  each password.
The performance impact of the client machine needing to compute  hash500(pwd) rather than hash( pwd) when the user is logging in is minimal.
 • Hash chains are used in post-quantum hash-based signatures (see §8.2 Hash-based Signa - tures ).
A signer signs the value n−i by revealing hashn−i(secret ).
 • Hash chains have been used as a way of proving, a limited number of times, that you know a  secret, without needing to use public keys.
A good example of this is Lamport’s hash (see  §9.12 Lamport’s Hash ).
A server Bob is configured with user Alice’s password hashed a lot  of times, e.g., hash1000(pwd), along with n, the number of times Bob believes the password is  hashed (in this case, n=1000).
When Alice attempts to authenticate, Bob sends n, and Alice  replies with her password hashed n−1 times.
Bob takes what he receives from Alice, hashes  it, and if the result matches his database, he replaces hashn(pwd) with the value he received  from Alice (hashn−1(pwd)) and decrements n.  5.4.5 Blockchain  A blockchain is a sequence of blocks, each containing the hash of the previous block.
This data  structure prevents someone from easily tampering with a block in the middle of the chain, because  they would have to recompute all the subsequent blocks.
See §15.3 Bitcoin .
 5.4.6 Puzzles  If a server Bob is being overwhelmed due to a denial of service attack, Bob can slow down requests  from each client by refusing a normal connection request, saying, “Try again, but this time include  a value whose hash has the bottom k bits = x.” He can choose how much to slow a client down by  choosing the number of bits that a client needs to solve.
Bob doesn’t want to remember which puz - zle he sent to which client, and he doesn’t want to give them all the same puzzle, so he’d probably  choose the puzzle to be some sort of secret he knows, hashed with the IP address of the client.
 While the previous paragraph adjusts the computation necessary to solve a puzzle by adjust - ing how many bits of a hash have to match a particular value, a variant (used by cryptocurrencies;  see §15.3.5 Mining ) adjusts how much computation is necessary to find a block by specifying a  maximum value of the hash of a block.
5.4.7 NIFTY THINGS TO DO WITH A HASH 113  5.4.7 Bit Commitment  Suppose Alice and Bob are talking on the phone, and Alice flips a coin to determine which of them  will get to keep the house after their divorce.
Bob will call “heads” or “tails”.
Alice will reveal the  result of the coin flip.
 The problem with that protocol is that if Bob tells Alice which he chooses (say, “heads”),  then Alice can say “It was tails” (regardless of what the coin flip actually was).
If instead, Alice  reveals the result of the coin flip before Bob reveals which he chose and she says that the result was  heads, then Bob can say “I chose heads.”
 Bit commitment is a very clever solution to this dilemma.
Alice reveals a quantity that com - mits her to the coin-flip value but does not reveal to Bob what the value is.
Alice chooses a random  number R and sends the hash of R to Bob.
If the low order bit of R is 0, Alice is committing to  heads, and if 1, to tails.
Alice sends hash( R) to Bob.
Alice can’t change her mind, since she cannot  find two numbers that have the same hash.
Knowing hash( R) does not help Bob choose.
Bob makes  the choice of heads or tails, and then Alice must reveal R (the jargon is that Alice opens the commit - ment ).
 5.4.8 Hash Trees  A single hash value for a large amount of data can be problematic, particularly if only a small piece  of the data needs to be read or written at any time.
Ralph Merkle invented a scheme in 1979 known  as a hash tree or Merkle tree [MERK79] (see Figure 5 -1*).
The idea is that each block of data is  hashed, then groups of hashes are concatenated and hashed, then groups of those hashes are hashed,  data data data data data data data data hash hash hash hash hash hash hash hash hash hash hash hash hash hash hash  Figure 5-1.
Hash Tree  *I3 think the root of the tree should be at the bottom and the leaves at the top, but I2 consulted the Internet and determined  that I3’m wrong.
114 C RYPTOGRAP HIC HASHES 5.4.9  and so on until there is only a single master hash, which is the root of the hash tree.
Given the mas - ter hash, you can verify any block of data if you have the data and all the hash values of siblings  along the path from the data to the root, because this allows you to compute the intermediate hash  values on the path and then compare your final value with the root hash.
Typically, a binary tree is  used so only one auxiliary hash value is needed per level in order to verify a data block.
You still  have to verify that you’ve obtained the actual value of the root hash.
 5.4.9 Authentication  Surprisingly, if there is a shared secret, a hash algorithm can be used in all the ways that secret key  block ciphers are used.
 In §2.2.3 Authentication , we discussed an example of how to use a block cipher for authenti - cation if Alice and Bob share a secret key KAB.
To authenticate Bob, Alice sends a challenge rA,  Bob encrypts rA with KAB, and Alice decrypts what she receives and verifies it matches rA.  Alice Bob  rA encrypted with KAB  If Bob and Alice share a secret KAB, they can authenticate using a hash algorithm instead of a  block cipher.
Since hash algorithms aren’t reversible, it can’t work quite the same way.
However,  the hash function can accomplish the same thing.
Alice still sends a challenge rA. Bob then concat - enates KAB with rA, computes a hash of that, and transmits the result.
Alice can’t “decrypt” the  result.
However, she can do the same computation and check that the result matches what Bob sent.
 Alice Bob  hash(KAB|rA)  5.4.10 Computing a MAC with a Hash  In §4.3 Generating MACs , we described how to compute a MAC with a block cipher, using a secret  KAB that Alice and Bob share.
Can we compute a MAC using a hash function instead of a block  cipher?
We will do roughly the same trick for the MAC as we did for authentication.
We concate - nate a shared secret KAB with the message M and use hash( KAB|M) as the MAC.
 This scheme should work, except for some idiosyncracies of most of the popular hash algo - rithms (including MD4, MD5, SHA-1, SHA2-256, and SHA2-512), which would allow an attacker rA  rA
5.4.10 NIFTY THINGS TO DO WITH A HASH 115  to be able to compute a MAC of a longer message beginning with M, given message M and  hash( KAB|M) .
 Assume hash is a function that acts as described in Figure 5-2.
Assume the input to hash is  KAB|M. Hash will add some padding at the end to make sure the input is an integral number of  blocks, but to make the example simpler, let’s ignore the padding for now.
Hash breaks the input  into blocks.
At each stage, hash uses a function we’ll call compress , which takes two inputs: the  next block of message and the output of the previous stage of the hash (called intermediate hash in  Figure 5-2 ), and outputs a new intermediate value.
The final intermediate hash is the hash of the  message.
 IV  M1 compress  intermediate hash  M2 compress  intermediate hash … … …  hash  M e s s a g e … pad  Figure 5-2.
Hash structure vulnerable to append attack  There’s an attack that we’ll call an append attack .
Let’s assume Carol (who does not know  KAB) would like to send a different message to Bob, and have it look like it came from Alice.
 Assume Carol doesn’t care what’s in most of the message.
She only cares that the end of the mes - sage says P.S. Give Carol a promotion and triple her salary .
Alice has transmitted to Bob some  message M, and hash( KAB|M).
Carol can see both of those quantities.
She concatenates whatever  she likes to the end of M and initializes the intermediate hash computation with hash( KAB|M).
In  other words, in Figure 5-2, she sets IV to the value that Alice sent to Bob as the MAC of M. Carol  does not need to know the shared secret KAB in order to continue the hash computation from the  end of M.  How can we avoid this flaw?
One technique that would work is to use only some of the bits of  the hash as the MAC.
For instance, use the low-order 64 bits of a 256-bit hash.
The attacker doesn’t  have enough information to continue the hash computation (other than correctly guessing the miss - ing 192 bits of the hash).
Using only 64 bits of the hash as a MAC is not any less secure since there
5.4.11 116 C RYPTOGRAP HIC HASHES  is no way, without knowing the secret, that an attacker can calculate or verify the MAC.
The best  that can be done is to generate a random 64-bit MAC for the message you’d like to send and hope  that you’ll be really, really lucky (1 in 264 lucky).
Note that SHA2-384 and (to a lesser extent  because it only truncates by 32 bits) SHA-224 effectively do this because they compute a larger  hash (512 and 256, respectively) and truncate.
 The solution the industry has settled on is HMAC (§5.4.11).
HMAC concatenates the secret  to the front of the message, hashes the combination, concatenates the secret to the front of the hash,  and hashes the combination again.
The actual construction is a little more complicated than this and  is described in §5.4.11.
HMAC has lower performance than truncating the hash because it does a  second hash.
But the second hash is only computed over the secret and a hash, so it does not add  much cost to large messages.
In the worst case, if the message concatenated with the key fit into a  single ( m-bit) block, HMAC would be four times as expensive as using a truncated hash.
However,  if many small messages are to be HMAC’d with the same key, it is possible to reuse the part of the  computation that hashes the key, so that HMAC would only be twice as slow.
With a large enough  message, HMAC’s performance is only negligibly worse.
 We call any hash combining the secret key and the data a keyed hash .
 5.4.11 HMAC  HMAC resulted from an effort to find a MAC algorithm that could be proven to be secure if the  underlying hash’s compression function was secure.
HMAC was proven to have these two proper - ties (given the underlying hash was secure):  • collision resistance (infeasible to find two inputs that yield the same output)  • an attacker who doesn’t know the key K cannot compute HMAC( K,x) for data x, even if they  can see the value of HMAC( K,y), for arbitrarily many values of y not equal to x  In essence, HMAC prepends the key to the data, hashes it, and then prepends the key to the result  and hashes that.
This nested hash with secret inputs to both iterations prevents the extension attacks  that would be possible if you simply hashed the key and message once.
In detail, HMAC takes a  variable-length key and a variable-sized message and produces a fixed-size output that is the same  size as the output of the underlying hash. (
See Figure 5-3 .)
 The number of bits in the output of HMAC is equal to the number of bits in the underlying  hash function.
HMAC also pads differently depending on the blocksize (in bits) of the underlying  hash.
Let’s call the blocksize B. B=512 bits (64 octets) in SHA-1, SHA2-224, and SHA2-256.
 B=1024 bits (128 octets) in SHA2-384 and SHA2-512.
B=1152 bits (144 octets) in SHA3-224.
 B=1088 bits (136 octets) in SHA3-256.
B=832 bits (104 octets) in SHA3-384.
B=576 bits (72  octets) in SHA3-512.
5.4.11 NIFTY THINGS TO DO WITH A HASH 117  If the key is smaller than B bits, HMAC pads it out to B bits by appending 0s.
If the key is  larger than B bits, HMAC first hashes the key, and if the hash is smaller than B bits, HMAC pads  the result out to B bits.
It then ⊕s (bitwise exclusive ors) the padded key with a constant bit string  repeating 00110110 , concatenates it with the message to be protected, and computes a hash.
It ⊕s  the padded key with another constant bit string repeating 01011100 , concatenates that with the  result of the first hash, and computes a second hash on the result.
 key 0  const 2 ⊕ ⊕ const 1  hash message  hash  HMAC(key, message)  Figure 5-3.
HMAC  Note that with HMAC, it is possible to find pairs of keys that result in the same HMAC values.
For  example, if a key K is shorter than B bits, say, 200 bits, then K |0, K|00,… will all result in the same  HMAC of any message.
Likewise, if K is longer than B bits, HMAC first hashes the key, and if the  hash is smaller than B bits, HMAC pads the result out to B bits.
Therefore, K and the hash of K  would be equivalent keys.
This issue is not a problem for most applications, especially if the appli - cation requires a fixed-size key.
 You can use HMAC with SHA-3.
However, since SHA-3 is not vulnerable to the append  attack, there’s no need for the added complication of hashing twice and using the key twice.
NIST  has specified a simple prepend MAC based on SHA-3 (with different padding for domain separa - tion) called KMAC [NIST16].
KMAC has the additional advantage that, in the unlikely event that  the equivalent key property of HMAC is a problem for your application, you don’t have to think  about it, because KMAC does not have the equivalent key problem (unless of course you can find  SHA-3 collisions).
5.4.12 118 C RYPTOGRAP HIC HASHES  5.4.12 Encryption with a Secret and a Hash Algorithm  “Encryption with a hash algorithm is easy!”
you say. “
But let me see you do decryption!”
Hash  algorithms are not reversible, so the trick is to design a scheme in which both encryption and  decryption run the hash algorithm in the forward direction.
The scheme we’ll describe is reminis - cent of CTR mode (§4.2.3) used by block ciphers.
 CTR mode generates a pseudorandom bit stream that can be used as a one-time pad.
Encryp - tion (and decryption) consist of ⊕ing the message with the pseudorandom bit stream.
CTR mode  uses a block cipher to generate the bit stream.
We can just as easily use a hash algorithm to generate  a pseudorandom bit stream.
 Alice and Bob need a shared secret, KAB.
Alice wants to send Bob a message.
She computes  hash( KAB|1).
That gives the first block of the bit stream, b1.
Then she computes hash( KAB|2) and  uses that as b2.
In general, bi is hash( KAB|i).
To encrypt multiple messages with the same key, a  unique IV is needed.
In that case the first block would be hash( KAB|IV|1).
 Note that there are not only many ways of doing this securely, but there are also ways of  doing it insecurely.
For example, if the secret, the IV, and the counter are variable length, concate - nation of these quantities could lead to different triples resulting in the same value to be hashed.
 Alice can generate the bit stream in advance, before the message is known.
Then when Alice  wishes to send the message, she ⊕s it with as much of the generated bit stream as necessary.
 b1 = hash( KAB|1) c1 = p1⊕b1  b2 = hash( KAB|2) c2 = p2⊕b2 … …  bi = hash( KAB| i) ci = pi⊕ bi … …  It is not secure to use the same bit stream twice, so Alice must use a unique key or a unique  IV for each message she sends to Bob.
She must transmit the IV to Bob.
Alice can generate the bit  stream in advance of encrypting the message, but Bob cannot generate the bit stream until he sees  the IV.
As with CTR mode, this mode of encryption gives no integrity protection, so it should be  used with a separate MAC scheme.
5.5 CREATING A HASH USING A BLOCK CIPHER 119  5.5 CREATING A HASH USING A BLOC K CIPHER  Secret key block ciphers do a great job of scrambling their inputs, but they don’t work as hash func - tions because they are reversible.
If you think of a block cipher as a function that takes two inputs  (plaintext and key) and produces one output (ciphertext), a design goal of a good block cipher is  that it is impossible to figure out what key is being used even if you see arbitrary amounts of plain- text and ciphertext.
That suggests that if you supply the quantity to be hashed as the key and  encrypt a constant, the output could be used as a hash of the input.
 The original UNIX password hash did just that.
It used a block cipher, a slightly modified  version of DES, to compute a hash of a password.
It never had to reverse the hash to obtain a pass- word.
Instead, when the user typed a password, UNIX used the same algorithm to hash the entered  quantity and compared the result with the stored password hash.
 The hashing algorithm first converted the password into a secret key.
This key was then used  to encrypt a block of 0s.
The method of turning a text string into a secret key was simply to pack the  7-bit ASCII associated with each of the first eight characters of the password into a 56-bit quantity  into which DES parity was inserted. (
UNIX passwords were allowed to be longer than eight charac - ters, but only the first eight characters were checked, so the passwords PASSWORDqv and  PASSWORDxyz would both work if the user’s password was PASSWORD .)
 A 12-bit random number, known as salt, was stored with the hashed password.
For an expla - nation of why salt is useful, see §9.8 Off-Line Password Guessing .
A modified DES was used  instead of standard DES to prevent hardware accelerators designed for DES from being used to  reverse the password hash.
The salt was used to modify the DES algorithm.
 To summarize, each time a password was set, a 12-bit salt was generated.
The salt was not  chosen by, or visible to, the user.
The first eight characters of the password was converted into a  secret key.
The salt was used to define a modified DES algorithm.
The modified DES algorithm  used the converted password as a key to encrypt a block of 0s.
The result was stored along with the  salt as the user’s hashed password.
 Block ciphers are used internally in many hash algorithms, as we’ll see in §5.6.1.
 5.6 CONSTRUC TION OF HASH FUNCTI ONS  As with encryption systems, hash functions (which can hash arbitrary-length messages) are con - structed from simpler functions that take fixed-length inputs.
The hash function applies the simpler function iteratively over fixed-size chunks of the (padded to an integral number of chunks) mes - sage.
In this section, we just give an intuitive understanding of these algorithms.
If you need to
120 C RYPTOGRAP HIC HASHES 5.6.1  implement them, you will need to refer to the standards.
Or, if you like to read code, you can find  (courtesy of me 3) Python implementations in https://github.com/ms0/crypto .
That code is  intended to be readable rather than efficient, and is also not designed to be secure against  side- channel attacks.
 5.6.1 Construction of MD4, MD5, SHA-1 and SHA-2  All these hash functions have the same structure, known as the Merkle–Damgård construction (see  Figure 5-4), but they have different padding, hash size, block size, and compression functions.
A  compression function takes n bits of input and produces m bits of output, where m < n.  IV  M1 compress  intermediate hash  M2 compress  intermediate hash … … …  hash  M e s s a g e … pad  Figure 5-4.
Merkle–Damgård Structure  A hash algorithm using this construction keeps an m-bit state, where m is the size of the hash  to be produced.
In Figure 5-4, we refer to the state as the intermediate hash.
The intermediate hash  is initialized to an m-bit constant defined by the algorithm.
We’ve called the initial state IV, since it  acts very similarly to an IV in an encryption algorithm.
The output of the final stage is the m-bit  hash.
 The (padded) message is broken into k-bit chunks.
At each stage, the compression function  takes as input the previous m-bit intermediate hash and k bits of the message and outputs the next  m-bit intermediate hash.
 The hash functions (MD4, MD5, SHA-1, and the SHA-2 family) build their compression  algorithm out of an encryption algorithm with a k-bit key and an m-bit blocksize.
The k-bit chunk of  the message is used to encrypt the m-bit intermediate state.
For more detail on the structure of their  encryption algorithms, see §5.8.1 and §5.8.2.
5.6.1 CONSTRUCTION OF HASH FUNC TIONS 121  You might think that the encryption algorithm could be the compression algorithm, but there  is a security problem with doing so.
Assume we have an encryption algorithm with an m-bit block- size.
We would like a brute-force search for a preimage of a particular m-bit hash value v to require  work approximately 2m .
However, if the compression function in Figure 5-4 were just an encryp - tion function, an attack known as the meet-in-the-middle attack (described in the next paragraph)  allows a brute-force search for a preimage to v to only require work proportional to about 2m/2.
 The meet-in-the-middle attack works as follows.
To review, the hash size is m, and the  block size of the message fed in at each stage of the hash is k. The preimage we will find with this  attack is a message that is two blocks (2 k bits) long.
It doesn’t matter how big the block size k is.
 The work required for this attack depends solely on m (the hash size).
The beginning state is con - strained to be the constant value specified as the IV, and the final output is constrained to equal v  (because we want to find a preimage of v).
 Choose 2m/2 random k-bit quantities and encrypt the IV with each one (so you will have 2m/2  encryptions of IV).
Then start with v, choose another 2m/2 random k-bit quantities, and decrypt v  with each.
Now, by the birthday problem, you will probably have a match, i.e., an intermediate hash  that equals the encryption of IV with key 1 and that also equals the decryption of v with key 2.
The  concatenation of key 1 and key 2 would therefore be a preimage of v.  The Davies-Meyer construction (Figure 5-5) defends against the meet-in-the-middle attack.
 IV  M1 key encrypt  ⊕  M2 key encrypt  ⊕ … … …  hash  Figure 5-5.
Davies-Meyer Hash Using Secret Key Cryptography  In the Davies-Meyer construction, the compression function is not simply an encryption function.
 Instead, at each stage, the intermediate state is not only encrypted with the next block of the mes - sage, but also ⊕’d into the output of the encryption function.
122 C RYPTOGRAP HIC HASHES 5.6.2  This prevents the meet-in-the-middle attack.
For each potential intermediate state resulting  from one of the encryptions of IV, the result needs to be compared, not with decryptions of v, but  with decryptions of v ⊕’d with the candidate intermediate state.
In other words, the attacker needs  to try decrypting 2m/2 different values with 2m/2 different keys, making the attacker’s work propor - tional to 2m .
Note that all the hash functions we discuss in this section use a variant of  Davies-Meyer where, instead of ⊕, + is used on each 32- or 64-bit word (throwing away the carry).
 Either operation has the same effect.
 5.6.2 Construction of SHA-3  SHA-3 is based on K ECCAK , the winner of NIST’s SHA-3 Cryptographic Hash Algorithm Compe - tition.
The specification can be found in [NIST15c].
The designers of the chosen algorithm were  Guido Bertoni, Joan Daemen, Michaël Peeters, and Gilles Van Assche.
 The hash algorithms we’ve discussed before take as input an arbitrary-sized input and gener - ate a fixed-length output.
In contrast, KECCAK is much more flexible, and allows both an arbi - trary-sized input and an arbitrary-sized output, as well as having parameters that allow trading off  performance against security.
 NIST standard FIPS 202 defines KECCAK parameter settings for SHA3-224, SHA3-256,  SHA3-384, and SHA3-512 (where the number after the hyphen refers to the number of bits in the  hash).
It also defines parameter settings for two additional functions called SHAKE128 and  SHAKE256.
Both of the SHAKE functions can generate an arbitrary number of hash bits.
The dif - ference is the security strength.
SHAKE128 is equivalent in security to AES128 and is faster than  SHAKE256, which is equivalent in security strength to AES256.
The SHAKE functions are known  as extendable output functions (XOF s).
If instead of a known message, the input were a secret  seed, the SHAKE functions could be used to compute a stream of pseudorandom bits, e.g., for a  stream cipher.
 The central part of the algorithm is called the SPONGE construction, so named because an  arbitrary number of input bits are absorbed and an arbitrary number of output bits are squeezed out.
 Referring to Figure 5-6, the intermediate state is 1600 bits, regardless of the desired size of the hash  or the desired security level.
The 1600 bits are partitioned into two parts, one of size r bits ( rate)  and one of size c bits ( capacity ).
If c is larger, then r will be smaller (because their sum is 1600),  and it will require more stages to compute the hash.
Let’s call the r-sized portion of the state the  r-bits and the c-sized portion of the state the c-bits.
Note that in the standard terminology, the c-bits  are referred to as the inner part , and the r-bits are referred to as the outer part , but we think our ter - minology is clearer.
In SHA-3, the number of c-bits is twice the size of the hash, and r is always  larger than the size of the hash.
In SHAKE128, c is 256, and in SHAKE256, c is 512.
 At each stage, the next r bits of the message are ⊕’d into the r-bits of the state.
Then the  entire 1600 bits of state (both the r-bits and the c-bits) are fed into a function f, which outputs 1600
5.6.2 CONSTRUCTION OF HASH FUNC TIONS 123 … squeeze absorb  O u t p u t …  f R a n d o m  f P s e u d o r bits c bits  0 0  ⊕  f  ⊕  f …  f  ⊕  f  SHA3  hash  … … P ad d e d M es sa g e r bits c bits  SHAKE  Figure 5-6.
SPONGE Construction  bits as the state for the next stage.
Note that both the input and the output to f are 1600 bits, so f is  not a compression function.
It is actually a permutation, a reversible mapping of each possible  value of the input to a unique value of the output.
f is defined as a mathematical function that is fast  to compute in both hardware and software.
 SHA-3 continues until all the bits of the message are absorbed.
Then the top bits of the state  are output as the SHA-3 hash.
The portion of the SPONGE construction that is ⊕ing bits of the mes - sage is known as the absorb phase .
 For SHAKE, the algorithm outputs bits after the message is completely absorbed.
This phase  is known as the squeeze phase .
At each stage in the squeeze phase, the 1600 bits of state are input  into f, and the r-bits of the state are copied into the output.
124 C RYPTOGRAP HIC HASHES 5.7  Notice that the larger the value of c, the more stages will be required to hash the message,  because only r bits of the message will be absorbed at each stage.
Also, during the squeeze phase, if  c is larger, the number of bits copied into the output at each stage ( r) will be smaller.
 If there were no c-bits, it would be easy to find a preimage (see Homework Problem 12).
 5.7 PADDI NG  Hash algorithms break a message into fixed-sized blocks and at each stage input the next block of  the message.
The message is always padded first, even if it is already a multiple of the block size.
 Suppose you simply padded a message with 0s to make it be a multiple of the block size.
In  that case, how would you distinguish a message that was an integral number of blocks, but just hap - pened to end in a 0, from one that was one bit short of an integral number of blocks with one bit of  padding added?
And if it ended in, say, five 0s, how would you know how many of those 0s were  part of the message and how many were padding?
Note that HMAC padding of the key does exactly  that (pad with 0s), and, therefore, it is possible to find different keys that will result in the same  HMAC (see §5.4.11 ).
 5.7.1 MD4, MD5, SHA-1, and SHA2-256 Message Padding  The message to be fed into the hash computation must be a multiple of 512 bits long.
The original  message is padded by adding a 1 bit followed by enough 0 bits to leave the message 64 bits less  than a multiple of 512 bits long.
Then a 64-bit big-endian number representing the bit-length of the  unpadded message is appended to the message. (
See Figure 5-7.)
 1 to 512 bits 64 bits  original message 1000…000 original length in bits  multiple of 512 bits  Figure 5-7.
Padding for MD4, MD5, SHA-1, and SHA2-256  The case that would need the most padding is if the final block of the message were 64 bits  less than a block.
Then there wouldn’t be room for a 1 followed by a 64-bit length, so the final mes - sage block would need to be padded with a 1 followed by 63 0s, and then a new final block would  need to have 448 bits of 0 followed by the 64-bit length.
5.7.1 PADDING 125  Note that since the length field is only 64 bits, the message must be less than 264 bits long.
 We don’t consider that a problem.
Such a message would take over a decade to transmit at a hun- dred gigabits per second.
As our grandmothers would surely have said, had the occasion arisen,  If you can’t say something in less than 264 bits, you shouldn’t say it at all .
 Note that SHA2-512 has a blocksize of 1024, and the padding has 128 bits for the original length.
 So if you really wanted to say something in more than 264 bits (but less than 2128 bits), you could  use SHA2-512.
And if you are nervous about being limited to 2128 bits, you will be pleased to know  that SHA-3 has no limit on message length at all.
 Why is the length field useful in the padding?
Given that these hashes use the Merkle–  Damgård Structure (see Figure 5-4 ), without the length field it would be slightly easier to find a  second preimage , which is a message M2, given a message M1, such that M2’s hash is the same as  M1’s hash.
Let’s assume M1 is, say, 256 blocks long.
That means hashing would produce 256 inter - mediate hashes.
So, if the attacker computes all 256 intermediate hashes for M1, say M1:1,M1:2,…,  M1:256, and if the attacker tries random messages and finds one, say M3, that matches any of these  intermediate hashes, say M1:207, then M2= M3|M1:208 |…|M1:256 would be a second preimage of  the hash of M1.
This means it would be 256 times easier to find a second preimage than a first pre - image, which translates to eight bits less security strength.
 Unfortunately, even with the length field, there is another somewhat more complicated attack  to find a second preimage.
Since finding collisions is so much easier than finding a preimage (2n/2  compared with 2n), we can think of searching for collisions as free.
Ignore padding for now.
Choose  a bunch of single-block and double-block messages at random, search for one of each size whose  SHA-2 hashes (without padding) match, and call the common SHA-2 value H1.
Now use H1 as an  IV, search for a collision of one-block messages and three-block messages, and call the common  SHA-2 value H2.
Now use H2 as an IV, search for a collision of single-block messages and  five-block messages, and call the common SHA-2 value H3.
With these three colliding pairs, you  can construct a message with anywhere from three to ten blocks with a hash of H3.
If you keep  doing this up to H8, you will be able to construct a message of any length from 8 to 263 blocks long  with hash H8 (by concatenating the correct-length messages from the collisions).
Now, as in the  attack without the length field, you can search for a block that maps from H8 to any of the interme - diate hashes of the message for which you’d like to find a second preimage (other than the last  eight).
If you can find one, you can construct a message that is the appropriate length with the cor - rect hash.
 So the length field in the padding doesn’t provide significant protection against any known  threat.
It does, however, allow for a nicer security proof.
In particular, with length padding it’s pos - sible to prove that the hash function is collision resistant as long as the compression function is col - lision resistant.
 In contrast, without the length padding, it’s possible to create a plausible-looking hash func - tion with an adversarially chosen IV that makes it easy to find collisions in the hash function
126 C RYPTOGRAP HIC HASHES 5.7.2  without finding collisions in the compression function.
To do this, use a Davies-Meyer compression  function (Figure 5 -5) and set the IV to the decryption of 0, using some message block M as a key.
 Then any message of the form M|X will have the same hash as M|M|X or M|M|M|X or ….
 5.7.2 SHA-3 Padding Rule  SHA-3 and SHAKE do padding differently than earlier hashes.
 • There is no length field.
The length field is unnecessary in SHA-3 because an attacker  attempting to find a second preimage would have to find a collision on the c-bits, which are  set in the SHA-3 standard to be twice as long as the full hash.
 • The pad field ends with 1 rather than 0.
This prevents someone from constructing two strings  where the SHA3-256 value of the first string equals the SHA3-512 value of the second string  (this applies to any two SHA3 lengths or SHAKE strengths).
 • The SHA-3 specification inserts an extra field between the message and the padding (that we  will lump in with the padding) that they call the domain separator .
This is intended to pre - vent a SHAKE output from having the same value as a SHA-3 hash.
The domain separator  for SHA-3 is 01.
The domain separator for SHAKE is 1111 .
They could have accomplished  the same thing by using a different initial state, but instead, SHA-3 (and SHAKE) always  start with the initial state equal to zero.
One might wonder why they didn’t use a single bit to  distinguish SHA-3 and SHAKE, but they are leaving room in the domain separator for distin - guishing potential additional uses of KECC AK.
 The padding after the domain separator consists of 100…001, where the number of 0s is between  0 and r−1, inclusive. (
Remember the state length is 1600 bits, consisting of some number of r-bits,  and the remainder being c-bits.
Given that c can be between 256 and 1024, r will be between 576  and 1344). (
See Figure 5 -8 and Figure 5-9.)
 m bits 4 to r+3 bits  unpadded bitstring 01100 …001  multiple of r bits  Figure 5-8.
Padding for SHA-3
5.8 THE INTERNAL ENCRYPTION ALGORITHM S 127  m bits 6 to r+5 bits  unpadded bitstring 1111100 …001  multiple of r bits  Figure 5 -9.
Padding for SHAKE  5.8 THE INTERNA L ENCRYP TION ALGOR ITHMS  Recall that SHA-1 and SHA-2 use the Davies-Meyer construction (Figure 5-5), which incorporates  an encryption function.
Although the encryption functions used by SHA1 and SHA2 differ, they are  both Feistel-like (see §3.3.3 Feistel Ciphers ) in that each round is reversible.
MD4 and MD5 are  similar, but with 128 bits instead of 160 and having fewer rounds.
But our description will only be  for SHA-1 and SHA-2.
 5.8.1 SHA-1 Internal Encryption Algorithm  The exact details are not important (unless you need to implement it, of course), but the structure is  rather interesting.
SHA-1’s hash is 160 bits, so it has a 160-bit intermediate state.
It treats the state  as five 32-bit words (labeled v0, v1, v2, v3, v4 in the diagram).
We’ll call the five words in the output  of a round w0, w1, w2, w3, w4. (
They become the vis for the next round.) (
See Figure 5-10.)
 ⊕ 16 words  of message 160-bit intermediate hash value  v0 v1 v2 v3 v4 gener ated datacomplicated  function  w0 w1 w2 w3 w4 ↵ 30 ↵ 1  +  Figure 5-10.
Inner Loop of SHA-1—80 Iterations per Block  The internal encryption algorithm inputs a 512-bit chunk of the message that is used to gener - ate eighty 32-bit per-round keys (because there are eighty rounds, and each round uses a 32-bit  key).
It treats the 5-word value ( v0, v1, v2, v3, v4) as the quantity to be encrypted.
128 C RYPTOGRAP HIC HASHES 5.8.2  The first sixteen round keys are simply the sixteen 32-bit words of the 512-bit message  chunk.
After that, the 32-bit key for round i is generated as the ⊕ of four previous 32-bit keys ( i-16,  i-14, i-8, and i-3), and then rotated left one bit.
 Each of the eighty rounds uses a complicated nonreversible function, which takes as input the  next 32-bit key and v0, v1, v2, and v3.
Note how the Feistel-like structure enables each round to be  reversible even though the complicated function is not.
Notice that starting with the output of a  round ( w0, w1, w2, w3, w4), the input values v0, v1, v2, and v3 are trivially derived from w1, w2, w3,  w4 by simply being copied into a different position, though when v1 is copied into the output posi - tion w2, it is rotated right by 2 bits (or equivalently, rotated left by 30 bits).
 So how can we recover v4 from w0, w1, w2, w3, w4?
We know all the inputs to the complicated  function (the 32-bit round key, v0, v1, v2, and v3) so we can calculate the complicated function in the  forward direction, and if we subtract the result from w0, we will get v4.
 If you are curious about the insides of the complicated function, it takes as input the 32-bit  quantities v0, v1, v2, v3, the round key, and a per-round constant.
The per-round constant is the same  for the first twenty rounds, then a different constant is used in the next twenty rounds, and so on, so  there are four different constants.
And to show nothing nefarious was done by choosing the con - stants, the four constants are the largest integer smaller than 230 times the square root of 2, 3, 5, and  10, respectively.
 The complicated function adds v0 (but rotated left 5 bits), 32 bits of key, the 32-bit constant,  and then a round-specific function of v1, v2, and v3 that is the same for each set of twenty rounds,  but then changes to a different function for the next twenty rounds.
The round-specific function is  either a bitwise ⊕ of v1, v2, and v3, or a bitwise majority, or a bitwise selection.
Majority means that  if two or three input bits are 1, then the output bit is 1, else the output bit is 0.
Selection means that  if the bit in v1 is 0, then the output is the corresponding bit in v2; otherwise, the output is the corre - sponding bit in v3.
 5.8.2 SHA-2 Internal Encryption Algorithm  SHA-2 is actually a family of algorithms producing different hash sizes.
SHA2-512 differs from  SHA2-256 in the number of rounds in the internal encryption algorithm (sixty-four for SHA2-256,  eighty for SHA2-512), and the sizes of the blocks are doubled in SHA2-512.
The intermediate state  in both consists of eight words ( v0, v1, v2, v3, v4, v5, v6, v7) with the state at the output of the round  being eight words ( w0, w1, w2, w3, w4, w5, w6, w7).
SHA2-256 uses 32-bit words; SHA2-512 uses  64-bit words.
SHA2-256 processes 512-bit chunks at each stage; SHA2-512 processes 1024-bit  chunks. (
See Figure 5-11.)
 The key schedule takes the 16-word message block and expands it with 48 more words (for  SHA2-256) or 64 more words (for SHA2-512).
5.9 SHA-3 F FUNCTI ON (ALSO KNOWN AS KECCAK -F) 129  v0 v1 v2 v3 v4 v5 v6 v7  complex  function1 complex  function2  + + Kt 16-word  message  block  Wt 48-or 64-word  expansion  w0 w1 w2 w3 w4 w5 w6 w7  Figure 5-11.
Inner Loop of SHA-2—64 or 80 Iterations per Block  Now, referring to Figure 5-11, just as with SHA-1, the Feistel-like structure allows reversing  a round, i.e., computing v0, v1, v2, v3, v4, v5, v6, v7 if you know w0, w1, w2, w3, w4, w5, w6, w7 and  the per-round key.
Note that all the vi s other than v3 and v7 are easily derived from the wi s, because  v0, v1, v2, v4, v5, v6 are simply copied (shifted by a word).
The tricky part is deriving v3 and v7.
 There are two nonreversible complex functions.
Complex function 1 takes as input v0, v1, v2.
 We know all those inputs, so we can compute complex function 1.
 Complex function 2 takes as input v4, v5, v6, which we also know, and the per-round key, and  a per-round constant.
So we can also compute complex function 2.
 Now we can compute v7 by subtracting the outputs of the two complex functions from w0.
 Now that we know v7, we can compute v3 by subtracting v7 and the output of complex function 2  from w4.
 If you’re still awake, you’ll notice that we haven’t described what’s inside the two complex  functions.
As with the SHA-1 functions, they are easy to compute (constructed out of ⊕, majority,  rotation, +, and selection).
Each complex function takes three of the vi s as input, and complex func - tion 2 adds in the per-round constant K and the round key Wt.t  5.9 SHA-3 f Function (Also Known as KECCAK -f)  In §5.6.2 Construction of SHA-3 , we described most of the operation of the SHA-3 (and SHAKE)  families.
We did not say what happens inside the function f (see Figure 5 -6).
The f function is a  one-to-one mapping of 1600-bit inputs to 1600-bit outputs.
In theory, this could be done with a sin- gle table lookup in a table with 21600 entries, but that would be an impractically large table.
130 C RYPTOGRAP HIC HASHES 5.9  As discussed in §3.7.2 , a common component of both secret key encryption algorithms and  hashes consists of alternating operations (usually called layers), where in each round, the input bits  are partitioned into small groups, and each group is operated on with a nonlinear operation (often  referred to as an S-box).
Then a linear layer spreads the bits around and mixes them with one  another so that the output bits of each S-box affect the inputs of multiple S-boxes in the next round.
 After enough rounds, every bit will influence all the bits of output.
 The design philosophy of the f permutation in SHA-3 is broadly similar to that of AES.
In  fact, one of the designers of AES, Joan Daemen, was also on the KECCAK design team.
The func - tion f is a one-to-one mapping of 1600-bit inputs to 1600-bit outputs, using twenty-four rounds  each comprising one substitution (nonlinear) operation and four linear operations.
All these opera - tions are reversible, so it’s easy to show f is one-to-one, which makes analysis of the algorithm  easier.
 The function f is called each time the next r-bit block of message is processed (in the absorp - tion phase) or each time r bits are output (during the squeezing phase).
The 1600 bits can be visual - ized as a three-dimensional array of 5 ×5×64 bits.
The SHA-3 specification [NIST15c] has lots of  pretty pictures, intended to help people visualize what the bit transformations are.
f is made up of  twenty-four nearly identical rounds.
Each round performs five bit-scrambling operations, known as  θ (theta), ρ (rho), π (pi), χ (chi), and ι (iota), which are done in that order.
 The three dimensions in the 5 ×5×64 array of bits are known as rows, columns, and lanes.
The  rows and columns are five bits long, and the lanes are 64 bits long.
Why are the bits organized this  way?
The reason is that 64-bit processors naturally organize data into 64-bit words, so the obvious  way to implement this data structure is as a 5 ×5 array of words (where each lane is represented by a  64-bit word).
In this arrangement, all the component functions of the round can be efficiently  implemented using common CPU instructions—in particular, bitwise ⊕ (exclusive or), bitwise ∧  (and), and bit rotation operations.
 Let’s look at the five component functions ( θ, ρ, π, χ, and ι) in order.
 θ is probably the least intuitive of the five operations.
A parity bit is computed for each col - umn by ⊕ing together the five bits in the column, and these parity bits are ⊕’d into all the bits in  two adjacent columns.
The point of this operation is to increase the number of bits that each bit can  affect by the end of the round.
θ is a linear transformation since each output bit is the ⊕ of eleven  specific input bits.
The fact that these eleven bits are all in different lanes and all in different rows  helps supplement the mixing provided by the other functions.
 In contrast, the ρ and π operations (also linear) really are what we would call bit shuffles  (since each output bit is determined by a single input bit).
They do not change any bits; they just  rearrange them.
The ρ operation rotates each of the 25 lanes, each by a different number of bits.
 The Greek letter ρ was presumably chosen because it sounds like the first syllable of “rotate”.
The  π operation takes each of the 25 bits in each 5 ×5 slice, and moves it to a new specified location  within the slice.
Together, ρ and π thoroughly mix the bits around in all three dimensions, though  the 64 bits in a lane remain together in a lane (albeit rotated).
5.9 SHA-3 F FUNCTI ON (ALSO KNOWN AS KECCAK -F) 131  χ is the only nonlinear operation.
It acts independently on each of the 320 5-bit rows, so it is  an S-box with 5-bit inputs and outputs.
Each bit in the row is affected by the two bits that follow  according to the formula b1=b1⊕ (~b2∧ b3).
In processing bits at the end of the row, the row is con - sidered to wrap around.
Since this S-box performs exactly the same operations on each of the 64  5×5 sheets in the 1600-bit state, it can be implemented efficiently by performing bitwise ∧ and ⊕  operations on the 25 lanes of the state.
This implementation strategy is called bit-slicing the S-box  layer.
This stands in contrast to the design strategy for the AES S-box.
The AES S-box was initially  intended to be implemented by table lookup, but it was soon found that due to the behavior of CPU  caches, a table lookup took a variable amount of time in a way that can allow side-channel attackers  to get secret information by carefully observing how long it took the encryptor and decryptor to  perform cryptographic operations.
 The ι function uses a 7-bit per-round constant.
It ⊕s those bits into seven particular bits in the  1600-bit structure.
The value of this per-round constant is the only difference between the opera - tions performed in the twenty-four rounds of the f permutation.
The ι function is linear.
 For the exact bit transformations of the five functions, we refer the reader to [NIST15c].
The  characteristics of these functions are:  • With θ, each bit affects ten nearby bits.
 • With π and ρ, bits are only rearranged and do not affect other bits.
 • With χ, each bit affects only two nearby bits and does so with probability only ½.  • ι touches at most seven bits in the whole structure in each round, and usually only about ½ of  those bits are flipped in a round.
Which bits are flipped depends only on the round number.
 If all the input bits were 0s, ι is the only function that can create any 1s, and it can create at most  seven of them in any given round.
It may seem surprising that the mixing is so thorough in a mere  twenty-four rounds, but if each bit can affect 32 others in each round, the compounding adds up.
In  fact, nine rounds are enough to avoid all known attacks costing less than 2512 operations.
132 C RYPTOGRAP HIC HASHES 5.10  5.10 HOMEWO RK  4: Lizard 1.
Draw the Merkle tree associated with a file containing eight data blocks ( b1,b2,…,b8).
Which  items do you need to know in order to verify that b3 has not been modified?
If you need to  modify b3, which items in the Merkle tree need to be modified?
 2.
Suppose Alice and Bob want to play Rock Paper Scissors Lizard Spock * (Figure 5-12).
 • Scissors cuts Paper 0: Rock • Paper covers Rock  • Rock crushes Lizard  • Lizard poisons Spock  • Spock smashes Scissors  • Scissors decapitates Lizard  • Lizard eats Paper  • Paper disproves Spock  • Spock vaporizes Rock  • Rock crushes Scissors  Figure 5 -12.
 Rock Paper Scissors Lizard Spock 1: Paper  Suppose they are talking on walkie talkies, where they have to take turns talking.
If Alice tells  Bob her choice, Bob can simply make a choice that beats Alice’s choice.
What protocol can  they use so that neither of them can cheat?
 3.
Why do SHA-1 and SHA-2 require padding for messages that are already a multiple of 512  bits?
 4.
Open-ended project: Implement one or more of the hash algorithms and test how “random”  3: Spock  the output appears.
For example, test the percentage of 1 bits in the output or test how many  2: Scissors  bits of output change with minor changes in the input.
Also, design various simplifications of  the hash functions (such as reducing the number of rounds) and see how these change things.
 5.
What are the minimal and maximal amounts of padding that would be required in each of the  hash functions?
 6.
In section §5.7.1, we stated that without a length field and if the attacker can choose the IV,  “any message of the form M|X will have the same hash as M|M|X or M|M|M|X”.
Assume a  hash that follows the Davies-Meyer construction ( Figure 5-5) and that the attacker can choose  the IV.
Explain why the attacker can find an IV and a block M such that repeating M will  result in collisions.
For example, what does the IV need to be?
What will the output of the  encrypt function be?
What will be the input of the second encrypt function be?
 *Rock Paper Scissors Spock Lizard was invented by Sam Kass and Karen Bryla.
After it was popularized by the TV show  Big Bang Theory , which called it Rock Paper Scissors Lizard Spock , most everyone uses that name.
5.10 HOMEWORK 133  7.
Assume a secure 128-bit hash function and a 128-bit value d. Suppose you want to find a  message that has a hash equal to d. Given that there are many more 2000-bit messages that  map to a particular 128-bit hash than 1000-bit messages, would you theoretically have to test  fewer 2000-bit messages to find one that has a hash of d than if you were to test 1000-bit mes - sages?
 8.
For you statistics fans, calculate the mean and standard deviation of the number of 1 bits in a  randomly chosen 256-bit number.
 9.
For purposes of this exercise, we will define random as having all elements equally likely to  be chosen.
So a function that selects a 100-bit number will be random if every 100-bit  number is equally likely to be chosen.
Using this definition, if we look at the function  x+ y mod 2100 , then the output will be random if at least one of x and y is random.
For  instance, y can always be 51, and yet the output will be random if x is random.
For the follow - ing functions, find sufficient conditions for x, y, and z under which the output will be random:  • ∼ x  • x⊕ y  • x∨ y  • x∧ y  • The choice function: Ch( x,y,z) = (x∧y) ∨ (∼x∧ z)  • The majority function: Maj( x,y,z) = (x∧ y) ∨(y∧ z) ∨(z∧ x)  • The parity function: Parity( x,y,z) = x⊕ y⊕z  • y⊕ (x∨∼z)  10.
Prove that the function ( x∧y)⊕ (x∧z)⊕(y∧z) and the function ( x∧y)∨ (x∧z)∨(y∧ z) are  equivalent. (
Sorry—this isn’t too relevant to cryptography, but we’d stumbled on two differ - ent versions of the majority function in different documentation, and we had to think about it  for a bit to realize they were the same.
We figured you should have the same fun.)
 11.
In §5.4.10 Computing a MAC with a Hash , we describe the append attack and listed the algo - rithms MD4, MD5, SHA-1, SHA2-256, and SHA2-512 as being vulnerable to this attack.
 Why aren’t SHA2-224 and SHA2-384 vulnerable to this attack?
 12.
Show how to compute a preimage of a hash in a modified form of SHA-3, where there are no  c-bits.
In other words, assume r=1600 and c=0, but otherwise, the algorithm is the same.
 13.
Show how in SHA-3 the work factor for finding a preimage is dependent on the size of c.  14.
Suppose Alice and Bob share a key KAB, and Alice uses, as a MAC for message M to Bob,  SHA-3( M|KAB).
Would this be vulnerable to the append attack discussed in §5.4.10?
 15.
In §5.8.1 SHA-1 Internal Encryption Algorithm , we mention three bitwise operations, ⊕,  majority, and selection.
Write the truth table for these three functions assuming the three
134 C RYPTOGRAP HIC HASHES 5.10  inputs are each one bit long.
Now what would the output be if the inputs a, b, and c are 32 bits  long, in these cases:  • Each of a, b, c is all 0s  • Each of a, b, c is all 1s  • a, b, and c are random numbers (meaning that for each of them, about half the bits are 0)  What is the probability of 0 versus 1 in a specified bit of the output under each of these  functions?
6 FIRST-GENERA TION PUBLIC  KEY ALGORITHMS  6.1 INTRODUC TION  This chapter describes the most common public key algorithms that have been deployed as of 2022  and that will probably remain the most common public key algorithms in use for several years.
As  we will show in Chapter 7 Quantum Computing , if someone could build a quantum computer of  sufficient size, the algorithms in this chapter would no longer be secure.
The world will soon be  converting to different public key algorithms (see Chapter 8 Post-Quantum Cryptography ).
But the  current algorithms, the focus of this chapter, are widely deployed and fascinating to understand.
 Public key algorithms are a motley crew.
All the hash algorithms do the same thing—they  take a message and perform an irreversible transformation on it.
All the secret key algorithms do  the same thing—they take a block and encrypt it in a reversible way, and there are modes (Chapter  4 Modes of Operation ) to convert the block ciphers into message ciphers.
But public key algorithms  look very different from each other not only in how they perform their functions but in what func - tions they perform.
The post-quantum algorithms are also public key algorithms, but we will defer  discussion of them until after we’ve discussed quantum computers in Chapter 7 Quantum Comput - ing.
In this chapter, we’ll describe:  • RSA, which does encryption and digital signatures  • ElGamal and DSA and ECDSA (elliptic curve DSA), which do digital signatures  • Diffie-Hellman and ECDH (elliptic curve Diffie-Hellman), which establish a shared secret  and can also do encryption  The thing that all public key algorithms have in common is the concept that a principal has a pair of related quantities, one private and one public.
 As we discussed in §2.7 Numbers , cryptographic algorithms require a type of mathematics in  which numbers can be represented exactly and with a guaranteed upper limit on the number of bits  required to represent them.
Most of them use modular arithmetic.
We introduced modular arith - metic in §2.7.1 Finite Fields , but we’ll go into a bit more detail here.
 135
136 F IRST-GENERATI ON PUBLIC KEY ALGORIT HMS 6.2  6.2 MODULAR ARITHMETIC  There was a young fellow named Ben  Who could only count modulo ten.
 He said, “When I go  Past my last little toe I shall have to start over again.”
 —Anonymous  Modular arithmetic uses the non-negative integers less than some positive integer n, performs ordi - nary arithmetic operations such as addition and multiplication, and then replaces the result with its  remainder when divided by n. The result is said to be modulo n or mod n. When we write “ x mod  n”, we mean the remainder of x when divided by n. Sometimes we’ll leave out “mod n” when it’s  clear from context.
 6.2.1 Modular Addition  Let’s look at mod 10 addition.
3 + 5 = 8, just like in regular arithmetic.
The answer is already  between 0 and 9.
In regular arithmetic, 7 + 6 = 13, but the mod 10 answer is 3.
Basically, one can  perform mod 10 arithmetic by using the last digit of the answer.
For example,  5 + 5 = 0 3 + 9 = 2 2 + 2 = 4 9 + 9 = 8  Let’s look at the mod 10 addition table (Figure 6-1 ).
 + 0 1 2 3 4 5 6 7 8 9  0  1 2 3 4  5  6 7 8 9 0 1 2 3 4 5 6 7 8 9  1 2 3 4 5 6 7 8 9 0  2 3 4 5 6 7 8 9 0 1  3 4 5 6 7 8 9 0 1 2  4 5 6 7 8 9 0 1 2 3  5 6 7 8 9 0 1 2 3 4  6 7 8 9 0 1 2 3 4 5  7 8 9 0 1 2 3 4 5 6  8 9 0 1 2 3 4 5 6 7  9 0 1 2 3 4 5 6 7 8  Figure 6-1.
Addition Modulo 10  Addition of a constant mod 10 can be used as a scheme for encrypting digits, in that it maps each  decimal digit to a different decimal digit in a way that is reversible; the constant is our secret key.
6.2.2 MODULAR ARITHMET IC 137  It’s not a good cipher, of course, but it is a cipher. (
It’s actually a Caesar cipher.)
Decryption would  be done by subtracting the secret key modulo 10, which is an easy operation—just do ordinary sub - traction, and if the result is less than 0, add 10.
 Just like in regular arithmetic, subtracting x can be done by adding −x, also known as the  additive inverse of x. The additive inverse of x is the number you’d add to x to get 0.
For example,  4’s additive inverse will be 6, because in mod 10 arithmetic 4 + 6 = 0.
If the secret key were 4, then  to encrypt we’d add 4 (mod 10), and to decrypt we’d add 6 (mod 10).
 6.2.2 Modular Multiplication  Now let’s look at the mod 10 multiplication table ( Figure 6 -2).
 × 0 1 2 3 4 5 6 7 8 9  0  1 2 3  4  5 6 7 8 9 0 0 0 0 0 0 0 0 0 0  0 1 2 3 4 5 6 7 8 9  0 2 4 6 8 0 2 4 6 8  0 3 6 9 2 5 8 1 4 7  0 4 8 2 6 0 4 8 2 6  0 5 0 5 0 5 0 5 0 5  0 6 2 8 4 0 6 2 8 4  0 7 4 1 8 5 2 9 6 3  0 8 6 4 2 0 8 6 4 2  0 9 8 7 6 5 4 3 2 1  Figure 6-2.
Multiplication Modulo 10  Multiplication by 1, 3, 7, or 9 works as a cipher (again, not a secure cipher), because it performs a  one-to-one substitution of the digits.
But multiplication by any of the other numbers will not work  as a cipher.
For instance, if you tried to encrypt by multiplying by 5, half the numbers would  encrypt to 0, and the other half would encrypt to 5.
You’ve lost information.
You can’t decrypt the ciphertext 5, since the plaintext could be any of {1, 3, 5, 7, 9}.
So multiplication mod 10 can be  used for encryption provided that you choose the multiplier wisely.
But how do you decrypt?
Well,  just like with addition, where we undid the addition by adding the additive inverse, we’ll undo the  multiplication by multiplying by the multiplicative inverse.
In ordinary (real or rational numbers)  arithmetic, x’s multiplicative inverse is 1 /x.
If x is an integer, then its multiplicative inverse is a frac - tion.
In modular arithmetic, though, the only numbers that exist are integers.
The multiplicative  inverse of x (written x −1) is the number by which you’d multiply x to get 1.
Only the numbers {1, 3,  7, 9} have multiplicative inverses mod 10.
For example, 7 is the multiplicative inverse of 3.
So  encryption could be performed by multiplying by 3, and decryption could be performed by multi - plying by 7.
9 is its own inverse.
And 1 is its own inverse.
Multiplication mod n is not a secure
138 F IRST-GENERATI ON PUBLIC KEY ALGORIT HMS 6.2.3  cipher, but it works in the sense that we can scramble the digits by multiplying by x and get back to  the original digits by multiplying by x −1 (assuming the “key” is a number with a multiplicative  inverse mod n).
 It is by no means obvious how you find a multiplicative inverse in mod n arithmetic,  especially if n is very large.
For instance, if n was a 100-digit number, you would not be able to do  a brute-force search for an inverse.
But it turns out there is an algorithm (see §2.7.5 Euclidean Algo - rithm ) that will efficiently find multiplicative inverses mod n. Given x and n, it finds the number y  such that x×y mod n = 1 (if there is one).
 What’s special about the numbers {1, 3, 7, 9}?
Why is it they’re the only ones, mod 10, with  multiplicative inverses?
The answer is that those numbers are all relatively prime to 10.
Relatively  prime means they do not share any common factors other than 1.
For instance, the largest integer  that divides both 9 and 10 is 1.
The largest integer that divides both 7 and 10 is 1.
In contrast, 6 is  not one of {1, 3, 7, 9}, and it does not have a multiplicative inverse mod 10.
It’s also not relatively  prime to 10 because 2 divides both 10 and 6.
In general, when we’re working mod n, all the num - bers relatively prime to n will have multiplicative inverses, and none of the other numbers will.
 Mod n multiplication by any number x relatively prime to n will work as a cipher because we can  multiply by x to encrypt, and then multiply by x −1 to decrypt.
Again let us hasten to reassure you  that we’re not claiming it’s a good cipher in the sense of being secure.
What we mean by its being a  cipher is that we can modify the information through one algorithm (multiplication by x mod n) and  then reverse the process (by multiplying by x −1 mod n).
 How many numbers less than n are relatively prime to n?
Why would anyone care?
Well, it  turns out to be so useful that it’s been given its own notation— φ(n).
φ is called the totient function ,  supposedly from total and quotient .
How big is φ(n)?
If n is prime, then all the integers  {1, 2,… n−1} are relatively prime to n, so φ(n) = n−1.
If n is a product of two distinct primes, say p  and q, then there are ( p−1)(q−1) numbers relatively prime to n, so φ(n) = (p−1)(q−1).
Why is that?
 Well, there are n = pq total numbers in {0, 1, 2,… n−1}, and we want to exclude those numbers that  aren’t relatively prime to n. Those are the numbers that are either multiples of p or of q. There are p  multiples of q less than pq and q multiples of p less than pq.
So there are p+q−1 numbers less than  pq that aren’t relatively prime to pq. (
We can’t count 0 twice!).
Thus φ(pq) = pq − (p+q−1) =  (p−1)(q−1).
 6.2.3 Modular Exponentiation  Modular exponentiation is again just like ordinary exponentiation.
Once you get the answer, you  divide by n and get the remainder.
For instance, 46 = 6 mod 10 because 46 = 4096 in ordinary arith - metic, and 4096 = 6 mod 10.
Let’s look at the exponentiation table mod 10.
We are purposely put - y+nting in extra columns because in exponentiation, xy mod n is not the same as x mod n. For  instance, 31 = 3 mod 10, but 311 = 7 mod 10 (it’s 177147 in ordinary arithmetic).
6.2.4 MODULAR ARITHMET IC 139  Let’s look at mod 10 exponentiation ( Figure 6 -3).
 yx 0 1 2 3 4 5 6 7 8 9 10 11 12  0  1 2 3  4  5 6 7 8 9 0 0 0 0 0 0 0 0 0 0 0 0  1 1 1 1 1 1 1 1 1 1 1 1 1  1 2 4 8 6 2 4 8 6 2 4 8 6  1 3 9 7 1 3 9 7 1 3 9 7 1  1 4 6 4 6 4 6 4 6 4 6 4 6  1 5 5 5 5 5 5 5 5 5 5 5 5  1 6 6 6 6 6 6 6 6 6 6 6 6  1 7 9 3 1 7 9 3 1 7 9 3 1  1 8 4 2 6 8 4 2 6 8 4 2 6  1 9 1 9 1 9 1 9 1 9 1 9 1  Figure 6-3.
Exponentiation Modulo 10  Note that exponentiation by 3 would act as an encryption of the digits in that it rearranges all the  digits.
Exponentiation by 2 would not, because both 22 and 82 are 4 mod 10.
 How would you decrypt if exponentiation was used to encrypt?
Is there an exponentiative  inverse like there is a multiplicative inverse?
Just like with multiplication, the answer is sometimes .
 (Note we invented the term exponentiative inverse , because it’s useful, it’s obvious what it means,  and there really is no other word.
Mathematicians might wince if you use the term, though.)
 6.2.4 Fermat’s Theorem and Euler’s Theorem  p−1Fermat’s theorem states that if p is prime and 0 <a <p, a =1 mod p. Euler generalized Fermat’s  theorem to non-prime moduli.
Euler’s theorem states that for any a relatively prime to n,  φ(n)a =1 mod n.  The implication is that if we can find two numbers e and d that are multiplicative inverses  mod φ(n), they will be exponentiative inverses mod n, meaning that if we exponentiate x mod n by  e, and then exponentiate the result mod n by d, we’ll wind up with x mod n.  The Euclidean algorithm (§2.7.5) allows us to calculate multiplicative inverses mod n. There  is no efficient algorithm for finding exponentiative inverses mod n directly.
However, if we know  how to factor n, we will be able to calculate φ(n), and we can then find multiplicative inverses mod  φ(n).
Therefore, if we know how to factor n, we will be able to calculate exponentiative inverses  mod n, but if we do not know how to factor n, we will not be able to calculate φ(n), and we will not  be able to efficiently find an exponentiative inverse mod n.  Armed with this knowledge, let’s look at RSA.
140 F IRST-GENERATI ON PUBLIC KEY ALGORIT HMS 6.3  6.3 RSA  RSA is named after its inventors, Rivest, Shamir, and Adleman.
It is a public key cryptographic  algorithm that does encryption, decryption, signature generation, and signature verification.
The  key length is variable.
Anyone using RSA can choose a long key for enhanced security or a short  key for efficiency.
The most commonly used key length for RSA is 2048 bits.
 The block size in RSA (the chunk of data to be encrypted) is also variable.
The plaintext  block must be smaller than the key length.
The ciphertext block will be the length of the key.
RSA  is much slower to compute than popular secret key algorithms like AES.
As a result, RSA is not  used for encrypting long messages.
Instead, RSA is used to encrypt a secret key, and then secret key  cryptography is used to actually encrypt the message.
Likewise, instead of signing a large message,  RSA is used to sign a hash of the message.
 6.3.1 RSA Algorithm  First, you need to generate a public key and a corresponding private key.
Choose two large primes p  and q (probably around 1024 bits each).
Multiply them together and call the result n. The factors p  and q will remain secret.
The modulus n will be part of your public key, so people will know n.  However, you won’t tell anybody the factors p and q, and it’s practically impossible to factor num - bers as large as a 2048-bit n.  You’ve now chosen the modulus n. To generate the rest of your public key, choose a number e  that is relatively prime to φ(n).
Since you know p and q, you can easily compute φ(n)—it’s  (p−1)(q −1).
Your public key is the pair 〈e,n〉.
 To generate your private key, use the extended Euclidean algorithm to find the number d that  is the multiplicative inverse of e mod φ(n).
Your private key is 〈d,n〉.
Someone that sees your public  key 〈e,n〉, will not be able to calculate your private key unless they can factor n.  Note that there is an optimization.
When Alice is computing her two exponents e and d, she  can compute e’s multiplicative inverse, d, mod λ(n) rather than φ(n).
The quantity λ(n) is the small - est integer x such that for every integer a with 1< a<n, ax mod n = 1.
With an RSA number n being  the product of two odd primes p and q, λ(n) will be at most ( p−1)(q −1)/2 and might be smaller if  p−1 and q−1 have other common factors.
The d she’d compute using φ(n) may be different from the  d she’d compute using λ(n), but either d will be an exponentiative inverse mod n of her chosen  exponent e. So that means that for a given e, there will be at least two exponentiative inverses  mod n.  To encrypt a quantity m, someone using your public key should compute ciphertext c =  me mod n. (Again, the quantity m being encrypted will usually not be an entire message, but will be  a single block that contains the AES key that will be used to encrypt a long message.)
Only you,
6.3.2 RSA 141  using your private key, will be able to decrypt c, by computing m = cd mod n. Also, only you can  create a signature s for a quantity m, using your private key, by computing s = md mod n. Anyone  can verify your signature by checking that m = se mod n. (Again, you would be signing a hash of a  message, and not signing a long message.)
That’s all there is to RSA.
Now, there are some ques - tions we should ask:  • Why does it work?
Will decrypting an encrypted message get the original message back?
 • Why is it secure?
Given e and n, why can’t someone easily compute d?
 • Are the operations encryption, decryption, signing, and verifying signatures all sufficiently  efficient to be practical?
 • How do we find big primes?
 6.3.2 Why Does RSA Work?
 RSA does arithmetic mod n, where n=pq.
We know that φ(n)=(p−1)(q−1).
We’ve chosen d and e  de such that de=1modφ(n).
Therefore, by Euler’s Theorem, for any x, x =x mod n. An RSA encryp - tion consists of taking x and raising it to e. If we take the result and raise it to the d (i.e., perform  e)d ed RSA decryption), we’ll get ( x , which equals x , which is the same as x. So we see that decryp - tion reverses encryption.
 In the case of signature generation, x is first raised to the d power to get the signature, and  then the signature is raised to the e power for verification; the result, x de mod n, will equal x.  6.3.3 Why Is RSA Secure?
 We don’t know for sure that RSA is secure.
We can only depend on the Fundamental Tenet of Cryp - tography—lots of smart people have been trying to figure out how to break RSA, and they haven’t come up with anything yet (§2.1.1 The Fundamental Tenet of Cryptography ).
 The real premise behind RSA’s security is the assumption that factoring a big number is hard.
 The best known factoring methods are really slow.
To factor a 2048-bit number with the fastest  known technique (the general number field sieve) would take more than 1020 MIPS-years.
We sus - pect that a better technique is to wait a few hundred years and then use the fastest known technique.
 If you can factor quickly, you can break RSA.
Suppose you are given Alice’s public key 〈e,n〉.
 If you could find e’s exponentiative inverse mod n, then you’d have figured out Alice’s private key  〈d,n〉.
How can you find e’s exponentiative inverse?
Alice did it by knowing the factors of n, allow - ing her to compute φ(n).
She found the number that was e’s multiplicative inverse mod φ(n).
She  didn’t have to factor n—she started with primes p and q and multiplied them together to get n. You  can do what Alice did if you can factor n to get p and q.
142 F IRST-GENERATI ON PUBLIC KEY ALGORIT HMS 6.3.4  We do not know that factoring n is the only way of breaking RSA.
We know that breaking  RSA (for example, having an efficient means of finding d, given e and n) is no more difficult than  factoring [CORM91], but there might be some other means of breaking RSA.
 Note that it’s possible to misuse RSA.
For instance, let’s say Bob is holding an auction where  bids must be an integral number of dollars and the item being auctioned is worth about $1000.
 Alice will bid $742.
Trudy would like to find out what Alice’s bid is, so that Trudy can bid just a  tiny bit more.
Let’s say Alice encrypts her bid (742) with Bob’s public key.
What can Trudy learn from eavesdropping on Alice’s encrypted message?
 Trudy can’t decrypt something encrypted with Bob’s public key, but she can encrypt using  his public key.
She can try all thousand possible bids that Alice might be making, encrypt each with  Bob’s public key, and find the one that matches the ciphertext that Alice sent.
 The defense that prevents Trudy from guessing and verifying plaintext is that Alice should  concatenate her bid with a large random number, say 128 bits long.
Then instead of a thousand pos - sible messages for Trudy to check, there are 1000 ×2128, and checking that many messages is com - putationally infeasible.
 6.3.4 How Efficient Are the RSA Operations?
 The operations that need to be routinely performed with RSA are encryption, decryption, generat - ing a signature, and verifying a signature.
These need to be very efficient because they will be used  a lot.
Finding an RSA key (which means picking appropriate n, d, and e) also needs to be reason - ably efficient, but it isn’t as performance critical as the other operations since it is done less fre - quently.
As it turns out, finding an RSA key is substantially more computationally intensive than  using one.
 6.3.4.1 Exponentiating with Big Numbers  Encryption, decryption, signing, and verifying signatures all involve taking a large number m, rais- ing it to a large power x, and finding the remainder mod a large number n. For the sizes the numbers  have to be for RSA to be secure, these operations would be prohibitively expensive if done in the  most straightforward way (multiplying m by itself x times, and then reducing mod n).
The follow - ing will illustrate some tricks for doing the calculation faster.
 Suppose you want to compute 12354 mod 678.
The straightforward thing to do (assuming  your computer has a multiple-precision arithmetic package) is to multiply 123 by itself 54 times,  getting a really big product (about 100 digits), and then to divide by 678 to get the remainder.
A  computer could do this with ease, but for RSA to be secure, the numbers must be on the order of  600 digits.
Raising a 600-digit number to a 600-digit power by this method would exhaust the
6.3.4.1 RSA 143  capacity of all existing computers for more than the expected life of the universe and thus would not  be cost-effective.
 Luckily, you can do better than that.
If you do the modular reduction after each multiplica - tion, it keeps the number from getting really ridiculous.
To illustrate:  1232 = 123 × 123 = 15129 = 213 mod 678  1233 = 123 × 213 = 26199 = 435 mod 678  1234 = 123 × 435 = 53505 = 621 mod 678  This reduces the problem to 54 small multiplications and 54 small divisions, but it would still  be unacceptable for exponents of the size used with RSA.
 However, there is a much more efficient method.
To raise a number m to an exponent that is a  power of 2, say 32, you could start with m and multiply by m 31 times, which is reasonable if you  have nothing better to do with your time.
A much better scheme is to first square m, then square the  result, and so on.
Then you’ll be done after five squarings (five multiplications and five divisions):  1232 = 123 × 123 = 15129 = 213 mod 678  1234 = 213 × 213 = 45369 = 621 mod 678  1238 = 621 × 621 = 385641 = 537 mod 678  12316 = 537 × 537 = 288369 = 219 mod 678  12332 = 219 × 219 = 47961 = 501 mod 678  What if you’re not lucky enough to be raising something to a power of 2?
First note that if  you know what 123x is, then it’s easy to compute 1232x—you get that by squaring 123x .
It’s also  easy to compute 1232x+1—you get that by multiplying 1232x by 123.
Now you use this observation  to compute 12354 .
 Well, 54 is 110110 2 (represented in binary).
You’ll compute 123 raised to a sequence of  powers— 12, 112, 110 2, 1101 2, 11011 2, 110110 2.
Each successive power concatenates one more  bit of the desired exponent.
And each successive power is either twice the preceding power or one  more than twice the preceding power:  1232 = 123×123 = 15129 = 213 mod 678  1233 = 1232 ×123 = 213×123 = 26199 = 435 mod 678  1236 = (1233)2 = 4352 = 189225 = 63 mod 678  12312 = (1236)2 = 632 = 3969 = 579 mod 678  12313 = 12312×123 = 579×123 = 71217 = 27 mod 678  12326 = (12313)2 = 272 = 729 = 51 mod 678  12327 = 12326 ×123 = 51×123 = 6273 = 171 mod 678  12354 = (12327)2 = 1712 = 29241 = 87 mod 678  The idea is that squaring is the same as doubling the exponent, which, in turn, is the same as shift - ing the exponent left by one bit.
And multiplying by the base is the same as adding one to the expo - nent.
6.3.4.2 144 F IRST-GENERATI ON PUBLIC KEY ALGORIT HMS  In general, to perform exponentiation of a base to an exponent, you start with your value set  to 1.
As you read the exponent in binary bit by bit from high-order bit to low-order bit, you square  your value, and if the bit is a 1, you then multiply by the base.
You perform modular reduction after  each operation to keep the intermediate results small.
 By this method you’ve reduced the computation of 12354 to eight multiplications and eight  divisions.
More importantly, the number of multiplications and divisions rises linearly with the length of the exponent in bits rather than with the value of the exponent itself.
 RSA operations using this technique are sufficiently efficient to be practical.
 Note that as we explained in §2.7.3 Avoiding a Side-Channel Attack , this implementation  would allow a side-channel attack, because the implementation would behave differently on each bit of the exponent, depending on whether the bit was a 0 or a 1.
An implementation should avoid  providing a side channel by behaving the same way on each bit, as explained in §2.7.3.
 6.3.4.2 Generating RSA Keys  Most uses of public key cryptography do not require frequent generation of RSA keys.
If genera - tion of an RSA key is only done, for instance, when an employee is hired, then it need not be as  efficient as the operations that use the keys.
However, it still has to be reasonably efficient.
 6.3.4.2.1.
Finding Big Primes p and q  There is an infinite supply of primes.
However, they thin out as numbers get bigger and bigger.
The probability of a randomly chosen num ber n being prime is approximately 1 /ln n. The natural loga - rithm function, ln, rises linearly with the size of the number represented in digits or bits.
For a ten-digit number, there is about one chance in 23 of it being prime.
For a 300-digit number (a size  of prime that would be useful for RSA), there is about one chance in 690.
 So, we’ll choose a random odd number and test if it is prime.
On the average, we’ll only have  to try 690 of them before we find one that is a prime.
So, how do we test if a number n is prime?
 One naive method is to divide n by all numbers ≤ n and see if there is always a nonzero  remainder.
The problem is that it would take several universe lifetimes (with numbers the size used  in RSA) to verify that a candidate is prime.
We said finding p and q didn’t need to be as easy as  generating or verifying a signature, but forever is too long.
 For a long time, there was no sufficiently fast way for absolutely determining that a number  of this size is prime.
Mathematicians have now invented such an algorithm [AGRA04 ], but almost  no one uses it.
That’s because there is a much faster test for determining that a number is almost  certainly prime, and the more time we spend testing a number the more assured we can be that the  number is prime.
6.3.4.2 RSA 145  p−1 n−1Recall Fermat’s Theorem : If p is prime and 0 < a<p, a = 1 mod p. Does a =1mod n  hold even when n is not prime?
The answer is—usually not!
A primality test, then, for a number n  is to pick a number a<n, compute an−1 mod n, and see if the answer is 1.
If it is not 1, n is certainly  not prime.
If it is 1, n may or may not be prime.
If n is a randomly generated number of about 300  digits, the probability that n isn’t prime but an−1mod n= 1, is less than 1 in 1040 [POME81,  CORM91].
Most people would decide they could live with that risk of falsely assuming n was  prime when it wasn’t.
The cost of such a mistake would be either that (1) RSA might fail—they  could not decrypt a message addressed to them, or (2) someone might be able to compute their pri - vate exponent with less effort than anticipated.
There aren’t many applications where a risk of fail - ure of 1 in 1040 is a problem.
 But if the risk of 1 in 1040 is unacceptable, the primality test can be made more reliable by  using multiple values of a. If for any given n, each value of a had a probability of 1 in 1040 of  falsely reporting primality, a few tests would assure even the most paranoid person.
Unfortunately,  n−1there exist numbers n that are not prime but which satisfy a = 1 mod n for all values of a. They  are called Carmichael number s. Carmichael numbers are sufficiently rare that the chance of  selecting one at random is nothing to lose sleep over.
Nevertheless, mathematicians have come up  with an enhancement to the above primality test that will detect non-primes (even Carmichael num - bers) with high probability and negligible additional computation, so we may as well use it.
 The method of choice for testing whether a number is prime is due to Miller and Rabin  [RABI80 ].
We can always express n−1 as a power of two times an odd number, say 2bc.
We can  then compute an−1 mod n by computing ac mod n and then squaring the result b times.
If the result  is not 1, then n is not prime and we’re done.
If the result is 1, we can go back and look at those last  few intermediate squarings. (
If we’re really clever, we’ll be checking the intermediate results as we compute them.)
If a c mod n is not 1, then one of the squarings took a number that was not 1 and  squared it to produce 1.
That number is a mod n square root of 1.
It turns out that if n is prime, then  the only mod n square roots of 1 are ±1.
Further, if n is not a power of a prime, then 1 has multiple  square roots, and all are equally likely to be found by this test.
A square root other than ±1 is known  as a nontrivial square root of 1.
So if you can find a nontrivial square root of n, you know n is not  prime.
For more on why, see §6.3.4.3 Why a Non-Prime Has Multiple Square Roots of One .
 So if the Miller-Rabin test finds a square root of 1 that is not ±1, then n is not prime.
Further - more, if n is not prime (even if it is a Carmichael number), at least ¾ of all possible values of a will  fail the Miller-Rabin primality test.
By trying many values for a, we can make the probability of  falsely identifying n as prime inconceivably small.
In actual implementations, how many values of  a to try is a trade-off between performance and paranoia.
6.3.4.3 146 F IRST-GENERATI ON PUBLIC KEY ALGORIT HMS  To summarize, an efficient method of finding primes is:  1.
Pick an odd random number n with the desired number of bits.
 2.
Test n’s divisibility by small primes and go back to step 1 if you find a factor. (
Obviously, this  step isn’t necessary, but it’s worth it since it has a high enough probability of catching some  non-primes and is much faster than the next step.)
 3.
Repeat the following until n is proven not prime (in which case go back to step 1) or as many  times as you feel necessary to show that n is probably prime:  Pick an a at random and compute ac mod n (where c is the odd number for which n − 1 = 2bc).
 cIf a = ±1 mod n, pick a different a and repeat.
Otherwise, keep squaring the result mod n  until you get a value that equals ±1 mod n, or until the exponent is n −1.
If the value is +1, the  value you just squared is a square root of 1 other than ±1, so the number is definitely not  prime.
If the exponent is n−1, the number is definitely not prime because an−1mod n ≠1.
Oth - erwise, n has passed the primality test for this a, and if you want, you can try another a.  6.3.4.3 Why a Non-Prime Has Multiple Square Roots of One  By the Chinese remainder theorem (§2.7.6), there are four ways that a number y can be a square  root of 1 mod n when n=pq:  1.
y is 1 mod p and 1 mod q. In this case, y will be 1 mod n.  2.
y is −1 mod p and −1 mod q. In this case, y will be −1 mod n.  3.
y is 1 mod p and −1 mod q. In this case, y will be a nontrivial square root of 1 mod n.  4.
y is −1 mod p and 1 mod q. In this case, y will be a nontrivial square root of 1 mod n.  Note that if you find a nontrivial square root of 1 mod n, you can factor n (Homework Problem 9).
 6.3.4.3.1.
 Finding d and e  How do we find d and e given p and q?
As we said earlier, for e we can choose any number that is  relatively prime to ( p−1)(q−1), and then all we need to do is find the number d such that  ed = 1 mod φ(n).
Τhis we can do with the extended Euclidean algorithm.
 There are two strategies one can use to ensure that e and ( p−1)(q−1) are relatively prime.
 1.
After p and q are selected, choose e at random.
Test to see if e is relatively prime  to (p−1)(q−1).
If not, select another e.  2.
Don’t pick p and q first.
Instead, first choose e, then select p and q carefully so that ( p−1) and  (q−1) are guaranteed to be relatively prime to e. The next section will explain why you’d want  to do this.
6.3.4.4 RSA 147  6.3.4.4 Having a Small Constant e  A rather astonishing discovery is that RSA is no less secure (as far as anyone knows) if e is always  chosen to be the same number.
And if e is chosen to be small, then the operations of encryption and  signature verification become much more efficient.
Given that the procedure for finding a 〈d, e〉  pair is to pick one and then derive the other, it is straightforward to make e be a small constant.
This  makes public key operations faster while leaving private key operations unchanged.
You might  wonder whether it would be possible to select small values for d to make private key operations fast  at the expense of public key operations.
The answer is that you can’t.
If d were a constant, the  scheme would not be secure because d is the secret.
If d were small, an attacker could search small  values to find d.  Two popular values of e are 3 and 65537.
 Why 3?
The number 2 doesn’t work because it is not relatively prime to ( p−1)(q−1) (which  must be even because p and q are both odd).
3 can work, and with 3, public key operations require  only two multiplications.
Using 3 as the public exponent maximizes performance.
 As far as anyone knows, using 3 as a public exponent does not weaken the security of RSA if  some practical constraints on its use are followed.
Most dramatically, if a message m to be  encrypted is small—in particular, smaller than 3 n —then raising m to the power 3 and reducing  mod n will simply produce the value m 3.
Anyone seeing such an encrypted message could decrypt  it simply by taking a cube root.
This problem can be avoided by padding each message with a ran- dom number before encryption so that m 3 is always large enough to be guaranteed to need to be  reduced mod n.  A second problem with using 3 as an exponent is that if the same message is sent encrypted to  three or more recipients, each of whom has a public exponent of 3, the message can be derived from  the three encrypted values and the three public keys 〈3,n1〉, 〈3,n2〉, 〈3,n3〉.
 Suppose a bad guy sees m 3 mod n1, m 3 mod n2, and m 3 mod n3 and knows 〈3,n1〉, 〈3,n2〉,  〈3,n3〉.
Then by the Chinese Remainder computation, the bad guy can compute m 3 mod n1n2n3.
 Since m is smaller than each of the ni s (because RSA can only encrypt messages smaller than the  3modulus), m 3 will be smaller than n1n2n3, so m 3 mod n1n2n3 will just be m .
Therefore, the bad  guy can compute the ordinary cube root of m 3 (which again is easy if you are a computer) to get m.  Now this isn’t anything to get terribly upset about.
In practical uses of RSA, the message to  be encrypted is usually a key for a secret key encryption algorithm and in any case is much smaller  than n. As a result, the message must be padded before it is encrypted.
If the padding is randomly  chosen (and it should be for a number of reasons), and if it is re-chosen for each recipient, then  there is no threat from an exponent of 3 no matter how many recipients there are.
The padding  doesn' t really have to be random—for example, the recipient’s ID would work fine.
 Finally, an exponent of 3 works only if 3 is relatively prime to φ(n) (in order for it to have an  inverse d).
How do we choose p and q so that 3 will be relatively prime to φ(n)=(p−1)(q−1)?
 Clearly, ( p−1) and ( q−1) must each be relatively prime to 3.
To ensure that p−1 is relatively prime
6.3.4.5 148 F IRST-GENERATI ON PUBLIC KEY ALGORIT HMS  to 3, we want p to be 2 mod 3.
That will ensure p−1 is 1 mod 3.
Similarly we want q to be 2 mod 3.
 We can make sure that the only primes we select are congruent to 2 mod 3 by choosing a random  number, multiplying by 3 and adding 2, and using that as the number we will test for primality.
 Indeed, we want to make sure the number we test is odd (since if it’s even it is unlikely to be  prime), so we should start with an odd number, multiply by 3, and add 2.
This is equivalent to start - ing with any random number, multiplying by 6, and then adding 5.
 An even more popular value of e is 65537.
Why 65537?
The appeal of 65537 (as opposed to  others of the same approximate size) is that 65537 = 216+1, and it is prime.
Because its binary rep - resentation contains only two 1s, it takes only 17 multiplications to exponentiate.
While this is  much slower than the two multiplications required with an exponent of 3, it is much faster than the  3072 (on average) required with a randomly chosen 2048-bit value (the typical size of an RSA  modulus in practical use today).
Also, using the number 65537 as a public exponent largely avoids  the problems with the exponent 3.
 The first problem with 3 occurs if m 3< n. Unless n is much longer than the 2048 bits in typi - cal use today, there aren’t too many values of m for which m 65537< n, so being able to take a normal  65537th root is not a threat.
 The second problem with 3 occurs when the same message is sent to at least 3 recipients.
In  theory, with 65537 there is a threat if the same message is sent encrypted to at least 65537 recipi - ents.
A cynic would argue that under such circumstances, the message couldn’t be very secret.
 The third problem with 3 is that we have to choose n so that φ(n) is relatively prime to 3.
For  65537, the easiest thing to do is just reject any p or q that is equal to 1 mod 65537.
The probability  of rejection is very small (2−16) so this doesn’t make finding n significantly harder.
 6.3.4.5 Optimizing RSA Private Key Operations  There is a way to speed up RSA exponentiations in generating signatures and decrypting (the oper - ations using the private key) by taking advantage of knowledge of p and q. Feel free to skip this sec - tion—it isn’t a prerequisite for anything else in the book, and it requires more than the usual level of  concentration.
 In RSA, d and n are 2048-bit numbers, or about 617 digits.
p and q are 1024 bits, or about 308  digits.
RSA private key operations involve taking some c (usually a 2048-bit number) and comput - ing cd mod n. It’s easy to say “raise a 2048-bit number to a 2048-bit exponent mod a 2048-bit num - ber”, but it’s certainly processor intensive, even if you happen to be a silicon-based computer.
A  way to speed up RSA operations is to do all the computation mod p and mod q, then use the Chi - nese Remainder Theorem to compute what the answer is mod pq.
 So suppose you want to compute m = cd mod n. Instead of computing cd mod n, you can take  c = c mod p and c = c mod q and compute m = cd mod p and m = cd mod q, then use the Chi -p q p p q q  dnese Remainder Theorem to convert back to what m would equal mod n, which would give you c  mod n. Also, it is not necessary to raise to the dth power mod p, given that d is going to be bigger
6.3.5 RSA 149  p−1than p (by a factor of about q).
Since (by Euler’s Theorem) any a =1 mod p, we can take d’s  value mod p−1 and use that as the exponent instead.
In other words, if d = k(p−1)+ r, then cd mod p  = cr mod p.  So, let us compute d = d mod ( p−1) and d = d mod ( q−1).
Then, instead of doing the p q  expected RSA operation of m = cd mod n (which involves 2048-bit numbers), we’ll compute both  m = c dp mod p and m = c dq mod q and then compute m from the Chinese Remainder Theorem.
p p q q  To save ourselves work, since we’ll be using d and d a lot (every time we do an RSA private key p q  computation), we’ll compute them once and remember them.
Similarly, to use the Chinese Remain - der Theorem at the end, we need to know p −1 mod q and q −1 mod p, so we’ll precompute and  remember them as well.
 All told, instead of one 2048-bit exponentiation, this modified calculation does two 1024-bit  exponentiations, followed by two 1024-bit multiplications and a 2048-bit addition.
This might not  seem like a net gain, but because the exponents are half as long, using this variant makes RSA about  twice as fast.
 Note that to do these optimizations for RSA operations, we need to know p and q. Someone  who is only supposed to know the public key will not know p and q (or else they can easily compute  d).
Therefore, these optimizations are only useful for the private key operations (decryption and  generating signatures).
However, that’s okay because we can choose e to be a convenient value (like  3 or 65537) so that raising a 2048-bit number to e will be easy enough without the Chinese Remain - der optimizations.
 6.3.5 Arcane RSA Threats  Any number x<n is a signature of xe mod n. So it’s trivial to forge someone’s signature if you don’t  care what you’re signing.
The following sections explain the vulnerabilities that PKCS #1 [see  §6.3.6 Public-Key Cryptography Standard (PKCS) ] is attempting to avoid.
 The goal is to constrain what is being signed so that a random number has negligible proba - bility of being a valid message.
For example, often what is being signed is a hash value, which is  sufficiently smaller than an RSA modulus that there is plenty of room for padding the hash before  digitally signing it.
If the pad in a valid signature is required to include hundreds of bits of a spe- cific constant, it is extremely unlikely that a random number will look like a valid padded hash.
An  attacker knowing only Alice’s public key would have to find a value x that when raised to e (mod n)  would have valid padding.
 Note: RSA deals with large numbers, and there is, unfortunately, more than one way to repre - sent such numbers.
In what follows, we have chosen to order the octets left to right from most sig - nificant to least significant.
This is called big-endian format.
6.3.5.1 150 F IRST-GENERATI ON PUBLIC KEY ALGORIT HMS  6.3.5.1 Smooth Numbers  A smooth number is defined as one that is the product of reasonably small primes.
There’s no  absolute definition of a smooth number, since there’s no real definition of reasonably small .
The  more compute power the attacker has at her disposal, and the more signatures she has access to, the  larger the “reasonably small” primes need to be, to prevent an attacker from taking advantage of this threat.
 The threat we are about to describe is known as the smooth number threat .
It is really only  of theoretical interest because of the immense amount of computation, the immense numbers of  signed messages that need to be gathered, and the luck involved.
However, it costs very little to  have an encoding that avoids this threat.
The smooth number threat was discovered by Desmedt and Odlyzko [DESM86].
 The first observation is that if you have signed m1 and m2, and a bad guy Carol can see your  signature on m1 and m2, she can compute your signature on m1× m2, on m1/m2, on m1j, and on  k m1j×m2 .
For instance, if Carol sees m1 d mod n (which is your signature on m1), then she can com - pute your signature on m12 by computing ( m1 d mod n)2 mod n (see Homework Problem 5).
 If Carol collects a lot of your signed messages, she will be able to compute your signature on  any message that can be computed from her collection by multiplication and division.
If the mes - sages you sign are mostly smooth, there will be a lot of other smooth messages on which she will  be able to forge your signature.
 Suppose she collects your signatures on two messages whose ratio is a prime.
Then she can  compute your signature on that prime.
If she’s lucky enough to get many such message pairs, she  can compute your signature on lots of primes, and then she can forge your signature on any mes - sage that is the product of any subset of those primes, each raised to any power.
With enough pairs,  she will be able to forge your signature on any message that is a smooth number.
 Actually, Carol does not have to be nearly that lucky.
With as few as k signatures on messages  that are products of different subsets of k distinct primes, she will be able to isolate the signatures  on the individual primes through a carefully chosen set of multiplications and divisions.
 The typical thing being signed with RSA is a padded hash.
If it is padded with zeroes, it is  much more likely to be smooth than is a random mod n number.
A random mod n quantity is  extremely unlikely to be smooth (low enough probability so that if you are signing random mod n  numbers, we can assume Carol would have to have a lot of resources and a lot of luck to find even  one smooth number you’ve signed, and she might need millions of them in order to mount the  attack).
 Padding on the left with zeroes keeps the padded message digest small and therefore likely to  be smooth.
Padding on the right with zeroes is merely multiplying the message digest by some  power of two, and so isn’t any better.
 Another tempting padding scheme is to pad on the right with random data.
That way, since  you are signing fairly random mod n numbers, it is very unlikely that any of the messages you sign
6.3.5.2 RSA 151  will be smooth, so Carol won’t have enough signed smooth messages to mount the threat.
However,  this leaves us open to the next obscure threat.
 6.3.5.2 The Cube Root Problem  Let’s say you pad on the right with random data.
You chose that scheme so that there is a negligible  probability that anything you sign will be smooth.
However, if the public exponent is 3, this enables  Carol to forge your signature on virtually any message she chooses!
 Let’s say Carol wants your signature on some message.
The hash of that message is h. Carol  pads h on the right with zeroes.
She then computes its ordinary cube root and rounds up to an inte - ger r. Now she has forged your signature, because re = r 3 = (h padded on the right with a seemingly  random number).
 6.3.6 Public-Key Cryptography Standard ( PKCS)  It is useful to have some standard for the encoding of information that will be signed or encrypted  through RSA, so that different implementations can interwork, and so that the various pitfalls with  RSA can be avoided.
Rather than expecting every user of RSA to be sophisticated enough to know  about all the attacks and develop appropriate safety measures through careful encoding, the stan - dard known as PKCS recommends encodings.
PKCS is actually a set of standards, called PKCS #1  through PKCS #15.
There are also two companion documents, An overview of the PKCS standards ,  and A layman’s guide to a subset of ASN.1, BER, and DER . (
ASN.1 = Abstract Syntax Notation 1 ,  BER = Basic Encoding Rules , and DER = Distinguished Encoding Rules —aren’t you glad you  asked?).
 There is a newer version of PKCS #1 documented in RFC 8017, but most implementations  use the format described here.
 The PKCS standards define the encodings for things such as an RSA public key, an RSA pri - vate key, an RSA signature, a short RSA-encrypted message (typically a secret key), a short  RSA-signed message (typically a hash), and password-based encryption.
 The threats that PKCS has been designed to deal with are:  • encrypting guessable messages  • signing smooth numbers  • multiple recipients of a message when e = 3  • encrypting messages that are less than a third the length of n when e = 3  • signing messages where the information is in the high-order part and e = 3
6.3.6.1 152 F IRST-GENERATI ON PUBLIC KEY ALGORIT HMS  6.3.6.1 Encryption  PKCS #1 defines standards for formatting a message to be encrypted with RSA.
RSA is not gener - ally used to encrypt ordinary data.
The most common quantity that would be encrypted with RSA is  a secret key, and for performance reasons, the actual data would be encrypted with the secret key.
 The format normally used is  0 2 at least eight  random nonzero octets 0 data  The data to be encrypted, usually a secret key, is much smaller than the modulus.
If it’s an  AES key, it’s 128, 192, or 256 bits.
Sometimes it is a seed to be used as input to a PRF (§2.6.3 Cal- culating a Pseudorandom Stream from the Seed ).
 The top octet is 0, which is a good choice because this guarantees that the message m being  encrypted is smaller than the modulus n. (If m were larger than n, decryption would produce m mod  n instead of m.) Note that PKCS specifies that the high-order octet (not bit!)
of the modulus n must  be non-zero, and given that the PKCS padding requires the top octet to be zero, this guarantees that  the thing being encrypted will be smaller than the modulus.
 The next octet is 2, which is the format type.
The value 2 is used for a block to be encrypted.
 The value 1 is used for a value to be signed (see next section).
 Each octet of padding is chosen independently to be a random nonzero value.
The reason 0  cannot be used as a padding octet is that 0 is used to delimit the padding from the data.
 Let’s review the RSA threats and see how this encoding addresses them.
 • encrypting guessable messages: Since there are at least eight octets of randomly chosen pad - ding, knowing what might appear in the data does not help the attacker who would like to guess the data, encrypt it, and compare it with the ciphertext.
The attacker would have to  guess the padding as well, and this is infeasible.
 • sending the same encrypted message to more than three recipients (assuming 3 is chosen for  e): As long as the padding is chosen independently for each recipient, the quantities being  encrypted will not be the same.
 • encrypting messages that are less than a third the length of n when e = 3: Because the second  octet is nonzero, the message will be guaranteed to be more than a third the length of n.  6.3.6.2 The Million-Message Attack  There was an attack on SSL (the precursor to TLS) that could have been interpreted as a flaw in an  implementation of SSL, but the world has come to see it as a flaw in the PKCS #1 encryption for - mat.
There is a PKCS #1 version 2 format that fixes the “flaw”.
The attack, published by Daniel  Bleichenbacher [BLEI98], is known as the million-message attack .
It is possible because SSL  made some incorrect assumptions about the services PKCS #1 padding provides.
This attack allows
6.3.6.3 RSA 153  an attacker (Trudy) to recover the key used for an Alice-Bob connection but requires Trudy to send  a million modified versions of the third message in the Alice-Bob communication.
 In the SSL protocol, the client (Alice) sends the server (Bob) a randomly chosen secret key  (S), padded according to PKCS #1 and encrypted using RSA.
SSL decrypts the value, and, if the  padding is correct, sends a response encrypted with S. If after the decryption the padding is not cor - rect, Bob sends an error message.
The problem is that this allows the attacker (Trudy) to use Bob as  an oracle.
Trudy can send Bob a message, and Bob will tell Trudy whether the message (when  decrypted) has proper PKCS #1 padding.
Some SSL servers were particularly helpful and would say whether the padding was wrong because the first two octets were something other than 0 and 2,  or whether it was wrong because the length of the encrypted quantity was something other than what was expected.
 Trudy can then carefully craft SSL connection requests with modified versions of the original  Alice-to-Bob encrypted message and note what error message she got from Bob.
If Bob gave a dif - ferent error message when the decrypted value began with the octets 0 and 2 (and on average one  message in 216 would have that form), Trudy could eventually figure out the encrypted key after  seeing the responses to about a million carefully crafted messages.
 Attacks of this sort can be avoided if the padding for encryption includes enough redundancy  that the probability of a randomly chosen value decrypting into something that looks like it is prop - erly padded is negligible. (
One in a million is not considered negligible to cryptographers.
To them  “negligible” should be less than one in 2100 or so.)
A particularly complex scheme for making the  probability negligible is specified in PKCS #1 version 2, also known as OAEP [BELL94], and stan - dardized in IEEE P1363 and RFC 8017.
 Because this is an obscure attack easily avoided by other means (like not being so helpful in  error messages when people send you invalid messages), the world has not scrambled to migrate to  OAEP.
But OAEP might be mandated in newly defined protocols where backward compatibility is  not an issue.
 6.3.6.3 Signing  PKCS #1 also defines standards for formatting a quantity to be signed with RSA.
Usually the data  being signed is a hash.
As with encryption, padding is required.
The format normally used is  0 1 at least eight octets of ff16 0 ASN.1-encoded  hash type and hash  As with encryption, the top octet of 0 ensures that the quantity to be signed will be less than n. The  next octet is the PKCS type; in this case, a quantity to be signed.
The padding ensures that the quan - tity to be signed is very large and therefore unlikely to be a smooth number.
 Inclusion of the hash type instead of merely the hash standardizes how to tell the other party  which hash function you used.
154 F IRST-GENERATI ON PUBLIC KEY ALGORIT HMS 6.4  6.4 DIFFIE-HELLM AN  Diffie-Hellman allows two individuals (Alice and Bob) to agree on a shared key even though they  can only exchange messages in public.
For instance, they might be sending messages across a net- work, on a telephone that might be tapped, or shouting at each other across a crowded room.
Alice  shouts “My number is 92847692718497.”
Bob shouts “My number is 28379487198225.”
Everyone  in the room hears what Alice and Bob said, but after that exchange, Alice and Bob know a secret  that nobody else in the room can feasibly calculate.
 The security of Diffie-Hellman depends on the difficulty of solving the discrete log problem ,  xwhich can be informally stated as, “If you know g, p, and g mod p, what is x?”
In other words,  what exponent did you have to raise g to, mod p, to get gx?
 Diffie-Hellman works with many types of groups ( e.g., integers with various moduli, polyno - mials, elliptic curves), but let’s assume for now that we are using integers with a prime modulus.
 Some values of p and g are more secure than others.
Typically, cryptographers define a few groups  to choose from, and you would choose one of those.
 So, there are integers p and g, where p is a large prime (say 2048 bits) and g is a number less  than p with some restrictions that aren’t too important for a basic understanding of the algorithm.
 The values p and g are known beforehand and can be publicly known, or Alice and Bob can negoti - ate with each other across the crowded room.
For instance, Alice could shout “I’d like to use any of  these published cryptographer-approved Diffie-Hellman groups”, and Bob can reply “Of your pro - posed groups, I choose the third one in your list”.
 Once Alice and Bob agree on a p and g, each chooses a large, say, 512-bit number at random  and keeps it secret.
Let’s call Alice’s private Diffie-Hellman key a and Bob’s private  Diffie-Hellman key b. Each raises g to their private Diffie-Hellman number, mod p, resulting in  their public Diffie-Hellman value.
So Alice computes ga mod p and Bob computes gb mod p, and  each informs the other (Protocol 6-4 ).
Finally, each raises the other side’s public value to their own  private value.
Bob Alice  Protocol 6-4.
Diffie-Hellman Exchange  Alice raises gb mod p to her private number a. Bob raises ga mod p to his private number b. So  Alice computes ( gb mod p)a = gba mod p. Bob computes ( ga mod p)b = gab mod p. And, of course,  gba mod p = gab mod p.  Without knowing either Alice’s private number a or Bob’s private number b, nobody else can  calculate gab mod p in a reasonable amount of time even though they can see ga mod p and gb mod a g mod p  gb mod p
6.4.1 DIFFIE -HELLMA N 155  p. We assume they can’t compute discrete logarithms, because of the Fundamental Tenet of Cryp - tography.
 6.4.1 MITM (Meddler-in-the-Middle) Attack  As explained in §1.6 Active and Passive Attacks , a MITM attack is where someone (say Trudy)  manages to get in the middle of a conversation between Alice and Bob, relaying messages between  them, but being able to decrypt or modify messages.
Diffie-Hellman alone does not prevent MITM  attacks, if we assume that Alice and Bob do not have credentials (such as public keys) for each  other.
 A classic case of a MITM attack (Protocol 6 -5) is where Alice and Bob do a Diffie-Hellman  exchange and use the resulting Diffie-Hellman key as their session key.
Alice will send ga mod p.  aBob will send gb mod p. But Alice doesn’t realize that she is really talking to Trudy.
She sends g  tmod p to Trudy.
Trudy responds with Trudy’s own Diffie-Hellman value, say g mod p. So the  Alice-Trudy connection will use the session key gat mod p. Trudy doesn’t know Alice’s private Dif - tfie-Hellman value a, so Trudy sends her own Diffie-Hellman value g mod p to Bob.
The  Trudy-Bob connection will use the session key gbt mod p. Trudy will be able to see the entire  Alice-Bob conversation because every message from Alice to Bob (encrypted with gat mod p) will  be decrypted by Trudy and re-encrypted with gbt mod p and then sent to Bob.
Alice “Alice”, g a mod p  Trudy “Alice”, g t mod p Bob  “Bob”, g t mod p “Bob”, g b mod p  session key = g at mod p session key = g bt mod p  Protocol 6-5.
Diffie-Hellman with Trudy in the Middle  How can Alice and Bob detect that there is a MITM?
Each of them think they have estab - lished a secure session to the other.
They could try sending each other passphrases that they had  agreed-upon for authentication—one password that Bob is to say to Alice, perhaps “The fish are  green”, and one that Alice is to say to Bob, for instance, “The moon sets at midnight”.
If Alice  receives the expected password, can she assume she is talking to Bob?
No.
Trudy is decrypting each message from Alice and encrypting it when forwarding it to Bob.
So the passphrases will get for - warded between Alice and Bob, and they will not detect MITM Trudy.
156 F IRST-GENERATI ON PUBLIC KEY ALGORIT HMS 6.4.2  Perhaps Alice and Bob could transmit their session key over the encrypted channel they have  established.
That won’t work either.
Alice sends the message “I think we are using gat mod p”.
 However, Trudy decrypts the message and edits it to be “I think we are using gbt mod p” before  encrypting and forwarding it on to Bob.
 Using Diffie-Hellman alone, there’s really nothing Alice and Bob can do to detect the MITM  through whom they are communicating.
As a result, this form of Diffie-Hellman is only secure  against passive attacks where the intruder just watches the encrypted messages.
 6.4.2 Defenses Against MITM Attack  One technique by which Diffie-Hellman can be secure against active attacks is for each person to  have a somewhat permanent Diffie-Hellman value instead of inventing one for each exchange.
The  public Diffie-Hellman values would then all be certified by some means that is assumed reliable (for instance, through a PKI; see §10.4 PKI).
This enables Alice and Bob to start communicating  securely without doing any handshake first!
Alice’s certified public key would be g a mod p. Bob’s  certified public key would be gb mod p. Alice and Bob would know that to communicate with each  other, they would use the session key gab mod p.  Always using the same session secret between Alice and Bob can create vulnerabilities, so  we’ll describe a more common technique.
If Alice and Bob know some sort of secret with which  they can authenticate each other, either a shared secret key or knowledge of the other side’s public  key ( e.g., RSA key) and their own private key, then they can prove they generated their  Diffie-Hellman value.
We call such an exchange an authenticated Diffie-Hellman exchange .
 Authentication can be done simultaneously with sending the Diffie-Hellman values or after the Diffie-Hellman exchange.
Examples are:  • Encrypt the Diffie-Hellman exchange with the pre-shared secret.
 • Encrypt the Diffie-Hellman value with the other side’s public key (Homework Problem 2).
 • Sign the Diffie-Hellman value with your private key.
 • Following the Diffie-Hellman exchange, transmit a hash of the agreed-upon shared  Diffie-Hellman value, your name, and the pre-shared secret.
 • Following the Diffie-Hellman exchange, transmit a hash of the pre-shared secret and the  Diffie-Hellman value you transmitted.
 Yet another defense against MITM is to use channel bindings. (
See §11.6 Detecting MITM .)
6.4.3 DIFFIE -HELLMA N 157  6.4.3 Safe Primes and the Small-Subgroup Attack  While Diffie-Hellman will “work” (in the sense of Alice and Bob agreeing on a common value to  use as a key) for any values of p and g, there will be security vulnerabilities unless certain other cri - teria are met.
In particular, there are known attacks on Diffie-Hellman if ( p−1)/2 is smooth (see  §6.3.5.1 Smooth Numbers ).
 Traditionally, Diffie-Hellman is done with primes where ( p−1)/2 is also prime.
A prime p  that satisfies this additional constraint is called a safe prime , while ( p−1)/2 is called a Sophie Ger - main prime .
The number of elements in a group is known as the order of the group.
If the group is  multiplication mod p, and p is prime, then the order of the group is p −1 (because 0 is not in the  multiplicative group mod p).
 As we will see, DSA (§6.5.1 The DSA Algorithm ) uses Diffie-Hellman-style key pairs, but  has an optimization that speeds up signing and verification, and it makes signatures smaller.
Instead  of having p be a safe prime, ( p −1)/2 only needs at least one reasonably large factor q, where the  length of q is twice the desired security strength.
So for 128-bit security, q would be 256 bits.
This  optimization makes DSA faster because some of the computation can be done mod q rather than  mod p. Since typically p will be 3072 bits, and q will be 256 bits, the exponents will be a twelfth the  size when operating mod q.  The number g is known as a generator .
If you take an element g of a group and multiply it by  itself over and over, you will get elements g 1, g 2, etc.
Once you get to 1 (the identity element), you  will have generated some subset of the group.
This is known as a cyclic subgroup .
This subgroup  will be closed under multiplication, meaning that multiplying any two elements of the subgroup  together will result in one of the elements of the subgroup.
The number of elements in the subgroup  is known as the order of the subgroup.
The order of a subgroup always divides the order of the  group.
 Recall, if p is a prime, then the order of the group is φ(p), which equals p−1.
If p is a safe  prime, then the only factors of p−1 are 1, 2, the prime ( p−1)/2, and p−1.
So the only subgroups are:  • The subgroup of order 1, generated by g=1 (since all powers of 1 = 1).
 • The subgroup of order 2, consisting of the elements ±1, and generated by g= −1.
 • The subgroup of order p−1, consisting of the entire group.
Almost half of the group elements  (those that are not squares, other than −1) will each generate the entire group.
 • The subgroup of (prime) order ( p−1)/2.
Again, almost half the group elements (those that are  squares, other than 1) will each generate this subgroup.
For instance, assume you have a  generator g that generates all p−1 elements of the group.
Then g 2 will only generate every  other element in the list generated by g.  It is computationally expensive to choose p and g. Theoretically, one only needs to choose them  once and keep using the same p and g. It could even be a constant in the standard, and everyone  could use the same p and g. The advantage of having everyone use the same p and g or choosing
158 F IRST-GENERATI ON PUBLIC KEY ALGORIT HMS 6.4.3  from a small set of standardized values is that if you get the values from a trusted source, you don’t  have to test whether p is appropriate, for instance, whether both p and ( p−1)/2 are prime.
 It turns out to be possible, though incredibly space- and computation-intensive, to calculate a  large table based on a single p, which would allow you to compute discrete logarithms for that p.  For example, a 1024-bit prime is too short, since it would take workfactor of about 80 bits to break  that prime, and then workfactor 50 bits to break Diffie-Hellman using that prime.
The recom - mended size of a Diffie-Hellman prime is at least 2048.
To break a prime of that size is workfactor  112 bits, and subsequently breaking a Diffie-Hellman exchange using that prime would be about 70  bits.
For a Diffie-Hellman prime of 3072 bits, breaking the prime has a workfactor of 128 bits, and  subsequently breaking Diffie-Hellman with that prime has a workfactor of 80 bits.
That is why  Diffie-Hellman uses 3072-bit primes in order to get 128-bit security.
 A similar scheme would allow you to break RSA in the sense of being able to calculate some - one’s private key based on their modulus.
There is not that much incentive for an attacker to do that,  however.
Since everyone is using a different RSA modulus, that would only allow the attacker to  break one person’s key.
If the p for Diffie-Hellman were broken, and many people were using the  same p, then every key exchange based on Diffie-Hellman with that modulus could be broken.
Sim - ply changing p occasionally will eliminate this threat.
 Now we’ll discuss the small group attack .
Things would certainly not be secure if one were  doing Diffie-Hellman in a group of really small order (such as if you were using the element −1 as  the generator g), because gab would have few possible values.
So it’s important for the subgroup in  which Diffie-Hellman is executed to be quite large.
 First, let’s assume p−1 is smooth, meaning that its factors are all small numbers, say f1, f2,…,  fk.
An eavesdropper, Trudy, that sees ga mod p will be able to calculate Alice’s private number a, as  follows.
 (p−1)/f.
Take any factor f of p−1.
There will be f elements generated by g .
Since f is small,  aTrudy can enumerate all of those elements.
Trudy can see what Alice’s a is, mod f, by taking g  mod p and raising it to the power ( p−1)/f .
By matching the result with the enumerated members of  the subgroup of size f , Trudy now knows that a is mod f.  Trudy does this with all the factors of p−1.
Now Trudy knows, for each factor f, what Trudy’s  a is mod that f. By the Chinese remainder theorem (§2.7.6), Trudy can now compute what a is mod  the product of all the fs, which is Alice’s number a.  So having p−1 be smooth would not be secure at all.
Any eavesdropper would be able to cal - culate Alice’s and Bob’s private values.
 Now let’s instead choose a p such that p −1 has one large factor q. It doesn’t matter what the  other factors of p−1 are, so let’s assume they are all small.
There will be a subgroup with q ele- ments.
An algorithm using this technique ( e.g., DSA) will be designed so that some of the opera - tions will still be done mod p, so the “prime breaking” we discussed in the beginning of the section  is not a problem.
The generator g that will be chosen for this kind of optimized Diffie-Hellman will  generate the order q subgroup.
6.4.4 DIFFIE -HELLMA N 159  Consider Protocol 6-6 : Alice g a mod p Bob  g b mod p, {“Hello?”}
g ab mod p  Protocol 6-6.
Diffie-Hellman Exchange  If Alice were malicious, and if she can interact with Bob lots of times, and if Bob keeps using the  same private number b, then Alice will be able to calculate b by using a technique similar to what  Trudy used when p −1 was smooth, as follows.
 Malicious Alice creates her public number A, not by raising the chosen generator g to a  power, but instead by choosing an A that generates a small subgroup of size f. With this A, Alice  can calculate what Bob’s value b is mod f, because he encrypts something recognizable using key  bAb—Alice goes through the f possible values of A and discovers which of those Bob used to  encrypt his message.
Since, for efficiency, these algorithms allow Alice and Bob to use reasonably  small exponents ( e.g., 256 bits), Alice only needs to get what Bob’s value is mod a set of fs such  that the product of those fs is at least 256 bits.
 So, Bob has to check whether Alice is sending him a malicious A. He does this by calculating  Aq mod p. If the result is 1, then A is assumed honestly chosen.
Otherwise, Bob recognizes that  Alice is not playing by the rules.
 6.4.4 ElGamal Signatures  Using the same sort of keys as Diffie-Hellman, ElGamal came up with a signature scheme  [ELGA85] to sign hash( m).
It is much harder to understand than the RSA signature scheme.
It is  useful to understand ElGamal signatures, though, because it will make it easier to understand DSA  and ECDSA (§6.5). (
ECDSA is the DSA algorithm using elliptic curves.)
 ElGamal signatures require using a well-known g and p. The signer (Alice) needs a long-term  public/private Diffie-Hellman key pair (the public key being A=ga mod p and the private key being  a as described for Diffie-Hellman).
She then signs message m by doing the following:  • Alice computes a hash of the message: M = hash( m).
 • She randomly chooses a per-message secret t relatively prime to p−1 and computes t−1  mod( p −1).
Since t is chosen to be relatively prime to p−1, this calculation is possible.
 • She computes T=gt mod p. The value T will be part of her signature on hash( m).
 • She computes S = (M −aT)×t−1 mod ( p−1).
 • Her signature on m is 〈T, S〉.
160 F IRST-GENERATI ON PUBLIC KEY ALGORIT HMS 6.5  • Bob verifies the signature by doing the following:  • He computes the hash of the message: M = hash( m).
 M• He verifies that g = AT× TS mod p.  • To prevent certain forgery attacks, he must also verify that T and S are each greater than  0 and less than p−1.
 Why does verification work?
 aT+tSmod p. • AT×TS mod p = (ga)T×(gt)S mod p = g  aT+tS mod p = gaT+tS mod (p−1)mod p. • Since gp−1 mod p = 1, g  • From how S was computed, we get tS = (t×(M−aT)× t−1)mod (p−1) = M− aT mod (p−1).
 aT+M−aT mod (p−1) mod p = gM mod (p−1) mod p. • Substituting for tS, we get AT× TS = g  It is not very intuitive why this is secure.
The important things to try to convince your self of are:  • If the signature is done correctly, the verification will succeed.
 • If the message is modified after being signed, the inputs to the signature function will have  changed, and with overwhelming probability the signature will not match the modified mes - sage.
 The things you have to believe based on the Fundamental Tenet of Cryptography are:  • Someone who doesn’t know a will not be able to find values for T and S that will satisfy the  above equation.
 • Seeing any number of signatures will not help an attacker compute Alice’s private key, a.  ElGamal signatures in this form are rarely used today because there are variations on this theme  (e.g., DSA) that have better performance that are used instead.
We mention ElGamal signatures  because they are historically important and easier to understand than the more advanced techniques  that follow.
 6.5 DIGITA L SIGNA TUR E ALGO RITH M (DSA)  NIST, the (U.S.) National Institute of Standards and Technology, published an algorithm for digital signatures in 1991 similar to ElGamal but with better performance and shorter signatures.
The algo - rithm is known as DSA , for Digital Signature Algorithm .
 The evolution of the design of cryptographic algorithms is guided by the work of cryptana - lysts.
As mentioned in §6.4.3 Safe Primes and the Small-Subgroup Attack , Diffie-Hellman and  ElGamal are traditionally done with safe primes because if ( p−1)/2 is smooth, there are some spe -
6.5.1 DIGIT AL SIGNATURE ALGORI THM (DSA) 161  cialized attacks possible.
Safe primes are as far as you can get from having ( p −1)/2 be smooth.
 Using safe primes turns out to be overkill, since those specialized attacks only work if all factors of  (p−1)/2 are smaller than twice the workfactor to break the scheme.
That means that if p is chosen  large enough to have a 128-bit workfactor to break (currently believed to be about 3072 bits), it’s  only important that ( p−1)/2 have at least one prime factor q that is at least 256 bits in size.
DSA  takes advantage of that to construct a variation of ElGamal signatures that is just as secure, while  being much faster to compute and verify.
 Instead of all calculations being done mod p (where p is a large prime), some are done mod q  (where q is a smaller prime that divides p−1 but is at least twice as big as the desired security  strength).
Given the best known attacks, to get security comparable to a 128-bit AES key, ElGamal  signatures would need a 3072-bit prime p, while DSA would need a 3072-bit prime p and a 256-bit  prime q. Because DSA can do a lot of its computation using a 256-bit q instead of ElGamal’s  3072-bit p, signing with DSA is six times faster, and the signatures are twelve times smaller.
 As with RSA and Diffie-Hellman, DSA is defined for a number of different-sized parameters,  trading off security against performance.
 6.5.1 The DSA Algorithm  As with ElGamal, the DSA algorithm requires choosing long-lived parameters g, p, and q that will  be public.
The process of choosing these parameters is computationally intensive, but generating  new key pairs, signing, and verification are relatively fast.
The procedure for finding p and q  involves first finding a 256-bit prime q and then searching for 3072-bit primes of the form qn+1.
 The generator g is chosen to ensure that the subgroup generated by g has order q. To generate a sig - nature, we can assume that the parameters p, q, and g are already known.
 As with Diffie-Hellman and ElGamal, our signer Alice will have a public/private key pair  where a is her private key, and A=ga mod p is her public key.
The signing procedure follows the  signing procedure for ElGamal except that some of the operations are done mod q, and many of the  exponents are q-sized rather than p-sized:  • Alice computes a hash of the message: M = hash( m).
 • She randomly chooses a per-message secret t (where 0 < t < q), and she computes t−1 mod q.  • She computes T= ((gt mod p) mod q).
The value T will be part of her signature on hash( m).
 • She computes S =(M +aT)×t−1 mod q.  • Her signature on m is 〈T, S〉.
 Bob verifies the signature with the following calculation (which is somewhat different from the  ElGamal verification):  • He computes the hash of the message m: M = hash( m).
162 F IRST-GENERATI ON PUBLIC KEY ALGORIT HMS 6.5.2  • He calculates x=M× S−1 mod q.  • He calculates y=T×S −1 mod q.  • He verifies that T=(gx ×Ay mod p) mod q,  Why does this work? (
Homework Problem 8 ).
 6.5.2 Why Is This Secure?
 What does it mean to be secure?
It means several things.
 • Signing something does not divulge Alice’s private key a.  • Nobody should be able to generate a signature for a given message without knowing a.  • Nobody should be able to generate a message that matches a given signature.
 • Nobody should be able to modify a signed message in a way that keeps the same signature  valid.
 Why does DSA have all these properties?
It is believed secure because of the Fundamental Tenet of  Cryptography.
DSA also has the blessing of the NSA, arguably the best cryptographers in the  world.
Unfortunately, it’s a mixed blessing, since some cynics believe NSA would never propose an  algorithm it couldn’t break.
 6.5.3 Per-Message Secret Number  Both DSA and ElGamal require that the signer generate a unique secret number t for each message.
 If the same secret number were used for two different messages, it would expose the signer’s pri - vate key.
Likewise, if a secret number t were predictable or guessable, the signer’s private key  would be exposed.
 How is Alice’s private key exposed if the secret number for a message is known?
In DSA, the  signature is S=(M+aT)×t−1 mod q. Remember that t is the secret number, T is gt mod p mod q, and  a is the signer’s private key.
So if t is known (or can be guessed), then we can compute  a=(St−M)T −1 mod q.  Knowing Alice’s private key a, we can forge DSA signatures on anything.
 How is the private key exposed when two messages share the same secret number?
In DSA, if  m and m′ are signed using the same secret number t, then we can compute  a=(S −S′)−1(hash( m)−hash( m′)) mod q.  Similar arguments exist for ElGamal signatures.
See Homework Problem 7.
 An application that NIST was particularly designing for was doing signatures on the type of  low-cost, low-performance smart card available in the 1990s.
For example, a smart card could be
6.6 HOW SECURE ARE RSA AND DIFFIE -HELLMA N?
163  embedded on a badge that would authenticate to a door.
It would be unpleasant if a person had to  wait many seconds for the door to open.
 DSA requires taking a multiplicative inverse mod q, which is quite expensive.
Both the signer  and the verifier will need to do this multiplicative inverse operation.
But DSA can be implemented  so that the signer can pre-compute a per-message secret t and the multiplicative inverse of t mod q.  There are several ways of generating a unique secret number for each message.
 • Use truly random numbers.
The problem with this is that it requires special hardware.
It’s dif - ficult enough to make hardware predictable, but it’s even harder to make it predictably unpre - dictable.
 • Use a cryptographic pseudorandom number generator.
The problem with this is that it  requires nonvolatile storage in order to store its state.
 • Compute t to be a cryptographic hash of a combination of the message and the signer’s pri - vate key.
The problem with this is that t (and therefore t−1) can’t be computed until the mes - sage is known.
 6.6 HOW SECUR E ARE RSA AND DIFFIE-HELL MAN?
 A brute-force attack (trying all possible keys) requires an exponential (in the length of the key)  work factor.
The security of RSA is based on the difficulty of factoring.
The security of  Diffie-Hellman is based on the difficulty of solving the discrete log problem.
The best-known algo - rithms for solving them on a classical computer are subexponential (less than exponential) but  superpolynomial (more than any fixed-degree polynomial).
Because the difficulty is subexponen - tial, the required size of the keys in these public key algorithms is much larger (say, 3072 bits) than  a corresponding secret key (say 128 bits).
 NIST has constructed tables estimating the keysizes needed in various algorithms to get vari - ous levels of security (Figure 6-7).
These may change if there are breakthroughs in cryptanalysis  but should not be affected by speedups in hardware (though the needed level of security will be).
 The table starts at 80 bits, which was considered adequate at one time, but going forward, a crypto - graphic strength of at least 128 bits should be used.
 Note, as we will explain in Chapter 7 Quantum Computing , a sufficiently large quantum com - puter would be able to factor and compute discrete logs efficiently enough that none of the algo - rithms in this chapter would be secure, though secret key algorithms and hashes would survive.
164 F IRST-GENERATI ON PUBLIC KEY ALGORIT HMS 6.7  Security Secret Hash RSA DH DSA ECC  Strength KeySize Size Modulus Exponent Modulus q p KeySize  80  112  128  192  256 80 160 1024 160 1024 160 1024 160  112 224 2048 224 2048 224 2048 224  128 256 3072 256 3072 256 3072 256  192 384 7680 384 7680 384 7680 384  256 512 15360 512 15360 512 15360 512  Figure 6-7.
Security Strength of Various Algorithms  6.7 ELLIPTI C CURV E CRYPTOGRA PHY (ECC)  As we said in §6.6 How Secure Are RSA and Diffie-Hellman? ,
there are known subexponential (but  superpolynomial) algorithms for breaking RSA and Diffie-Hellman based on modular arithmetic  on a classical computer.
Elliptic curve cryptography (ECC) is important because no one has (yet?)
 found subexponential algorithms (on a classical computer) for breaking it.
Therefore, it is believed  to be secure against attacks with classical computers with much smaller keysizes, which is impor - tant for performance.
ECC is a popular replacement for public key cryptographic schemes like  RSA, Diffie-Hellman, ElGamal, DSA, etc.
For some of these cryptographic schemes, you can  replace modular multiplication by elliptic curve multiplication directly, resulting in algorithms  referred to as ECC Diffie-Hellman (or ECDH) and ECC DSA (or ECDSA).
Of course, none of  these would be secure if there were a sufficiently large quantum computer running Shor’s algorithm  (§7.3 Shor’s Algorithm ).
 In general, an elliptic curve is a set of points on the coordinate plane satisfying an equation  of the form y 2+axy+by = x 3+cx 2+dx+e, though only a few special forms of elliptic curves are used  in cryptography.
Note that ellipses are not elliptic curves.
While some of the original mathematical  exploration of elliptic curves was related indirectly to ellipses, there is no relationship that is useful  in cryptography.
Originally, elliptic curves were conceived where the two coordinates x and y were  real or complex numbers, but cryptography uses elliptic curves where x and y are members of a  finite field whose size is either a large prime p or a number of the form 2n .
With finite field coordi - nates, there is no way to make a sensible graph that looks like curves, although they are still called “curves”.
The currently most popular curves (because they give the best performance for a given  2level of security) have formulas of the form y = x 3 + ax + b and use a finite field with mod p arith - metic where p is a large prime very close to (but less than) a power of 2.
Elements on the curve are  points with an x and y coordinate .
Because the formula defining the curve has y 2 on the left side of
6.7 ELLIP TIC CURVE CRYPTOGRAPHY (ECC) 165  the equation, if the point 〈x, y〉 is on the curve, then 〈x, −y〉 is also on the curve.
These will be the  only points on the curve with that particular x coordinate, so specifying the x coordinate and the  sign of the y coordinate is enough to determine the point.
When encoding the value of an elliptic  curve point, it is common to represent it as an x coordinate that is an integer between 0 and p and a  single bit that distinguishes between the two possible y values.
 In order to use elliptic curves for, say, Diffie-Hellman, there needs to be some mathematical  operation on two points on the curve that will always produce a third point on the curve.
Let’s call  that operation “multiplication”, although in ECC it will not look like any multiplication you are  used to.
 Be warned when reading other papers about elliptic curve cryptography, most authors refer to  the operation applied to points on the curve as addition rather than multiplication.
Where we talk about raising g to some power, they will speak of scalar multiplication of g by an integer.
This is  just a matter of notation used and does not change any of the results.
We believe the use of multipli - cation for the operation on points makes things clearer because it makes the parallels between the  mod p and the elliptic curve versions of Diffie-Hellman and DSA more obvious.
And while they  call the repeated operation of a point with itself scalar multiplication , they still call the process of  reversing that operation (which is needed to break the cryptosystem) computing a discrete loga - rithm rather than scalar division .
When describing elliptic curves with the operation being addition,  the identity element of the group is referred to as 0.
If the operation is referred to as multiplication,  the identity element is referred to as 1.
 The operation (that we call multiplication) has to be associative, so that you can use the  repeated squaring trick to raise a number to a large power in time linear with the length of the expo - nent.
In other words, to “exponentiate” a point by 128, you should be able to multiply the point by  itself (you’ve now raised it to the power 2), then multiply the result by itself (you’ve now raised it to  the power 4), multiply the result by itself (to have raised it to the power 8), and so on.
Since this  x) y xy y)xmultiplication operation is associative, it will be true that ( g = g = (g .
And it is also impor - xtant that doing discrete logs is hard (knowing g and g , it is difficult to compute x).
We’ll give the  formulas for the multiplication operation shortly.
 Before giving the formulas, there is one more important detail.
To make the set of points sat - isfying the elliptic curve formula a group, it is necessary to define one additional point, called the  point at infinity , and come up with some unique representation for it in memory when doing this  arithmetic on a computer.
This is the identity element of the group, so we’ll call it 1.
Multiplying  any point by 1 is easy.
For any point P, P× 1 = 1×P = P .
To multiply any two points 〈x1,y1〉 and  〈 x2,y2〉 where x1≠ x2, the result is 〈x3,y3〉 where:  • x3=((y2− y1)/(x2−x1))2−x1−x2  • y3=((y2− y1)/(x2−x1))× (x1−x3) − y1
166 F IRST-GENERATI ON PUBLIC KEY ALGORIT HMS 6.7.1  These formulas don’t work when x1=x2.
If x1=x2, then either y1=y2 or y1= −y2.
If y1= −y2, then the  two points are inverses, and 〈x1,y1〉×〈 x1,−y1〉=1.
Multiplying a point by itself has a special for - mula: 〈x1,y1〉×〈x1,y1〉 =〈x2,y2〉 where:  2• x2=((3x12+a)/2y1)2−2x1 where a comes from the formula y =x 3+ax+b  • y2=((3x12+a)/2y1)×(x1−x2) −y1  With point multiplication so defined, the set of points on an elliptic curve form a group, complete  with an identity, inverses, and a multiplication operation that is commutative and associative.
While  more complex to understand, it is faster, at least for private key operations, since until someone  comes up with a subexponential algorithm for breaking ECC, the keys can be smaller.
For public  key operations, such as signature verification, RSA is likely to be faster, even with larger keys,  because it can use a small public exponent.
 Note that since the difficulty of Shor’s algorithm (§7.3) depends on the size of the key  (whether doing ECC or modular arithmetic), ECC, with its smaller keys, will likely be broken before RSA.
 When doing cryptography with elliptic curve groups, we don’t always use all the elements of  ithe group.
We pick some element g of the group and use only elements that can be computed as g  for some integer i. This will be a subgroup of the elliptic curve group (sometimes the entire group).
 With the right choice of g, a, b, and p, the result will be a subgroup whose size is a prime about the  same size as p.  6.7.1 Elliptic Curve Diffie-Hellman (ECDH)  Unlike mod p Diffie-Hellman and ElGamal, no one has discovered any separate operation to  “break” an elliptic curve with an expensive operation that makes future breaking of individual pub - lic keys or exchanges using that curve easy.
As a result, almost all uses of elliptic curve cryptogra - phy use one of the pre-calculated curves published by NIST [NIST13—Appendix D].
They have  published a series of curves with different sizes (for different security strengths) and based on dif- ferent finite fields.
 The math associated with ECDH (Protocol 6-8 ) looks just like the math associated with  Diffie-Hellman.
Alice and Bob each choose a random number of twice the needed security strength  (e.g., 256 bits for security comparable to AES-128).
Alice raises the generator of the elliptic curve  group to the power a, Bob raises it to the power b, and they exchange values.
They then each com -
6.7.2 ELLIP TIC CURVE CRYPTOGRAPHY (ECC) 167  pute gab by raising the value they receive to the power of their private exponent.
But an eavesdrop - per who sees ga and gb will not be able to compute gab without an extraordinary effort.
AliceBob  Protocol 6-8.
Elliptic Curve Diffie-Hellman Exchange  6.7.2 Elliptic Curve Digital Signature Algorithm (ECDSA)  The formulas for ECDSA are remarkably similar to those for DSA.
It’s just that they use a different  form of arithmetic.
Given an elliptic curve group with some prime number n of elements and a gen- ierator g, all the elements can be expressed as g , where 0≤ i< n. The exponents are integers and  nsince g =1, you can do arithmetic with them mod n.  The points on the elliptic curve will have x and y coordinates and be expressed as an ordered  pair 〈x, y〉.
Alice will choose a public/private key pair where her private key will be a randomly cho - sen integer a between 2 and n−1.
In order to sign a message m, she must perform the following  steps:  • Alice computes a hash of the message: M=hash( m)  a• Alice has a long-term public key A=g .
She chooses a per-message secret t.  t• She computes T=the x-coordinate of g .
The value T will be part of her signature on m.  • She computes S=(M +aT)× t−1 mod n  • Her signature on m is 〈T, S〉.
Note that these are two integers—no curve points involved.
 The calculations Bob does to verify the signature also follow the same formulas as for DSA:  • He calculates x=M×S −1 mod n.  • He calculates y=T×S −1 mod n.  x• The signature is valid if T =the x-coordinate of g Ay  As with DSA, Bob needs to do some additional checks to ensure that Alice’s value is properly gen - erated (see §6.4.3 Safe Primes and the Small-Subgroup Attack ).
a g  b g
168 F IRST-GENERATI ON PUBLIC KEY ALGORIT HMS 6.8  6.8 HOMEWO RK  1.
In mod n arithmetic, why does x have a multiplicative inverse if and only if x is relatively  prime to n?
 2.
In section §6.4.2 Defenses Against MITM Attack , it states that encrypting the Diffie-Hellman  value with the other side’s public key prevents the attack.
Why is this the case, given that an  attacker can encrypt whatever it wants with the other side’s public key?
 3.
In RSA, given that the primes p and q are approximately the same size, approximately how  big are p and q compared to n?
How big is φ(n) compared to n?
 4.
What is the probability that a randomly chosen number would not be relatively prime to some  particular RSA modulus n?
What threat would finding such a number pose?
 5.
Suppose there is no enforced padding structure such as in PKCS #1.
Suppose Fred sees your RSA signature on m 1 and on m2(i.e.,
he sees m1 d mod n and m2 d mod n).
How does he com - pute the signature on each of m1j mod n (for positive integer j), m1 −1 mod n, m1× m2 mod n,  and in general m1j×m2 k mod n (for arbitrary integers j and k)?
 6.
Suppose we have the encoding that enables Carol to mount the cube root attack (see §6.3.5.2  The Cube Root Problem ).
If Carol sends a message to Bob, supposedly signed by you, will  there be anything suspicious and noticeable about the signed message, so that with very little additional computation, Bob can detect the forgery?
Is there anything Carol can do to make  her messages less suspicious?
 7.
In ElGamal, how does knowing the secret number used for a signature reveal the signer’s pri - vate key?
How do two signatures using the same secret number reveal the signer’s private key?
Hint: p−1 is twice a prime.
Even though not all numbers have inverses mod p−1, divi - sion can still be performed if one is willing to accept two possible answers. (
We’re neglecting  the case where the divisor is ( p−1)/2, since it’s extremely unlikely.)
 8.
Show that verification works in DSA.
 9.
Suppose (see §6.3.4.3 Why a Non-Prime Has Multiple Square Roots of One ) you know a non - trivial square root, y, of n. Show how this will let you factor n. Hint: ( y 2−1) mod n = 0.
7 QUANTUM  COMPUT ING  7.1 WHAT IS A QUANTU M COMPUTER ?
 Quantum mechanics predicts that it should be possible to build a computer that can do certain cal - culations much faster than would be possible on a conventional (classical) computer.
Aspects of  quantum mechanics may seem nonintuitive, but all evidence supports it.
In this chapter, we describe  how a quantum computer differs from a classical computer and give intuitive descriptions of the  quantum algorithms most relevant to cryptography.
 There are entire books about quantum mechanics.
Our goal isn’t to pack years of physics and  math into a few pages but to give some insight into the concepts, terminology, and notation, as well  as the algorithms that run on quantum computers.
And for readers who want to study the topic more  deeply, hopefully this introduction will make it easier to understand the deeper literature.
 When I 2 challenged me 4 to write something “really short” about what a quantum computer is,  I4 wrote “a quantum computer is a magic box that factors numbers quickly.”
We want to go into  more detail than that, but keep the chapter reasonably intuitive and short.
 7.1.1 A Preview of the Conclusions  First, we’ll hint at the conclusions, and then we’ll explain why they are true.
 A common misconception is that a quantum computer is faster than a classical computer, and  that because Moore’s law is slowing down, eventually all computers will be replaced by quantum  computers.
This is definitely not true.
There is only a narrow set of problems for which a quantum  computer would be faster.
The property of a quantum computer that makes it excel at some tasks is  that it can compute, with only storage size n, as if it were operating on 2n values in parallel.
How - ever, as we’ll see later in this chapter, it also has serious limitations, such as if you read the state you  will see only one value and the others disappear.
The magic of a typical quantum program is for the  169
170 Q UANTUM COMPUTI NG 7.1.2  quantum computation to raise the likelihood that when you finally read a result, it will be a useful  value.
 Another common misconception is that a quantum computer can solve NP-hard problems  (such as the traveling salesman problem) in polynomial time.
This is almost certainly not true.
 Although nobody has proven that it is impossible, no known quantum algorithm for solving such  problems is that powerful.
Indeed, a quantum algorithm that powerful would be nearly as surpris - ing, given our current state of knowledge, as a classical algorithm that was that powerful.
 Although a quantum computer can, in principle, do any calculation that a classical computer  can do, for most calculations quantum computers would be no faster than conventional computers.
 Also, in practice, quantum computers are likely to be much more expensive to build and operate.
 For example, most designs would need to operate at temperatures very close to absolute zero.
So it  is not likely that quantum computers will ever serve more than a small niche market doing  extremely CPU-intensive calculations on tiny amounts of data.
However, there are two important  quantum algorithms that are relevant to cryptography.
 · Grover’s algorithm, which makes brute-force search faster.
This might seem worrisome for  things like encryption or hashes, where a brute-force search can find a key or a pre-image.
 But the limit of Grover’s algorithm is that it only reduces the search time to the square root of  the size of the search space.
Squaring the size of the space being searched (for instance, using  an encryption key twice as long) is more than adequate to protect against Grover’s algorithm.
 · Shor’s algorithm, which can efficiently factor numbers and calculate discrete logs.
Shor’s  algorithm, run on a sufficiently large quantum computer, would break all our widely used  public key algorithms ( e.g., RSA, Diffie-Hellman, elliptic curve cryptography, ElGamal).
At  the time of this writing, no quantum computer large enough to break the currently deployed  public keys has ever been publicly demonstrated, and some experts remain skeptical that one  ever will be built.
There are a number of difficult engineering challenges that remain, and it  may never be economically viable to overcome them.
But because such a computer might be  possible, it is important to convert to quantum-safe public key algorithms well before a quan - tum computer (of sufficient size) might exist.
The cryptographic community is actively devel - oping and standardizing such algorithms, which we describe in Chapter 8 Post-Quantum  Cryptography .
 7.1.2 First, What Is a Classical Computer?
 Classical computers compute on information stored in bits.
Each bit stores either a 0 or a 1.
A clas- sical computer with n bits can be in one of 2n states.
For instance, with three bits, the possible states  are 000, 001, 010, 011, 100, 101, 110, 111.
A classical computer operates on bits using gates  (such as AND and NOT ) that take some number of bits as input and then output some number of
- - -7.1.3 WHAT IS A QUANTUM COMPUT ER?
171  bits.
A cla ssica l gate can be describe d wit h a tabl e tha t indicates , give n the valu e of the inpu t bit(s),  what the valu e of the output bit(s ) are.
For instance , the cla ssical AND gate’ s tabl e is:  Input Out put  00 0  01 0  10 0  11 1  In quantu m com putation , ther e is an intuitively simila r notion of a gate .
However, wherea s a  classica l gate read s inputs an d write s the corres pondi ng out put valu e to a differen t location , a quan - tum gat e operate s on a collecti on of qubit s and change s the state o f those qubits .
We’l l describe  more about that later.
 7.1.3 Qubits and Superposition  Instea d of bits , a quantu m com puter uses qubits , wher e a qubit’s stat e can be a mixtur e of a 0 and a  1.
This is k nown as superposition .
The standard notati on fo r describi ng superpositi on is know n as  ket notati on.
A ke t is a sym bol o r valu e written betwee n a vertica l bar and a righ t angle bracke t rep- resenti ng a state .
The stat e of one o r mor e qubit s is usuall y denoted by |y æ .
The standar d notati on  for a single qubit’s stat e is |y æ = a |0æ +b |1æ .
The coefficient s (a and b ) determin e how likel y the  measure d valu e wil l be 0 versus 1.
Measurin g a qubit (readi ng it) destr oys th e superpositio n info r- mation, and the qubit take s on the valu e read—eithe r completely 0 (writte n as 1|0æ +0|1æ , or simply  |0æ ) or completely 1 (written as 0|0æ +1|1æ, or simpl y |1æ ).
 The sta ndard notatio n for the stat e of two qubit s is |y æ = a |00æ +b |01æ+g |10æ +d |11æ .
The  coefficient s (a , b , g , d ) determin e how likely the measure d valu e wil l be 00, 01,10, or 11.
 Once measured , a qubit is no differen t from a classica l bit—it’ s either 0 or 1.
How doe s a  qubit becom e a mixtur e of 0 and 1?
A quantu m com puter can com pute wit h gate s tha t creat e supe r- positi on, a s we wil l sho w in §7.1.5.
 The coe fficient s (a and b ) of the qubit stat e are called probability amplit udes , and their  square d absolut e value s are probabil ities.
Give n tha t the tota l probabi lity mus t be 1, |a |2+|b |2=1.
 The probabi lity tha t a qubit in state a |0æ +b |1æ would be rea d as 0 is |a |2, and th e probabilit y tha t it  1 1 would be rea d as 1 is |b |2.
For example , if a = ------and b = ------, then the probabil ity of th e state a |0æ 2 2  + b |1æ being rea d as 0 is ½, and th e probabilit y of i t bein g rea d as 1 is also ½. In mos t literatur e the  coefficient s are refe rred to as amplit udes.*
 * For a while , we’ll us e the term coefficient rathe r than amplitude because a in the expre ssion a |0æ describe s both the mag - nitud e and the phas e (see Figure 7-2) , and the words ampl itude and magnitude are ofte n use d as synonyms i n nont echnica l  English.
m 172 Q UANTUM COMPUTI NG 7.1.3  The coefficients are actually complex numbers.
But, for ease of drawing a figure, and because  until we explain Shor’s algorithm we only need to use real coefficients, assume for now that the  coefficients are real numbers.
Given that the probabilities | a |2+|b |2 need to add up to 1, if you were  to graph all the potential (real) values of a and b , you’d wind up with a circle of radius 1.
Note that  if you change the sign of a or b , the probability of reading 0 or 1 is not changed.
For instance, in  the left half of Figure 7-1, we show a |0æ +b |1æ.
And in the right half of Figure 7-1, we show  a |0æ -b |1æ .
In both cases, the probability that the qubit would be measured as 0 is |a |2, and the prob - ability that the qubit would be measured as 1 is |b |2.
 |1> |1>  ab a| 0>+b| 1>  |0> |0>a -b a| 0>-b| 1>  Figure 7-1.
Real Coefficients  If two coefficients are different, but they have the same absolute value, the two coefficients  are said to have different phases .
If the coefficient is a real number, the phase is just a choice of plus  or minus.
For complex numbers, there is a continuum of possible phases. (
If we plot a complex  coefficient x+yi as a vector in two dimensions, its phase is the angle the vector makes with the real  axis, and its absolute value is the vector’s length.
See Figure 7-2 and note that a coefficient can have  absolute value no more than 1 and so can only exist within the radius-1 outer circle; any coefficient  on the radius- m inner circle has absolute value m.)  i  -1 0  -i yi  x x+yi  q 1  Figure 7-2.
A Complex Number x+yi with Absolute Value (aka Magnitude) m and Phase q  Why should we care about the phase of a coefficient if it doesn’t change the probability of  that value being read?
The answer is that in useful quantum computations the state is repeatedly  altered by transformations called quantum gates before the state is measured, and the probability  of reading a certain value after the gates are applied generally depends on what the phases of the
7.1.3.1 WHAT IS A QUANTUM COMP UTER?
173  coefficients were before the gates were applied.
We’ll talk more about quantum gates later in the  chapter.
 7.1.3.1 Example of a Qubit  A photon behaves as a qubit, where the polarization of the photon can be thought of as its quantum  state.
It can be polarized to be up/down (which we’ll interpret as 1), right/left (which we’ll interpret  as 0), or anything in between.
If a polarizing filter is exactly aligned with the photon, the photon  will definitely pass through the filter.
If the filter is 90 ° off from the photon’s polarization, the pho - ton will definitely not pass through.
If the filter is 45 ° off from the polarization of the photon, the  photon will pass through with probability ½. More generally, if the photon is polarized at angle f  relative to the filter, then the probability that the photon passes through the filter is cos2f .
 What’s happening is that the polarizing filter is measuring the photon.
The values 0 and 1 are  relative to the alignment of the filter; the photon’s state is sin f |0æ +cosf |1æ .
If the photon passes  through, it means it has been measured as 0 relative to the polarizer.
And since measurement  destroys the 1 component of the photon, the photon will now be exactly aligned with the filter.
 This behavior can easily be demonstrated in real life with three polarizing filters.
If you put  two filters aligned 90 ° from each other on top of each other, no light passes through (none of the  photons get through).
However, if you insert the third filter between those two, and align it 45 ° from  the others, light will pass through the combination of the three filters (½ will pass through the first  filter, then of those, ½ will pass through the second filter, and then of those, ½ will pass through the  third filter). (
See Homework Problem 1.)
 Note that what you define as 0 and 1 is arbitrary, as long as what is defined as 0 is orthogonal  to what is defined as 1.
 7.1.3.2 Multi-Qubit States and Entanglement  Another strange concept in quantum computers is entanglement —where a collection of qubits has  a state that can be described collectively but cannot be described by specifying the states of the  individual qubits.
For instance, three qubits can be in a superposition of all eight possible classical  states: 000, 001, 010, 011, 100, 101, 110, 111.
To express the state | y æ of the set of three qubits,  the notation would be  |y æ = a |000æ +b |001æ +g |010æ +d |011æ +e |100æ +z |101æ +h |110æ +q |111æ .
 The probability is 1 that the set will be in one of those eight states.
So,  |a |2+|b |2+|g |2+|d |2+|e |2+|z |2+|h |2+|q |2 = 1,  and the probability that the set of three qubits would be measured as 011 is |d |2.
 It requires 2n complex numbers (coefficients) to express the state of n entangled qubits.
How - ever, if the qubits are not entangled, i.e., are independent of each other, their state can be expressed  more compactly, with only 2 n coefficients.
The state of the first qubit can be expressed as
174 Q UANTUM COMPUTI NG 7.1.4  a 1|0æ +b 1|1æ .
The state of the second qubit can be expressed as a 2|0æ +b 2|1æ .
The state of the third  qubit can be expressed as a 3|0æ +b 3|1æ , and so on.
However, to specify the state of ten entangled  qubits, it requires 1024 coefficients (one coefficient for each of the 210 possible values of ten bits).
 In contrast, if the ten qubits are unentangled, the state of each of the qubits can be expressed with  two coefficients, requiring only 20 coefficients.
 While the compact notation using 2 n coefficients cannot be used to describe an entangled  state, the less compact notation using 2n coefficients can be used to describe unentangled states.
In  an unentangled state, you can derive the coefficients of all 2n classical states from the coefficients  of |0æ and |1æ in each of the n unentangled qubit states.
For instance, with three unentangled qubits  in states a 1|0æ +b 1|1æ , a 2|0æ +b 2|1æ, a 3|0æ +b 3|1æ , respectively, the coefficient of state | 000æ in the  collective state of three qubits would be a 1a 2a 3.
Likewise, the coefficient of | 001æ in the collective  state would be a 1a 2b 3, the coefficient of | 010æ would be a 1b 2a 3, and so on. (
See Homework Prob - lem 2.)
 7.1.4 States and Gates as Vectors and Matrices  The quantum state of a collection of qubits can be represented as a column vector of coefficients,  one for each bit combination.
By convention, we’ll order the coefficients according to the corre - sponding bit combinations, so, for one qubit the order is just 0,1; for two qubits it’s 00,01,10,11.
 You can think of this as a shorthand to avoid writing out the bit combinations every time.
For exam - ple, we can write a |0æ +b |1æ as  a  b  A quantum gate can be represented as a matrix of multipliers.
For a single-qubit gate, the first  column gives the coefficients of final states | 0æ and |1æ when the initial state is | 0æ , while the second  column gives the final state coefficients when the initial state is | 1æ.
Again, this is a shorthand to  avoid writing out the bit combinations.
When we describe gates, we’ll show their matrix representa - tions as well.
For example, we can write the NOT gate as  é 0 1ø  êœ1 0ëß  An example of a 2-qubit gate is known as CNOT (controlled not ) (Figure 7-3 ), which flips  the second qubit between 0 and 1 if the first qubit is 1.
It can be defined by how it acts on | 00æ , |01æ ,  |10æ , and |11æ.
In the matrix representation, the first column gives the coefficients of final states  |00æ , |01æ , |10æ, and |11æ when the initial state is | 00æ.
The second column gives the coefficients of  final states | 00æ , |01æ , |10æ , and |11æ when the initial state is | 01æ, and so forth.
7.1.5 WHAT IS A QUANTUM COMPUT ER?
175  initial state final state é 1000ø  |00æ |00æ ê œ0100|01æ |01æ ê œ  ê 0001œ|10æ |11æ ê œ  |11æ |10æ ë 0010ß  Figure 7-3 .
CNO T Gate  The beaut y of thi s representati on is tha t the app licatio n of a gat e to a state i s accomplishe d by  multiplyi ng th e gate matri x time s the state vector .
For instance , to apply th e NOT gat e to a qubit in  state a |0æ +b |1æ , you’d multipl y the NO T matri x by th e colum n vecto r representi ng th e qubit state.
 The resu lt of applyi ng a sequenc e of gate s is give n by the produc t of the gate matrice s in  right-to-lef t order.
 7.1.5 Becomin g Sup erpose d and Entangled  Entanglemen t and superpositio n are wha t can mak e a quantu m com puter mor e powe rful tha n a  classica l computer .
How do qubit s becom e superposed and entangled ?
You’d likely star t a com puta- tion on a quantu m com puter by in itializi ng th e qubit s to a know n state , whic h would be done by  measuri ng al l the qubits .
Now none o f them are enta ngled , and the y are all firmly 0s or 1s.
 Ther e are variou s operati ons (gates ) one ca n apply to qubit s tha t wil l caus e superpositi on  and/or enta nglement .
For instance , superpos ition ca n be create d on a single qubit using a quantum  gate know n as a Hadamar d gate, whic h take s a qubit that is solidl y zero o r solidl y one a nd set s it  to be a n equal mixtur e of 0 and 1.
Th e Hadamar d gat e operati on is shown in Figur e 7-4:  initial state final state  |0æ+ |1æ |0æ- |1æ1 2 ------ -1 2----- - - 1 1 êê  ë - 21 2 1 2 2 2 ------ - 2 ----- - -é 1 1 ø  |0æ œ  œ  |1æ ß  truth tabl e representati on matri x representati on  Figur e 7-4 .
Hadamard Gate  Gate s tha t operat e on a set of qubit s can entangl e the set .
For instance , if you hav e unentan - gled qubits x, y, and z and operat e on x and y, then x and y may becom e enta ngled .
If you the n ope r- ate on y and z, then all thre e qubits (x, y, and z) may now be enta ngled .
A gat e migh t also unentangle  previ ously enta ngled qubits.
 The trut h tabl e representa tion show s wha t the Hadamar d gat e doe s to a qubit initia lized to 0  or 1.
If the qubit starte d out wit h a superposed state , for instance , a |0æ +b |1æ , then to calculat e the  outpu t of the Hadamar d on a |0æ +b |1æ , you a dd th e out puts of |0æ and |1æ in proportion to th e coe ffi-
- - - - - - - - -176 Q UANTUM COMPUTING 7.1.6  cient s of the input state .
Sinc e the coefficien t of |0æ in the input is a , and the output o f |0æ is  1 1 a a -----|0æ + ------|1æ, you multiply th e output o f |0æ by a to obtain ------|0æ + -----|1æ .
Likewise , you multiply2 2 2 21 1the out put o f |1æ (whic h is ------|0æ-------|1æ ) by b .
2 2  Addin g the tw o results , the Hadamar d gate’ s outpu t on a qubit wit h state a |0æ +b |1æ is  a + ba – b ------------|0æ + ------------ -|1æ .
2 2  This can als o be calculate d by multiplying th e matri x representatio n of th e Hadamar d gat e by  the colum n vecto r representi ng th e qubit state a |0æ +b |1æ :  aé  êë = = êê  œœ ß ê ê  ë -œß ø  êëé œœ ß-- 22 2 1  212 2  2 1  2 1 2 2 babaa  ba1 1 øé 1 + 1 bø é+bø  ê œ  ê œ  ë œß  The reas on w e can multiply th e outpu t of a state b y the coefficien t of that superpose d stat e in  the input is due t o one o f the propertie s of quantu m gate s know n as linearity , describe d in th e next  section.
 Now , for somethi ng really fascinating , wha t happen s whe n you a pply a Hadamar d gate  twice ?
Hadamar d is its own inverse ! (
See Homewor k Problem 7.)
 7.1.6 Linearity  Quantu m gate s satisf y the propert y of linearity .
The intui tion for linearity is tha t the output o f a  quantu m gat e is a wei ghted sum of the outputs of all the superpose d classica l input states , wei ghted  by the coe fficient s of the superposed cla ssica l input states .
So, if you kno w how th e gate wil l ope r- ate on th e possibl e classica l input states , you can multiply th e outpu t of eac h classica l input stat e by  the coefficien t of that classica l input state , and th e sum wil l be the quantu m stat e of the resu lt.
 Linearit y is nice , becaus e it mean s tha t we can describe n-qubit quantu m gate s by table s that  only describ e the output s produce d whe n the input is on e of the 2n classica l states .
The tabl e is suf- ficien t to determin e wha t the gat e wil l do to any in put state , becaus e all the possible quantu m states  can be e xpresse d as a superposition (linea r combinati on) o f the 2n classica l states .
For example , for  the CNO T gat e (Fi gure 7-3) , if the 2-qubit input stat e is a |00æ +b |01æ +g |10æ +d |11æ, linearity  implie s tha t the out put stat e is a |00æ +b |01æ +d |10æ + g |11æ.
 7.1.6.1 No Clonin g Theo rem  An interesti ng consequenc e of linearit y is tha t it is impossibl e to clone a qubi t (or set of enta ngled  qubits) .
If you ha d a qubit in state a |0æ +b |1æ , you migh t thin k you c ould Å it into a qubit in stat e |0æ  and wi nd up w ith two qubits , eac h in state a |0æ +b |1æ .
But instead , you’d wind up with tw o entan - gled q ubits in state a |00æ +b |11æ . (
See Homewor k Problem 8.)
7.1.7 WHAT IS A QUANTUM COMP UTER?
177  An important implication of the no cloning theorem is that you can’t read a quantum state  more than once.
If you could clone the state, you could make lots of copies, and by reading them,  read multiple of the superposed values.
If you could read enough copies, you could then derive the  probabilities of the various superposed states.
But you can’t.
Reading the quantum state destroys  the superposition, and because of no cloning, you can only read it once.
 7.1.7 Operating on Entangled Qubits  Quantum gates seldom operate on more than one or two qubits, because the engineering challenge  of building a quantum gate increases wildly with the number of qubits it is acting upon.
One could  imagine a quantum gate that operates on n qubits, but in practice it would almost certainly be built  out of gates that operate on one or two qubits at a time.
The running time of a quantum algorithm  employing a virtual n-qubit gate is adjusted to account for the actual gate configuration.
So, given  that we will only be using 1- or 2-qubit gates, what happens if you operate on a subset of entangled  qubits?
 For instance, suppose there were three entangled qubits with state a |000æ +b |011æ +g |100æ .
 Now suppose we use a NOT gate on the first qubit.
The result would be a |100æ +b |111æ +g |000æ .
 See Homework Problem 5 for a more complicated example.
 Measurement typically happens one qubit at a time.
Suppose you measure one qubit of an  entangled set of qubits and get the value 1.
A side effect of the operation of measuring that qubit is  that the coefficients of all the states that don’t match the measured value go to 0, and the coeffi - cients of all the states that do match the measured value are increased linearly to make the sum of  their squared absolute values equal to 1.
Note that this linear scaling does not change the phases of  the remaining coefficients. (
See Homework Problem 6.)
 7.1.8 Unitarity  Linearity (§7.1.6) constrains the possible things a quantum gate can do, such as preventing cloning  qubits.
Another constraint on quantum gates is that they must be unitary .
Unitarity is the property  of a linear gate that says if the input state of the gate is normalized ( i.e., the sum of the squared  absolute values of the coefficients is 1), then the output state is normalized.
These constraints are  not arbitrarily imposed by us or by some committee.
Instead, they result from the known laws of  physics.
 Unitarity seems like a sensible property.
It seems rather absurd to imagine that the probability  of all measurement outcomes could add to something other than 1 after you do a physically possi - ble operation.
Nonetheless, there are some operations that are forbidden by this constraint that seem
178 Q UANTUM COMPUTI NG 7.1.9  like they should be legal operations.
For example, suppose we tried to make a linear gate out of the  operation that sets a qubit to 0.
We will call this the “zeroize” gate, and its truth table will be  Input Output  0 0  1 0  Such a gate would violate unitarity, since when we apply the rule for linearity to the above truth  3 4table, we find the gate would take the normalized state - - -|0æ + 5- - -|1æ to the unnormalized state 53 4 7 3 4 - - -|0æ + - - -|0æ = - - -|0æ . (
Note that - - -|0æ + - - -|1æ is normalized because 32+42 = 52.)
5 5 5 5 5  Classical operations such as the zeroize gate that can’t be straightforwardly turned into a uni - tary gate are called irreversible operations.
What they have in common is that they take at least two  different inputs to the same output.
In contrast, reversible classical operations, those that take every  possible input to a different output, can be straightforwardly turned into valid unitary quantum  gates.
In the next two sections, we will show how to implement even irreversible classical opera - tions with quantum components, first by measurement and next by converting the irreversible clas - sical operation into a reversible classical operation.
 7.1.9 Doing Irreversible Operations by Measurement  One way to perform classical operations that are irreversible (from quantum building blocks) is to  use measurement.
For instance, the zeroize operation can be applied to a qubit as follows:  1.
Measure the qubit  2.
If you measure a 1, apply the NOT gate: | 0æ® |1æ, |1æ® |0æ .
If you measure a 0, do nothing.
 However, this way of implementing classical operations has a major drawback.
When you measure  a qubit in a superposition of multiple classical states, you only get information about one of the  classical states in that superposition.
Moreover, if the qubit you measured was entangled with some  other qubits, it won’t be entangled with them after you make the measurement.
Since superposition  and entanglement are necessary for quantum computing to be more powerful than classical comput - ing, it would be unfortunate if measurement was the only way to do certain classical computations  you wanted to use as part of a quantum algorithm.
 7.1.10 Making Irreversible Classical Operations Reversible  When we want to incorporate an irreversible classical computation into a quantum algorithm, it is  typical to add some extra qubits to the operation in order to make it reversible (and therefore uni - tary).
- - - - - -7.1.11 WHAT IS A QUANTUM COMPUT ER?
179  Suppos e you wanted t o implemen t an irreversibl e function that , in the classica l world , used i  input bit s and o outpu t bits .
One wa y tha t this can be mad e into a reversibl e functio n is by modify - ing th e function so tha t it takes i+o input bits , compute s the irreversibl e functio n on th e first i bits,  and Å s the resul t into the last o bits.
If the o bits are initialize d to 0, the computa tion is the same,  and th e new functi on is clearl y reversibl e since i f you execut e it twic e you ge t the origina l input.
 In a quantu m versio n of this , assum e the i input qubit s are in a superpositi on an d the o output  qubit s are 0s.
Afte r the functio n is executed , the i+o entangled qubit s will be in a superpositi on of  the same numbe r of states , with th e sam e coe fficients .
Eac h superposed valu e of the i+o entangled  qubit s consists o f one of the origina l superposed value s of the i input qubits, wit h the o qubits  entangle d wit h tha t stat e corresp onding to the out put o f the functi on on tha t input.
 Ther e are know n algorithm s for converti ng any computabl e cla ssica l functio n into a similarly  efficien t quantu m circui t that acts in the wa y we jus t described.
 7.1.11 Univ ersal Gat e Sets  The terms circu it and gate used in quantu m com puting wer e chosen to sou nd familia r to people  used to conventi onal com puters .
In principle , all classical operati ons c ould be buil t out o f NAND  gates , and a circui t would carry ou t a mor e comple x calculati on (such a s adding togethe r two 32-b it  quan tities ) using a large numbe r of physica l gate s wire d together.
 Similarly , in a quantu m computer , all circuit s can be a pproximate d with a relativel y sma ll  numbe r of gat e types .
An exampl e of a universa l quantu m gat e set (a collec tion of gate s from  whic h any desired circui t can be constructed ) is a Hadamar d gate , a CNO T gate , and a p /8 gate.
 Thes e gate s can be define d as follows:  Hadamar d gate  initial state final state  1 1 é 1 1 ø  |0æ ------|0æ+ -----|1æ ê 2 2 œ2 2 1 1  |1æ ------|0æ------|1æ ë 2 2 ß1 1 ê -œ  2 2  CNO T gate  initial state final state é 1000ø  |00æ |00æ ê œ0100ê œ|01æ |01æ ê 0001œ  |10æ |11æ ê œ0010 ë ß|11æ |10æ
180 Q UANTUM COMPUTI NG 7.2  p /8 gate  initial state final state é 1 0 ø  |0æ |0æ ê 1+i œ  1 + i 0 |1æ ---------- -|1æ êë 2 œß2  The kinds of gates implemented in an actual quantum computer would depend on engineering  tradeoffs between the cost of having more gates and the performance penalty of requiring more  steps to implement a circuit.
 The above description is a little misleading because there is a continuum of possible quantum  gates, and almost none of them can be constructed from finite sequences of these universal gates.
 But any possible quantum gate can be approximated as closely as desired with a finite sequence of  universal gates.
 State and Gate Geometry  In Figure 7-1, we showed the state of a qubit as a point on a unit circle.
More generally, since  amplitudes can be complex numbers, the state of a qubit is actually a point on a  4-dimensional unit sphere, with the real and imaginary parts of each complex amplitude con - sidered as separate coordinates.
Similarly, the state of a collection of n qubits is a point on a  2n+1-dimensional unit sphere.
It’s a unit sphere because states are normalized, i.e., the sum of  the squared absolute values of all the amplitudes must be 1.
 A quantum gate must be unitary, which means that it is linear and maps states to states.
 The vector from the origin to any state has length 1, so the gate maps that vector to another  vector of length 1.
Because it is linear, the gate must map the vector between any two states to  another of the same length.
In other words, the gate preserves the distance between states.
So  the sphere of states remains rigid under gate application, which means that it can only rotate  or flip.
And if several states lie in a plane, they will still lie in a (likely different) plane after  gate application.
 7.2 GROVER ’S ALGOR ITHM  Grover’s algorithm [ GROV96] is a quantum algorithm that can do brute-force search in the square  root of the time it would take on a classical computer.
For instance, assume we want to know which  n-bit secret key k encrypts plaintext m to ciphertext c. On a classical computer, we could try all pos - Nsible N=2n keys, and on average, it would take --2- -guesses to find the right key (and worst case, N).
 On a quantum computer running Grover’s algorithm, the number of iterations to get n qubits into a  2n/2 state where it is very probable that the state will be read as the key k is proportional to N = .
- -7.2 GROVER’S ALGORITHM 181 probability amplitude Grover’ s algorith m begin s by initializi ng n qubit s suc h tha t if the set wer e rea d to produc e an  n-bit value , eac h of the N=2n possibl e value s would be equa lly likely .
It then iterativel y boost s the  probabilit y of readi ng th e value k (the answe r we are searchi ng for) , little by little, until after  roughly N =2n/2 iterati ons th e likeli hood of readi ng k will be clos e to 1.
 To superpose all N=2n possible n-bit value s onto n qubits, we begi n by zeroi ng eac h qub it  (measuri ng it, and if it’s 1, inverti ng it) and the n applyi ng a Hadamar d gat e to eac h qubit .
The left - most part of Figur e 7-5 show s the amplitude s of eac h of the N=2n superpose d value s at this point,  whic h wil l be ------1 =2-n/2 for each.
N  Now we wil l loop ove r the followin g two operati ons (th e res t of Figur e 7-5):  1.
Multipl y the amplit ude o f |kæ by -1  2.
Reflec t the amplit udes of all N=2n states acros s the mea n of al l the amp litudes  +  mean  0 k k k  -start of iteration amplitude of |k> negated amplitudes flipped about mean  Figure 7-5 .
First Itera tion of Grover’ s Algorithm , n=4, N=16, k=5  Note tha t negati ng th e amp litude o f one stat e (|kæ) is a unitar y operatio n (th e probabilitie s still add  up to 1 afterwards ) sinc e the square d absolut e value o f the amplitud e of |kæ will not cha nge when  the amplitud e is mul tiplied by -1.
It ma y be somewha t mysteri ous a t this point how to creat e a  quantu m circui t that will rec ogniz e and negat e the ampl itude of |kæ , but we’l l explai n how to do that  in §7.2.2.
 Now we wil l reflect eac h of th e amplit udes acro ss the mea n of al l the ampl itudes .
The mean  will be ver y clos e to the commo n amplit ude o f ever y stat e excep t |kæ , becaus e ther e are N-1 of t hose  and onl y one instanc e of |kæ .
The mea n wil l be slightl y belo w all of them , becaus e the amplit ude of  |kæ (whic h is now nega tive) wil l bring down th e average .
You might b e skeptica l that thi s operati on  is unitary , but it is .
In the nex t secti on, w e wil l give an alternat e descripti on of thi s operati on as a  sequenc e of simpl e unitar y operati ons.
 The rightmos t par t of Figure 7-5 show s the amplit udes afte r the firs t iterati on.
Notic e that  we’v e now mad e all the amplitude s (othe r tha n |kæ ’s) sma ller by a tiny am ount (becaus e the mean  was so clos e to the amp litude o f ever y othe r state) .
But we’v e made th e amplitude o f |kæ a lot bigger,  becaus e it was furthe r from the mean .
Now , the amplit ude o f |kæ is bigge r by appr oximately a factor  of three .
Becaus e the probabilit y is the square of the amplitude , the chance s of measuri ng k goes up  by approximatel y a facto r of nin e afte r the firs t iterati on.
Despit e that , it will still be ver y improba - ble, if we rea d the n qubits , that we’l l get k (assuming n is large) .
To mak e the figur e legible , we’ve  used n=4 qubits .
However , to mak e Grover’ s useful , a valu e of n =128 woul d be more realistic.
-182 Q UANTUM COMPUTING 7.2.1  The nex t tim e we do th e two operati ons  1.
Multipl y the amplit ude o f |kæ by -1  2.
Reflec t the amplit udes of all N=2n states acro ss the mea n of al l the amp litudes  all the amp litudes (othe r tha n |kæ ’s) agai n get a little bi t smaller , and |kæ ’s amplit ude increase s (but  the increas e is a littl e less tha n befor e sinc e the mea n is a little smalle r).
Afte r iteratin g the optimal  numbe r of times , the amplit udes of eac h of th e state s (othe r than |kæ) are nearl y 0, a nd th e amplit ude  of |kæ is nearly 1.
 Interesti ngly, if we continue iterati ng beyo nd th e optima l numbe r of times , the mea n becomes  negative .
Then, whe n we reflec t acro ss the mean , the amplitud e of |kæ decreases , and th e amplit ude  of the othe r state s increases .
If we oversh oot th e optima l numbe r of iterati ons, then for a whil e it  will becom e less an d less likel y tha t whe n we measur e the qubit s we’l l read k. Figure 7-6 shows  how th e probabil ity o f readi ng k changes as th e algorith m proceeds .
Not e tha t the probabil ity of  reading k increase s to a maximu m the n decrease s as mor e iterati ons ar e performed , and the n cycles.
 So it’s importan t to kno w whe n to stop iterati ng in orde r to hav e the bes t chanc e of readi ng th e cor- rect result .
You onl y get one chance t o read a result.
 1  0  number of iterations  Figure 7-6 .
Overvie w of Grover’ s Algorithm , n=8  7.2.1 Geometri c Description  In general , a quantu m stat e of n qubits , superposi ng N =2n classica l states , is represente d by a point  on a 2 N dimensi onal unit s phere .
The reaso n it’s 2N (rathe r than N) is tha t the amplit udes, in the  genera l case , are comple x numbers .
However , for Grover’s , we can focu s on jus t one unit circl e on  that 2N dimensiona l sphere .
Tha t circl e lies in a plan e we’l l call Plan e K (becaus e the vertica l coor- dinat e is the amp litude o f |kæ).
The amplitud e of |anythi ng-but-kæ (an equa l superpos ition of al l clas - sical state s othe r than |kæ ) is the horiz ontal coordinate.
 The initia l stat e in Plan e K (afte r initializin g each qub it to 0 and the n applyi ng Hadamard to  1 each ) is show n in Figur e 7-7 .
The ampl itude of |kæ is tiny ( -------).
So the state i s jus t slightly above N  and co unterclockwis e from the horizonta l axis .
Let’ s call that angle a. If we wer e usi ng Grover’ s to  searc h for a 128-bi t key , angle a woul d be wa y too smal l to sho w in a diagram , so we'v e chosen  n =6 (N=26=64) fo r the figures .
Figure 7-8 show s a full run of Gr over’s.
probability of success0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30
amplitude of |k> a  Plane K  amplitude of |k> 0 1 2 3 45 6  a numberofiterations  Plane K                                                        - -7.2.2 GROVER’S ALGORITHM 183  initial state Grover’s algorithm, N = 64  amplitude of |anything-but-k> amplitude of |anything-but-k>  Figure 7-7 .
Grover’ s Plan e K Initial State Figure 7-8 .
Grover’ s Plan e K Stat e Progre ssion  So, th e stat e in Plan e K will star t at a ngle a. Eac h iterati on wil l rotat e the stat e counterclock - wise by an a ngle 2a.
After -4-p--a -iterations , the state i n Plan e K wi ll hav e travele d roughly -2 p-around  the circle , arrivi ng nea r the top of th e circle , so the probabil ity of readi ng k will be nearl y 1.
 Since the arc of the circl e is nearl y vertica l at the initia l stat e and level s off to nea r horiz ontal  near th e top of th e circle , the amp litude o f |kæ (the ver tical coordinate ) increase s mor e at th e begin - ning than i t doe s whe n the stat e near s the top of th e circle .
If ther e are too many iterations (goi ng  past the top of th e circle) , the amplit ude o f |kæ starts decreasing .
If yo u kep t doing iterati ons, you’d  cycle around th e circle , with the probabilit y of succe ss decreasi ng to nea r 0 befor e increasin g agai n  once the amplit ude o f |kæ goes negative.
 That is th e high-leve l summary .
Now we wil l explai n how we ca n manag e to do thes e opera - tions.
 7.2.2 How t o Negat e the Amplit ude of |kæ  We assum e tha t ther e is som e efficien tly com putabl e function f that take s an n-bit classical bitstri ng  s and return s 1 if s has the correct valu e and return s 0 otherwise .
For example , if we were searchi ng  for an s for whic h hash( s)=h, then f(s) woul d retur n 1 i f hash( s)=h and retur n 0 otherwise .
If we  want to kno w what n-bit secre t key encr ypts plaintext p to ciphertext c, then f(s) would retur n 1 if p  encrypted wit h key s is c and retur n 0 otherwise.
 For simplicity , we wil l assum e tha t we know that s is an n-bit intege r and tha t ther e is exactl y  one value , k, suc h that f(k)=1.
Tha t is, f (s)=1 if s=k and f(s)=0 otherwise.
- - -184 Q UANTUM COMPUTING 7.2.3  In orde r to construc t a quantu m versi on of f, say fQ, we wil l use an extr a qubit, whic h we’ ll  refer to as the ancill a qubit , and a quantu m circui t that will operat e on n +1 qubit s (the n qubits of s  plus the one ancilla) .
Usi ng th e notati on s;b to mean th e value s togethe r wit h the ancilla qubit b, fQ  flips the ancill a between 0 and 1 when s=k, and leave s the ancill a alone fo r all othe r value s of s.  After performing fQ, we wil l have n +1 enta ngled qubits .
If we rea d the qubit s afte r only a few  iterations of Grover’s , we’ d almos t certainl y read a value s differen t from k in the first n qubits and  0 in the ancilla .
But if w e wer e luck y enough t o read k, we’ d get k;1.
 We now describ e ho w to negat e the ampl itude of |k;1æ and leave the othe r amp litudes  uncha nged .
We will nee d a quantu m gat e calle d a Z gate , whic h act s on a single qubit and take s |0æ  to |0æ and |1æ to -|1æ .
 To negat e the amp litude o f |k;1æ , appl y a Z gat e to the ancilla .
Sinc e the ancilla i s enta ngled  with the othe r qubits , this multiplie s the amplitude o f stat e |k;1æ by -1 and does not cha nge an y of  the amp litudes for |s;0æ. (
Al l state s othe r than s=k will hav e the ancill a equa l to 0.)
 Now apply fQ again .
Thi s leaves th e anci lla unchange d for all value s except k but flips the  ancill a from 1 back to 0 for |kæ.
Now the anc illa is solidly 0 for all state s and is therefor e no longer  entangled wit h the othe r qubits . (
See Homewor k Proble m 11.)
 The circu it describe d above i s not th e mos t efficien t way to negat e the ampl itude of |kæ , since  it require s us t o compute fQ twice.
Ther e is a clevere r way to negat e the amplit ude o f |kæ .
It turn s out  1 1that if we set the ancilla qubit to Hadamard(| 1æ)= -----|0æ-------|1æ , then we only nee d to apply fQ once.
2 2  It’s not obvious , but it work s (see Homewor k Problem 12).
 So now we k now how t o negat e the amplitud e of |kæ, or to put it a nothe r way , we know how to  reflec t the amplit ude o f a selecte d stat e (in this cas e |kæ ) acro ss the horiz ontal axis.
 7.2.3 How t o Reflec t All the Amplit udes Across th e Mean  This is accomp lishe d in a wa y ver y sim ilar to negati ng th e ampl itude of |kæ but is slightly more  complicate d to explain .
Wha t we reall y wan t to do i s reflec t the stat e acros s the line showin g the  initial stat e in Figur e 7-7 .
We know from the previou s sec tion (§7.2.2 ) how to reflec t an amplit ude  acros s the horiz ontal axis in a plane .
But the line in Figur e 7-7 that we wan t to reflec t acro ss is not  horiz ontal.
 The tric k we wil l use is to visi t anothe r plane , whic h we’ ll cal l Plan e Z (for zero), in which  the horizonta l axis is the amplit ude o f |0æ, and the vertica l axis is the amp litude o f |anythi ng-but-0æ .
 The line w e wante d to reflec t acro ss in Plan e K wil l be the horizonta l axis in Plan e Z.  Figure 7-9 show s the initia l stat e (in Plan e Z) at the beginnin g of Grover’s , whe n all the qubits  are initia lized to zero—th e ampl itude o f |0æ is 1, and the ampl itude o f all othe r state s is 0 .
 To trave l betwee n Plan e Z and Plan e K, we apply a Hadamar d to eac h of the n qubits .
As the  state travel s from Plan e Z to Plan e K, it rotate s counterclockwis e by angle a. If w e apply  Hadamard s wh ile in Plan e K, it will retur n to Plan e Z, rotate d clockwis e by angle a.
7.2.3 GROVER ’S ALGORITHM 185 amplitude of |anything-but-0> Plane Z  amplitude of |0>  Figure 7-9.
Grover’s Plane Z Initial State  The line we want to reflect across in Plane K is now horizontal in Plane Z, and we know how  to reflect states across the horizontal axis.
 We mark which states we wish to reflect across the horizontal axis by flipping the ancilla  between 0 and 1 for all states other than |0 æ.
We use a quantum function, say gQ, which flips the  ancilla between 0 and 1 when the value of the n qubits is not equal to 0, and leaves the ancilla alone  for |0æ.
 Now we apply a Z gate to the ancilla, and we’ve reflected all values (other than |0 æ ) across the  horizontal axis.
Then we need to apply gQ again to reset the ancilla to 0 for all states.
 Now we return to Plane K by applying Hadamards to the n qubits.
The combination of the  three operations  · apply Hadamards to get from Plane K to Plane Z  · reflect all states other than |0 æ across the x axis  · apply Hadamards to get back to Plane K  accomplishes the goal of reflecting all states across the mean of their amplitudes in plane K.  This is depicted graphically in Figure 7-10.
 start of iteration reflect across horizontal axis K to Z reflect across horizontal axis Z to K  Plane K Plane K a Plane Z Plane Z a  Plane K  Figure 7-10.
 One Iteration of Grover’s
186 Q UANTUM COMPUTI NG 7.2.4  7.2.4 Parallelizing Grover’s Algorithm  Grover’s algorithm speeds up key search quadratically.
Does that mean that we need to double our  keysizes for secret key algorithms?
Not necessarily.
Apart from the very real possibility that quan - tum gates might be more difficult to build than their classical counterparts, there is a more funda - mental difference between Grover’s algorithm and classical key-search algorithms that makes  Grover’s less practical—Grover’s algorithm doesn’t parallelize well.
 To provide a concrete example, DES with its 56-bit key is considered to be completely inse - cure.
However, if all you had was a single CPU, trying one key after another, it would take decades  to get through all the keys, even if it was a pretty good CPU.
Nonetheless, DES keys are routinely  broken in a matter of hours.
This is done by using multiple CPUs (or GPUs, or specialized hard - ware) to try many keys at once.
For example, if you can try a hundred thousand keys at the same  time, you can break a DES key in a few hours.
There is no similar way to speed up Grover’s algo - rithm.
 You could, of course, divide the key space into segments.
For example, if you tried to search  for a 128-bit key using a million quantum processors running in parallel, each with a few hundred  qubits, you could get the first processor to run Grover’s algorithm under the assumption that the  first twenty bits of the key were 0…00, you could get the next one to assume the first twenty bits  were 0…01, and so on.
However, doing this would only reduce the number of iterations of Grover’s  algorithm by a factor of about a thousand (from about 264 to about 254.)
You made your computer a  million times bigger, but it only made the search a thousand times faster.
One might hope that there  would be a more efficient way to parallelize Grover’s algorithm.
However, Zalka [ZALK99]  showed that there isn’t one.
 Therefore, doubling the size of secret keys and hashes is more than sufficient to defend  against the threat of quantum computers running Grover’s algorithm.
 7.3 SHOR’S ALGOR ITHM  Shor’s algorithm [SHOR95], which can efficiently factor integers and find discrete logarithms, is  the most important application of quantum computing for cryptography.
Shor’s algorithm, com - bined with a general-purpose quantum computer of a few thousand logical qubits and a quantum  program that applies billions of logical gates (see §7.6 Quantum Error Correction ), would render  insecure all current widely deployed public key cryptography.
Shor’s algorithm finds the period of a  periodic function.
We’ll explain in §7.3.1 how RSA relates to a periodic function, and we’ll explain  in §7.3.2 why knowing the period of that function helps us factor the RSA modulus.
7.3.1 SHOR’S ALGORITHM 187  Then starting in §7.3.3 , we describe how Shor’s algorithm can factor a b-bit RSA modulus n.  Typically, RSA moduli are 2048 bits long, but to make the numbers easy, we’ll use b=2000 in  examples.
The version of Shor’s algorithm that finds discrete logs is a little more complicated but  conceptually similar to the one for factoring.
 7.3.1 Why Exponentiation mod n Is a Periodic Function  An RSA modulus n is (usually) the product of two odd primes p and q, i.e., n=pq.
As we described  in §6.2 Modular Arithmetic , f (n) (the totient function) is the number of positive integers less than n  that are relatively prime to n. For n=pq, f (n)= (p-1)(q -1).
Euler’s theorem states that for any a rel- f (n) x+f (n) x+2f (n) x+3f (n) atively prime to n, a =1 mod n. So for any value x, ax mod n=a =a =a , and in  x+kf (n) general, for any integer k, ax mod n equals a .
In other words, ax mod n is a periodic function  of x that repeats every f (n).
 xIn fact, although a mod n indeed repeats every f (n), the actual period will be a divisor of  l (n), the least common multiple of p -1 and q-1.
So l (n) is smaller than f (n) by a factor of  gcd( p-1,q-1).
For instance, since p and q will be odd numbers, both p -1 and q-1 will be even  (having a common factor of 2), so l (n) can be at most half of f (n).
Also, if a is, for instance, a  square mod n, the period of ax mod n will be half that of its square root, so the period might again  be halved.
Shor’s algorithm will compute the period of ax mod n. We’ll call the period d.  x7.3.2 How Finding the Period of a mod n Lets You Factor n  First we’ll show how, knowing d (the period of ax mod n) for a suitable choice of a, you can find  some number y that is a nontrivial square root of 1 mod n, i.e., a number, other than ±1 mod n, such  that y 2=1 mod n. Then we’ll explain why knowing y lets you factor n.  Because d is the period of ax mod n, we know that ad mod n=1.
If ad mod n=1, then if d is  even, a d/2 mod n is a square root of 1.
By the Chinese remainder theorem (§2.7.6), there are four  ways that a number y can be a square root of 1 mod n when n =pq:  1.
y is 1 mod p and 1 mod q. In this case, y will be 1 mod n.  2.
y is -1 mod p and -1 mod q. In this case, y will be -1 mod n.  3.
y is 1 mod p and -1 mod q. In this case, y will be a nontrivial square root of 1 mod n.  4.
y is -1 mod p and 1 mod q. In this case, y will be a nontrivial square root of 1 mod n.  In the first case ( y = 1 mod n), if the exponent is even, you can divide the exponent by 2 again to get  ad/4 mod n and hope to get a nontrivial square root of 1.
But if the exponent is odd, or in the second  case ( y = -1 mod n), you can’t go further, and you need to choose a different a and start over.
188 Q UANTUM COMPUTI NG 7.3.3  Note that there’s no reason to start with ad/2 mod n. Instead, you can first find the largest odd  divisor of d by dividing d by 2 until you get an odd number, say w. Then you calculate aw mod n. If  it’s 1, choose a different a. Otherwise, keep squaring the result until you get 1 or -1.
If you get -1,  choose a different a. If the first time you get 1 is after calculating, say, a 8w mod n, then a 4w mod n is  your nontrivial square root of 1 mod n. (It’s no coincidence that this procedure is reminsicent of the  Miller-Rabin primality test [RABI80] described in §6.3.4.2.1 Finding Big Primes p and q .)
 So now you’ve found a nontrivial square root of n. Let’s call that y. By definition of being a  square root of 1, y 2=1, or equivalently, y 2-1=0.
Factoring y 2-1, the equation becomes ( y+1)(y-1)  mod n=0.
 If the product of two numbers mod n is 0, there are two cases:  · One of the numbers ( y+1 or y-1) is 0 mod n.  · One of the numbers is a multiple of p, and the other is a multiple of q.  But since y is a nontrivial square root of 1, we know it is the second case.
Therefore, taking  gcd(y+1,n) or gcd( y-1,n), you’ll get p or q. And, of course, once you know one of the factors of n,  you can divide by that to get the other.
 7.3.3 Overview of Shor’s Algorithm  For factoring a b-bit RSA modulus n, we’ll need 3 b qubits.
The first 2 b qubits, which we’ll call the  exponent qubits , will be initialized to hold an equal superposition of all 22b classical values of t.  Note that we’re using a double-length exponent t so that the periodic function at mod n repeats  many times.
The remaining b qubits, which we’ll call the result qubits , will be initialized to 0.
We  will Å at mod n into the b result qubits.
We will then have 3 b entangled qubits.
 Assuming a 2000-bit modulus, that would be 6000 qubits—4000 exponent qubits to hold t,  plus 2000 result qubits to hold at mod n. We’ll use N to denote the number of classical values of the  exponent t. So N = 22b , (or 24000 for a 2000-bit n).
 Initialize all 3 b qubits to 0 (measure each and invert if not 0).
Then set the exponent qubits to  an equal superposition of all N possible classical values of t by applying a Hadamard to each one.
 Next, we randomly choose an x relatively prime to n, and use a quantum circuit that calcu - lates xt mod n and Å s the result into the result qubits.
Now we have 3 b entangled qubits, where the  2b exponent qubits hold an equal superposition of all N classical values of t, and the b result qubits  hold xt mod n for each possible value of t.  In other words, if we were to measure the 3 b qubits at this point, we’d get some random num - ber j in the 2 b exponent qubits, and xj mod n in the b result qubits.
That would not be very useful.
 Using a classical computer, we could have picked a random number j, computed xj mod n, and got - ten the same information.
So we’re not going to do that.
7.3.3 SHOR’S ALGORITHM 189  Instead of measuring all 3 b qubits at this point, we are only going to measure the b result  qubits.
We’ll get some random uninteresting number u. Because the 2 b exponent qubits are entan - gled with the b result qubits, the result of reading the result qubits and getting the value u is that the  amplitudes of all the classical states of t for which xt mod n is not equal to u go to zero.
The result - ing graph of the amplitudes of all the superposed values of t will be a periodic function with spikes  every d. For reasons that will become clear later, we call this graph (Figure 7 -11) the time graph .
If  we could now somehow read the value of d, we’d be able to factor n. Although the spikes will be d  apart, the location of the first spike will be at some random value t , which is the first value of t for  which xt mod n = u. So reading the qubits at this point will not let us find d.  ¬ t ¬ ¬ d ¬ . . .
 0 t t+d t+2d t+3d t+4d t+5d t+6d  Figure 7-11.
Time Graph  If we could get the locations of two of the spikes, we could take the difference, which would  be a multiple of d, and that would be useful.
But, once we measure, all the other amplitude  spikes disappear, and due to the no-cloning theorem (§7.1.6.1), we can’t copy the state before mea - suring.
 Shor, inspired by discrete Fourier transforms, observed that there is a way to convert the  quantum state from what we have in the time graph into a new quantum state whose graph (Figure  7-12) we’ll call the frequency graph (for reasons that will become clear later).
The nice thing about  the frequency graph is that it also has spikes at regular intervals, ( f =N/d apart), but the first spike  . . .
¬ ¬ f  0 f 2f 3f 4f 5f 6f 7f 8f 9f  Figure 7-12.
Frequency Graph
190 Q UANTUM COMPUTI NG 7.3.4  occurs at the value 0, so each spike is at an integer multiple of f. That means that if we read the state  at this point, we’ll almost certainly get a number that is a multiple of f.  There are a few subtleties.
Unfortunately, N/d will almost certainly not be an integer, since N  will almost certainly not be a multiple of d. As a result, the spikes in the frequency graph are actu - ally spread over a few values, as you can see from Figure 7-12.
But that’s okay.
If we read the  qubits, with high probability we’ll read a value v sufficiently close to a multiple of f =N/d that we  will be able to compute d with a simple classical algorithm using the continued fraction of N/v.  Another subtlety is that, in the time graph, all the amplitudes are positive real numbers.
In the  frequency graph, the amplitudes are complex numbers.
In Figure 7-12 we’re only showing the  absolute values of the spikes in the frequency graph, because that’s all that matters.
The only thing  we do once the quantum state is as represented by the frequency graph is read the qubits.
 The next section will give a classical algorithm, hopefully optimized for comprehensibility,  for converting the time graph into a frequency graph.
This is not how a quantum computer would do  it, or even how a classical computer would do it, but rather how it’s simplest to understand.
The  description involves doing things a quantum computer can’t do (like reading the amplitudes) and a  thoroughly impractical number of operations (on the order of 28000 to factor a 2000-bit number).
 On a quantum computer, Shor’s algorithm for factoring a b-bit number would run in time propor - tional to b3.
In fact, the most expensive part of Shor’s algorithm is the modular exponentiation, not  the conversion of the time graph to the frequency graph.
 7.3.4 Converting to the Frequency Graph—Introduction  We’ll call the values plotted along the horizontal axis in the time graph t and the values plotted  along the horizontal axis in the frequency graph s. Both t and s are integers between 0 and N-1.
 When we start, the quantum state of the 2 b qubits of t is represented by the time graph.
Later, the  quantum state of the same 2 b qubits will be represented by the frequency graph.
 Throughout this section, we will use vectors to represent complex numbers.
A complex num - ber x+yi can also be described as a vector (x,y).
The vector (x,y) starts somewhere on a plane and  goes x to the right and y up.
 The amplitude at each s in the frequency graph will be the sum of a bunch of vectors, as we’ll  describe shortly, normalized so that the s probabilities add to 1.
The phase of the sum (the direction  in which the vector points) does not affect the probability of reading the value.
However, Shor’s  algorithm demonstrates how an amplitude can be a complex number.
If the normalized sum vector  of s is the vector (a,b), that means the amplitude of | sæ is the complex number a+bi.
 To add a set of vectors, you add the x values to get the x value of the sum and add the y values  to get the y value of the sum e.g., (x1,y1)+(x2,y2) +(x3,y3)=(x1+x2+x3, y1+y2+y3).
In our case, all  the vectors being added into the sum for a given s will have the same magnitude but may be point - ing in different directions.
If n vectors point in the same direction, their sum will be a vector n times
7.3.5 SHOR’S ALGORITHM 191  as long (see Figure 7-13a).
If two vectors point in opposite directions, say (x,y) and (-x,-y), they  will cancel each other out, resulting in a zero-length vector.
Likewise, if n equal-sized vectors are  equally spaced around a circle, they will cancel each other out (see Figure 7-13c).
If the n vectors  are equally spaced within an arc of a circle, then the direction of the sum vector will be in the mid - dle of the arc (see Figure 7-13b), and the length of the sum will depend on how large the arc is.
If  the arc is smaller, the sum is larger.
 a c b  Figure 7-13.
Adding Vectors  7.3.5 The Mechanics of Converting to the Frequency Graph  Imagine the time graph as a giant springy string with spikes corresponding to the spikes in the time  graph.
To calculate the amplitude at s in the frequency graph:  · Initialize the sum vector S to (0,0).
 · Stretch the time graph string so that it wraps around the circle exactly s times.
 · For each spike in the string, add to S the vector pointing in the same direction as the spike,  and of length equal to the size of the spike (in our case, all the spikes are the same length).
 · Normalize S by dividing by N so that the sum of the squared absolute values of the Ss is 1.
 For s=0, the string is shrunk to a point, so the spikes all point in the same direction.
See Figure  7-14.
Note the vector inside the circle in Figure 7-14 is the normalized sum of the spikes.)
 Sum of coincident spikes = c×1/Ö c = Ö c  Normalized sumofspikes  Figure 7-14.
Frequency Graph Calculation for Stretch Factor s = 0
-192 Q UANTUM COMPUTING 7.3.5  For s=1, the stri ng wrap s around the circl e exactl y once.
Sinc e the spike s are space d symmetrically  around th e circle , as vectors the y will essentiall y cance l eac h othe r out, and so the resulti ng ampli - tude wil l be ver y nearl y 0. (
See Figur e 7-15.)
 Normalized sumofspikes  Figure 7-15.
Frequenc y Gra ph Calcula tion fo r Stretc h Factor s = 1  Similarly , for s=2 (see Figure 7-16), the norma lized sum of spike s is basicall y zero , so all we see  insid e the circle in Figure 7-16 is a point.
 Normalized sumofspikes  Figure 7-16.
Frequenc y Gra ph Calcula tion fo r Stretc h Factor s = 2  But for s » f (or s being clos e to an intege r mu ltiple of f ), the spike hit s the circle i n approxi- mately th e sam e place on each wrap .
Since f is not usuall y an integer , s (when s is clos e to a multi - ple of f ) is either a little smalle r than a multipl e of f or a little large r than a multipl e of f. If sma ller,  each succe ssive spike wi ll unders hoot th e previou s spik e a little , whil e if larger , eac h successive  spike will overs hoot th e previ ous spik e a little.
I n eithe r case , the spike s will fall on an ar c of the cir- cle.
The length of th e sum vecto r wil l be somewha t smalle r than if they wer e pointi ng in exactly the  same directi on. (
See Fi gure 7-17.)
The case wher e the hei ght o f the spik e wil l be smalles t is whe n a  multipl e of f is a hal f integer s o tha t the spike s spa n hal f the circle , reducin g the magnit ude o f the  2 sum by a facto r of roughly -p -(see Fi gure 7-18.).
Even for value s of s for whic h the spike s go  completel y around th e circle , they migh t not exactl y cance l out, so the sum could be ver y smal l but
7.3.5 SHOR’S ALGORITHM 193  Normalized sumofspikes  Figure 7-17.
Frequency Graph Calculation for a Stretch Factor s near f  Frequency Graph Calculation for a Stretch Factor s nearly ½ away from a multiple of f Normalized sumofspikes  Figure 7 -18.
 nonzero.
Visit https://rawcdn.githack.com/ms0/docs/main/dft.html for an animation of this  process.
 When we’ve computed the amplitude for each s, we have the frequency graph.
As we’ve seen,  it will have spikes near each integer multiple of f. The differences between the time graph (Figure  7-11) and the frequency graph (Figure 7-12) are  · The time graph starts with a spike at a random place that depends on the value read from the  result qubits.
The frequency graph starts with a spike at 0.
 · The spikes in the time graph are spaced d apart.
The spikes in the frequency graph are spaced  f = N --- -d apart.
 · Each spike in the time graph is a single value of t. Each spike in the frequency graph com - prises a few values of s near an integer multiple of f, with the values of s closest to such a  multiple providing the largest amplitudes.
 · The time graph amplitude at each non-spike t is 0.
The frequency graph amplitudes away  from each spike are nearly 0.
 · The spikes in the time graph all have identical amplitudes (both magnitude and phase).
The  spikes in the frequency graph vary in magnitude and phase.
- -194 Q UANTUM COMPUTING 7.3.6  7.3.6 Calculatin g the Period  With high pr obabi lity, whe n we rea d the qubit s in the stat e represente d by th e frequenc y graph , we  N will get a value v that is clos e to an intege r multipl e of f, where f = -- -.
Wha t we wan t is th e denom -d  inator d.  Ther e is a smal l probabilit y tha t the v that we rea d will be 0, whic h is not useful , but given  that ther e are s o many othe r spike s wit h simila r sizes , we almos t certainl y won’t be tha t unlucky.
 There’ s als o a smal l probabilit y tha t we’ll rea d a value tha t isn’ t nea r a spike , because , although  small , those stil l hav e nonzer o amp litudes .
If we manag e to read a non-usefu l value , we jus t run the  algorith m agai n (starti ng w ith ra ndoml y choosin g a number a to exponentiate).
 Once we rea d a value v that is clos e to an intege r mul tiple of f, we nee d to calculate d.  N NRemembe r that f = --- -.
We expand - - -as a con tinue d fracti on a nd us e tha t to ge t good rationald v  approximations .
The numerato r of one o f thos e rati onal approximati ons is usuall y goi ng to be d,  althoug h it might occasi onall y be a diviso r of d.  The choice t o use a 2b-bit exponen t to facto r a b-bit modulu s is mad e because tha t make s it  very likely tha t the measured v is clos e enough t o an intege r mul tiple of f for the continue d fracti on  technique t o efficientl y find d.  7.3.7 Quantu m Fourie r Transform  The algorith m we’v e describe d for transformin g the tim e grap h into the frequenc y gra ph is actually  performi ng a discret e Fourie r transform .
Althoug h I3,4 cannot imagin e anyone tha t isn’ t completely  comfortabl e wit h discret e Fourie r transforms , I1,2 spen t a lot of energ y on th e previ ous secti on  tricki ng potentially math- phobi c reader s into understa nding discret e Fourie r transform s befor e sur- prisin g the m by telli ng them , in this secti on, tha t they now understa nd them.
 A discret e Fourie r transfor m com putes a frequenc y gra ph fro m a tim e gra ph usin g a formula  that might lo ok intimidati ng at firs t glance.
 -1 i st N -2pa ~(s) = 1 åa (t)×eN  N t=0  But let’ s brea k the formul a down .
It is describin g the com putati on w e describe d in §7.3.4.
 The amplit ude o f each s (writte n as a ~(s) to the left of the equa l sign) i s the sum of N values , each  of whic h depe nds on a (t) (the amplit ude of t in the tim e gra ph) from t =0 to t =N-1.
 To com pute a ~(s) , we are going to trave l around a uni t circl e and , for each t, calculat e a com - plex numbe r to add to our sum .
For eac h of the N value s of t, the arc tha t we trave l over is s/N of the  circle .
So for s=1, we trave l around th e circle exactl y once , whil e for s=3, we trave l around the cir- cle exactl y thre e times .
Also, recal l that th e circumferenc e of a unit circl e is 2p , and th e units of dis- tance along th e circumference tha t add up t o 2p (on a un it circle ) are know n as radians .
Tha t means
7.4 QUANTUM KEY DISTR IBUTION (QKD) 195  that for a given value of s, we want to travel s/N of the circle for each increment of t, i.e., 2p s/N  radians for each increment of t. So after t steps we’ve traveled 2 p st/N radians.
 We’ve explained most of the exponent of e in the formula.
The minus sign is the convention  for the (forward) Fourier transform.
Without the minus sign, it would be equivalent to wrapping  around the circle counterclockwise.
Each a ~(s) would have the same magnitude with or without the  minus sign, but its phase would be different, since it would be reflected across the horizontal axis.
 For our purposes, only the magnitude matters. (
It turns out that the inverse Fourier transform, which  recovers the time graph from the frequency graph, just leaves out the minus sign.)
 But what is i, the square root of -1, doing in the exponent?
This is an application of Euler’s  formula, which states that for any real number q , eiq = cosq + i sinq .
What Euler’s formula says is  that there are three ways of expressing a point on a unit circle:  · as coordinates: (x,y) =(cosq ,sinq )  · iqas e  · as the complex number cos q + i sinq  Each value we add into the sum is the complex number representing the angle at which t hits the  unit circle times the amplitude at t in the time graph.
While in this case the amplitudes in the time  graph are real, in general, when you multiply complex numbers, you multiply their magnitudes and  add their angles.
 The only remaining part of the formula is dividing by N to make the probabilities add up to  1.
So the resulting transform is unitary!
 7.4 QUAN TUM KEY DISTRIBUTION (QKD)  In 1984, Bennett and Brassard [BENN84] came up with a way for two parties to establish a secret  that would be impossible for an eavesdropper to see, by leveraging an existing shared key together  with a medium (such as a fiber optic link) for sending quantum information (such as a stream of  photons).
The goal is similar to using authenticated Diffie-Hellman, leveraging an existing secret,  to create a new secret for perfect forward secrecy.
 The theory behind quantum key distribution is that if an eavesdropper Eve attempted to read  the secret bits Alice is sending to Bob, Eve would introduce enough errors in the information  stream that Alice and Bob would detect the interference based on seeing more than the expected  number of errors (and they would fail to agree on a new secret key).
 The idea is fairly simple.
Alice will carefully send a single photon at regular time intervals,  randomly polarizing that photon to be up (|), 45 ° ( ¤ ), -45° (\), or sideways (–).
196 Q UANTUM COMPUTI NG 7.4  Bob will read the photon after passing it through a polarizing filter in one of two positions he  randomly chooses at each time interval—vertical or 45 ° diagonal.
If Bob chooses to read one of the  photons with a vertical filter, then  · If Alice sent |, Bob will see the photon.
 · If Alice sent –, Bob will definitely not see the photon.
 · If Alice sent \ or ¤, then Bob has probability ½ of seeing the photon.
 Similarly, if Bob chooses to use a 45 ° diagonal filter, then  · If Alice sent | or –, Bob has probability ½ of seeing the photon.
 · If Alice sent ¤ , Bob will see the photon.
 · If Alice sent \, Bob will not see the photon.
 Next, Bob tells Alice which sequence of filter positions he used, say | | ¤ | | | ¤ ¤ | ¤ | ¤ .
This message  (where Bob reports the filter positions he chose) must be authenticated using the pre-shared secret  we are assuming Alice and Bob have been configured with before this exchange.
Otherwise, our  eavesdropper, Eve, can simply do this protocol acting as a meddler-in-the-middle, establishing a  separate secret between Bob-Eve and Eve-Alice.
 When Bob reads with a filter ±45 ° off from the polarization of the photon, it’s totally random  whether he sees a photon or not, so those bits are useless.
Therefore, Alice checks the sequence of  filter positions that Bob reports and tells Bob to ignore bits for which his filter was ±45 ° off of the  polarization of the photon Alice transmitted.
 If Alice sent | or – and Bob chose ¤ for that time slice, Alice tells Bob to ignore that bit.
Sim - ilarly, if Alice sent ¤ or \ and Bob chose | for the filter, Alice tells Bob to ignore that bit.
 For the remaining bits, Bob (or Alice) sends a checksum computed using an error-correcting  code.
Since sending single photons will be a somewhat noisy channel under the best of circum - stances (no eavesdropper), they will expect a certain number of errors, and the error-correcting code  should be able to correct more than the expected number of benign errors but not as many errors as  would be introduced by an eavesdropper.
 Why would eavesdropper Eve introduce errors?
She might try to read the photons, and allow  a photon that makes it through her filter to continue on to Bob.
However, she has to guess what con - figuration to set her filter.
If the photon passes through her filter, and the filter is not exactly aligned  with the photon, she’ll have twisted the photon, so that even if Bob’s filter is aligned with Alice’s  choice, Bob may not be able to read the now-twisted photon.
Or if Bob should have read a 0, the  now-twisted photon might pass through Bob’s filter.
7.4.1 HOW HARD ARE QUANTUM COMPUTERS TO BUILD?
197  7.4.1 Why It’s Sometimes Called Quantum Encryption  Why do people worry about making high performance QKD (as described above)?
If QKD were  only used to send, say, 256 bits, to be used as a secret key to encrypt data over a traditional commu - nication channel, high performance of the quantum channel is unnecessary.
 A system has information-theoretic security if even an attacker with unlimited compute  power cannot get any information from seeing ciphertext ( i.e., there are no attacks based on going  through all possible keys and recognizing plaintext when the correct key is tried).
Å with a  one-time pad has that property.
If the goal is to use the quantum channel to establish a one-time pad  (and achieve information-theoretic security), the name quantum encryption makes sense.
The  one-time pad needs to be as long as the amount of data to be sent.
High performance is required in  this case, because the quantum communication channel (over which the one-time pad will be estab - lished) will need to have several times the desired bandwidth of the channel over which the  encrypted data will be sent, because of all the bits that will need to be discarded or used for error  correction.
 7.4.2 Is Quantum Key Distribution Important?
 Quantum key distribution has been highly advertised as an important breakthrough with guaranteed  security, but in our opinion, it’s not as useful or as secure as claimed.
 QKD depends on having established a pre-shared secret.
If you believe that cryptography  works, any mechanism described elsewhere in this book for Alice and Bob to securely communi - cate will work.
And these traditional mechanisms can work over a traditional multi-hop network,  whereas QKD requires (at a minimum) a very expensive special direct link.
At the time of our writ - ing, deployed QKD systems have additional problems.
Typically, end-to-end QKD only works over  a limited distance (no more than a few hundred kilometers).
Middleboxes, called trusted  repeaters , that are capable of decrypting traffic are required for longer-distance communication.
 QKD systems may also be vulnerable to side-channel attacks that learn the shared secret, not by  measuring the transmitted photons, but by observing the behavior of the transmitting and receiving  devices.
 7.5 HOW HARD ARE QUANTU M COMPUTER S TO BUILD?
 We haven’t talked much about how a quantum computer might be implemented.
We have implicitly  promised that all these superposed and entangled manipulations of quantum information are consis -
198 Q UANTUM COMPUTI NG 7.5  tent with physics as we know it, but this raises the question: If all this is consistent with physics,  why hasn’t anyone built a quantum computer big enough to break 2048-bit RSA?
What’s so hard  about it?
What are the challenges involved, and how might they be overcome?
 The first challenge is coming up with physical systems that can reliably store and manipulate  qubits.
Then the qubits must be isolated from their environment, because any interaction between a  qubit and its environment affects the state of the qubit.
But, in order to apply gates, it is necessary to  interact with the qubits in a carefully controlled way.
 One way of isolating qubits from their environment is to choose qubits that interact very  weakly with their environment, e.g., polarized photons in empty space or in optical fiber.
This  makes the qubits very hard to manipulate, especially when we need to apply two-qubit gates that  require the qubits to interact with each other.
Often, reducing unwanted interactions between qubits  and the environment requires extreme cooling.
At normal temperatures, there are too many particles  bouncing around to allow qubits to stay unmeasured for very long.
Qubits encoded in very small  physical systems like atoms and ions can sometimes deal with higher temperatures provided they  are separated from other atoms by a significantly large region of high-quality vacuum.
 Unfortunately, even without a noisy environment, most qubits will decay (or decohere ) on  their own.
If the | 1æ state is slightly more energetic than the | 0æ state, for example, it will have a cer - tain nonzero probability of decaying to the | 0æ state in any given interval of time, and that probabil - ity will be directly proportional to the energy difference between the two states.
The downside,  however, of having a small energy gap between possible states of the qubit is that it tends to make  applying gates to the qubit very slow.
 Despite all these challenges, there are a number of ways to create qubits that do the right  thing about 99% of the time.
Serious proposals have been based on photons traveling through an  obstacle course of half-silvered mirrors and polarizers (optical quantum computers), manipulating  the states of valence electrons in trapped ions using lasers (ion-trap quantum computers), nanoscale  circuits involving semiconductors (single-electron transistors, quantum dots) or superconductors  (superconducting quantum computing), and pushing around excitations of planar solids in  braid-like trajectories (topological quantum computers).
 At the time of this writing, the most promising candidate is superconducting quantum com - puting.
Here the qubits are represented by tiny oscillating currents in superconducting circuits.
The  circuits need to be cooled to around .01kelvin.
The qubits can interact using resonators and they  can be manipulated by hitting them with lasers or simply by running a pulse of current near them.
 So, qubits that can be manipulated to do the right thing 99% of the time have been produced  (with some difficulty), but this on its own is not good enough.
A cryptographically relevant quan - tum computation like Shor’s algorithm involves billions of gates.
Doing something a billion times  that has a 1% chance of completely ruining your computation is pretty much guaranteed to ruin  your computation.
Solving this problem has led to the development of advanced schemes for  achieving fault tolerance using error-correcting codes (see §7.6 Quantum Error Correction ), where  a group of flaky physical qubits act as a single well-behaved qubit known as a logical qubit .
7.6 QUANTUM ERROR CORRE CTION 199  The challenges involved in building a quantum computer appear daunting but not insur - mountable.
In fact, it seems increasingly likely that a cryptographically relevant quantum computer  will be built in the next few decades, and if built, such a computer would render all widely used  public key cryptography insecure.
We live in interesting times.
 7.6 QUAN TUM ERROR CORR ECTION  When building conventional computers, we can build circuitry whose error rate is almost arbitrarily  low, but sometimes it is more cost effective to build something with a higher error rate and use  error-correcting codes to compensate for the small number of expected errors.
In traditional com - munications and classical memory, typically less than 10% overhead is needed for error-correction  bits.
Some classical technologies, such as DRAM, need to be refreshed by being read and rewritten  periodically, because otherwise the state decays.
 Quantum error correction is much harder, for various reasons:  · The intrinsic error rate is much higher (at least given current qubit and quantum gate technol - ogy).
While a conventional computer might have one error bit in a million, all known quan - tum gates have error rates no better than one in a thousand.
 · A quantum state cannot be read and rewritten to refresh the state before it decays beyond  repair (as is done with classical DRAM).
 · A conventional computer bit can only have one kind of error (a bit is flipped).
In contrast, a  qubit can drift into an infinite number of states, and its entanglement can change.
 It is somewhat surprising that quantum error correction is possible at all.
Quantum error correction  is an active area of research.
With all known technology, qubits have a very high error rate.
There - fore, the only hope for doing complex computations is to have a set of intrinsically flaky physical  qubits act as a group to behave like a single more-stable logical qubit.
A quantum error correction  scheme is characterized by a threshold , which is the maximum error rate for physical qubits that  will result in logical qubits being more reliable than the physical qubits they are based on.
When  physical qubits and gates have error rates below the threshold, it is possible to adjust the error cor - rection method to make the error rate for logical qubits arbitrarily low without requiring exponen - tially more physical qubits than logical qubits.
As of 2022, the best quantum error correction  schemes have a threshold corresponding to something like a 1% error rate.
 How much error correction is needed (and the size of the physical qubit group creating a log - ical qubit) depends on the fidelity of the gates acting on the physical qubits and how many gates the  logical qubits need to experience while keeping the resulting quantum states sufficiently accurate.
 While some of these quantities are subject to change due to better technology or improvements in
200 Q UANTUM COMPUTI NG 7.6  optimizing quantum algorithms, academics have been giving estimates for some time.
These can  perhaps give some idea of the scale of the numbers involved and how much or little they’ve  changed over the years.
For example, a highly cited 2012 paper [FOWL12] said a practical quan - tum computer for factoring would require at least a hundred million physical qubits, while a more  recent analysis from 2019 [GIDN21] said factoring a 2048-bit RSA number would only require  about twenty million physical qubits.
These numbers may be compared to the several thousand log - ical qubits required to run Shor’s algorithm against a 2048-bit RSA number.
 Acting on one or two logical qubits with a logical gate to perform the equivalent of acting on  physical qubits with a physical gate requires more physical gates.
Therefore, the physical qubits  must have high enough fidelity to withstand the extra gates required to have the logical qubit group  be operated upon by at least one logical gate.
If acting on a logical qubit with a logical gate has suf - ficient fidelity, then the error correction algorithm can be applied recursively to produce a higher  level of logical qubits with higher fidelity still.
This in theory results in logical qubits with arbi - trarily high fidelity.
There is a trade-off between the fidelity that can be achieved by the physical  qubits and the number of them needed.
The frontiers of quantum computer design involve increas - ing both the number of qubits and their fidelity.
 For a simplified example of a quantum error correction algorithm that will correct for a single  qubit’s bit flip (flipping between | 0æ and | 1æ ) but not correct for other types of errors such as  changes in phase (where the coefficient of | 0æ or |1æ is multiplied by a complex number of absolute  value 1), use three physical qubits (which we’ll call a, b, and c) to represent one logical qubit.
The  correct state of the three qubits is that they are supposed to be equal, so the two superposed states  are |000æ (indicating the logical qubit is | 0æ) and |111æ (indicating the logical qubit is | 1æ ).
So the  quantum state of the entangled group of three qubits might be a1|000æ +b1|111æ.
 Some quantum gates can act on logical qubits in a way that doesn’t cause single qubit errors  to propagate from one physical qubit to the other physical qubits in the same logical qubit.
For  example, a NOT gate can be performed by applying NOT to each of the three physical qubits.
If  there were no errors, a, b, and c should all be identical after the gate, so the only two superposed  states should still be | 000æ and |111æ , possibly with different coefficients, say a2|000æ +b2|111æ .
 In fact, any quantum gate that is equivalent to a classical gate can be performed in this way.
In other  words, our simplified example is good enough to do error-corrected classical computation.
 We assume the probability of more than one bit flip is very low.
Suppose qubit b suffers a bit  flip, so now the state might be a2|010æ +b2|101æ.
If we read any of the qubits, we’ll get 0 or 1 and  destroy the superposition.
So instead, there is a clever way of detecting which of the qubits dis - agrees, by measuring both aÅ b and bÅ c (in a nondestructive way*).
If there was no error, both  measurements ( a Å b and bÅ c) will be 0; if bit a flipped, we will measure 1 and 0; if bit b flipped,  we will measure 1 and 1; and if bit c flipped, we will measure 0 and 1.
Thus, we can apply a NOT  *Note that to measure aÅ b, you use an extra qubit, let’s call it d, initialized to 0.
First, apply a CNOT gate to a and d, then  apply a CNOT gate to b and d.
-- - - - - - - -7.7 HOMEWORK 201  gate to flip th e affecte d physica l qubit , and the stat e of the logica l qubit wil l be repaire d to be  a2|000æ +b2|111æ .
 Unfort unately , ther e are way s a singl e qubit can be damage d othe r tha n a simple bi t flip,  whic h is why th e above triple t schem e doe s not wor k in practice .
For example , an error coul d turn  3 4 4 3|0æ into - - -|0æ-- - -|1æ and tur n |1æ into - - -|0æ + - - -|1æ.
Or it coul d turn a |0æ +b |1æ into a |0æ -b |1æ.
The5 5 5 5  error correcti on scheme s tha t wor k correc tly ar e somewha t mor e complicate d to explai n and use  more qubits to constitut e a logica l qubit .
Thos e erro r correcti on scheme s limit the variet y of 1- and  2-qubit gate s tha t can be fau lt tolerant .
However , ther e are error correcti on scheme s tha t allo w a  universa l gate set.
Tha t is to say , they cannot produc e any 2-qubit gate you mi ght writ e a trut h table  for, but they can , with logarithmi c overhead , produc e somethi ng tha t is clos e enou gh to the desired  gate.
So even i f man y of thes e gate s are required , the overal l com putati on wil l succee d with hi gh  probability.
 7.7 HOMEWORK  1.
Suppos e you us e four polarizi ng fi lters, wit h eac h one 30° off from th e previ ous one .
What  percentag e of photons w ould pass thr ough th e four fi lters? (
See §7.1.3.1.)
 2.
Suppos e ther e are thre e unentangle d qubit s wit h qubi t 1 havi ng state a 1|0æ +b 1|1æ, qubit 2  having state a 2|0æ +b 2|1æ, and qubit 3 havi ng state a 3|0æ +b 3|1æ.
Write ou t the amplit udes of  each of th e eight possibl e classica l value s for the thre e qubits , expre ssed in term s of a 1,b 1,  a 2,b 2,a 3,b 3.
 3.
Suppos e we mak e a 2-qubit gate tha t (on cla ssica l inputs) Å s the value o f qubit 1 into qubi t 2,  while leavin g qubi t 1 alone.
Wr ite ou t the trut h tabl e for this 2-qubit gate .
Hav e we see n it  before ?
Wha t is th e outpu t of thi s gat e whe n qubit 1 and qubi t 2 are initialize d to  1 1 1 1Hadamard(| 0æ )= --- --|0æ + ------|1æ and Hadamard(| 1æ)= -----|0æ-------|1æ, respec tively ?
Ar e the two 2 2 2 2  qubit s enta ngled in the output state ?
Did the stat e of qubit 1 cha nge?
Wha t about qubit 2?
 4.
Conside r a Hadamar d gat e operati ng on a qubi t with rea l amp litudes a and b , so the state of  the qubit is a poin t on a un it circle .
The Hadamar d gat e reflect s the stat e acros s a line through  the origin .
At wha t angle is this line ? (
Hint : Wher e does the Hadamar d move the stat e |0æ ?
 Wher e does it move th e stat e |1æ ?)
 5.
Suppos e we hav e thre e entangle d qubit s in state a |001æ +b |010æ +g |100æ .
Wha t would the  state of the qubits be afte r operatin g on th e firs t qubit with a Hadamar d gate ? (
Hin t: Compute  the resul t for eac h of th e three s uperposed state s and add th e result s in proportion to their  coefficients .
For instance , operatin g on th e firs t qubit of a |001æ with a Hadamar d result s in  a a -----|001æ + ------|101æ.)2 2
202 Q UANTUM COMPUTI NG 7.7  6.
Suppose we have the same entangled set of three qubits in state a |001æ +b |010æ +g |100æ .
 What is the state of the set of three qubits after we measure the first qubit and get 1?
What  would the state of the set of three qubits be if we measured the first qubit and got 0?
 7.
For each of the following states of a qubit, show the result of applying Hadamard once, then  show the result of applying Hadamard twice:  a. 1|0æ  b. 1|1æ  c. a |0æ +b |1æ  8.
What is the state of two unentangled qubits, each in state a |0æ +b |1æ expressed as coefficients  of |00æ , |01æ , |10æ, and |11æ ?
 9.
Show that any linear quantum gate on n qubits whose truth table is a one-to-one mapping of  classical input states to classical output states is unitary.
 10.
If you choose two random 128-bit blocks x and y, is it always true that there is exactly one  128-bit AES key k that maps plaintext x to ciphertext y?
How about with 256-bit AES?
 11.
In §7.2.2 , what would happen if instead of performing fQ a second time, you simply read the  ancilla, and if it’s 0, then leave it as 0, and if it’s 1, then perform NOT on the ancilla?
 12.
Show that the optimization in §7.2.2 (initializing the ancilla to Hadamard(| 1æ) and performing  fQ once) indeed negates the amplitude of | kæ .
8 POST-QUANTUM  CRYPTOGRAPHY  As we described in Chapter 7 Quantum Computing , a sufficiently large quantum computer imple - menting Shor’s algorithm would break our currently deployed public key algorithms.
However,  long before that can happen, the world will (hopefully) have converted to replacement algorithms.
 The replacement algorithms will be based on math problems that (hopefully) not even a combina - tion of classical and quantum computers would be able to solve in a reasonable amount of time.
 These new algorithms are known by several equivalent names: quantum-resistant,  quantum-safe, or post-quantum cryptography (PQC).
The world seems to have settled on the term  post-quantum , so that is what we will use, even though we have noticed that the term post-quantum  sometimes confuses people into thinking these new algorithms run on quantum computers.
 It is important to start migrating away from the current public key algorithms well before a  sufficiently large quantum computer might exist.
One reason is that conversion will be slow.
 Another reason is that there might be data that should be kept secret for many years, and if that data  were encrypted with today’s public key algorithms, it could be stored now and decrypted later,  when quantum computers do exist.
Given that it is not possible to know when (or even if) quantum  computers might realistically threaten our current public key algorithms, people should convert to  new public key algorithms very soon.
However, before conversion to new algorithms can happen,  the world needs to standardize on some replacement algorithms.
 The National Institute of Standards and Technology (NIST) has played an important role in  standardization of cryptography ( e.g., AES and SHA) and is playing an important role in the stan - dardization of post-quantum algorithms.
In late 2017 (the deadline for submissions), NIST received  about 80 proposed schemes.
Rather than picking a single “winner” scheme, as NIST did for AES,  NIST will standardize several.
There would be no way to pick a single “best” scheme, because the  post-quantum algorithms have such different properties, such as dramatic differences in key size,  signature size, and computation required.
 The intention of this chapter is to give some intuition into how these algorithms work and to  be comprehensible to people who have not recently been taking advanced math courses.
For those  who want mathematical rigor, complete specifications, and security proofs, there are (and will be)  203
204 P OST-QUANTUM CRYPTOGRAP HY 8.1  many excellent resources.
For example, NIST’s post-quantum site [NISTPQC ] gives pointers to  detailed information about all the submissions.
 In subsequent sections, we’ll discuss four of the best-known families of schemes: hash-based  (§8.2), lattice-based (§8.3), code-based (§8.4), and multivariate cryptography ( §8.5).
 In addition to the four major types of post-quantum algorithms we discuss at length in this  chapter, there is a fifth major type—isogeny-based cryptography.
An isogeny is a particular kind of  mapping between elliptic curves.
Isogeny-based cryptography uses elliptic curves, but unlike tradi - tional elliptic curve cryptography, does not depend on the difficult of the (elliptic curve) discrete  log problem. (
Recall that the elliptic curve discrete log problem would be easily solvable with a  quantum computer using Shor’s algorithm.)
Isogeny-based cryptography, in contrast, relies on the  hardness of various problems such as constructing an isogeny between two specific elliptic curves  that are known to have an isogeny between them.
Isogeny-based cryptography is relatively well  studied.
A number of promising schemes have been proposed, including the encryption scheme  Supersingular Isogeny Key Exchange (SIKE), which is, at the time of this writing, an alternate can - didate in the NIST PQC standardization process.
Several other encryption and signature schemes  have also been proposed using isogenies.
Proposed schemes typically have small public keys,  ciphertexts, and signatures compared to other post-quantum schemes, but also tend to be slower,  although much progress has been made in creating more efficient implementations of  isogeny-based schemes.
We omit more extensive discussion of isogeny-based cryptography, not  because we think it is less important than the other major areas of post-quantum cryptography, but  because we think providing enough mathematical background to do these schemes justice will  seem tedious to most of our readers, and readers who are interested will have many places to read  the details.
 8.1 SIGNA TURE AND/OR ENCRY PTION SCHEMES  With RSA, any key pair can be used for either signatures or encryption.
It’s not considered good  security practice to use the same key pair for both purposes, but mathematically, it can be done.
In  contrast, most post-quantum algorithms are only good for one or the other (signatures or encryp - tion).
That means that in the post-quantum world, entities that do both signatures and encryption  will likely need not just two different key pairs, but also two different algorithms.
 Any public key scheme requires an algorithm for key pair generation.
A signature scheme  additionally requires algorithms for signature generation and signature verification.
An encryption  scheme requires algorithms for encryption and decryption.
Since public key schemes are slower  than secret key schemes, a public key signature scheme will usually only sign a hash of the
8.1.1 SIGNATURE AND /OR ENCRY PTION SCHEMES 205  message, and a public key encryption scheme will usually only use the public key encryption to  establish a secret S and then encrypt the message with S.  Sometimes cryptographers refer to a third type of scheme, key encapsulation mechanism  (KEM ).
KEM schemes can be converted to encryption schemes and vice versa.
The distinction is  that with an encryption scheme, one side chooses S and encrypts S using the other party’s public  key.
In contrast, in a KEM scheme, a shared secret is derived from information that one or both  sides contribute to the exchange.
We will not use the term KEM further, and simply refer to both  types of schemes as encryption .
 8.1.1 NIST Criteria for Security Levels  NIST defined five security strength categories as a coarse-grained measure of how hard it would be  to break the algorithm.
The post-quantum algorithm submissions are specified with parameter sets  for some or all the categories.
e.g., to meet category 1, use these values of the parameters.
The cate - gories are defined as:  1.
At least as hard as key search against a 128-bit block cipher ( e.g., AES128)  2.
At least as hard as collision search against a 256-bit hash function ( e.g., SHA256)  3.
At least as hard as key search against a 192-bit block cipher ( e.g., AES192)  4.
At least as hard as collision search against a 384-bit hash function ( e.g., SHA384)  5.
At least as hard as key search against a 256-bit block cipher ( e.g., AES256)  In terms of security against classical attack, categories 1 and 2 are essentially the same (128-bits of  security), as are categories 3 and 4 (192 bits of classical security).
However, when we consider  quantum attacks ( e.g., Grover’s algorithm), category 2 might be considered more secure than cate - gory 1, and category 4 might be considered more secure than category 3.
 8.1.2 Authentication  With all public key schemes, Alice and Bob need to reliably know each other’s public keys in order  to do secure signatures, encryption, or authentication.
An example method is having them present  certificates to each other, signed by an entity the other trusts.
The public key of the certifying entity  might be embedded in the software.
At any rate, this is not an issue specific to post-quantum public  key algorithms, and we will not discuss it further.
206 P OST-QUANTUM CRYPTOGRAP HY 8.1.3  8.1.3 Defense Against Dishonest Ciphertext  Public key encryption schemes have to be carefully designed to defend against a type of attack  known as a chosen-ciphertext attack .
An attacker, say Trudy, might gain information by sending  Alice ciphertexts that were not generated according to the rules of the algorithm.
This attack usu - ally requires Trudy sending many ( e.g., millions of) ciphertexts to Alice and depends on Trudy  being able to figure out from Alice’s behavior whether the decryption succeeded.
In some public  key encryption algorithms, there is a possibility that even honestly generated ciphertexts might fail  to decrypt, and Trudy may be able to gain information even if she does generate her ciphertexts  according to the rules of the algorithm, provided she sends enough ciphertexts so that it is likely  that Alice will fail to decrypt some of them.
The probability that an honestly generated ciphertext  will fail to decrypt is called the decryption failure rate .
 The information that Trudy might gain with this attack might allow her to learn Alice’s pri - vate key, or, if Trudy sees an encrypted message that honest Bob had sent to Alice, Trudy might  learn Bob’s plaintext by sending lots of slight variants of Bob’s ciphertext and seeing whether Alice  can decrypt them.
 This attack is not specific to post-quantum encryption algorithms.
For instance, an attack of  this form, known as the million-message attack , was successfully demonstrated against a particu - lar implementation of RSA [BLEI98].
 Any public key encryption algorithm should contain explicit countermeasures to this sort of  attack.
One simple countermeasure to prevent chosen ciphertext attacks is having Alice change her  public key for every single communication, but this can be impractical or hard to enforce.
In cases  where algorithm designers want it to be safe to reuse a key pair, there are provably secure system - atic constructions for providing security against chosen ciphertext attacks.
The general principle  behind these constructions is for Alice to refuse to decrypt a ciphertext unless she can verify that it  was honestly generated.
Typically, the way Bob proves to Alice that he generated his ciphertext  honestly is as follows:  • Bob will typically need to generate random numbers in the process of creating a ciphertext.
 Instead of generating these numbers truly randomly, he will pseudorandomly derive them  from a seed in a way that is dictated by the algorithm specification.
 • Rather than directly encrypting a plaintext or a shared secret directly, Bob will instead  encrypt the seed from which other quantities are derived.
 • After Alice decrypts the ciphertext to obtain Bob’s random seed, she verifies that the cipher- text sent by Bob is indeed derived from the seed.
If so, she proceeds to pseudorandomly  derive a shared secret (also in a way dictated by the algorithm specification) from the seed.
 • If the recreated ciphertext does not match what Alice received, Alice does not want to give an  attacker any information about whether or why the decryption failed.
This behavior of Alice  is known as implicit rejection .
This means that rather than sending an error message, Alice
8.2 HASH-BASED SIGNATURE S 207  pretends everything is fine and uses a random number for the shared secret, so whoever is  speaking with Alice will perceive this as Alice speaking gibberish ( i.e., Alice communicating  using an encryption key the other side does not know).
In contrast explicit rejection means  that Alice sends an error message or terminates the connection.
Using implicit rejection  instead of explicit rejection is believed to make it easier to avoid implementation errors that  might unintentionally leak information to an attacker.
 All the encryption schemes that made it to the third round of NIST’s PQC Standardization process  use this or a similar construction.
It should be noted that these sorts of constructions only effec - tively protect against chosen ciphertext attacks if the decryption failure rate for honestly generated  ciphertexts is very low, e.g., one failed decryption per 2128 ciphertexts.
Therefore, rigorous analysis  of the decryption failure rates of these schemes is necessary for analyzing their security, and not  just their reliability.
 For simplicity, when presenting the various families of post-quantum schemes, we will  present simplified versions that do not include these protections against chosen ciphertext attacks.
 8.2 HASH-BASED SIGNAT URES  This family of algorithms is based on the assumption that it is computationally infeasible to find a  pre-image of a hash h, i.e., a value v, such that hash( v)=h.
The proposed hash-based algorithms are  only for public key signatures (not encryption).
 Cryptographic hashes have been well studied, and are believed to be quantum-resistant.
They  are a conservative choice since, if properly implemented, they are less likely to be broken than  other signature schemes—hash-based signature schemes only rely on the security of a hash func - tion, while other signature schemes rely not only on the security of a hash function, but also on the  hardness of one or more additional computational problems.
There are two types of hash-based sig - natures schemes:  • Stateful , meaning that a signer must be extremely careful to keep track of how many items  were previously signed, or else the scheme is insecure.
 • Stateless , meaning that a signer need not keep track of how many things it has signed in the  past.
 The stateful schemes are vastly more efficient than the stateless ones, but stateful implementations  have to be incredibly careful to keep accurate state.
It might be easy for an implementation to lose  track of how many things have been signed if a process crashes and restarts or if there are multiple  instances of a service using the same public key.
208 P OST-QUANTUM CRYPTOGRAP HY 8.2.1  We’ll start our description with schemes that are impractical but easy to understand, and then  explain some optimizations to make these more efficient.
 Hash-based signatures tend to be described with lots of parameters.
This enables variants that  can trade off things like security, signature size, and computation, but all this flexibility can lead to  descriptions that are hard to read.
Since we’re primarily intending to give intuition rather than exact  specification, we’ll choose numbers for parameters to make our descriptions more readable.
 8.2.1 Simplest Scheme – Signing a Single Bit  The first concept is a one-time signature of a single bit.
An example of signing one bit of informa - tion is where Alice will announce which of two candidates won an election—candidate 0 or candi - date 1.
Alice’s private key will consist of two randomly chosen 256-bit numbers p0 and p1, and her  public key will consist of the 256-bit hashes of each of those— h0 and h1.
In other words,  hash( p0)=h0, and hash( p1)=h1.
To sign the value 0, Alice reveals the pre-image of h0, namely p0.
 To sign the value 1, Alice reveals the pre-image of h1, namely p1.
 With this scheme, Alice can only sign one bit, since knowledge of both p0 and p1 allows  someone to sign either a 0 or a 1 bit.
If Alice wants to certify the results of another election, she  needs to create a new public key.
Assuming we are using 256-bit ps and hashes, the size of a public  key would be 512 bits (consisting of h0 and h1), a private key would also be 512 bits (consisting of  p0 and p1), and a signature would be 256 bits (either p0 or p1, depending on whether she was sign - ing 0 or 1).
 8.2.2 Signing an Arbitrary-sized Message  Next, let’s build on the single-bit scheme to enable Alice to sign a single arbitrary-sized message.
 As with other public key algorithms that we are familiar with ( e.g., RSA), Alice doesn’t directly  sign the message, but rather, signs a hash of the message.
For example, if she uses the hash algo - rithm SHA-256, she’ll need to be able to sign 256 bits.
 To be able to sign 256 bits, Alice chooses two 256-bit secrets for each of the 256 bits.
So, for  i i ibit i, she’ll choose the pair of secrets 〈 p0, p1 〉.
If bit i is 0, she’ll reveal p0 .
If bit i is 1, she’ll reveal  i 0 0 1 1 255 255 p1 .
This will result in 512 secrets that we’ll call 〈 p0, p1 〉, 〈 p0, p1 〉,…, 〈 p , p 〉.
She 0 1  0 0 1 1 255 255 hashes each of these 512 secrets to obtain 〈 h0, h1 〉, 〈 h0, h1 〉,…, 〈 h 0, h 1 〉.
That’s 512  0 0 1 1 255 255hashes, and the hash of all of them, H = hash( h0| h1| h0| h1|…|h 0| h ), will be her 256-bit 1  public key.
 To sign a 256-bit quantity, say 001011101 …1, Alice does the following for each of the 256  i ibits: If the ith bit is 0, Alice reveals p0 and hi  1 .
If the ith bit is 1, Alice reveals hi  0 and p1 .
Bob (the  verifier) hashes each of the 256 pis Alice reveals, in order to now know all the 512 hashes ( h0 0 , h0 1 ,
8.2.3 HASH-BASED SIGNATURE S 209  1 1 255 255 0 0 1 1 255 255 h0, h1, … , h 0, h 1 ).
Then Bob computes hash( h0| h1| h0| h1|…|h 0| h 1 ) and veri - fies that the result is indeed Alice’s public key H.  Alice can only sign one message digest, because if she signs two different message digests,  where bit i is 1 in one digest and 0 in the other, both p 0 0 and p 0 are revealed, and then someone can 1  sign either a 0 or 1 for bit i. Note that each time Alice signs something, she reveals half of the  pre-images.
If she were to sign two messages with this scheme, then probably half of the bits in the  hash of the second message to be signed will be equal to those bits in the first message she signed,  so there would still be a quarter of the pre-images that were not yet revealed.
If Eve wants to forge  Alice’s signature, and Eve knows all 512 pre-images, she can sign anything.
But if there are, say n  unrevealed pre-images, then Eve has to keep testing messages until she can find one whose hash  has bits for which all the pre-images were revealed.
 For this scheme, the size of a public key is 256 bits, the size of a private key is 128K bits (256  i i〈 p0, p1 〉 pairs, which is 256 ×512 bits), and the size of a signature is also 128K bits.
Verifying a  signature requires hashing the message and doing an additional 256 hashes (of either the pre-image  i i p0 or p1 , depending on whether that bit in the message hash is a 0 or 1, and then hashing the 512  hashes.
 8.2.3 Signing Lots of Messages  A scheme that only allows Alice to sign a single message would not be very useful.
So we’ll modify  the scheme, using hash trees (also known as Merkle trees; see §5.4.8) to allow Alice to sign a lot of  messages.
In Figure 8 -1, she can sign four different messages (each with a 256-bit digest), and her  public key is still just 256 bits (the value of the tree root).
 Starting at the top of the tree, Alice’s public key is HAlice, the root node.
HAlice is equal to  hash( H0|H1).
H0 is hash( H00|H01).
Each of the four nodes at the second level equals the hash of its  512 children.
This tree will allow Alice to sign four different messages using the subtrees H00, H01,  H10, or H11.
 We’ll use the terminology of a treelet as the bottom level of the tree (with 512 children).
In  order to make a tree with n levels have 2n treelets, we won’t count the root or the leaves as a level.
 Therefore, the tree in Figure 8 -1 has two levels and four treelets.
 Alice can use use the treelet under H01 to sign a message as follows.
For each of the i bits in  i ithe message digest of the message to be signed, if the ith bit is 0, then Alice reveals p0 and h1.
If  ithe ith bit is 1, then she reveals hi  0 and p1 .
Bob (the verifier) hashes each p to obtain the corre - sponding h, so Bob now knows all 512 children of H01 and therefore can compute H01.
Bob also  has to be told H01’s sibling ( H00) so he can compute H0 (which is hash( H00|H01), and has to be told  H0’s sibling ( H1) so he can compute hash( H0|H1) and verify that the result is HAlice.
 To summarize, a public key in this scheme is a 256-bit hash, the root of the hash tree.
A sig- i inature (of a 256-bit digest) consists of 512 256-bit quantities—for each bit, a pair 〈 p0, h1 〉 if the
210 P OST-QUANTUM CRYPTOGRAP HY 8.2.3  Htreelet  treelet detail …  000 000 001 001 … 255 255h0h1h0h1 h0h1  …  000 000 001 001 255 255 …p0p1p0p1 p0p1  Figure 8-1.
 Hash Tree for Signing  ibit is 0, or 〈 h0 i , p1 〉 if the bit is 1.
The signature also needs the sibling hashes in the hash tree, con - sisting of a 256-bit quantity for each level in the tree.
Even for deep trees, signatures will only be a  little larger than 128K bits (16K octets).
 Suppose Alice wants to be able to sign a really large number of messages, say a million.
Gen - erating and storing the 20-level hash tree will be expensive (more than 130 billion bits—a million  times 512 times 256).
Another issue is that Alice might not know how many messages she will need  to sign.
She could overestimate, at the expense of creating and storing a bigger tree than she  needed.
But what if the number of messages exceeds the size of the tree she initially computed?
 Suppose she built a tree big enough for 1024 signatures with root R1.
Her public key would  then be R1.
What can she do if she needs to sign more messages than that?
 After signing 1023 messages, she can create a new hash tree with 1024 treelets and root R2  and use the last treelet in her first hash tree to sign the root of the new hash tree (effectively signing preimages hashes treelet roots  … … … … … … … … … … … … … … … … H00 H01 H10 H11 H0 H1 HAlice
8.2.4 HASH-BASED SIGNATURE S 211  a message saying “ R2 is also my public key”).
This enables her to sign an additional 1023 messages  using R2, plus leave room for signing the root of a third hash tree for the next 1023 messages.
Sig- i inatures for the first 1023 messages would consist of the 512 values ( p0 and hi  1, or hi  0 and p1 ) for  each of the 256 bits of the hash, plus the sibling hashes of the ancestor nodes in the first tree.
This  works out to 133632 bits, or 16704 octets.
 For the next 1023 messages, the signature would have to be double-sized, because not only  would it need to consist of the signature of the message being signed using the second tree, but also  the signature for the root of the second tree (that was signed using the last treelet in the first tree).
 This comes out to 33408 octets.
 Then for the next 1023 messages, the signature would be triple-sized, and so forth.
 An alternative strategy is that Alice could use the first tree only to sign roots of 1024 trees,  with each tree having 1024 treelets.
Then she’d be able to sign a million messages, each of whose  signatures would be 33408 octets.
If that’s not enough, a three-level structure would allow a billion  signatures, each of which would be 50112 octets.
 8.2.4 Deterministic Tree Generation  Generating a tree deterministically from a secret seed S enables Alice to only need a small secret  (e.g., 256 bits), which will be used to generate the hash trees as needed, instead of having to  pre-generate and store all the hash trees.
This saves Alice storage for her private key but costs com - putation because she needs to regenerate the entire tree to sign anything.
 To generate a tree with ten levels, Alice uses S to compute each of the leaf pre-image values  as a function of S, the treelet number, and an indicator of whether the pre-image is for 0 or 1.
For  example, p7 of treelet 42 might be hash( S|7|1|42).
1  Alice will need to generate the entire tree in order to sign a message.
But after signing (and  carefully remembering how many messages she has signed), she can forget the tree she generated  and just remember S and the number of messages she has signed.
There is a trade-off between how  much data she stores and how much computation she does.
If she remembers all the peer hashes  from the hash tree but recomputes the treelet she needs for each signature, she will need only a  moderate amount of stored data and computation.
 Suppose she is using a two-level tree of trees (where the treelets in the first tree are used to  sign second-level trees).
For each signature, she will need to generate (or store) the upper tree and  the one lower tree she is currently working on.
The secret for generating each tree has to be unique,  of course, so the single secret S would have to be hashed with the tree number to create the secret  for generating that tree.
 As with the single-tree case, a lot of computation can be saved by keeping track of all the sib - ling hashes already computed and computing sibling hashes for new trees as needed.
212 P OST-QUANTUM CRYPTOGRAP HY 8.2.5  This concept of deterministic tree generation will be particularly important for the stateless  schemes (see §8.2.7.1 Stateless Schemes ).
 8.2.5 Short Hashes  Using shorter hashes, e.g., 128 bits instead of 256 bits, would obviously improve performance.
 Treelets could have half as many leaf nodes, and each leaf value could be half the size.
But can a  scheme that uses 128-bit hashes have a security level of 128 bits?
Recall that finding a hash colli - sion for an n-bit hash requires only work 2n/2, whereas finding a pre-image of a specific n-bit hash  value requires work 2n .
 First, let’s make it safe for Alice to sign 128-bit hashes of messages rather than 256-bit  hashes, where by “safe” we mean maintaining NIST’s security category 1.
We need to ensure that  an attacker has to find a pre-image of a specific 128-bit hash rather than find any 128-bit hash colli - sion.
 We can force Ivan (the attacker) to find a pre-image of a specific hash instead of finding any  collision by doing the following.
To sign a message, Alice chooses a random number R (similar to  an IV), and her signature consists of R, the message, and the signature on hash( R|message ).
This  solves the collision issue, since Ivan cannot, in advance, find any two messages with the same  hash—he has to wait for Alice to sign a message and choose an R, and then Ivan has to find a  pre-image of hash( R|message ).
 Since Ivan cannot predict which random R Alice will choose, he cannot create an evil mes - sage with the same hash as the combination of R and a message Alice will sign.
Instead, Ivan will  have to find an evil message and an R such that hash( R|message ) is one of the hashes Alice has  already signed.
If Alice only signed a single message, Ivan would have to match the one hash she  signed ( i.e., find a pre-image of that hash), and this would be secure.
 There is an additional attack we should be aware of that makes finding a pre-image a little  easier.
This is known as the multi-target attack .
Suppose Ivan doesn’t need to find a pre-image of  one specific hash, but instead has a list of a million possible hashes for which finding a pre-image  will work for his purposes.
For example, Alice might have signed a million different messages, and  Ivan, with a list of a million hashes that Alice has signed, just needs to find a message that is a  pre-image of any of those million hashes.
Each time Ivan does a hash of a trial message, he can  compare the result against the entire list.
Since a million is approximately 220, this reduces the  number of messages he needs to try from 2128 to 2108 .
 To prevent the multi-target attack, Alice should also include the treelet number in the quantity  that she hashes.
Note that to enable Bob to verify Alice’s signature, the treelet number must already  be part of what is specified in Alice’s signature.
So, Alice’s ith signature will include message Mi,  random number Ri, treelet number used ( i in this case), hash tree sibling hashes, and the treelet  secrets revealed based on hash( Ri|Mi|i)).
8.2.6 HASH-BASED SIGNATURE S 213  Now let’s modify the design so that Alice can also use 128-bit hashes inside her hash tree and  still avoid the multi-target attack.
The hashes will need to be customized with a unique value (simi - lar to the purpose of salt in the Unix password hash—see §5.5 Creating a Hash Using a Block  Cipher ).
And the list of hashes that would be useful for Ivan to match might not only be hashes that  Alice has signed, but hashes that other authorized entities have signed.
So, for Alice’s hash tree, an  additional input into each hash should be the name Alice , and the location in Alice’s tree.
 These optimizations reduce the size of a signature by a factor of four, since each constant  released is only half as many bits and there are only half as many bits to sign.
So the size of a signa - ture can be about 4K octets for signing up to a thousand messages, 8K for signing up to a million,  and 12K for signing up to a billion.
 8.2.6 Hash Chains  A hash chain is computed by taking a value p and hashing it multiple times.
Our notation is that  hash7(p) means computing hash(hash(hash(hash(hash(hash(hash( p))))))).
Someone who knows p  can calculate hashi(p) for any value i, but someone that sees hashi(p) cannot calculate anything in  the chain before i. For example, if someone is told hash3(p), they’d be able to compute hash4(p),  hash5(p), hash6(p), hash7(p), but not p, hash( p), or hash2(p).
Note that, as in the previous section,  in order to avoid the multi-target attack, the hashes have to be “customized” by including some con - stant specific to the hash chain in each hash calculation.
For example, in addition to the customiza - tion in the previous section ( e.g., Alice’s name and the treelet number), the third hash of p should  include 3.
So hash3(p) would be hash( Alice |treelet number |3|hash2(p)).
However, we’ll leave all  that customization out of our notation to keep things simple.
 We can use hash chains to make signatures smaller, at the cost of more computation.
Previ - ously, Alice signed one bit by releasing two numbers—either p0 and h1 for 0, or h0 and p1 for 1.
 Suppose Alice wants to sign four bits at once.
The 4-bit chunk will have one of the values  {0000 ,0001 ,…,1111 ).
Let Alice choose a single secret, say p, and hash p fifteen times.
If the 4-bit  chunk equals 7, she reveals hash7(p).
If the 4-bit chunk equals 6, she reveals hash6(p), and so forth,  until, if the chunk equals 0, she reveals p. This isn’t secure yet, though, since if she signs 0, once  she has revealed p, a forger could sign any other value for the chunk.
If she signed 13, a forger  would be able to sign 14 or 15.
 To fix that security problem, Alice uses two hash chains, based on secrets that we’ll call pup  and pdown.
To sign the 4-bit value i, Alice divulges hashi(pup), and hash15−i(pdown).
Now a forger  cannot sign extra values.
If Alice signed i, the forger would need to find pre-images of hashi(pup) to  sign a value smaller than i and would need to find pre-images of hash15−i(pdown) to sign values  larger than i.  Using this technique with 4-bit chunks, a treelet for signing 128 bits only requires 64 values:  a pair of values—hash15(pup) and hash15(pdown)—for each of the 32 4-bit chunks.
In contrast,
214 P OST-QUANTUM CRYPTOGRAP HY 8.2.7  signing each of the 128 bits individually requires 256 values.
This optimization of using 4-bit  chunks reduces the signature size from 8512 octets to 2368 octets in the case where up to a million  signatures can be generated per public key.
 A variant of this scheme, credited to Winternitz, makes the signature even smaller.
Again,  assuming 4-bit chunks, Winternitz still uses hash15(pup) for each chunk but requires only a single  pdown chain, which signs the sum of all the chunks.
Since there are 32 4-bit chunks in a 128-bit  value, each with a value between 0 and 15, the sum of the chunks will be a number between 0 and  15×32 = 480.
A treelet would only need 33 values: hash15(pup) for each chunk and a single value  hash479(pdown) for the sum.
 To save the work of having to compute up to 480 hashes, the sum can be represented in binary  with three 4-bit chunks, requiring three values of hash15(pdown), one for each of those chunks.
A  treelet for signing a 128-bit value would then consist of 35 values: hash15(pup) for each of the 32  chunks in the 128-bit value and three values of hash15(pdown) for the sum.
This optimization  reduces the signature size from 2368 octets to 1440 octets in the case where up to a million signa - tures can be generated per public key.
 How many bits are in each chunk is one of the parameters of a hash signature algorithm that  trades off signature size against computation.
Using 8-bit chunks instead of 4-bit chunks results in  signatures that are half as big (because there are half as many chunks) but increases the computa - tion cost by a factor of eight (since 256 hashes are required to complete a chain rather than 16).
 Most standardized schemes tend to use chunk sizes between one and eight bits.
 8.2.7 Standardized Schemes  RFCs 8391 (XMSS) and 8554 (LMS) each specify a stateful hash-based scheme.
They differ in  their details such as how they pad the data being hashed, and they each offer a wide range of param - eter values trading off number of signatures with a single key, compute time, and signature size.
 RFC 8554 signature sizes range from 1616 octets (to sign up to 32K messages) to 3652 (to sign a  trillion).
RFC 8391 signature sizes range from 2500 octets (to sign up to 1K messages) to 27688  octets to sign up to 260 messages.
 8.2.7.1 Stateless Schemes  The schemes we’ve outlined so far require Alice to remember how many messages she’s ever  i isigned, so that she doesn’t ever reuse the same treelet and wind up divulging both p0 and p1 for the  same bit.
For some applications, such as root certificates and code signing, this might be an accept - able requirement, since the application inherently requires strong version control, backups, and  record keeping.
However, for other applications, where signatures are created more dynamically
8.2.7.1 HASH-BASED SIGNATURE S 215  (e.g., TLS authentication, where the parameters for each new connection must be signed), using the  stateful protocol is very likely to lead to security problems.
 In the schemes we’ve looked at so far, the way we assured that we never used the same treelet  to sign two different hashes was to keep track of how many things we have signed and never use a  treelet more than once.
But what if Alice is stateless?
 One example of a stateless approach is as follows.
Suppose Alice has a tree with 128 levels,  so there are 2128 treelets, one for each possible hash value (assuming 128-bit hashes).
To sign a  hash with value X, she’d use the Xth treelet.
Alice would not need to keep track of how many hashes  she’s signed, because a treelet could only be used to sign a single value.
Signatures would be fairly  small (just 127 sibling hashes and the hashes or pre-images in the treelet), but Alice would need to  generate the entire tree beforehand, which is obviously infeasible.
 But if Alice used a multi-level tree, using the technique that we described in §8.2.3 Signing  Lots of Messages , Alice need not generate the entire tree in advance and can instead deterministi - cally generate trees as needed from a secret seed S. Alice’s top-level tree might have 28 treelets  (eight levels).
To sign a hash that has the first eight bits equal to X, Alice uses the Xth treelet in the  top-level tree to sign the root of a tree TX .
The tree TX is used for signing hashes starting with X.  Tree TX will also have eight levels, and the Yth treelet in tree TX will be used for signing the  root of a tree to be used for signing message digests that start with X|Y. And so forth, through six - teen trees, to get up to the size of a 128-bit hash.
 As we described before, in order to force Ivan to find a pre-image of a specific hash rather  than find a collision, Alice will need to choose a random R when signing message m, and her signa - ture will include R, m, and her signature on hash( R|m).
 Although Alice will never use the same treelet to sign two different hash values, there is still  the problem of the multi-target attack.
If Alice has signed, say a million 〈R,m〉 pairs, Ivan could try  〈R′,m′〉 pairs, and see if hash( R′|m′) matches any of the hash( R|m) values in any of Alice’s signa - tures.
 The defense is to increase the size of the hash according to how many signatures Alice is  expected to sign.
If she is likely to sign a million times (~220), then the hash size should be  increased by twenty bits, and the number of levels in the multi-level tree should be increased by  twenty.
 One optimization is to note that if Alice only uses the treelet hash( R|m) for signing the value  hash( R|m), there is no need for the treelet to have more than a single leaf—treelet i need only con - tain a single hash value, say Hi, and Alice can sign the value i by revealing the pre-image of Hi .
 This scheme is attributed to John Kelsey, who called it Pyramid.
 An alternative way to build a stateless hash-based signature scheme is that instead of having  Alice use treelet Z to sign the hash value Z, Alice could choose a treelet, say treelet T, at random,  compute Z=hash( R|m|T), and sign the value Z using treelet T. Then the number of treelets does not  need to depend on the length of the hash, but instead on the number of things that Alice will sign.
 For instance, if Alice will never sign more than a million messages (220), and the desired
216 P OST-QUANTUM CRYPTOGRAP HY 8.3  probability that Alice accidentally choose the same treelet twice should be less than, say, 2−64, then  the number of treelets required should be at least 2104 .
How do we get the exponent 104?
By the  birthday problem (§5.2 ), the probability of Alice accidentally choosing the same treelet if there are  240 treelets with Alice signing 220 messages is about ½. To make the probability less than 1 in 264  that she’ll accidentally choose the same treelet, we need to add 64 to 40.
 One of the proposals submitted to the NIST competition is a stateless hash signature scheme  known as SPHINCS+.
It defines some additional optimizations specific to the case of a stateless  hash and allows up to 264 signatures to be generated that are only 7856 octets long.
Proposed  schemes often have parameters that can be modified to trade off security, signature sizes, and com - putation, and often the authors of the schemes make modifications to the schemes to improve per - formance or security.
So these numbers are just approximate, and we are providing them just to  give the reader a feel for the performance.
 8.3 LATTIC E-BASED CRYPTOGRAP HY  What is a lattice?
A lattice is a set of points in n-dimensional space.
A point in an n-dimensional  space is represented by an n-tuple of numbers 〈x1,x2,…,x 〉.
A point can also be thought of as a vec -n  tor.
For example, 〈x1,y1, z1〉 is the vector that gets you from the origin 〈0,0,0〉 to the point 〈x1,y1, z1〉.
 To add vectors, you add the ith coordinate of each to get the ith coordinate of the sum, e.g.,  〈x1,y1, z1〉 +〈x2,y2, z2〉 = 〈x1+x2,y1+y2, z1+z2〉.
 A lattice is closed under addition and subtraction, which means that if two points  v1=〈x1,y1, z1〉 and v2= 〈x2,y2, z2〉 are points in a lattice, then so are v1+v2 and v1−v2.
It follows then,  that all the following linear combinations of v1 and v2 are also in the lattice:  • v1−v1 which equals 〈0,0,0〉  • − v1 which equals 〈−x1,−y1,−z1〉  • 2v1− 3v2 which equals 〈2x1−3x2,2y1−3y2,2z1− 3z2〉  A basis of a lattice is a set of lattice vectors b1,b2,…,b such that any point in the lattice can n  be written in exactly one way as an integer linear combination of the n basis vectors.
For example,  if c1,c2,…,c are integers, then c1b1+c2b2+…+c b is a point in the lattice.
A lattice is completely n n n  specified by giving a basis for it.
 A basis of an n-dimensional lattice will consist of n vectors, each with n components.
A com- mon way to specify a basis is as an n×n matrix, where the ith row in the matrix represents the ith  basis vector.
 There are many different bases that specify the same lattice.
For example, the 2-dimensional  lattice in Figure 8-2 can be specified with the two short basis vectors in the left diagram or the two
8.3.1 LATTI CE-BASED CRYPTOGRAPHY 217  long vectors in the right diagram.
Either basis generates the same lattice.
A basis consisting of short  vectors is known as a “good basis” while a basis consisting of long vectors is known as a “bad  basis”.
 8.3.1 A Lattice Problem  A lattice problem that is considered difficult: Given a bad basis for a lattice in an n-dimensional  space and some non-lattice point in that space, find a nearby lattice point.
Given a good basis, how- ever, it’s easy (see §8.3.4.2).
This leads to an intuition for a lattice-based scheme: have Bob’s pri - vate key be a good basis for a lattice and his public key be a bad basis that generates the same  lattice.
If Alice knows Bob’s public key (a bad basis for his lattice), she can establish a secret m  with Bob, as follows (see Figure 8-3):  • Alice uses Bob’s public key (the bad basis) to compute some random point P in Bob’s lattice.
 • She chooses a random small n-dimensional vector m.  • She computes X =P+m.
Point X will not be in Bob’s lattice.
 • Bob uses his private key (the good basis) to find the nearest lattice point to X, which is P .
 • Bob computes X −P to get m.  Lattice-based encryption schemes that work this way include the Goldreich, Goldwasser, Halevi  cryptosystem [GOLD97] and the NTRU cryptosystem that we will discuss shortly.
Figure 8-2.
Lattice Bases  P m X  Figure 8-3.
Message Encoded as Offset from a Lattice Point
218 P OST-QUANTUM CRYPTOGRAP HY 8.3.2  8.3.2 Optimization: Matrices with Structure  One optimization cryptographers use to make schemes more practical is to use matrices with struc - ture so that only a small amount of the matrix needs to be stored and transmitted, and the rest can be  derived.
 This optimization is such an important part of lattice schemes that we describe it before  describing the schemes.
To specify a general n×n matrix requires n 2 elements, each element requir - ing some number of bits to specify its value.
Unfortunately, in order to make the lattice problems  hard enough to be cryptographically strong, we need a lot of dimensions.
For example, with  n =1000, element size twelve bits, this would require twelve million bits.
Using structured matrices  can vastly improve the efficiency of lattice-based cryptosystems.
While the most popular ways of  using structured matrices are not known to significantly weaken security, some cryptosystems using  structured matrices have been broken.
Paranoid lattice cryptosystems that forgo the use of struc - tured matrices entirely have been proposed.
Nonetheless, given the lack of known attacks and the  vast efficiency improvements, it seems likely that the most widely used lattice cryptosystems in the  future will use structured lattices.
 One example of a structured matrix is known as a circulant matrix , where only the top row  needs to be specified, and each subsequent row is created by rotating the entries in the previous row  one column to the right, with the element that falls off the end rotating into the leftmost position.
 An example of a circulant matrix is:  ⎡  ⎢  ⎢  ⎢  ⎢  ⎢  ⎢⎣ 17 33 5 0 −12  −12 17 33 5 0  0 −12 17 33 5  5 0 −12 17 33  33 5 0 −12 17               The way to think of this mathematically is that each row represents a polynomial of degree n−1,  where n is a prime, with the elements being the coefficients of the n terms of the polynomial, con - stant term being on the left.
If the row has elements r0,r1,r2,…,rn−1, then the polynomial repre - sented by that row is r0+r1x+r2x 2+…+rn−1.
And to be more tangible, the top row of the above n−1x  4circulant matrix would represent the polynomial 17+33 x+5x 2–12x .
 Each row (other than the top one) is calculated by multiplying the polynomial represented by  the previous row by x, and then reducing modulo xn−1.
This results in just what we want.
All the  values will shift one position to the right, and the element shifted off the end will simply be shifted  into the leftmost position.
This technique is also known as lattices from polynomial rings , or ideal  lattices .
 Suppose you have two circulant matrices, M1 and M2, where the polynomial representing the  top row of M1 is p1, and the polynomial representing the top row of M2 is p2.
It is easy to show that
8.3.3 LATTI CE-BASED CRYPTOGRAPHY 219  M1M2 is a circulant matrix, and the top row of M1M2 is p1p2 mod xn−1.
That means that not only  do you save storage when using a circulant matrix (it only takes 1/ n of the storage to specify the top  row), but it saves computation to do addition or multiplication of circulant matrices, because all you  need to do is multiply or add the polynomials representing the top rows of the matrices.
 For example, all the following are circulant:  • The identity matrix  • The sum of two circulant matrices  • The product of two circulant matrices  While in general, matrix multiplication is not commutative, multiplication of circulant matrices is  commutative (Homework Problem 8).
While not all circulant matrices have inverses, for those that  do, the inverses are also circulant matrices (Homework Problem 9 ).
 Because circulant matrices can be specified by just specifying the top row, this greatly  reduces key sizes.
For instance, with 1000 dimensions and 12-bit elements, it only takes 12000 bits  to specify a circulant matrix, rather than 12000000 bits to specify a general 1000 ×1000 matrix of  12-bit elements.
 In a typical lattice-based cryptography scheme using circulant matrices, one or more circulant  matrices with small elements are chosen by Bob to be his private key.
Then he calculates a public  key with large elements from the private key.
The math for obfuscating the private key (to transform  it into a public key) is chosen such that the public key will also consist of one or more circulant  matrices, but with large elements.
 8.3.3 NTRU-Encryption Family of Lattice Encryption Schemes  Now we will describe an actual scheme, NTRU (named for Nth degree TRUuncated polynomial  ring) [HOFF98 ].
The original scheme has been improved for efficiency, and there are various pro - posals based on the original scheme.
We will call the scheme in our description NTRU, even though  NTRU describes a family of similar schemes, and if there is a standardized scheme named NTRU, it  is likely to differ from the description here in some details.
The variant we propose here is chosen to  be simple to understand.
It is similar to, but not identical to, the scheme called NTRU that is a NIST  submission.
It will not at first be obvious how NTRU relates to lattices, but we’ll explain that after  we explain our NTRU variant.
 NTRU is described using degree n−1 polynomials.
Arithmetic on the coefficients of the poly - nomials is done mod q, where n is a prime and q is a power of 2.
A typical value of n is 509 and a  typical value of q is 2048 (which would result in elements requiring eleven bits).
After multiplica - tion of polynomials, the result is reduced mod xn−1, so that the result will be a degree n−1 polyno - mial with coefficients interpreted as 11-bit signed integers ranging from −1024 through 1023.
For
8.3.3.1 220 P OST-QUANTUM CRYPTOGRAP HY  readability, we will avoid repeating everywhere that polynomial multiplication is mod xn−1 and  arithmetic on the coefficients is done mod q.  8.3.3.1 Bob Computes a (Public, Private) Key Pair  There are some parameters that are well known, i.e., specified as part of the algorithm.
These  parameters are the degree, n, of the polynomial modulus; the coefficients’ modulus, q; and a  smaller modulus, p, which is 3 in most variants.
To make the description easier to read, we’ll cut  down on the number of variables and use p=3 in our description.
n is a prime number, and q is a  power of 2.
In the several variants proposed to NIST, which trade off performance versus security,  〈n,q〉 is 〈509,2048 〉, 〈677,2048 〉, 〈701,8192 〉, or 〈821,4096 〉.
Although we will set p to 3, we will  use n and q rather than choosing specific numbers for n and q in our description.
 To create his key pair, Bob chooses two polynomials, f and g, each of degree n−1, with small  integer coefficients.
For polynomial g, the coefficients will be −3, 0, or 3.
Likewise for f, except the  constant coefficient of f will be −2, 1, or 4.
As a result of this choice, g will be 0 mod 3 (all the coef - ficients of the polynomial g will be 0 mod 3), and f will be 1 mod 3 (all the coefficients of f will be  0 mod 3, except the constant term, which will be 1 mod 3).
The fact that f =1 mod 3 and g=0 mod 3  will be relevant later.
 −1The polynomial f needs to have the extra property that it has an inverse, a polynomial f  such that f ×f −1 = 1 (mod xn−1 mod q, as always in NTRU, though for readability we’ll elide these  moduli henceforth).
If the coefficients Bob chose for f result in a polynomial that does not have an  inverse, he chooses a different f. It is not difficult for Bob to calculate f −1 (see Homework Problem  10).
 To turn f and g into a public key, Bob multiplies g by f −1 to get a new polynomial, which  −1we’ll call H. So, H=f g .
 Bob’s public key is H, which will be an n−1 degree polynomial with large coefficients (in the  range of −q/2 through q/2−1).
Bob’s private key is f.  8.3.3.2 Alice encrypts m with Bob’s public key  Alice knows H. She chooses two ( n−1)-degree polynomials, r and m, both with small coeffi - cients.
The polynomial m is the encoding of the message she is encrypting for Bob, and its coeffi - cients are all chosen to be 1, 0, or −1.
 Alice sends rH+m to Bob.
Only Bob, with knowledge of his private key f, will be able to  obtain m.
8.3.3.3 LATTI CE-BASED CRYPTOGRAPHY 221  8.3.3.3 How Bob Decrypts to Find m  Bob multiplies the polynomial he receives ( rH+ m) by his private key f, getting f × (rH+ m).
Since  −1 −1 −1 −1H=f g , the result is f ×(rf g+ m) = f × rf g + fm = f ×f rg+ fm =rg+fm.
He then reduces  the coefficients in rg+fm mod 3.
Since g=0 mod 3 (all the terms in polynomial g are 0 mod 3), and  since f =1 mod 3, the result of reducing the coefficients in rg +fm mod 3 is m.  It might be surprising that reduction mod 3 works.
It wouldn’t work if the coefficients had  been first reduced mod q, because if z is divisible by 3, z−q won’t be.
So why can Bob reduce  rg+fm mod 3?
 The size of the coefficients in r, g, f, and m are chosen to be small enough that the coefficients  in rg+fm would not need to be reduced mod q, if rg+fm were calculated directly (without having  intermediate multiplication by f −1).
Although the journey by which Alice and Bob each did part of  the computation to get rg+fm did involve mod q arithmetic, the result ( rg+fm) was the same as if  Bob had directly calculated rg+fm.
Since direct calculation of rg+fm will not involve mod q, Bob  can take what he calculated and reduce mod 3.
 8.3.3.4 How Does this Relate to Lattices?
 When we introduced lattices, we described the basis of an n-dimensional lattice as n n-element vec - tors, or alternatively, the n rows of an n×n matrix.
The definition of lattices might seem to say that  the elements (the coordinates of the points in the lattice) are real numbers, which would be awk - ward for implementations, because we’d like to express an element in a small fixed number of bits.
 If the basis vectors for a lattice all have integer elements, the lattice points would all have integer  elements; but this would still be awkward because the integers would be of unbounded size.
 Modular arithmetic would be ideal, but if we simply defined an n-dimensional lattice with  mod q arithmetic, any n basis vectors that are linearly independent mod q would result in a very  uninteresting lattice—every point with integer coordinates would be a lattice point.
 Bob’s lattice is actually 2 n-dimensional, with infinitely many lattice points, including lattice  points with elements that are arbitrarily large (larger than q).
However, NTRU uses a trick so that  elements in the basis vectors, and in the computed lattice points, are small enough that they can be  represented as mod q integers.
 A basis for Bob’s 2 n-dimensional lattice consists of 2 n basis vectors, each 2 n-dimensional.
 The basis can be represented by the 2 n ×2n matrix constructed by gluing four n× n matrices  together:  • An identity matrix on the top left  • H on the top right (recall that H is Bob’s public key) ⎡ HI   ⎢• 0 on the bottom left 0 qI⎣  • q times an identity matrix on the bottom right
8.3.3.4 222 P OST-QUANTUM CRYPTOGRAP HY  Recall that given a basis for Bob’s lattice, replacing any basis vector with that basis vector plus any  linear combination of other basis vectors will result in another basis for the same lattice.
Likewise,  adding or subtracting a basis vector to any lattice point yields another lattice point in Bob’s lattice.
 In NTRU, the elements of H (the elements in the top right of this matrix) can be kept within  the desired range by adding or subtracting the relevant rows of the bottom half.
For example, to  reduce the rightmost element in the top row mod q, subtract the bottom row as many times as nec - essary.
Therefore, with the trick of using the bottom half of the basis vectors to reduce elements  with size outside the range, the NTRU implementation computes a public key with elements in the  proper range and finds lattice points with elements in the desired range ( −q/2 through q/2−1) by  adding or subtracting the relevant bottom rows.
 In our description of NTRU, we said that Alice chooses two n-dimensional vectors, r and m.  What is actually happening in the 2 n-dimensional lattice?
Since Bob’s lattice is really  2n-dimensional, Alice would be choosing two 2 n-dimensional vectors, which we’ll call r′ and m′:  • r′ is the n elements of r followed by n zeroes, which we’ll write as r′=r|0n .
 • m′ is the n elements of −r followed by the n elements of m, which we’ll write as m′=−r|m.
 Note that m′ consists of small elements since both r and m consist of small elements.
 The vector r′ is used to compute a lattice point in Bob’s lattice.
Multiplying r′ by Bob’s  2n-dimensional public key matrix results in a 2 n-dimensional vector consisting of r concatenated  with rH, which we’ll write as r|rH, which is a lattice point in Bob’s 2 n-dimensional lattice.
Alice  can use the bottom half of Bob’s public matrix to reduce any of the elements in the last half of the  vector mod q, which will compute another lattice point in Bob’s lattice, where the elements will be  in the desired range.
 The vector m′ is the offset from a point in Bob’s lattice.
Even if Alice has reduced the ele - ments of rH mod q, adding m′ might require her to use the bottom half of Bob’s public matrix to  reduce some more but will still result in another lattice point, where m′ will be the offset from that  lattice point.
 The result of Alice adding m′= −r|m to the lattice point r|rH will be the 2 n-dimensional vec - tor that is n zeroes followed by rH+m (mod q), or equivalently, 0n|rH+m (mod q).
This is a point  in 2n-dimensional space that is close to a point in Bob’s lattice (it’s close because m′, as we said  before, consists of all small elements).
There’s no reason for Alice to send n zeroes, so she just  sends rH+m.
 What does the good basis for Bob’s 2 n-dimensional lattice look like?
His private key consists  of f and g. The top half ( n rows) of the matrix representing his good basis consists of two n× n cir- culant matrices—a circulant matrix on the left with f as the top row and a circulant matrix with g as  the top row on the right.
Because both f and g were chosen to have small elements, this will be n  vectors in a good basis.
To see that the n rows of this n× 2n matrix indeed represent the same lattice  points as Bob’s public key, note that multiplying the top half of Bob’s public matrix ( I|H) by the
8.3.4 LATTI CE-BASED CRYPTOGRAPHY 223  circulant matrix with f as the top row, and using the bottom half of his (bad) basis as necessary to  reduce elements by mod q, results in the top n rows of the good basis.
 This is only half of Bob’s private basis (because there are only n rows).
Bob can get another n  vectors that are nearly as short to fill out his basis.
These extra rows are not required for decryption.
 The extra rows are used in signatures based on the NTRU lattice, but the process used to generate  them is somewhat tedious, so we will not discuss it.
 8.3.4 Lattice-Based Signatures  Just as with encryption, the first lattice-based signature scheme we describe will consist of Bob’s  private key being a good basis for a lattice and his public key being a bad basis for the same lattice.
 First, we’ll present a simple, intuitive signature scheme that was shown to be insecure.
Then we’ll  show how it was fixed.
 8.3.4.1 Basic Idea  We assume there is a special hash function whose output contains the correct number of bits to be  interpreted as a random point P in an n-dimensional space, and we assume that Bob’s lattice is  n-dimensional.
 To sign a message M, Bob uses that hash function to find the corresponding P=hash (M) in  n-dimensional space.
Bob signs M by showing a lattice point L in his lattice near to P. So Bob’s sig - nature consists of 〈M,L〉.
This proves Bob knows the private key because only someone that knows  the good basis can calculate a nearby lattice point.
 To verify Bob’s signature on M, Alice uses the hash function to find P from M, verifies that  P−L is small, and using Bob’s public key (the bad basis), can verify that L is in Bob’s lattice.
 8.3.4.2 Insecure Scheme  How does Bob use his short basis to find a nearby lattice point?
One simple method, proposed in  the Goldreich, Goldwasser, Halevi signature scheme [GOLD97], works as follows:  Recall, given a basis b1,b2,…,b for a lattice, that c1b1+c2b2+… +c b is a point in the lattice n n n  as long as c1,…,c are integers.
Suppose b1,b2,…,b is Bob’s good basis.
Bob can find a lattice n n  point near P by  1.
Solving for real numbers x1,…,x such that x1b1+…+x b =P. This step involves solving lin -n n n  ear equations of the real numbers to limited precision, which is easy.
 2.
Rounding each of x1,…,x to the nearest integer to get integers c1,…,c .n n  3.
Multiplying the basis vectors by c1,…,c to get the lattice point L = c1b1+c2b2+… +c b .n n n
8.3.4.3 224 P OST-QUANTUM CRYPTOGRAP HY  Rounding xi to get ci moves L away from P by at most half the length of a basis vector.
Since Bob’s  good basis consists of short vectors, the total distance from L to P isn’t very big.
If Trudy (who  doesn’t know a good basis for Bob’s lattice) had tried the same process using Bob’s bad public  basis, she would likely have gotten an L that was much further away.
So Alice just needs to check  that P is as close to L as would be expected if Bob’s good basis was used.
 This proposed lattice-based signature scheme was shown to be insecure by Gentry and Szy- dlo in 2002 [GENT02], and the attack was improved by Nguyen and Regev in 2006 [NGUY06] so  that breaking a private key required the attacker seeing only 400 of Bob’s signatures.
 Why is this insecure?
Unfortunately, the public key is not the only information an attacker  might have.
Typically, the attacker also has access to some appreciable number of previously  signed messages, and so 〈P,L〉 pairs.
If the vectors consisting of P−L for each of Bob’s signatures  are graphed, they are all confined to a parallelepiped whose edges are parallel to the good basis vec - tors in Bob’s private key.
The attacker can do statistics on P−L for a moderate number of signatures  to learn the shape of this parallelepiped ( Figure 8 -4)..  Figure 8 -4.
How Seeing Multiple Signatures Divulges the Private Key  8.3.4.3 Fixing the Scheme  In 2008, Gentry, Peikert, and Vaikuntanathan [GENT08] published a method that avoids the flaws  in the first scheme.
The basic idea is that Bob should not always give the closest lattice point to  hash (M), but instead he should give a somewhat nearby lattice point.
Bob has to be careful to  always choose the same L for a given P; otherwise, if Bob signed P with L1, and later signed P  with L2, the two lattice points L1 and L2 would be sufficiently close to each other that an attacker  that saw both signatures could compute L1−L2, which would be a small basis vector for Bob’s lat - tice.
 This provably secure hash-then-sign signature is the basis for the NIST candidate Falcon.
 There are other efficient provably secure lattice-based signature constructions, such as the one pro - posed in 2012 by Lyubashevsky [LYUB12] that is the basis for the NIST candidate Dilithium.
By  provably secure , as usual, we mean the scheme is secure assuming certain assumed-hard problems  are actually hard.
In both these cases, however, the proofs are strong enough to rule out the possibil - ity that seeing signed messages in addition to the public key could significantly help the attacker.
8.3.5 LATTI CE-BASED CRYPTOGRAPHY 225  8.3.5 Learning with Errors (LWE)  Another encryption scheme in the lattice family is known as learning with errors (LWE ).
As with  NTRU, there are many variants.
 LWE schemes always use modular arithmetic.
As with NTRU, they can be translated to a lat- tice problem, and the result looks very similar to the 2 n-dimensional lattice in §8.3.3.4.
 LWE is intuitively similar to Diffie-Hellman (§6.4), in that in both algorithms Alice and Bob  each choose a secret, generate a public message derived from that secret, transmit their public mes - sage, and use the received public message and their own secret to generate a shared secret.
Recall  that in traditional Diffie-Hellman protocol with modular exponentiation, the non-secret parameters  are a generator g and a prime p. Alice chooses a secret A and Bob chooses a secret B. The public  message Alice sends is gA mod p. Bob sends gB mod p, and they agree on the secret g AB mod p.  In contrast, in LWE, the non-secret parameters include a modulus size q and an n×n matrix A  with elements between 0 and q−1.
The value of n is a parameter of the scheme and is typically in  the range 500–1000.
In our example, we use q=215 and n=1000.
 It is safer to agree upon a new randomly generated matrix for each exchange, and designs  ensure that it is inexpensive to do so.
The matrix A will not be secret, but Alice and Bob need to  agree upon the same matrix.
Rather than enumerate all n 2 elements of A, the matrix can be speci - fied by having Alice transmit a 256-bit seed to Bob and having a deterministic publicly  agreed-upon algorithm for generating A using the seed.
So Alice, Bob, and any eavesdropper will  be able to compute A. When we say below that Alice transmits A to Bob, we mean that she chooses  a seed that generates A and sends the seed to Bob.
 In LWE, Alice and Bob each generate a secret that is a pair of n-length vectors with small ele - ments.
The specific limit on “small” from which the elements are chosen ( e.g., −2, −1, 0, 1, or 2) is  specified by the scheme.
Schemes also specify a distribution of elements.
For instance, if a scheme  specifies a uniform distribution, there should be an equal probability of choosing each value for  each element.
A binomial distribution would give higher probability to 0, lower probability to ±1,  even lower to ±2.
 Using terminology in most of the papers, we’ll call Alice’s secret vectors r and eA. We’ll call  Bob’s secret vectors s and eB. The vectors eA and eB are error vectors .
Their elements are mod q  integers.
Alice’s vector r will multiply as a row vector ( i.e., a 1×n matrix), while Bob’s vector s will  multiply as a column vector ( i.e., an n×1 matrix).
 The protocol steps are:  1.
Alice chooses n-element vectors r and eA, both with small coefficients, and a matrix A.  2.
Alice computes, and transmits to Bob, A and the n-dimensional vector rA+eA.  3.
Bob chooses n-element vectors s and eB, also both with small coefficients, and transmits to  Alice the n-dimensional vector As+eB.
8.3.5.1 226 P OST-QUANTUM CRYPTOGRAP HY  4.
Alice multiplies r by what she received from Bob (the n-dimensional vector As+eB) to get  the scalar rAs+reB.  5.
Bob multiplies the n-dimensional vector he received from Alice ( rA+eA) by s to get the sca - lar rAs+eAs.
 6.
Since r, s, and the error vectors are all small, both Alice and Bob will calculate scalars that  are close to rAs (mod q).
 Let’s give the value rAs the name Z (which is what Alice and Bob would have both computed if it  weren’t for adding in the error vectors).
If q is 215 , Z will be 15 bits long.
Alice and Bob each have  computed a value that is approximately equal to Z.  How far apart might “approximately equal” be?
Call Alice’s value ZA and Bob’s value ZB.
 ZA−Z (Alice’s difference from Z) is reB. In the worst case (all the coefficients in r and e are, say, 2,  and with n=1000), the product can have magnitude at most 4000.
Likewise, the maximum magni - tude for eAs (Bob’s difference from Z) is also 4000, so the maximum magnitude for ZB−ZA is 8000.
 But since elements are supposed to be distributed among { −2,−1,0,1,2}, the actual difference will  usually be much smaller than that.
 To send a single bit to Alice, Bob adds a small number ( e.g., −2, −1, 0, 1, or 2) to ZB to trans - mit 0, and he adds a large number to ZB(q/2+ one of −2, −1, 0, 1, or 2)) to transmit 1.
When Alice  receives this number from Bob, she can tell whether the result is closer (mod q) to ZA (in which  case Bob is sending 0) or closer to ZA plus half the maximum value of an element (in which case  Bob is sending 1).
 8.3.5.1 LWE Optimizations  8.3.5.1.1.
 Reusing 〈r,eA〉 and 〈s,eB〉 Values  Using the strategy in the previous section, Alice and Bob each need to send an n-element vector in  order to agree on a single secret bit.
If n=1000, with elements that are 15-bits long, each vector is  15000 bits.
 In typical uses, Alice and Bob really need to agree on a 256-bit secret (rather than a single  bit).
Rather than doing the strategy in the previous section 256 times, once for each bit, where Alice  and Bob would each have to transmit 256 different n-element vectors, the optimization in this sec - tion allows Alice and Bob to agree upon a 256-bit secret and each only transmit sixteen n-element  vectors!
 Alice chooses sixteen pairs 〈r0,eA0〉,〈r1,eA1〉,…,〈r15,eA15〉.
Bob chooses sixteen pairs  〈s0,eB0〉,〈s1,eB1〉,…,〈s15,eB15〉.
Each 〈i, j〉 combination yields a different approximate Z, resulting  in 256 different values for Z, i.e., Zij =riAsj for 0≤ i≤15 and 0≤ j≤15.
8.3.5.1 LATTI CE-BASED CRYPTOGRAPHY 227  At this point there are 256 Z values, which means 256 ZA values known to Alice and 256 ZB  values known to Bob.
For each of the 256 ZB values, Bob adds either approximately 0 (for a 0) or  approximately q/2 (for a 1).
Alice and Bob each transmit sixteen n-element vectors, and Bob addi - tionally transmits 256 scalars.
With n =1000, and elements being fifteen bits each, Alice will trans - mit 16×15× 1000 bits for her sixteen vectors (240000 bits), and Bob will also transmit 240000 bits,  plus 256 15-bit scalars (about 4000 bits).
 8.3.5.1.2.
Sending Multiple Bits for each Z  Assuming the scheme is carefully specified so that the difference between ZA and ZB is sufficiently  small relative to the modulus, another optimization is possible.
Instead of Bob partitioning each ZB  into two ranges—one range for 0 and the other for 1—the set of numbers can be partitioned into  more ranges, say sixteen, in order to send four bits at a time ( 0000 ,0001 ,…,1111 ).
So, instead of  Bob adding approximately q/2 to send a 1, he adds approximately iq/16 to send the 4-bit value i. A  specific scheme will carefully choose the parameters so that it is possible to partition the values into  the specified number of pieces. (
See Homework Problem 14.)
 8.3.5.1.3.
 Ring LWE  As with NTRU, LWE can be further optimized by using structured matrices.
Note that a circulant  matrix as described in §8.3.2 is not the only type of structured matrix.
A variant that is popular for  ring LWE is one in which the polynomial representing row k is derived from the polynomial repre - senting row k−1 by multiplying the polynomial in row k−1 by x and reducing mod xn+1 (instead of  xn−1 as before).
The result of reducing mod xn+1 is that the element that gets shifted off the right is  brought into the leftmost position, but with the opposite sign.
In this form of structured matrix, n is  usually a power of 2 (instead of n being a prime as before).
As with circulant matrices, arithmetic  can be done using only the top row of the structured matrix, treated as a polynomial.
An example of  this kind of matrix is  ⎡ 17 33 5 −12  ⎢ 12⎢ 17 33 5   ⎢ − 5 12 17 33   ⎢− 33⎣ − 5 12 17   Alice chooses a single pair of degree n−1 polynomials with small coefficients, r and eA. Bob  chooses a single pair of degree n−1 polynomials with small coefficients, s and eB. What was the  matrix A for unstructured LWE is a degree n−1 polynomial A for ring LWE.
As before, to allow A  to be specified with fewer bits, the coefficients of A can be generated from a 256-bit seed.
8.3.5.1 228 P OST-QUANTUM CRYPTOGRAP HY  Alice computes rA, resulting in a degree n−1 polynomial, and adds eA, resulting in a degree  n−1 polynomial).
Likewise, Bob computes the degree n−1 polynomial As+eB. Note that since poly - nomial multiplication is commutative, it doesn’t matter whether Alice computes rA+eA or Ar+eA.  Likewise, Bob could compute either sA +eB or As+eB. Polynomials are reduced to a degree n−1  polynomial (reduced mod xn +1 or xn− 1, depending on the type of structured matrix used), and  arithmetic on the coefficients is done mod q.  Alice takes the n−1 degree polynomial she received from Bob and multiplies that by r. Bob  takes the polynomial he received from Alice and multiplies that by s (again reducing the polyno - mial to a degree n −1 polynomial and reducing the coefficients mod q.)  Now Alice and Bob each have computed n−1 degree polynomials with coefficients between 0  and q−1, that will both be approximately rAs.
 Each of the n coefficients in that polynomial serves the purpose of the Zs in the previous sec - tion—a value that Alice and Bob will agree on approximately, but an eavesdropper will not be able  to compute.
If it weren’t for the error polynomials ( eA and eB), Alice and Bob would have com - puted the same polynomial ( rAs), but because of the error polynomials, the n coefficients of rAs  will be close to what each of Alice and Bob will have computed.
Therefore, Bob can use these coef - ficients to send up to n secret bits to Alice, again, by adding approximately 0 to a coefficient to send  a 0 and adding approximately q/2 to send a 1.
 8.3.5.1.4.
 Structured LWE  Ring LWE drastically reduces the keysize and ciphertext size as compared to unstructured LWE,  but there is a broader class of similar systems (including but not limited to ring LWE) called struc - tured LWE.
An example of a structured-LWE cryptosystem that is not a ring-LWE cryptosystem is  the NIST submission CRYSTALS-Kyber*, which uses a structured variant of LWE called module  LWE.
In Kyber, the matrix A is structured as a k× k array of 256 × 256 submatrices, where k is 2, 3,  or 4, depending on the security level.
For instance, for NIST security level 1, k is 2 and so A is  512× 512.
Each of the submatrices is structured so that arithmetic on the submatrices can be done  with polynomials, as with ring LWE, and some cryptographers think Kyber is a little less risky  because the matrix A is a little less structured than with ring LWE.
For example, when k is 4, there  are 4 times as many independent coefficients in A as there would be for a same-sized A using ring  LWE.
 *CRYSTALS is an acronym for CRYptographic SuiTe for ALgebraic LatticeS.
8.3.5.2 CODE-BASED SCHEMES 229  8.3.5.2 LWE-based NIST Submissions  LWE based on structured matrices is generally believed to be secure and is much more efficient  than unstructured LWE.
A NIST submission based on structured LWE is CRYSTALS-Kyber.
A  submission based on unstructured matrices is called Frodo.*
 In Frodo-640 (the Frodo variant for NIST level 1 security, n is 640, elements are mod 215 (so  fifteen bits long), Alice and Bob each transmit eight vectors, and two bits are sent each time.
So, to  transmit the vectors, Alice and Bob each need to transmit 8 ×15×640 bits, or approximately 10000  octets.
 In Frodo-1344 [the variant to match AES-256 security (NIST level 5)], n=1344, elements are  mod 216 (so 16 bits long), Alice and Bob each transmit eight vectors, and four bits are sent at a  time.
So to transmit the vectors, Alice and Bob each need to transmit 8 ×16× 1344 bits, or approxi - mately 22000 octets.
 In Kyber-512 (the CRYSTALS-Kyber variant for NIST level 1 security), Alice and Bob each  transmit a 2-dimensional vector of degree-256 polynomials whose coefficients are modulo 3329 (so  12 bits).
Alice and Bob each need to transmit a little more than 12 ×512 bits, or approximately 800  octets.
In Kyber-1024 (the CRYSTALS-Kyber variant for NIST level 5 security), Alice and Bob  each transmit a 4-dimensional vector of degree-256 polynomials whose coefficients are modulo  3329.
Alice and Bob each need to transmit a little more than 12 ×1024 bits, or approximately 1600  octets.
 8.4 CODE-BASED SCHEMES  Code-based cryptographic schemes use error-correcting codes in their construction.
 Error-correcting codes were originally invented to solve the problem of communication media or  storage systems that might corrupt a small portion of the bits. (
See Figure 8-5.)
An error-correcting  code expands a k-bit string into an n-bit string known as a codeword .
There is also an inverse func - tion that can take an n-bit codeword with up to t errors (flipped bits), find which bits were flipped,  and recover the original k-bit string.
The greater the number of redundant bits added to a string, the  more errors the error-correcting code can correct.
If too many errors occur, the error-correcting  code may report that the error is unrecoverable, or it may recover incorrect information.
 When used as the basis of a cryptographic scheme, the error-correcting code is not used to  correct accidentally introduced errors.
Instead, errors are deliberately introduced, and these deliber - ately introduced errors are the secret being transmitted.
 *The name is a Lord of the Rings reference, where Frodo lets go of the ring.
230 P OST-QUANTUM CRYPTOGRAP HY 8.4.1  k-bit string n-bit codeword  recovered k-bit string n-bit codeword with up to t errors  Figure 8-5.
Error-Correcting Code  Code-based schemes are very similar to lattice-based schemes.
The scheme we describe  (based on the MDPC-McEliece scheme of Misoczki et al [MISO13], which subsequently was sub - mitted to NIST as BIKE) is very similar to the lattice-based scheme NTRU.
In NTRU, the secret is  the offset from a lattice point.
In the scheme we describe, the secret is the offset from a codeword.
 8.4.1 Non-cryptographic Error-correcting Codes  First, to build intuition, we’ll describe a non-cryptographic error-correcting code.
 The codeword might be in a form that consists of the original k-bit string with n−k check bits  appended.
That is known as systematic form (see Figure 8-6).
If the algorithm chosen for convert - ing a k-bit string into an n-bit codeword does not generate systematic form codewords, a codeword  will look (to a human) like an n-bit quantity not obviously related to the original string.
 k-bit string n-bit codeword  k-bit string n−k check bits  Figure 8-6.
Systematic Form  The simple non-cryptographic error-correcting scheme we describe in this section is inspired  by moderate density parity-check (MDPC) schemes.
Later, we’ll show how to turn our  non-cryptographic scheme back into the public key encryption scheme that inspired it.
 The algorithms for creating codewords and for correcting errors (in this non-cryptographic  scheme) are both public.
The codewords generated are in systematic form.
The steps in the algo - rithm, each of which will be described below, are:  • Invention step: Define an error-correcting code.
 • Codeword creation step: Input a k-bit string M and output an n-bit codeword Y in systematic  form.
The codeword Y is M |C, where M is the k-bit string, and C is n−k check bits.
 • Misfortune step: Flip up to t bits of the codeword Y, producing what we’ll refer to as the  “mangled codeword” Y′. Think of Y′ as equal to Y⊕E, where E is an n-bit error vector
8.4.1.1 CODE-BASED SCHEMES 231  whose 1s flip the bits in Y. Since the error vector might have 1s in both the M and the C por- tions of Y, the mangled codeword Y′ will be M′|C′.  • Diagnosis step: Calculate the error vector E.  Note that in the original use for which error-correcting codes were designed, the goal is to recover  M. However, for the cryptographic scheme we will be describing, the value of M is unimportant.
 We can think of M as a random seed for calculating a codeword in Alice’s error-correcting code.
 The actual message Bob will want to send to Alice is the error vector that Bob will add to the code - word.
 8.4.1.1 Invention Step  To create an error-correcting code, Alice creates a matrix G, known as the generator matrix .
G  will be a binary matrix (entries are 0 or 1), sparse (only about 1% of the entries being 1), and of  size k×n.
We treat a binary string as a binary row vector and vice versa.
Multiplying a k-bit string  by G will produce an n-bit codeword.
Note that G will have thousands of rows and columns.
 Arithmetic is mod 2.
This means that ⊕, +, and − are all equivalent.
So, for instance, if you  added a matrix X to itself, i.e., X +X, you’d get 0.
Likewise X−X=0.
 We want G to produce systematic form.
In order to produce systematic form codewords, the  left side of G will be the identity matrix I (where the diagonal entries are 1 and the rest 0).
 The rest of G will be a randomly generated sparse ( n−k) × k matrix that we’ll call Q (see Fig- ure 8-7).
We apologize for the figure.
Q should be sparse (only 1% of the entries should be 1), but  that would require Q in the figure to have hundreds of columns, and that would be unreadable, so  please imagine a much larger Q, one with hundreds of columns and with only about 1% of the  entries being 1.
 8.4.1.2 Codeword Creation Step  This is easy.
We take the k-bit string M and multiply it by matrix G to get a codeword Y. Because  the left part of G is the identity matrix, MG will be in systematic form, so Y will be M |C, where C  is the n−k check bits.
 8.4.1.3 Misfortune Step  Select an n-bit error vector E with up to t 1s.
Compute mangled codeword Y′=Y⊕E. Since E can  mangle bits in both the first k bits of Y (which is the string M) and the n−k check bits (which is the  string C), Y′=M′|C′. (See Figure 8-8.)
8.4.1.4 232 P OST-QUANTUM CRYPTOGRAP HY  k×n matrix G  1 0 0 0 0 … 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0  0 1 0 0 0 … 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0  … 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 … … 0 0 0 0 0 0 1  … 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0  0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0  … …… … . . . … …… …… …
 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0  … 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0  … 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  … 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0  … 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0  k×k Identity k×(n-k) matrix Q  Figure 8-7.
Generator Matrix for Systematic Form  k-bit string M n−k check bits C  n-bit codeword Y  n-bit error vector E ⊕with up to t 1s  mangled codeword Y′  M′ C′  Figure 8-8.
Generating Mangled Codeword  8.4.1.4 Diagnosis Step  Alice receives codeword Y′ with possibly some errors (if there were no errors, Y′=Y).
She wants to  calculate E.  Alice, knowing she’s receiving something in systematic form, starts by assuming that the first  k bits of the codeword ( M ′) is the string M. She uses G to compute the codeword for M ′ and gets  check bits C″. Then she computes C′⊕ C″, which is known as a syndrome , and is of length n−k.
If  there were no errors in the codeword, then C, C′, and C″ would all be equal, so the syndrome  would be 0.
8.4.1.4 CODE-BASED SCHEMES 233  If the syndrome is not 0, then Alice will use the sparse matrix Q to diagnose where the errors  are.
This is not a simple calculation, and, indeed, would be computationally infeasible if Q were not  sparse.
 Consider what happens to the syndrome if a single bit (call it bit i) of the codeword is flipped  in the mangling step.
Before any bits are flipped, the syndrome will be 0.
 • If the flipped bit is in the last n−k bits of the codeword (the check bits), this will only flip a  single bit in the syndrome. (
See Homework Problem 17.)
 • If the flipped bit is in the first k bits of the codeword, this will cause all the columns where  there are 1s in the ith row of Q to be flipped in the syndrome. (
See Homework Problem 18.)
 Alice will try to compute the error vector E with the fewest nonzero bits that results in the discov - ered syndrome.
The syndrome will be the sum of all the rows of Q corresponding to the error bits in  M, plus the individual bits corresponding to error bits in C. If Q is sparse enough, we can usually  derive E from the syndrome by a process of progressive approximation, looking for rows of Q with  a lot of bits in common with the syndrome bits.
 First, Alice attempts to find the 1s in the first k bits of E. She chooses the row of Q that, when  ⊕’d with the syndrome, makes the biggest reduction in the number of 1s in the syndrome.
Some of  the 1s in the chosen row of Q might not have corresponding 1s in the syndrome.
This will cause  those bits of the syndrome to become 1s, but if this is indeed the best row of Q, the total number of  1s in the syndrome will decrease.
Then Alice tries another row of Q. Each row of Q represents a  corresponding 1 in the first k bits of E.  Once Alice has guessed, say, s bits in the first k bits of E so that there are less than t−s  remaining 1s in the syndrome, she can assume that the remainder of the 1s are due to 1s in the final  n−k bits of E.  It is possible for Alice’s algorithm to incorrectly choose a bit to flip and will find that unflip - ping that bit later will be the choice that most reduces the number of 1s in the syndrome.
It is possi - ble that the process will loop and never terminate.
And it is possible that she will get a wrong  answer, i.e., a different codeword happens to be within t bits of what she received from Bob, so  she’ll compute a different error vector than Bob had chosen.
 The fact that the algorithm might not terminate, or might get the wrong answer, could seem  somewhat disturbing.
However, with the parameter values in the proposed code-based post-quan - tum schemes, the probability of an issue is estimated to be very small ( e.g., for NIST security level  1, less than 1 in 2128).
 Not all error correction schemes are probabilistic.
The one we are describing is based on  MDPC codes and is probabilistic.
However, there are other types of error correction codes.
The  original McEliece cryptosystem, published in 1978, was based on Goppa codes, and the cryptosys - tem called classic McEliece also uses Goppa codes.
Goppa codes are better behaved (always termi - nating and never having multiple potential codewords within t bits).
They also correct more errors  for a given message and codeword size than MDPC codes.
However, since the math for
234 P OST-QUANTUM CRYPTOGRAP HY 8.4.2  understanding Goppa codes is somewhat harder than the math for understanding MDPC codes, we  will not be explaining Goppa codes in this book.
Additionally, the important key-size reduction  optimization, circulant matrices, works well for MDPC codes, but not nearly as well for Goppa  codes*, so the MDPC scheme we describe has significant advantages besides being easier to  explain.
 8.4.2 The Parity-Check Matrix  The usual implementation of an MDPC scheme involves a second matrix, known as a parity-check  matrix , which we’ll call H. Using the parity-check matrix H is a more elegant way to get the syn - drome than the procedure explained in §8.4.1.4 Diagnosis Step .
With the parity-check matrix H,  Alice just needs to multiply the (possibly mangled) codeword Y′ by H and she’ll directly get the  syndrome.
She will still have to do the same work to figure out which bits were flipped.
 The matrices G and H are related.
Indeed, in the scheme we’ve been explaining so far, the  only difference between G and H is where we glue the identity matrix onto our matrix Q. In G, it’s  on the left.
In H, it’s on the bottom.
Generator and parity-check matrices that are constructed by  gluing an identity matrix onto another matrix this way are said to be in systematic form . (
See Fig- ure 8-9.)
 In our cryptographic scheme that we will explain shortly, Alice will use a parity-check matrix  that is not in systematic form to generate a syndrome.
So, we will give a more general definition of  what makes something a parity-check matrix and how it is related to the generator matrix: G will  always be a k×n matrix (so that when k-bit string M is multiplied by G we’ll get an n-bit codeword  Y).
H will always be an n× (n−k) matrix with linearly independent columns.
H’s size is n× (n−k) so  that when an n-bit, possibly mangled, codeword is multiplied by H, we’ll get an ( n−k)-bit syn - drome.
The crucial relation between G and H is that their product is a zero matrix, i.e., GH = 0.
 This guarantees that the syndrome for any unaltered codeword is 0.
It also guarantees that for a  mangled codeword Y′, the syndrome Y′H is the same as EH, where E is the error vector. (
See  Homework Problem 19.)
 A scheme that produces a codeword by multiplying a message by a generator matrix is known  as a linear code .
All linear codes have parity-check matrices. (
Indeed, any linear code typically has  a large number of different parity-check matrices.)
But, what makes an MDPC code special is that  it has at least one sparse parity-check matrix.
The fact that the parity-check matrix H is sparse  makes it computationally feasible to decode a mangled codeword Y′.  *It may be possible to get some keysize reduction by making parts of the public key of a Goppa-code-based cryptosystem  circulant, but not nearly as much as with moderate density parity-check codes.
The first-round NIST submission BIG - QUAKE attempts to do this, resulting in a public keysize of about 2 ×105 bits instead of about 2 ×10 6.
For comparison,  MDPC codes using circulant matrices typically achieve keysizes on the order of 104 bits for the same security level.
Previ - ous attempts to get smaller keysizes with Goppa codes have proved insecure.
8.4.3 CODE-BASED SCHEMES 235  k ×n generator matrix G n ×(n−k) parity-check matrix H  k × k Identity k ×(n−k) matrix Q k × (n−k) matrix Q  1 0 0 … 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1  … 0 1 0 0 0 0 1 1 0 1 … … 0 1 1 1 1 1 0 1 … … 0 1 1 1  0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0  … … . . . ……… … … …
 0 0 0 1 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0  … 0 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 1  0 0 0 … 0 0 1 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1  1 0 0 0 … 0 0 0 0  0 1 0 0 … 0 0 0 0  0 0 1 0 … 0 0 0 0  0 0 0 1 0 0 0 0  (n−k) × (n−k) Identity ………. . . ………
 0 0 0 0 1 0 0 0  0 0 0 0 0 1 0 0 …  … 0 0 0 0 0 0 1 0  … 0 0 0 0 0 0 0 1  Figure 8 -9.
 Generator Matrix and Parity-check Matrix for Systematic Form  How can a generator matrix G have lots of different parity-check matrices?
If H is a  parity-check matrix for G, and if R is any invertible square matrix with the same number of col - umns as H, then HR will also be a parity-check matrix for G, because YH= 0 iff Y(HR)= 0. (
See  Homework Problem 20.)
 8.4.3 Cryptographic Public Key Code-based Scheme  So far, we’ve described how Alice can create an MDPC code, but the scheme we described so far  would not work as a cryptographic scheme, because anyone that could create a codeword would  also be able to find errors.
To create a public key scheme, we need to modify our scheme so that  Bob can produce codewords (using Alice’s public key), but only Alice (using her private key) can  find codewords that differ from mangled codewords by fewer than t bits.
 Alice’s public key will be a generator matrix G. To establish a shared secret with Alice, Bob  chooses an error vector E with at most t 1s, computes a codeword, mangles that codeword by ⊕ing  with E, and the secret that Alice and Bob will share is a hash of E, e.g., SHA-256.
8.4.3.1 236 P OST-QUANTUM CRYPTOGRAP HY  To create her key pair, Alice will start by creating a sparse parity-check matrix H, which will  be her private key.
Then she will produce a generator matrix G that is compatible with H (i.e.,  GH=0), but where G is not sparse.
This G will be her public key.
 Alice’s sparse parity-check matrix H (her private key) will not be in systematic form.
How - ever, her public key, the generator matrix G, will be in systematic form.
Having G be in systematic  form allows Alice’s public key to be smaller (since she doesn’t need to send the k×k identity  matrix), and it also allows a very cute optimization due to Neiderreiter (described in the next sec - tion), which reduces the size of the ciphertext.
We’ll describe this optimization before explaining in  detail how to compute G from H.  8.4.3.1 Neiderreiter Optimization  Remember that Bob is going to choose an error vector E with at most t 1s, and the secret Alice and  Bob will share is hash( E).
Niederreiter observed in his paper [NIED86] that if Bob uses the first k  bits of E as the string M, and if the generator matrix produces systematic form codewords, then  when E is ⊕’d with the resulting codeword ( M |C), the first k bits of the mangled codeword will be  0.
Therefore, if this optimization is built into the scheme (everyone uses the first k bits of E as the  string M), Alice can pretend that the n−k bits of ciphertext she receives from Bob was actually an  n-bit ciphertext with the first k bits all zero.
Since typically k is about half n, this means that the size  of what Bob needs to transmit to Alice is half as big. (
See Figure 8-10.)
 n-bit error vector E  with up to t 1s  first k bits of E  codeword for  first k bits of E  mangled codeword multiply by G  ⊕  M C  0  M′ C′  Figure 8-10.
Niederreiter Optimization  We will build Niederreiter’s optimization into the scheme we are describing.
 8.4.3.2 Generating a Public Key Pair  Alice creates a randomly generated sparse n×(n−k) matrix H, which will be her private key.
Let’s  break H into two parts (see Figure 8-11):
8.4.3.2 CODE-BASED SCHEMES 237  •  • Matrix A of size k×(n−k)  Matrix B of size ( n−k)×(n−k)  0 0 n ×(n−k) matrix H  0 0 1 0 0 0 0 0  0 0 0 0 0  k×(n−k) matrix A 0 0 0 0 0 … … … 0 0 0 0 0  0 0 0 0 1  … …  0 0 0 0 0 0 0 0 1 0  0 0 1 0 0 0 0 0 0 0  0 0 0 1 0 0 0 0 0 0  0 0 0 0 0 0 0 0 0 0  0 0 0 0 0 0 1 0 0 0  … …(n−k)× (n−k) matrix B  0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0  0 0 0 0 0 0 0 0 0 0  0 0 0 1 0 0 0 1 0 0  0 1 0 0 0 0 0 0 0 0  Figure 8-11.
Sparse Parity-Check Matrix H  Alice will now obfuscate H, meaning that she will turn it into a parity-check matrix for the same  code that will not be sparse (and will be in systematic form).
 8.4.3.2.1.
Calculating the Public Key  −1 −1Alice calculates B .
By definition of inverse, BB =I, the identity matrix.
She multiplies her pri - vate key matrix H by B−1 and gets the result in Figure 8 -12.
 Not only does multiplying by B −1 turn the top half into a dense matrix, and therefore not use - ful for decoding, but the advantage of turning the bottom half into an identity matrix is that it will  be really easy to create the systematic form generator matrix G by using the top half, AB−1, as what  we called the matrix Q in Figure 8-9.
 −1Her public key, therefore, is just the matrix AB .
The generator matrix G is created by glu - ing a k×k identity matrix to the left (see Figure 8-13).
Her private key is H. The reader may check  that G and H have the required relationship of a generator and a parity-check matrix, GH = 0. (
See  Homework Problem 21.)
8.4.3.2 238 P OST-QUANTUM CRYPTOGRAP HY  n×(n−k) matrix HB−1  0 0 0 0 1 1 0 1 0 1  1 1 0 1 1 …1 0 1 1 1  0 0 0 1 0 0 0 0 1 0  … … k×(n−k) matrix AB−1  … 1 0 0 0 0 0 0 0 0 0  … 0 1 0 0 0 0 0 0 0 0  … 0 0 1 0 0 0 0 0 0 0  … 0 0 0 1 0 0 0 0 0 0  0 0 0 0 1 0 0 0 0 0  (n−k)×(n−k) Identity … …… … . . . … …… …
 0 0 0 0 0 1 0 0 0 0  0 0 0 0 0 0 1 0 0 0 …  … 0 0 0 0 0 0 0 1 0 0  … 0 0 0 0 0 0 0 0 1 0  … 0 0 0 0 0 0 0 0 0 1  Figure 8-12.
Obfuscated H  k×n matrix G  1 0 0 0 0 0 0 0 0 0 … 0 0 0 0 1 0 1 1 1 1 0 1 0 1  0 1 0 0 0 0 0 0 0 0 … 1 1 0 1 1 1 0 0 1 1 0 1 1 1  … 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1 … … 1 0 0 0 0 1 0  … 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 0 0 0 1  0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1  … …… … . . . … …… …… …
 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 1  … 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 0 0 0 1  … 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 1 0 1 0 1 1 1 1 0  0 0 0 0 0 … 0 0 0 1 0 1 1 0 1 0 0 0 1 1 0 0 1 1 1  0 0 0 0 0 … 0 0 0 0 1 1 0 0 0 1 1 1 0 1 0 0 1 0 1  k×k Identity k×(n-k) matrix AB −1 (Alice’s public key)  Figure 8-13.
Alice’s Generator Matrix
8.4.3.3 MULTIVARI ATE CRYPTOGRAPHY 239  Here are the steps in the protocol, using public key G and private key H to establish the  shared secret hash( E):  1.
Alice sends Bob her public key G. (Since the first k columns of G are an identity matrix,  Alice only needs to send the last n−k columns.)
 2.
Bob chooses an n-bit error vector E, with at most t 1s.
 3.
Bob multiplies the first k bits of E by G to get n-bit codeword Y. Then he ⊕s E with Y to get  Y′. (Since this construction forces the first k bits of Y′ to be zero, Bob only needs to send the  final n−k bits of Y′.)  4.
Alice then takes Y′ (the quantity she received from Bob, with k bits of 0 inserted at the begin - ning) and multiplies it by H to get the syndrome. (
Since the first k bits of Y′ are zero, Alice  can get the same syndrome by multiplying the last n−k bits of Y′, i.e., what she received from  Bob, by B.) (See Homework Problem 22.)
 5.
She then uses the sparse H to calculate E.  8.4.3.3 Using Circulant Matrices  The scheme we described can easily use circulant matrices.
Alice creates her sparse parity-check  matrix (see Figure 8-11) as two sparse circulant matrices A and B. Since B is circulant, B−1 will  −1also be circulant, as will AB −1, her public key.
So her public key is just the top row of AB .
 The NIST submission that is closest to what we’ve described is BIKE.
It uses both circulant  matrices and the Niederreiter optimizations.
Its k is half n. Therefore, the public key is of size k, and  for NIST security levels 1, 3, and 5, k is 12323, 24659, and 40973 bits, respectively.
 8.5 MULTIVARIA TE CRYP TOGRA PHY  Multivariate cryptosystems are based on the difficulty of solving systems of nonlinear (usually qua - dratic) equations over many variables.
Linear equations are easy to solve, but nonlinear equations  are not.
 In a linear (degree 1) equation, each term is a constant, or the product of a constant and a sin- gle variable.
For example, x+15y−2z+3q =15.
 In contrast, in a nonlinear equation, variables may be multiplied by themselves or each other.
 For example, a degree 3 equation might have terms such as 4 x 3, 19xyz, 3xy 2.
 Multivariate public key signature schemes ( e.g., Unbalanced Oil and Vinegar (UOV) and  Rainbow [DING05]) use degree 2 (quadratic) equations with more than a hundred variables.
UOV  and Rainbow have very large public keys, but they have fast signing and verification and small
240 P OST-QUANTUM CRYPTOGRAP HY 8.5.1  signatures, and so it seems like they could be useful for applications where the public key doesn’t  have to be downloaded every time a signature needs to be verified ( e.g., software or firmware  updates).
 What about multivariate encryption schemes?
There are some proposed schemes, but of the  ones that have not been broken, their performance characteristics do not seem promising compared  to encryption schemes based on codes or lattices.
 We will first describe the intuition behind multivariate signature schemes in general.
Then we  will explain one of the oldest, and arguably the simplest, multivariate signature scheme, UOV.
 8.5.1 Solving Linear Equations  It is easy to solve a set of n (linearly independent) linear equations in n variables.
For example, con - sider these two linear equations in two variables:  3x+2y=19  4x+y=22  Use the second equation to solve for y in terms of x, getting y=22− 4x.
Then substitute 22 −4x for y  in the first equation to get 3 x+2(22−4x)=19.
Simplify that to get 44 −19=5 x, then divide by 5 on  each side to get x=5, and substitute 5 for x in the equation y=22−4x to get y=2.
 If the equations are linearly independent and we have as many equations as variables, we can  solve for all the variables.
 8.5.2 Quadratic Polynomials  A quadratic polynomial over two variables might look like x 2+3xy+2y 2+3y+4.
A quadratic equa - tion sets the polynomial to some value, so a quadratic equation might be x 2+3xy+2y 2+3y+4=17.
 In a quadratic polynomial (or quadratic equation), each term has at most two variables multi - plied together (for instance, x 2 is x multiplied by itself, and 3 xy has x and y multiplied together).
 A quadratic polynomial over the four variables x, y, z, q might be 5 xq+7z 2+4q 2+5xz+7y 2+3y+2x.
 It becomes a quadratic equation in four variables if we specify a value, e.g.,  5xq+7z 2+4q 2+5xz+7y 2+3y+2x=85.
 8.5.3 Polynomial Systems  A polynomial system is a set of polynomials over some number of variables.
It is characterized by  its degree (the maximum number of variables that are multiplied together in any term), the number
8.5.4 MULTIVARI ATE CRYPTOGRAPHY 241  of variables, and the number of polynomials.
So for instance, the following polynomial system has  degree = 2 (it’s quadratic), number of variables = 2, and number of polynomials = 3.
 x 2+3xy+2y 2+3y+4  xy+y 2+x+2y  5x 2+3y 2+7xy+2x  As with other algorithms used in cryptography, in multivariate schemes, the arithmetic is typically  done over a small finite field, e.g., GF(64), the 6-bit finite field.
See §M.7.2 Finite Fields .
 Solving a set of polynomial equations means finding values of the variables that satisfy all  the equations.
In general, this is very difficult.
The best-known techniques are not much better than  using brute force to choose values of the variables.
With typical parameters for a multivariate signa - ture scheme ( e.g., more than 100 variables and 50 equations), this is computationally infeasible.
 This is wonderful for us, because it means it can be used to create a public key cryptosystem.
 8.5.4 Multivariate Signature Systems  Alice’s public key is the set of coefficients of the terms in a set of polynomials.
Her private key in  this sort of cryptography is some sort of trapdoor function that enables her to efficiently solve a set  of equations based on the polynomials in her public key.
 In a given scheme, the size and representation of the finite field, the number of variables, the  number of polynomials, the maximum degree of the equations, and so on, are parameters.
For read - ability, though, we’ll choose specific numbers in our description.
 In a general quadratic polynomial with n variables, there are approximately n 2/2 terms—  n(n−1)/2 terms with two different variables, n terms with one squared variable, n linear terms, and  one constant term, for a total of n 2/2+3 n/2+1 terms.
With 150 variables, that’s 11476 terms per  polynomial.
For 6-bit coefficients, each polynomial contributes about 69 thousand bits to the public  key.
So with 50 polynomials, the public key is about 3.45 million bits.
 8.5.4.1 Multivariate Public Key Signatures  Numbers of variables, size of coefficients, and size of hash are all parameters of a specific scheme.
 We will pick values that are easy to visualize, but they are just for explanation.
 Suppose Alice’s public key consists of 50 quadratic polynomials with 150 variables, using  6-bit arithmetic and a 300-bit hash function.
To sign a message M, Alice hashes M to get a value H.  Alice will partition the 300-bit H into fifty 6-bit chunks, say H1,H2,…,H50.
She’ll create 50 equa - tions by setting each polynomial to the corresponding chunk of H.  Alice’s signature consists of values of the 150 variables that satisfy those 50 equations.
 To verify a signature on M, the verifier computes the 300-bit hash of M, sets each of the poly - nomials in Alice’s public key to the corresponding chunk of the hash of M, plugs the specified val -
8.5.4.1 242 P OST-QUANTUM CRYPTOGRAP HY  ues of the variables into the equations, and verifies that each equation is satisfied.
If there are 150  variables, each requiring six bits to specify, the signature consists of 900 bits.
 This small signature size is a very nice feature of multivariate signature schemes, although  they unfortunately do have very large public keys.
 8.5.4.1.1.
 Creating a (Public, Private) Key Pair  To create a multivariate key pair, one needs to produce a system of polynomials with a trapdoor that  allows someone with knowledge of the secret to solve equations based on these polynomials but is  infeasible to solve without the secret.
 The simplest apparently secure way of creating a trapdoor is a scheme known as unbalanced  oil and vinegar [KIPN99].
To create a public key, the oil and vinegar approach starts with, say, 150  variables, and partitions the variables into two categories— oil and vinegar .
There will be more vin - egar variables than oil variables.
Let’s use 50 oil variables and a 100 vinegar variables.
Alice creates  what we’ll call constrained quadratic polynomials for her private key, by randomly choosing  coefficients for all possible terms of degree two or less, with the restriction that oil variables are  never multiplied together—any term with an oil variable consists of that variable alone, or that vari - able times a vinegar variable.
Vinegar variables, however, can be squared, multiplied by each other,  or multiplied by an oil variable.*
 Suppose we have oil variables o1 and o2 and vinegar variables v1,v2,…,v5.
Then this is an  example of a constrained quadratic: 3 o2v5+5v2v3− 7+v2+2o1+4v52.
Examples of terms that are not  2 2 2allowed: o1o2, o1, o1v1, v1v4.
 As we’ll see, the purpose of the restriction (that oil variables not be multiplied by each other,  or squared), is that with constrained quadratic equations, if values are chosen for the vinegar vari - ables, it will be easy to solve for the oil variables, because the equations at that point will be linear  in the oil variables.
But it wouldn’t be secure if the constrained quadratic polynomials were Alice’s  public key, because then anyone would be able to forge Alice’s signature.
Therefore, the polynomi - als in her private key have to be transformed so that the polynomials in the public key will be qua - dratic (not constrained quadratic), and it will not be computationally feasible (without the private  key) to transform the public polynomials back into a form where it is possible to tell which are the  oil variables.
 In our example, we are using the parameter values of 50 oil variables, 50 polynomials, 100  vinegar values, and a hash function of 300 bits.
To create her private key, Alice randomly selects 50  constrained quadratic polynomials with 50 oil and 100 vinegar variables.
Knowledge of this set of  polynomials will be part of her private key.
Let’s call these variables u1,u2,…,u150 (the first 50 of  *Note that whoever came up with this terminology seems to think that vinegars mix with each other, oils mix with any vine - gar, but oils do not mix with each other.
We admire their cryptographic skills (this scheme is very clever), but we wouldn’t  trust them in our kitchen.
8.5.4.1 MULTIVARI ATE CRYPTOGRAPHY 243  which are oil and the others are vinegar).
As we’ll see, the public key will be a transformation of the  constrained quadratics into a set of unconstrained quadratics, so that only Alice will be able to cre - ate a signature.
 8.5.4.1.2.
 Solving Constrained Quadratics  The first step in creating a signature for message M will be solving for the variables in the con - strained quadratics in her private key.
 • Alice computes hash( M) and partitions the result into 6-bit chunks (which, with a 300-bit  hash, will be 50 chunks).
 • She sets each of the constrained quadratic polynomials in her private key to one of the 6-bit  chunks of the hash( M).
 • She randomly chooses values for the 100 vinegar variables and substitutes those values in the  equations, resulting in 50 linear equations with 50 variables.
 • She solves these linear equations, so that she has values for all the oil variables that satisfy the  equations.
 • She now has values for all the variables in her private key (the 100 vinegar variables for  which she chose random values and the 50 oil variables she solved for).
 However, she has to transform the polynomials and variables from her private key into the obfus - cated polynomials and variables that will be her public key.
 8.5.4.1.3.
 Obfuscating to Create a Public Key  Alice is going to convert the 150 u variables in the constrained quadratic polynomials of her private  key into variables that we’ll call x variables.
The result will be unconstrained quadratic polynomi - als, also in 150 variables.
This is done as follows:  • Alice randomly selects 150 linear equations, each one setting one of the ui to a linear combi - nation of the x variables, e.g., u3=3x1+17x2+x4+…+9x150.
These equations will also be part  of the private key.
 • Alice substitutes the value (in terms of x variables) of each of the u variables in each of the 50  constrained quadratic polynomials in her private key.
The result will be 50 (unconstrained)  quadratic polynomials with variables x1,…,x150.
Those 50 polynomials will be the public  key.
Note that the polynomials in the public key will be quadratic ( i.e., substituting a linear  combination of x variables for any of the u variables will not result in any terms of degree  larger than two).
244 P OST-QUANTUM CRYPTOGRAP HY 8.6  As we will see, Alice will solve equations based on the u variables in her constrained quadratics of  her private key but will need to demonstrate values of the x variables for her signature.
 Alice can solve for each of the x variables in the 150 linear equations that express each u in  terms of x variables.
Now she has 150 linear equations that express each x variable in terms of u  variables.
 Once Alice has solved her constrained quadratics to find values of all the u variables, she uses  these equations to calculate the values of the x variables.
 Alice’s signature consists of the values of the x variables that satisfy the quadratic equations  in her public key.
 8.6 HOMEWO RK  Hash-based signatures  1.
Consider the following variant of the original scheme (no hash tree).
Instead of Alice’s pub - 0 0 1 1 255 255 lic key consisting of 512 quantities h 1, h0, h1, … , h , h , use only a single hash 0, h 0 1  0 1 255 ifor each bit, i.e., h , h , … , h .
If bit i is 0, she reveals pi, and if i is 1, she reveals h .
Why  is this not secure?
 2.
Now we’ll modify the scheme in Homework Problem 1 by using an additional eight values to  256 257 263 sign the sum of the number of 0 bits.
We’ll call these extra eight values h , h , …, h ,  256 257 computed as the hashes of p , p , ….
If the total number of 0s in the signed message is  256 257 120 (decimal), that would be 01111000 binary.
Therefore, Alice will divulge p , h ,  258 259 260 261 262 263 h , h , h , p , p , and p .
Is this scheme secure?
 3.
In §8.2.3 Signing Lots of Messages , we compared having a single tree with at least as many  treelets as messages to be signed, versus reserving one slot in the tree for signing a root of a  new tree, versus having a single tree whose treelets are used solely to sign roots of other  trees.
Suppose we have a 3-level structure, where the primary tree has, say k levels (2k tree- lets), with all the treelets of that primary tree used to sign roots of secondary k-level hash  trees, all of whose treelets are used to sign roots of tertiary k-level hash trees.
The tertiary  k-level hash trees are used to sign messages.
How many messages could be signed with this  scheme?
How big would signatures be?
 i i4.
Suppose Alice is using the scheme where each treelet has 512 children, a pair 〈 h0, h1 〉 for  each of the 256 possible bits.
Suppose Alice signs a message using that treelet.
How many  messages (approximately) would Ivan need to try in order to find a message that he can forge  i iusing that treelet (given that for each i, either p or p1 would have been divulged)?
What if 0
8.6 HOMEWORK 245  Alice accidentally uses the treelet twice for two different messages?
How many hashes would  Ivan need to try?
What about if Alice signed three different messages with that treelet?
 Lattice-based cryptography  5.
Multiply the polynomial 3–12 x+2x 2+5x 4 by x and reduce mod x 5−1.
 6.
Show that the sum of two circulant matrices is a circulant matrix.
 7.
Show that the product of two circulant matrices is a circulant matrix whose polynomial is  equal to the product of the corresponding polynomials of the two matrices, reduced mod  xn−1.
 8.
Show that multiplication of circulant matrices is commutative.
 9.
Show that if a circulant matrix has an inverse, the inverse will be circulant.
Hint 1: Matrix  inverses are unique.
Hint2: Show that the circulant matrix formed from the first row of the  inverse is an inverse.
 10.
For a polynomial f over a field like Z2, we can calculate the inverse of f mod xn−1 by using  the Euclidean algorithm (§2.7.5) to get polynomials a and b such that a ×f + b×(xn−1) = 1,  provided that f is relatively prime to xn−1.
But Z is not a field, so how do we get f −1?
It’s q  actually quite clever.
First, we calculate the inverse mod 2, which we can do since Z2is a  field.
Let a be that inverse; its coefficients are all zeroes and ones, but we will consider the  coefficients mod q. We then iterate ⎡log2 log2 q times: a=a ×(2−f × a) mod xn−1 mod q. Why  does this work?
Hint: What’s special about the coefficients of the error polynomial 1 −f × a  before the first iteration and then after each successive iteration?
 11.
In the LWE cryptosystem (see §8.3.5) would it be insecure if Alice and Bob did not add their  error vectors ( i.e., used 0 as their error vector)?
For simplicity, assume the matrix A is invert - ible.
 12.
Why would it be insecure for Alice to send multiple bits using the same Z?
 13.
Suppose Alice and Bob want to agree on 256 secret bits, but the bandwidth from Alice to Bob  is much greater than the bandwidth from Bob to Alice.
Would the scheme work if Alice trans - mitted 256 vectors, and Bob would send only one vector?
If Bob only sends four vectors, how many vectors would Alice need to send to Bob in order for them to agree on 256 secret bits?
 14.
If Bob sends four bits at a time (see §8.3.5.1.2), how many 〈i,j〉 pairs are required to send 256  secret bits?
Assuming Alice and Bob send equal numbers of vectors, how many vectors does  each need to send?
 Code-based schemes  15.
Multiply the bit vector 11001 by the 5× 5 identity matrix.
What is the result?
 16.
Suppose we modified the generator matrix from Figure 8 -7 so that the identity matrix was on  the right instead of the left.
What would the codeword for string M look like?
246 P OST-QUANTUM CRYPTOGRAP HY 8.6  17.
Suppose the error vector has only a single 1, and it is in the last n−k bits. (
See §8.4.1.4.)
Show  that the syndrome will have only a single 1 bit.
 18.
Suppose the error vector has only a single 1, say bit j, and j is in the first k bits.
Show that the  syndrome will be the jth row of Q. (See §8.4.1.4.)
 19.
Show that for a mangled codeword Y′ = MG+E, the syndrome Y′H is the same as EH, where  E is the error vector.
Hint: GH=0.
 20.
Show that if H is a parity-check matrix for G, and R is an invertible square matrix with the  same number of columns as H, YH=0 iff Y(HR)=0 for any (possibly mangled) codeword Y.  What if R isn’t invertible?
 21.
As described in §8.4.3.2.1 Calculating the Public Key , Alice’s public key is just the matrix  −1AB .
The generator matrix G is created by gluing a k×k identity matrix to the left.
Her pri - vate key is H, where the top half of H is the sparse matrix A, and the bottom half is the sparse  matrix B. Verify that GH=0.
 22.
As described in §8.4.3.2.1 , Alice computes the syndrome by taking the n-bit quantity Y′ (the  quantity she received from Bob, with k bits of 0 inserted at the beginning) and multiplies it by  H. Verify that since the first k bits of Y′ are zero, Alice can get the same syndrome by multi - plying the last n−k bits of Y′, i.e., what she received from Bob, by B.  23.
Suppose n=2k (so in Figure 8-9, Q will be the same size as I).
Show that GH=0.
Now sup - pose k is not half of n (so Q and I will not be the same size).
Will GH in Figure 8-9 still be 0?
 −124.
Assume that n=2k.
What is GH from Figure 8-13 (where G=I|AB , and H is A on top and  B on the bottom)?
What is GHB−1 from Figure 8-13?
 25.
Can Bob, knowing only AB −1 and not the sparse H, be able to tell whether an n-bit quantity is  a codeword?
 26.
Suppose an attacker Eve can send arbitrary ciphertext messages to Alice and be informed by  Alice when there is a decryption failure.
Suppose also that Alice doesn’t change her public  key.
How can an attacker that sees the ciphertext message Y′ that Bob sends to Alice eventu - ally find the secret Bob was sending to Alice?
Assume that Y′ is a mangled codeword with  exactly t errors and that any codeword with t or fewer errors will decrypt properly, while any  mangled codeword with more than t errors will produce a decryption failure.
Minor hint:  Each message Eve sends allows her to recover one bit of the error vector used to produce Y′.  27.
Bob purposely mangles the codeword in places where the error vector has 1s.
What happens  if the communication channel between Alice and Bob introduces more errors?
If, for an n-bit  message, the channel might introduce, say, a hundred bit errors, should Alice and Bob use an  error-correcting code that can find up to t+100 errors?
8.6 HOMEWORK 247  Multivariate cryptography  28.
Suppose variables a, b, c are oil variables, and w, x, y, z are vinegar variables.
Which of the  following terms would be allowable in a constrained quadratic: 5 wz, 12a, 4az, 3a 2, 6aw 2, 2bc,  8w?
 29.
Suppose the parameter set in an oil-vinegar scheme has more oil variables than equations.
 How could Alice solve her constrained quadratics?
For example, suppose there were 80 oil  variables, 80 vinegar variables, and 50 constrained quadratic equations in her private key.
 30.
Suppose there are fewer oil variables than equations.
For example, suppose Alice’s con - strained quadratics had 20 oil variables and 100 vinegar variables, with 50 equations?
Can  Alice solve these constrained quadratics?
 31.
Once the constrained quadratics (in u variables) are translated into x variables, why will the  result still be a quadratic?
9 AUTHENT ICATION  OF PEOPLE  Humans are incapable of storing high-quality cryptographic keys and they  have unacceptable speed and accuracy when performing cryptographic  operations.
They are also large, expensive to maintain, difficult to manage,  and they pollute the environment.
It is astonishing that these devices con - tinue to be manufactured and deployed, but they are sufficiently pervasive  that we must design our systems around their limitations.
 —Radia Perlman  Authentication is the process of reliably verifying the identity of someone (or something).
There  are lots of examples of authentication in human interaction.
People who know you can recognize  you based on your appearance or voice.
A guard might authenticate you by comparing you with the  picture on your badge.
A mail order company might accept as authentication the fact that you know  the number and expiration date on your credit card.
 We sometimes use the names Alice and Bob for machines.
We clarify examples when neces - sary by using the term the human Alice or the user Bob when we are referring to a human.
However,  in this chapter, Alice is always a human, and she is not authenticating to a human, so we will name  what she is authenticating to as Steve to make it clear Steve is not a human, but is a server some - where.
We do not intend to imply that everything named Steve is not a human.
 There are special challenges when humans are involved.
If it were simply two computers  communicating across a network, they are perfectly capable of storing high-quality cryptographic  keys and doing cryptographic authentication.
 There are various scenarios in which humans might authenticate to something:  • A human authenticating to a local resource to unlock a door, a car, or a phone.
 • A human authenticating to a remote resource across the network, using a personal device  (e.g., a laptop, a phone) that will act on the human’s behalf.
The device might store creden - tials for the user, making remote authentication on the user’s behalf more user-friendly.
But a  complication is that the user might want to use several personal devices and have a newly pur - chased device somehow obtain the credentials for the user.
 249
250 A UTHENTI CATION OF PEOPLE  • A user might want to use a public device, such as a hotel lobby computer, or someone else’s  computer.
This device will not have stored credentials for the user, and the user will need to  enter credentials in order to access remote services.
Hopefully the device will forget the cre - dentials after this one use.
Note our use of hopefully .
If the device is not trustworthy, it could  remember the credentials, or send them off to some criminal organization.
 The traditional categories of techniques for authenticating a human are: what you know, what you  have, what you are, and where you are.
Passwords fit into the what-you-know category.
Physical  keys or ATM cards fit into the what-you-have category.
Biometric devices, such as voice recogni - tion or fingerprint scanners, fit into the what-you-are category.
Having a resource only reachable  from a user located at a physically protected location or having resources authenticate a user (or  decide what the user should have access to) based, for example, on the source IP address in received data packets count as where you are .
 Each of these categories has problems.
A secret ( what you know ) can be accidentally over - heard or forgotten.
A physical thing ( what you have ) can be stolen or lost or broken.
A biometric  tends to have false positives and false negatives (especially if some injury changes the biometric,  such as having a bandage on your finger, or a cold that changes your voice), and may require spe - cial scanners.
Therefore, it is more secure to use multifactor authentication , in which the user is  authenticated using mechanisms from multiple categories.
Some companies advertise their systems as being multifactor when they involve multiple passwords, but that is not what most people think  should count as multifactor.
A multifactor authentication system should involve different catego - ries.
For example, a credit card might include a picture or a signature, so in theory it can combine  biometrics with physical access to the card (assuming the person at the store checkout has the time  to stare at the tiny picture or do handwriting analysis).
Another form of multifactor authentication is  having a website send a PIN to the user Alice’s phone, so that someone attempting to authenticate  as Alice would not only need to know her password but would also have to have access to Alice’s  phone.
 Although multifactor authentication is more secure, it tends to make things less convenient  for the user and multiplies reliability issues if any of these systems fail, since with multifactor authentication, all the factors should work.
 In many scenarios in this chapter, a user Alice will be authenticating across the network to a  service S. She will be entering her credentials into some sort of device, such as a tablet, phone, or  laptop.
We will refer to that device as a workstation or device , but the term is intended to include all  the devices Alice might use.
 Also, when Alice’s device is communicating with a service on the web, it will usually be  using an encrypted connection such as TLS (see Chapter 13 SSL/TLS and SSH ).
But there are still  potential security issues, as we will discuss in the examples.
9.1 PASSWOR D-BASED AUTHENTICAT ION 251  9.1 PASSWORD -BASED AUTHEN TICATIO N  It’s not who you know.
It’s what you know.
Steve If authentication of a human is based on something the human can remember, it is usually a pass- word (or other string of characters that a human can remember and type).
 I’m Alice; my password is fiddlesticks .Alice  Unfortunately, even computer-computer authentication is often based on passwords.
Some - times, cryptography is not used because the protocol started out as a human-computer protocol and  was not redesigned when its use got expanded to computer-computer communication.
 There are some cases in which it is surprising that the designers opted for a simple  password-based scheme.
For instance, the first generation of cellular phones transmitted the tele - phone number of the phone and a password (unencrypted) when making a call, and if the password  corresponded to the telephone number, the phone company let the call go through and billed that  telephone number.
The problem was, anyone could eavesdrop on cellular phone transmissions and  clone such a phone, meaning they could make a phone that used the overheard 〈telephone number,  password 〉 pair.
Indeed, it was a problem—criminals did clone phones for stealing phone service  and/or making untraceable calls.
It would have been technologically easy to implement a simple  cryptographic challenge-response protocol in the phone, and, indeed, all newer cell phones do this.
 Note that without a secure connection between Alice and Steve, an eavesdropper will be able  to see the password.
Even with a secure connection, if Trudy impersonates Steve and tricks Alice  into thinking she is talking to Steve, (perhaps because Trudy has obtained a DNS name that looks to  human Alice like “Steve”), Alice will be divulging her password to Trudy, who in our examples, is  always evil.
 9.1.1 Challenge-Response Based on Password  Note that authenticating using a password can be more secure than simply transmitting the pass - word.
Server Steve can send a challenge (a random number), and Alice’s machine can compute a  function of Alice’s password and Steve’s challenge (Protocol 9-1).
This is basically how the CHAP  protocol [RFC 1994] works .
 As in the previous example, if Alice is using TLS, there is no security issue with eavesdrop - pers.
However, if Alice can be tricked into talking to Trudy instead of server Steve, Trudy will not
252 A UTHENTI CATION OF PEOPLE 9.1.2  I’m Alice  a challenge R  f(password , R) AliceSteve  Protocol 9-1.
Challenge-Response Based on Password  directly learn Alice’s password using this protocol, but she could do a dictionary attack. (
See §9.8  Off-Line Password Guessing ).
 9.1.2 Verifying Passwords  Suppose Alice has a single network password, as might be done for accessing resources within the  company in which she works.
We are not discussing single sign-on here, where Alice types her  password once.
Instead, we are assuming Alice will need to separately type her password at each of  the company resources, but it will be the same password for all of them.
 How does a server know that the string Alice typed is her valid password?
There are several  possibilities:  1.
Alice’s authentication information is individually configured into every server Alice will use.
 2.
One location that we’ll call an authentication storage node stores Alice’s information, and  servers retrieve that information when they want to authenticate Alice.
This model is rarely  used today.
 3.
One location that we’ll call an authentication facilitator node stores Alice’s information,  and a server that wants to authenticate Alice sends the information received from Alice to the  authentication facilitator node, which does the authentication and tells the server yes or no.
 An example of this is RADIUS [RFC 2865].
 In cases 2 and 3, it’s important for the server to authenticate the authentication storage or facilitator  node, since if the server were fooled into thinking a bad guy’s node was the authentication storage  or facilitator node, the server could be tricked into believing invalid information, and therefore let  bad guys impersonate valid users.
Or the server might be tricked into forwarding the user’s pass - word (or information that would allow a dictionary attack §9.8) to the fraudulent authentication  node.
 Regardless of where authentication information is stored, it is undesirable to have the data - base consist of unencrypted passwords, because anyone who captured the database could imperson - ate all the users.
Someone could capture the database by breaking into the node with the database or  by stealing a backup.
In the first case (authentication information individually configured into every
9.2 ADDRESS -BASED AUTHENTICAT ION 253  server), capturing a server’s database (of unencrypted passwords) would enable someone to imper - sonate all the users of that server.
Also, if a user had the same password on multiple servers, that  user could then be impersonated at the other servers as well.
In the second and third cases, many  servers would use the one location, and capturing its database would enable someone to imperson - ate the users of all those servers.
There’s a trade-off, though.
It might be difficult to physically pro - tect every server, whereas if all the security information is in one location, it is only necessary to  protect that one location.
 Put all your eggs in one basket, and then watch that basket very carefully.
 —Anonymous  An alternative to directly storing passwords is to store hashes of passwords.
A password  would be verified by hashing it and comparing it with the stored value, or using the hashed pass - word as a shared secret in a challenge-response protocol.
Then if anyone were to read the password  database they could do off-line password-guessing attacks (§9.8), but would not be able to obtain  passwords of users who chose passwords carefully.
With hashed passwords an intruder who can  read the hashed password database can do a password-guessing attack because the intruder will  likely know the hash function (the function itself would not be secret).
 Note that if the hashed password is transmitted to Bob instead of the actual password, or if it  is used as a shared secret in a challenge-response protocol, then the hashed password is known as a  password equivalent .
That means that if Trudy steals Bob’s password database, she can use  Alice’s hashed password to directly impersonate Alice, even if Trudy can’t guess Alice’s actual  password.
However, since Alice’s client software only allows Trudy to input the actual password  (not the hashed password), Trudy will have to modify the client software to bypass the step where it  hashes the user input and uses the result for authentication.
 9.2 ADDR ESS-BASED AUTH ENT ICATION  It’s not what you know.
It’s where you are.
 Address-based authentication does not rely on sending passwords around the network but rather  assumes that the identity of the source can be inferred based on the network address from which  packets arrive.
At one point it was a widely used method of authentication, and it is still sometimes  used today.
For instance, firewalls might allow access based solely on the source address in the IP  header.
254 A UTHENTI CATION OF PEOPLE 9.2.1  In some cases, it might be somewhat safe to rely on address-based authentication, for  instance, if the network is completely isolated, or assuming you trust all the bridges (switches) in a  bridged Ethernet, you might trust that the network protects you from receiving any packets that  originated outside your VLAN.
 9.2.1 Network Address Impersonation  Location, location, location  How hard is it for Trudy to impersonate Alice’s network address?
Generally, it is easy to transmit a packet claiming any address as the source address.
Sometimes it is more difficult.
A router could  have features to make it difficult for someone to claim to be a different network address.
If a router  has a point-to-point link to an endnode, it could refuse to accept a packet if the source address is not  coming from the expected direction.
 It is often more difficult for Trudy to receive messages sent by Steve to Alice’s network  address than for Trudy to send packets using Alice’s address as the source address.
If Trudy  attempts to send packets with Alice’s source address and cannot receive packets from Steve to  Alice, usually the TCP protocol will fail to work for Trudy.
This is because TCP connections start with random sequence numbers.
Trudy would not be able to send TCP acknowledgments to Steve’s  packets, because she would not know the sequence number chosen by Steve.
So, Trudy will need to  both be able to forge Alice’s source address and be able to receive packets from Steve to Alice’s  address.
If Trudy is on the path between Alice and Steve, it is easy, because packets from Steve to  Alice will go through Trudy.
If Trudy is not on the path, she might be able to get a colluding router  on the path between Steve and Alice to hijack packets and forward them to Trudy.
Or Trudy might  be able to compromise the routing protocol to cause packets to go through Trudy.
 Compromising the routing protocol is a common problem with BGP [RFC 4271].
BGP is  very fragile and configuration intensive.
For instance, in 2008, Pakistan was attempting to block  access to YouTube from within Pakistan, so they had their BGP router advertise that it was the best  path to YouTube’s IP address.
Since Internet routing prefers the most specific destination address regardless of distance, and since everyone else was advertising an address block containing You - Tube’s address, all traffic worldwide for YouTube was routed to Pakistan’s BGP router, resulting in accidentally blocking access to YouTube worldwide.
 Mobile IP [RFC 5944] is designed to allow a node X to keep its IP address and move any - where on the Internet.
There will be a server S somewhere on the Internet that will receive traffic  addressed to a block of addresses (including X).
X acquires a temporary address T that is specific to  the location where X is currently residing, and X lets S know that X is currently reachable at  address T. When Steve sends a packet to X, it will be received by S and then tunneled to X with an
9.3 BIOME TRICS 255  outer header with destination= T. However, return traffic from X to Steve will have source= X and  destination= Steve.
If routers really tried to enforce that source address X would need to arrive from  the expected direction, this would not work.
Because enforcing the direction from which an address  is received would break mobile IP, most routers do not enforce source address direction.
 9.3 BIOMETR ICS  Biometric devices authenticate you according to what you are .
They measure your physical charac - teristics and match them against a profile.
Some people imagine biometrics being used for remote  authentication.
Although biometrics are useful for local authentication ( e.g., unlocking a smart - phone or unlocking a door), they are not useful as a “secret” with which to authenticate remotely,  because biometrics cannot be assumed to be secret.
If you want to know someone’s fingerprint, just  offer them a drink of water and get their print off the glass they used.
Remote authentication by  asserting the value of a biometric can be done safely if the measuring device is trustworthy and has  an authenticated connection to the remote server.
Whether local or remote, the measuring device  must be designed and policed somehow to prevent someone tricking it with a rubber finger with  someone else’s fingerprint, or by presenting the user’s severed finger.
 Examples of biometrics include fingerprint scans, iris scans, retinal scans, facial features, or  hand geometry.
DNA would be quite accurate, other than the case of identical twins, though the  technology is not quite available.
Then there are behavior-measuring strategies, such as typing  rhythm, mouse movements, gait, signature, and voice.
I2 don’t quite believe behavior measuring  will work.
Will my 2 gait change if I 2 have a blister, or if I 2’ve sprained my ankle, or if due to hell  freezing over I 2 am wearing high-heeled shoes?
 9.4 CRYPTOGRAP HIC AUTHE NTICA TION PROTOC OLS  It’s not what you know or where you are, it’s  %zPy#bRw Lq(ePAoa&N5nPk9W7Q2EfjaP!yDB$S  Cryptographic authentication protocols can be much more secure than either password-based or  address-based authentication.
The basic idea is that Alice (Alice’s machine) proves Alice’s identity  to server Steve by performing a cryptographic operation on a quantity Steve supplies.
The crypto - graphic operation performed by Alice is based on Alice’s secret.
We talked in Chapter 2
256 A UTHENTI CATION OF PEOPLE 9.5  Introduction to Cryptography about how hashes, secret key cryptography, and public key cryptogra - phy could all be used for authentication.
The remainder of this chapter discusses some of the more  subtle aspects of making these protocols work.
 9.5 WHO IS BEING AUTH ENT ICATED?
 Suppose user Alice is at a workstation and wants to access her files at a file server.
The purpose of  the authentication exchange between the workstation and the file server is to prove Alice’s identity.
 The file server generally does not care which workstation Alice is using.
 There are other times when a machine is acting autonomously.
For instance, directory service  replicas might coordinate updates among themselves and need to prove their identity to each other.
 Sometimes it might be important to authenticate both the user and the machine from which  the user is communicating.
For example, a bank teller might be authorized to do some transactions  but only from the machine at the bank where that teller is employed.
Or Alice might be allowed to  download a movie but only from a device that is trusted (by the content provider) to enforce rules  such as deleting the movie after some period, or never copying the movie.
 A computer can do cryptographic operations on behalf of the human, but it has to be designed  to somehow acquire a credential from the human to prove that the computer is operating on behalf  of that human.
 9.6 PASSWORD S AS CRYPTOGRAP HIC KEYS  Cryptographic keys, especially those for public key cryptography, are specially chosen very large  numbers.
A normal person would not be able to remember such a quantity.
But a person can  remember a password.
It is possible to convert a text string memorizable by a human into a crypto - graphic key.
For instance, to form an AES-128 key, a possible transformation of the user’s password  is to do a cryptographic hash of the password and take 128 bits of the result.
It is much trickier (and  computationally more expensive) to convert a password into something like an RSA private key,  since an RSA key has to be a carefully constructed number.
In theory, the password could be used as seed for a pseudorandom number generator that will deterministically compute an RSA key pair  from that seed.
 Schemes based on this idea (direct conversion of the user’s password to a public key pair) are  not used because they perform poorly and because knowledge of the public key alone gives enough
9.7 ON-LINE PASSW ORD GUESSING 257  information for an off-line password-guessing attack.
Although conversion to an AES key can be  computationally inexpensive, it is also vulnerable to password guessing, because  human-memorizable and typeable passwords do not have sufficient entropy.
An approach to slow  down password-guessing attacks is to purposely design the password-to-key conversion to be  expensive, such as by hashing the password 10000 times.
 Another alternative is to store the cryptographic key encrypted with a function of the user’s  password rather than trying to directly derive the key from the password.
To retrieve the key  requires the human to remember their password and have access to the encrypted key.
It also means  that anyone with access to the encrypted key can do an off-line password-guessing attack.
 There are clever schemes for leveraging a password (a weak secret) to establish a high-quality  key between the user’s machine and the server (see §9.17 Strong Password Protocols ).
 9.7 ON-LINE PASSWORD GUESSIN G  Sorry, but your password must contain an uppercase letter, a number, a  haiku, a gang sign, a hieroglyph, and the blood of a virgin.
 —anonymous  Trudy can impersonate you if she can guess your password.
And that might not be hard.
On some  systems, passwords are administratively set to a fixed attribute of a person, such as their birthday or  their badge number.
This makes passwords easy to remember and distribute, but it also makes them easy to guess.
I 1 once worked on a system where students’ passwords were administratively set to  their first and last initials.
Faculty accounts were considered more sensitive, so they were protected  with three initials.
 Many systems use common attributes of a person to authenticate them, such as their birthdate  or social security number.
When many systems use the same information about a person to authen - ticate them, this information is no longer secret.
 A popular mechanism today, usually used as backup authentication to allow someone to reset  their forgotten password, is to ask a human to choose answers to security questions, from an extremely small list, usually consisting of questions like “What is your favorite ice cream flavor?”
 The human might sometimes like vanilla and sometimes mint chip, so this is difficult for the actual user to remember but easy for someone to guess given the limited set of choices.
This is an actual set of questions that a system was forcing me 2 to choose from:  • Favorite sports team? (
What’s a “sport”?)
 • Name of your veterinarian? (
I2 don’t have a pet.)
258 A UTHENTI CATION OF PEOPLE 9.7  • Name of your second grade teacher? (
I2 couldn’t even remember my second grade teacher’s  name when I 2 was in second grade!)
 • Your middle name? (
Aha!
I 2 do have a middle name!
It’s Joy.
So I typed “Joy”, thrilled that  they gave me 2 a question I 2 could answer.
The system replied Not enough letters .)
 Even if passwords are chosen so they are not obvious, they may be guessable if the impostor gets  enough guesses.
In fact, given enough guesses, any password, no matter how carefully chosen, can  be guessed (by enumerating all finite character sequences until you hit on the correct password).
 Whether this is feasible depends on how many guesses it takes and how rapidly passwords can be  tested.
 In some military uses of passwords, guessing is not a problem.
You show up at the door.
You  utter a word.
If it’s the right word, they let you in; if it’s the wrong word, they shoot you.
Even if  you know the password is the month in which the general was born (so there are 12 possibilities), guessing is not an attractive pursuit.
 Most machines do not have an “execute user” function controllable from the remote end (as  useful as that might be), so this mechanism for limiting password guessing is not available.
Given  that people’s fingers slip or they forget that they changed their password, it’s important that they get  more than one chance anyway.
There are ways to limit the number or rate of guesses.
The first is to  design the system so that guesses have to actually be typed by a human.
Computers are much faster  and more patient than people at making guesses, so the threat is much greater if the impostor can  get a computer to do the guessing.
 One seemingly attractive mechanism for limiting password guessing is to keep track of the  number of consecutive incorrect password guesses for an account.
When the number exceeds a  threshold, say five, “lock” the account and refuse access, even with a correct password, until the  system is administratively reset.
This technique is used with PINs on ATM cards—three wrong  guesses and the machine eats your card.
You have to show up at the bank with ID to get it back.
An  important downside of this approach is that a vandal (someone who simply wants to annoy people)  can guess five bad passwords and lock out a user.
 Another approach to slow down a guesser is to lock the account for some time after a few bad  password guesses.
 Speaking on behalf of humans, I 2 think repeated guessing of the same wrong password  should not count against me 2.
An imposter won’t guess the same wrong password repeatedly, but  the actual user might, because she isn’t sure whether she might have mistyped her password.
See Homework Problem 7.
 The expected time to successfully guess a password is the expected number an impostor has  to guess before getting it right divided by the guess rate.
So far, we’ve concentrated on limiting the  rate of password guesses.
Another promising approach is to ensure that the number of passwords an  attacker would need to search is large enough to be secure against an off-line, unaudited search with
9.7 ON-LINE PASSW ORD GUESSING 259  a lot of CPU power.
Sometimes systems assign randomly generated character strings for a user’s  password.
These would not be vulnerable to password guessing, but they have another problem:  Users hate them…and forget them…and write them down.
Sometimes the random password  generator is clever enough to generate pronounceable strings, which makes it a little easier for the  human to remember the password.
Constraining the generated passwords to pronounceable strings  of the same length limits the number of possible passwords by at least an order of magnitude, but since a 10-character pronounceable string is probably easier to remember than an 8-character com - pletely random string and is about as secure, generating pronounceable strings is a good idea if the  administrator wants to impose passwords.
 Another approach is to let users choose their own passwords but to warn them to choose  “good” ones and enforce that choice where possible.
One strategy is to use a “pass-phrase” with  intentional misspelling or punctuation and odd capitalization, like GoneFi$hing or MyPassworD - isTuff! ,
or the first letter of each word of a phrase, like Mhall;Ifwwas (Mary had a little lamb; Its  fleece was white as snow).
Most systems today require a password to contain upper- and lower-case  letters, numbers, and special characters.
However, when most users take the password they’d like to  use (“password”) and change it to “Password1!”
to meet the requirements, these rules do not ensure  hard-to-guess passwords.
There is great wisdom to be found in xkcd cartoons, e.g., see xkcd.com /  936.
 The program that lets users set passwords typically checks for easy-to-guess passwords and  disallows them.
There are resources that have compiled dictionaries of user passwords.
A large dic - tionary might contain 500000 potential passwords, and it is not difficult for a computer to check  that many.
 Many systems, in a misguided attempt to make things more secure, actually impose rules that  make things less usable and less secure.
Every system administrator thinks their system is the only  one that a user needs to access, but, in fact, users need passwords at zillions of different systems.
 Users are told not to use the same password or similar passwords at multiple systems, to change the  password at each system every few months (see §9.10), and to never reuse a password.
Systems  have long memories and will catch you if you try to reuse a password.
Servers enforce rules that  really make Alice think she is living in a Kafka novel.
If she forgets her password and goes through  an arduous password-resetting ritual to convince the system she is really Alice, the system will  allow her to reset her password but won’t allow her to set it to the password that she momentarily  had forgotten.
That password would have been considered secure if she had remembered it, but now she’s not allowed to ever use it again, perhaps to punish her for having momentarily forgotten it.
To  increase the absurdity, every system has different rules for passwords ( e.g., must contain special  characters, must only contain English letters, must be at least n characters long, must not be longer  than m characters).
Alice might remember the password she chose if the system would tell her what  its password rules are if she asks, but it won’t tell Alice the rules until she needs to set a new pass - word.
There was even a system I1 was using that wouldn’t ever state its password rules.
When I1  attempted to create a password, it said “Unacceptable password.
Choose something else.”
After
260 A UTHENTI CATION OF PEOPLE 9.8  several attempts with what I 1 considered high-quality passwords, I 1 finally in frustration tried set - ting my 1 password to “something else”, and it was happy with that.
I 1 assume all users of that sys - tem wound up with the password “something else”.
 9.8 OFF-LINE PASSWORD GUESSIN G  In the previous section, we discussed on-line password guessing, where guessing can be slowed  down and audited.
Passwords do not have to be very strong if the only threat is on-line password  guessing.
 But sometimes it is possible for an attacker, through either eavesdropping or stealing a data- base, to obtain a quantity such as a cryptographic hash of the password.
In such a case, even though  the attacker cannot reverse the hash of the password, the attacker can guess a password, perform the same hash, and compare it with the stolen quantity.
The attacker can perform password guessing  without anyone knowing it and at a speed limited only by procurable compute power.
This attack is  known as a dictionary attack , or an off-line password-guessing attack .
 One way to slow down this type of attacker is to make it computationally expensive to com - pute each hash, for example, by having the stored hash consist of the password hashed 10 000 times.
 The extra computational burden will not be noticeable to a user when she logs in, but this will be  10 000 times as much work for the attacker to create the hash of all the passwords in his dictionary.
 Unfortunately, it also makes the work for the server 10 000 times as much when it needs to authen - ticate users, which might be a real burden if users log on frequently or someone is mounting an  on-line password-guessing attack (see Homework Problem 10 ).
 When disclosure of whole files full of hashed passwords is a concern, a useful technique is to  apply salt.
Rather than guessing passwords against a single user account, an attacker who has stolen  a file full of hashed passwords might hash all the words in a dictionary and check to see whether  any of the passwords match any of the stored hashed values.
This can be prevented as follows:  When a user chooses a password, the system chooses a number unique to that user (the salt).
It  need not be secret—just unique, so badge number or user ID would be fine.
It then stores both the salt and a hash of the combination of the salt and the password.
When the user supplies a password  user ID salt value password hash  Mary 2758 hash( 2758|passwordMary)  John 886d hash( 886d|password John)  Ed 5182 hash( 5182|password Ed)  Jane 1763 hash( 1763|password Jane)
9.9 USING THE SAME PASSWOR D IN MULTI PLE PLACE S 261  during authentication, the system computes the hash of the combination of the stored salt and the  supplied password and checks the computed hash against the stored hash.
The presence of the salt  does not make it any harder to guess any single user’s password, but it makes it impossible to per - form a single cryptographic hash operation and see whether a password is valid for any of a group  of users.
 An attacker can use a time/space trade-off by pre-computing a huge table of hashes and the  corresponding passwords.
These tables are called rainbow tables .
They make password guessing  cheap when hashed passwords are discovered.
Use of salt also prevents the creation of rainbow  tables.
 9.9 USING THE SAME PASSWORD IN MULTIP LE PLAC ES  One of the tough trade-offs is whether to recommend that users use the same password in multiple places or keep their passwords different for different systems.
All things being equal, use of differ - ent passwords is more secure because if one password is compromised, it only gives away the  user’s rights on a single system.
Things are rarely equal, however.
When weighed against the likeli - hood that users will resort to writing passwords down if they need to remember more than one, the  trade-off is less clear.
 An issue that weighs in favor of different passwords is that of a cascaded break-in.
An  attacker that breaks in to one system may succeed in reading the password database.
If users have  different passwords on different systems, this information will be of no use except on that one sys - tem.
But if users reuse passwords, a break-in to a system that was not well-protected because it con - tained no “important” information might, in fact, leak passwords that are useful on critical systems.
 9.10 REQU IRING FREQUE NT PASSWO RD CHANG ES  Security is a wet blanket. —
apologies to Charles Schulz  A technique that is intended to enhance security, but winds up making things so much less usable  that it decreases security, is requiring frequent password changes.
The idea behind frequent pass - word changes is that if someone does learn your password, it will only be useful until it next  changes.
This protection may not be worth much if a lot of damage can be done in a short time.
262 A UTHENTI CATION OF PEOPLE 9.11  The problem with requiring frequent password changes is that users are more likely to write  passwords down and less likely to give much thought or creativity to choosing them.
The result is  observable and guessable passwords.
It’s also true that users tend to circumvent password change  policy unless enforcement is clever.
In a spy-versus-spy escalation, the following is a common sce - nario:  1.
The system administrator decides that passwords must be changed every 90 days, and the system enforces this.
 2.
The user types the change password command, resetting the password to the same thing as before.
 3.
The system administrator discovers users are doing this and modifies the system so that it makes sure the password, when changed, is set to a different value.
 4.
The user then does a change password procedure that sets the password to something new, and then immediately sets it back to the old, familiar value.
 5.
The system administrator then has the system keep track of the previous n password values  and does not allow the password to be set to any of them.
 6.
The user then does a change password procedure that goes through n different password val - ues and finally, on the n+1st password change, returns the password to its old familiar value.
 7.
The system is modified to keep track of the last time the password was changed and does not allow another password change for some number of days.
 8.
The user, when forced to change passwords, constructs a new password by appending 1 to the  end of the previous password.
Next time the user replaces the 1 with a 2, and so on.
 9.
The system, in looking for guessable passwords, looks for passwords that look “too much like” one of the previous n passwords.
 10.
The users throw up their hands in disgust, accept impossible-to-remember passwords, and post them on their terminals.
 9.11 TRICKING USER S INTO DIVULGING PASSWORDS  A threat as old as timesharing is to leave a program running on a public terminal that displays a  login prompt.
An unsuspecting user then enters a user name and password.
The Trojan horse pro - gram logs the name and password to a file before the program terminates in some way designed to  minimize suspicion.
9.12 LAMP ORT’S HASH 263  Today, popup boxes that can be posted by any program or web page ask the user to type cre - dentials.
An example is a common email program.
At unpredictable (to the user) times, such as a  server somewhere going down, it displays a popup box asking the user to enter their password.
The  user did not do anything that would cause them to expect such a message.
Since users want to  receive their email, they will dutifully tell whatever displays such a popup box their password.
 They have become used to having these boxes pop up at any time, and there is no way for a user to  distinguish an honest popup (such as the one from their email program) from a malicious one.
 Another example is maliciously sent email (phishing) where the sender hopes to trick the user  into divulging her credentials.
The email might claim to be from the user’s bank, asking the user to  click on a link to verify a recent transaction or something.
The link will take the user to a page that  looks exactly like the login page of the bank, and they will type their username and password into a  malicious site that now knows the user’s credentials and can access the user’s account at the real  bank.
 9.12 LAMPORT’S HASH  It’s a poor sort of memory that only works backwards .
 —The White Queen (in Through the Looking Glass )  Leslie Lamport invented an interesting one-time password scheme [LAMP81].
Although not  widely used, it is a very elegant cryptographic trick.
The scheme allows server Steve to authenticate  Alice in a way that neither eavesdropping on an authentication exchange nor reading Steve’s data - base enables someone to impersonate Alice, and it does it without using public key cryptography.
 (See Protocol 9-2 .)
Alice (a human) remembers a password.
Steve (the server that will authenticate  Alice) has a database where it stores, for each user:  • username  • n, an integer that decrements each time Steve authenticates the user  • hashn(password ), i.e., hash(hash(…(hash( password ))…))  First, how is the password database entry associated with Alice configured?
Alice chooses a pass- word, and a reasonably large number n (like 1000) is chosen.
The user registration software com - putes x1 =hash( password ).
Then it computes x2 = hash( x1).
It continues this process n times,  resulting in x =hashn(password ), which it sends to Steve, along with n.n  When Alice wishes to prove her identity to Steve, she types her name and password to her  workstation.
The workstation then sends Alice’s name to Steve, which sends back n. Then the  workstation computes hashn−1(password ) and sends the result to Steve.
Steve takes the received
264 A UTHENTI CATION OF PEOPLE 9.12  quantity, hashes it once, and compares it with its database.
If it matches, Steve considers the  response valid, replaces the stored quantity with the received quantity, and replaces n by n−1.
Alice Alice, pwd  Alice’ s Workstati on Alice Steve n  x = hashn−1(pwd) knows 〈n, hashn(password )〉  compares hash( x) to hashn(password );  if equal, replaces  〈n, hashn(password )〉 with 〈n−1, x〉  Protocol 9-2.
Lamport’s Hash  If n ever gets to 1, then Alice needs to set her password again with Steve.
Alice can choose a  new password, compute hashn(new password ), and transmit hashn(new password ) and n to Steve.
 An enhancement is to use salt (see §9.8).
Instead of Alice’s machine computing  hashn(password ) when she sets her password, it could use her username (presumably a unique  value) as salt, and compute hashn(password | Alice ).
If she would like to securely use the same  password on multiple servers, she could include the name of the server in the computation, i.e.,  compute hashn(password | Alice | Steve ).
See Homework Problem 5 .
 Lamport’s hash has interesting properties.
It is similar to public key schemes in that the data - base at Steve is not security sensitive (for reading—other than dictionary attacks to recover the  user’s password).
It has several disadvantages relative to public key schemes.
One problem is that  you can only log in a finite number of times before having to reinstall password information at the  server.
 Another problem is there is no mutual authentication, i.e., Alice does not know she is defi - nitely talking to Steve unless she is talking over TLS.
 There’s another security weakness that we’ll call the small-n attack .
Suppose an intruder,  Trudy, were to impersonate Steve’s network address and wait for Alice to attempt to log in.
When  Alice attempts to log into Steve (but she’s really talking to Trudy), Trudy sends back a small value  for n, say, 50.
When Alice responds with hash50(password ), Trudy will have enough information to  impersonate Alice for some time, assuming that the actual n at Steve is greater than 50.
What can be  done to protect against this?
Alice’s workstation could display n to the human Alice and give her a  chance to object.
If Alice remembers that n was approximately 850 the last time she connected to  Steve, then when Trudy is impersonating Steve, the human Alice might be suspicious to see a much  smaller value of n displayed.
 Lamport’s hash can also be used in environments where the workstation doesn’t calculate the  hash, for example, when:
9.13 PASSW ORD MANAG ERS 265  • Alice is logging in from a workstation that does not have Lamport hash code, or  • Alice is logging in from a workstation that she doesn’t trust enough to tell her password  We’ll call this the human and paper environment and call the other environment the work - station environment.
The way Lamport’s hash works in the human and paper environment is that  when the information 〈n,hashn(password )〉 is installed at the server, all the values of  hashi(password ) for i<n are computed, encoded into a typeable string, printed on a paper, and given  to Alice.
When she logs in, she uses the string at the top of the page and then crosses that value out,  using the next value the next time.
This approach automatically protects against the small n attack.
 Of course, losing the piece of paper, especially if it falls into the wrong hands, is a problem.
 It is interesting that the human and paper environment is not vulnerable to the small n attack,  since the human just always uses the next value on the list and can’t be tricked into sending an item  further down on the list.
 There is a deployed version of Lamport’s hash, known as S/Key, implemented by Phil Karn.
 It was standardized in RFC 1938.
It operates in both the workstation and human and paper environ - ments.
It makes no effort to address the small n attack, but it certainly is an improvement over  cleartext passwords.
 9.13 PASSWORD MANAG ERS  Security administrators have a dream.
Users are supposed to choose long unguessable passwords at  every site.
The passwords should not be similar to each other and they should be changed periodi - cally.
Each site has its own rules about how long passwords should be and what characters they must or must not contain.
Unfortunately, this dream is a user’s nightmare.
 So various technologies have been developed to help users cope.
A user could personally  maintain a file containing her username and password for each site.
This is, of course, dangerous if  anyone were to steal this file.
To make it less vulnerable, the user could encrypt the file with a very  good password.
Users can remember and type one really good password, provided it is only one.
 The user would want to surf the Internet from various devices, so she’d have to move the file  between her devices.
And if she created a username/password at a new service that needed to be  added to the file, she’d have to make sure this entry was included in all the copies of the file.
This is  sufficiently cumbersome that users seldom do this.
 Instead, there are various password manager services, usually implemented in a user’s  browser.
We are not describing specific implementations, but rather conceptual issues.
 If user Alice agrees, the browser she is using might remember her username/password at  websites on which Alice has accounts.
When Alice is asked to log into a service, the browser might
266 A UTHENTI CATION OF PEOPLE 9.14  offer to autofill her username and password on the site.
This is not only convenient, but has the  security advantage that the password manager would not be tricked into sending Alice’s username  and password at website X to a phishing site with a name that looks similar to a human.
 The browser’s database of Alice’s username/passwords might be local to that one machine, or  the browser might back it up in the cloud.
If Alice explicitly logs into that browser on a different  device, the browser can download Alice’s username/password list from the cloud.
Sometimes Alice  might not be aware that she has logged into the browser, because the same company that provides the browser might also provide email service, and when Alice logs into her email, she will also be  logging into the browser.
 The disadvantage of a password manager is that a single password, if guessed or divulged,  reveals all the user’s passwords to the attacker.
Likewise, if the user’s device is infected with mal - ware, or if the user walks away from their device while it is unlocked, all the user’s passwords will  be compromised.
Note that password managers often offer to choose a completely unguessable  password for each site, but all these incredibly secure passwords are locked by a single password  that the user has to be able to remember and type.
 9.14 WEB COOKIE S  When user Alice is browsing the web, she is using a protocol known as HTTP.
HTTP is stateless,  which means that it treats each HTTP request independently from any other HTTP request.
The two main HTTP request types are GET and POST .
The best way to think of them is that GET is for  reading a web page and POST is for sending information to a web server.
The response contains  information such as the content requested and status information (such as OK or not found or unau - thorized ).
One status that might be included in a response is a redirect.
This informs the browser  that it should go to a different URL.
If redirected, the browser will then go to the new URL as if the  user had clicked on a link.
 Note URL stands for Uniform Resource Locator and is used by the HTTP protocol to find  the resource (the URL contains a DNS name that can be looked up to find an IP address, and addi - tional information interpreted by the resource, which helps it do things like find the specific web  page being requested).
 A website can include a string of octets in the HTTP response that is known as a cookie .
The  browser need not interpret the cookie in any way, but it must store it along with the DNS name of  the site that gave it that cookie.
When the browser sends an HTTP request to a DNS name (inside a  URL), it sends any cookies it has for that DNS name along with the HTTP request.
 This enables a user to have the illusion of a long-lived connection with service Steve, so Alice  only needs to log in to Steve occasionally.
Once she logs into Steve, Steve sends a cookie, and on
9.15 IDENTITY PROVIDERS (IDP S) 267  each subsequent HTTP request to Steve, the browser will include that cookie, indicating to Steve  that this is Alice.
Steve can have policies about how long Alice’s browser should keep the cookie.
If  the cookie expires (Alice’s browser deletes the cookie, or Steve includes a timestamp in the cookie  and rejects cookies that are too old), then when Alice’s browser makes a request to Steve, Steve will  redirect her to a login page.
 9.15 IDENTITY PROVIDERS (IDP S)  An alternative way of making authentication convenient for the user Alice is to have Alice link her  account at a site known as an identity provider with her account at other websites.
What this looks  like to Alice is that when she attempts to log in to Steve, Steve might give her the choice of logging  in with username/password or with one of several companies that offer identity provider service, such as Google or Facebook.
 When Alice creates an account at Steve, she can choose to create a username and password,  or she might choose to authenticate with IdentityProvider4.
If she chooses IdentityProvider4, then  Steve’s webpage will send her to IdentityProvider4.
If she has logged into IdentityProvider4 before,  her browser will have stored a cookie from IdentityProvider4 and will send that cookie to  IdentityProvider4.
If her browser does not have a cookie from IdentityProvider4, she will be  directed to a login page at IdentityProvider4.
IdentityProvider4 will now know which  IdentityProvider4 user Alice is and that she was sent there from Steve.
IdentityProvider4 will ask  Alice if she wants to log into Steve.
If Alice says yes, then IdentityProvider4 will send her back to  Steve, this time with information tacked onto the end of the URL directing her to Steve, which is a  signed statement from IdentityProvider4 saying that this user is Alice at IdentityProvider4.
 This is convenient for the user, since she need only remember how to log into  IdentityProvider4.
It does have privacy implications, in that IdentityProvider4 will know all the  websites that Alice has visited using IdentityProvider4 as the identity provider.
And it will be easy  for websites to notice that the user that website X knows as Alice at IdentityProvider4 is the same  user that Y knows as Alice at IdentityProvider4 .
It also means that if anyone were to break into  IdentityProvider4, many users would be compromised at many different websites.
 Note that if Alice has accounts at several identity providers, she needs to remember which  identity provider she used when creating an account at Steve.
A different identity provider would  know Alice by a different name, and Steve would not be able to recognize Alice’s account.
 The protocols enabling identity providers were designed to fit with mechanisms already in  browsers (HTTP, cookies, redirects).
There are various standards, including OpenID Connect and  SAML.
OpenID Connect is an authentication protocol that is layered on top of OAuth 2.0.
SAML
268 A UTHENTI CATION OF PEOPLE 9.16  has pretty much the same functionality as OpenID/OAuth but with different syntax (SAML uses  XML) and defined by a different standards organization.
 9.16 AUTHE NTICA TION TOKENS  An authentication token is usually a physical thing that a person carries around and uses in authen - ticating.
NIST refers to these devices as authenticators .
In the breakdown of security according to  what you know , what you have , and what you are , authentication tokens fall in the what-you-have  category.
Although we will describe various broad categories, many products do not fit nicely into a single category.
And again, we are not describing specific products.
We are discussing the con - ceptual issues and trade-offs and potential features of such tokens.
We will refer to the machine  from which Alice is connecting to the Internet as her device , with example devices being a laptop, a  tablet, or a phone.
We will use the term token to refer to either a physical hardware widget used for  authentication or a software implementation of it.
 9.16.1 Disconnected Tokens  The older type of token (though some are still in use) is a little thing that Alice carries around that  cannot directly communicate with Alice’s device.
Alice has to act as the interface between the  token and her device.
 Alice’s token displays a string of perhaps eight numbers, and Alice will type the value dis - played on her token into her device.
Each token has an internal secret configured into the token  before it is given to Alice.
There is a server somewhere on the Internet that manages a particular set  of tokens ( e.g., all the ones from a particular company).
Let’s call that the token server.
The token  server has a database consisting of a set of 〈userID , secret〉 pairs.
The userID is an ID for the human  Alice, and the secret is the secret configured into Alice’s token.
When Alice logs into a site, say,  Steve, that wants Alice to use the token as one of the factors of authentication, she will input the value displayed on the token, and Steve will query the server about whether Alice’s token value is correct.
There are at least two designs of these tokens.
 One type is time-based .
The value displayed by the token is a hash of the time of day (usu - ally in minutes) hashed with the token’s secret.
Sometimes the user types a PIN into the token, and  the input into the hash includes the PIN as well as the current time in minutes and the token secret.
 If the user mistypes the PIN, the only feedback she will get is that authentication will not work  (because the token is displaying the wrong value).
But authentication might not have worked because she might have mistyped the 8-digit number displayed on the token, so Alice has to decide
9.16.1 AUTHENTICATI ON TOKENS 269  whether she should retype the PIN into the token.
An interesting challenge with time-based tokens  is that the clocks can drift.
Typically, a solution to this is that if the value from the token seems to be  more than a minute too old or new, the token server asks the user to input both what the token is  currently displaying and what the next token value is.
The token server computes and stores the  time adjustment for Alice’s token so that it can again be in sync.
 Another type is challenge-response .
The token has a little keyboard, like a calculator.
When  using the token to authenticate, Steve (the service Alice is authenticating to) sends a number (per - haps four digits) to Alice’s workstation.
Alice types that value into her token, and the token com - putes a function of the challenge and the token’s secret, displaying the result.
Alice types the result  into her workstation.
This form of token is less convenient for Alice because she not only has to  type the value from the token into her device, but she has to type the challenge displayed on her  device into the token.
It is also more expensive than the time-based one, because it needs to include a keyboard.
It is less secure, too.
If an eavesdropper Eve were to observe a few hundred challenge/  responses from Alice, Eve might be lucky enough when attempting to impersonate Alice to get a challenge from Steve that Eve knows the answer to.
And if Eve doesn’t know the answer to a partic - ular challenge, she can just try logging into Steve again to get a new challenge.
If Eve gave a wrong  response to the challenge, Alice’s account might get locked after some number of attempts, but  Steve does not typically count a non-response as a wrong response.
The reason these tokens were  deployed was to avoid patent issues, but since relevant patents have expired, they are rarely used  anymore.
 It is inconvenient for Alice to have to read a display from the token and type the result into  her device.
Users also frequently misplace the tokens.
Although these types of tokens were origi - nally physical little things, they are now commonly implemented as software on the user’s device.
If Alice is communicating to service Steve using the same device on which the software token is  running, she can cut and paste the displayed value into the login page presented by service Steve.
 Having a software implementation is less expensive to implement than a physical token.
If  Alice is only accessing Steve from one device, this is very convenient.
Unfortunately, Alice is likely  to want to access service Steve from multiple devices ( e.g., laptop, tablet, phone), and for some rea - son, providers of this software do not want to allow Alice to have token software running on more  than one of her devices.
If the token software is running on her laptop and she wants to browse  using her phone, the laptop becomes an extremely cumbersome token that Alice will need in order  to access Steve from her phone.
 A security implication of these secret-based tokens is that there needs to be a server on the  Internet (the token server) that knows the secrets for all the tokens.
If this database is stolen, then all  tokens are compromised.
9.16.2 270 A UTHENTI CATION OF PEOPLE  9.16.2 Public Key Tokens  Public key tokens need to be connected to Alice’s device, because public key signatures are too  long for a human to type.
The tokens connect to a device using technologies such as Bluetooth,  NFC, or USB.
Given that they are based on public keys, there is no need for an on-line server that  knows the secrets for all the tokens.
 The token might be multifactor, in that Alice might need to activate it by inputting a pass- word, scanning her finger, or merely pressing a button on the token to give it permission to authen - ticate on her behalf.
Sometimes the physical token can be implemented in software on Alice’s device.
This is convenient, but it has the security issue that malware on Alice’s machine might be  able to steal the private key(s).
 Whether the token is a physical token or software, malware does not need to steal the private  key(s) from the token in order to cause security issues.
If malware can ask the token to authenticate on Alice’s behalf, the malware doesn’t need to know the token’s secrets.
Therefore, there is usually  some sort of feedback to Alice when the device is acting on her behalf.
 The most secure option would be for the token to have a display and display “Request to log  in to BigBank?”
Alice can then hit a button on the token for yes or no.
Most implementations have  Alice’s device (rather than the token) display what the token is being asked to do on Alice’s behalf, even if the permission button is on the token.
The problem is that malware on Alice’s device can ask Alice “Did you ask to log in to  weather.com ?”
when in fact the malware will ask the token to  authenticate to BigBank.
Although it would be more secure to have a display on the token, this  would make the token larger and more expensive.
 The FIDO alliance ( fidoalliance.org ) has developed standards for communication between a  browser or OS and such a device.
The W3C organization has developed standards for how the cre - dentials are used for web authentication.
The standards include many variations.
There are also public key authentication tokens that do not fit with the FIDO specifications.
We will not describe  the specific standards, but instead discuss some of the features such devices might have.
 The token could, in theory, have a single private key for Alice, and she could use that to  authenticate to all sites that support authenticating using the token.
Some tokens, such as credit cards with embedded computer chips, U.S. government CACs (Common Access Cards), or PIV  (Personal Identity Verification) cards, do not have different key pairs for each site that Alice will  connect to.
However, the PIV and CAC cards have several PIN-protected private keys, including  one for authentication, one for signing documents, and one for decrypting documents.
There is also  a non-PIN protected private key for local authentication, say, for opening a door.
 For consumer use, it is assumed that Alice will want to have a different private key for each  service to which she will authenticate, so that it won’t be easy to know which accounts at site A and  site B are the same user (though people seem to think that using identity providers is not a privacy  issue).
9.16.2 AUTHENTICATI ON TOKENS 271  With the consumer type of public key token, when Alice creates an account at a site Steve  that supports authentication using the token, Alice’s token creates a new public/private key pair, and  sends a key handle and public key to Steve.
Later, when she needs to authenticate to Steve, she will  tell Steve her username, and Steve will send the key handle associated with Alice’s account to  Alice’s workstation, which will pass this information to the token.
The token can then perform  authentication using that private key.
 A cute feature of the FIDO design is the option to have very little storage on the token, so that  the token does not remember all the private keys it has created for all the sites.
Instead, the token just needs to remember a single secret key S. The key handle the token sends to Steve can be the  newly created private key for Alice’s token, encrypted with S. When Alice logs into Steve, Steve  sends the key handle, and the token can retrieve Alice’s private key by decrypting the key handle.
 Another issue is that Alice might purchase a counterfeit, possibly malicious token.
How  would she possibly know?
The proposed solution is for a legitimate manufacturer (say M) to install  an attestation key into each token, along with a certificate saying “This device, model number X,  was manufactured by M.” The characteristics of that model from that manufacturer ( e.g., how  tamper-resistant the token is, or whether the token requires a fingerprint from Alice to activate it)  can be looked up somewhere.
Alice might be able to detect that the token is counterfeit because her device has the ability to query the token as to its heritage, or perhaps when Alice is enrolling at  website Steve, Steve might query the token.
Steve might only allow Alice to do certain operations if  her token is extra-securely manufactured.
Or Steve might warn Alice “I think your token is counter - feit” or “That token does not meet my standards or is not manufactured by one of the organizations  I trust.”
 If every token had its own unique attestation key, then various sites would be able to figure  out that Alice’s account at one site was the same human (using the same token) as an account at  another site.
To prevent linking accounts this way, large batches of tokens can be configured with  the same attestation key.
 Another interesting issue is preventing evil Trudy from acting as a meddler-in-the-middle  (MITM).
Suppose Trudy can trick Alice into thinking that Trudy is BigBank.com .
Perhaps Trudy  has acquired a DNS name that looks similar to a human, like BiqBank.com (the letter q looks like  the letter g if Alice doesn’t look carefully).
Alice connects to Trudy (intending to connect to Big- Bank).
Suppose Trudy wants to impersonate Alice at BigBank.
When Alice connects to Trudy, Trudy connects to BigBank, and says “I’m Alice.”
BigBank sends Trudy a challenge and key han - dle for Alice’s token.
Trudy forwards the challenge and key handle to Alice.
Alice’s token signs the  challenge, and Trudy can then use it to complete the authentication as Alice to BigBank.
 There are various ways of solving the MITM attack.
One is for Alice’s device to inform the  token of the DNS name that the device thinks it is communicating with.
BiqBank might look like  BigBank to Alice, but it is a different string.
If Alice’s token signs a hash of the challenge and the  DNS name BiqBank, then Trudy won’t be able to forward this to BigBank, because the response  will not be correct (it will be a hash containing the incorrect DNS name).
Another trick is to have
272 A UTHENTI CATION OF PEOPLE 9.17  the endpoints of a secure connection (such as TLS or IPsec) sign a hash of the secret key estab - lished for the secure session.
If Trudy were acting as a MITM, she would establish a different key  between herself and BigBank than the key she established between herself and Alice’s token.
Trudy  would not be able to trick Alice’s token into signing the key between Trudy and BigBank.
If the  protocol enables Trudy to force the secret key to be a particular value, this technique would not  detect her.
Having the endpoints sign a transcript of all the messages so far is likely to be more  secure.
Additional issues:  • What can be done if Alice loses her token, or the token breaks after Alice leaves it in her pocket and sends it through the laundry?
There should be some way for Alice to recover, by  purchasing another token, preferably without having to re-enroll at all the Internet sites she  has been using.
This might be solved by having her token back up all the private keys it has created, but if there is an interface for reading the private keys from the token, this will make the token less secure.
 • Can Alice have multiple tokens (because she is nervous she might lose one, or because she keeps one token plugged into her laptop USB port and the other taped to her phone)?
Can  protocols manage to allow her to use her multiple tokens interchangeably at all her sites?
 9.17 STRON G PASSWORD PROTOCOLS  Strong password protocols are designed so that, even though Alice is authenticating to a remote  resource using a password and not using something like TLS, someone who eavesdrops on an  authentication exchange or impersonates either end will not obtain enough information to do  off-line verification of password guesses.
An eavesdropper should not be able to gain any informa - tion from observing any number of legitimate exchanges.
Someone impersonating either endpoint will be able to do a single on-line password guess.
There’s really no way to avoid that.
If someone  correctly guesses the password, they will be able to successfully authenticate.
If they guess incor - rectly, they will know that they have not successfully authenticated, and therefore their guess must not be the user’s password.
A false guess will result in an authentication failure, which should gen - erate alarms if they occur in large numbers.
 These are sometimes referred to as PAKE (password-authenticated key exchange).
The first  such protocol, named EKE (for Encrypted Key Exchange ), was published by Bellovin and  Merritt [ BELL92a ]. (
See Protocol 9 -3.)
There are other protocols that are conceptually similar.
The  idea of EKE is that Alice and server Steve share a weak secret W that is a hash of Alice’s password.
 Steve knows W because at some point Alice set her password at Steve, and Steve stores (Alice, W)  in a database.
The user Alice knows her password.
The device from which Alice is authenticating to  Steve learns W because it computes it based on the password Alice typed.
9.17 STRONG PASSW ORD PROTOC OLS 273  choose random a  choose challenge C2 share weak secret W = f (pwd)  a Alice “Alice”, { g mod p}W  {gb mod p, C1}W  K = gab mod p Steve  {C1, C2}K  {C2}K choose random b and challenge C1  Protocol 9-3.
Basic EKE Protocol  The two devices do a Diffie-Hellman exchange, encrypting the Diffie-Hellman numbers with W,  and then do mutual authentication based on the agreed-upon Diffie-Hellman shared secret, which is  a strong secret.
After the authentication, they can use the agreed-upon Diffie-Hellman value K to  encrypt the remainder of their conversation.
 This protocol is quite subtle.
The reason it is secure from an eavesdropper is that a Dif- fie-Hellman transmitted value looks like a random number.
An eavesdropper doing a trial decryp - tion of { ga mod p}W and { gb mod p}W cannot verify a password guess because decrypting with  any password will still just look like random numbers.
And someone impersonating one side or the  other can verify a single password guess as incorrect or correct, but this is an on-line, auditable  guess.
There is no way to do an off-line dictionary attack.
The Diffie-Hellman secret K is a strong  secret because an attacker would both have to guess the password and break Diffie-Hellman.
 The reason it is secure from someone, say, Trudy, impersonating either Alice or Steve is that  Trudy only knows x for one value of gx mod p. Once Trudy encrypts with W, she is committing to a  single password guess. (
See Homework Problem 16.)
 Several years later, more or less simultaneously, two other strong password protocols were  invented.
One was done by Jablon and was called SPEKE ( simple password exponential key  exchange ) [JABL96 ], and the other was done by Wu and known as SRP ( secure remote password )  [WU98 ].
We’ll describe SRP in section §9.17.3.
Several years after SRP and SPEKE, we1,2  designed PDM ( password derived moduli ) [KAUF01 ].
 SPEKE uses W (the weak secret derived from the password) in place of g in the  Diffie-Hellman exchange, so rather than transmitting { ga mod p}W and agreeing upon K = gab mod  b p (as would be done in EKE), SPEKE transmits Wa mod p and W mod p and agrees upon the key  K = Wab mod p.
9.17.1 274 A UTHENTI CATION OF PEOPLE  PDM chooses a modulus p derived deterministically from the password and uses 2 as the  base, so the Diffie-Hellman numbers transmitted are 2a mod p and 2b mod p, and the agreed-upon  Diffie-Hellman key is 2ab mod p.  9.17.1 Subtle Details  There is more to making these schemes secure than the basic idea.
The original EKE paper pro - posed many variants of the protocol, many of which were later found to be flawed.
The successor  protocols have had similar difficulties.
To be secure, a protocol must carefully specify some imple - mentation details to avoid an eavesdropper being able to eliminate password guesses.
Subsequent papers noted other potential implementation issues.
For instance, assume a straightforward encryp - a ation of g mod p with W. Since g mod p will be less than p, an eavesdropper that does a trial  decryption with a guessed password and obtains a value greater than p can eliminate that password.
 If p were just a little more than a power of 2, an incorrect password would have almost a 50%  achance of being eliminated.
Each time an eavesdropper saw a value W{g mod p} (each time pre - sumably with a different a), the eavesdropper could eliminate almost half of the passwords.
With a  dictionary of, say, 50000 potential passwords, the eavesdropper would only need to see about  twenty exchanges before narrowing down the possibilities in the dictionary to a single choice.
 If SPEKE were not designed carefully, it would also have a flaw whereby an eavesdropper  might be able to eliminate some password guesses based on seeing Wa mod p. The flaw can be  eliminated by making sure that W is a perfect square mod p. Some numbers are generators mod p (g  1 2 3 p−1is a generator if g , g , g ,…,g mod p cycles through all the values from 1 through p−1).
If g is  a generator mod p, then its even powers are the perfect squares mod p (so half of all numbers mod p  are perfect squares, and any power of a perfect square is also a perfect square).
If some of the Ws  generated from passwords for use in SPEKE were perfect squares and some not, then if an eaves - dropper saw a value Wa mod p that was not a perfect square, she would know that none of the pass - words that resulted in Ws that were perfect squares could have been Alice’s password (since such a  password could not have generated a value that was not a square). (
To tell if a number is a perfect  square mod p, raise it to the power ( p−1)/2 and see if the result is 1 mod p.) This is a less serious  vulnerability than the EKE vulnerability in the previous paragraph, because in each EKE exchange  a different half of the passwords could be eliminated.
But in this SPEKE vulnerability, half the  passwords (the ones for which W would be a square) would be eliminated if a value Wa mod p that  was not a perfect square were seen by an eavesdropper, but there would be no further narrowing  down the possibilities no matter how many exchanges were observed.
 Both the vulnerabilities mentioned are easily avoided.
The EKE vulnerability is avoided by  choosing a p that is just a little less than a power of two.
The SPEKE vulnerability is avoided by  ensuring that W is a perfect square—hash the password and then square it mod p to get W.
9.17.2 STRONG PASSW ORD PROTOC OLS 275  To build a workable protocol from the basic idea of PDM (generating the modulus determin - istically from the password with reasonable performance) involves some math that we won’t go  into in detail because it’s not that important for the purpose of this chapter. (
For subtle reasons,  using 2 for g, the modulus p has to be a safe prime, i.e., (p −1)/2 must also be prime, and p must be  equal to 11 mod 24.)
 Adoption of all these protocols was slowed down by patent issues.
However, the relevant pat - ents have expired at this point.
 9.17.2 Augmented Strong Password Protocols  With the schemes in the previous section, if someone stole the server database and therefore knew  W, they could impersonate the user.
Bellovin and Merritt designed a strong password protocol with  an additional security property that they called augmented EKE [BELL93].
The additional property  prevents someone who has stolen the server database from being able to impersonate the user.
The  information in the server database would still allow an attacker, Trudy, to do a dictionary attack.
If  Trudy’s dictionary attack found the user’s password, then Trudy could impersonate the user.
But  using an augmented form of a strong password protocol, if Trudy’s dictionary attack on the stolen  server database was unsuccessful, she would not be able to impersonate the user to the server.
 All the basic schemes (EKE, SPEKE, and PDM) can be modified to have the augmented  property.
Another protocol that we’ll describe (SRP) only has an augmented form.
The idea is for  the server to store a quantity derived from the password that can be used to verify the password, but  the client machine is required to know the actual password (in addition to the derived quantity  stored at the server).
The scheme in the published augmented EKE is a bit complicated, so we will  instead show a simpler scheme with the same properties.
The strategy in this simpler protocol will  work with any of the schemes (EKE, SPEKE, and PDM), but we’ll show it with PDM and let the  reader adapt EKE and SPEKE in Homework Problem 11.
 In the augmented form of PDM, the server (Steve) stores p, the safe prime derived from the  user Alice’s password.
The server will also store 2W mod p, where W is a hash of Alice’s password.
 The exchange is as follows (see Protocol 9-4):  After the Diffie-Hellman exchange (using PDM’s modulus p derived from Alice’s password),  both Alice and Steve can compute 2ab mod p and 2bW mod p. (see Homework Problem 12 ).
Notice  that each side sends a hash of (2ab mod p, 2bW), so it has to be a different hash or else whoever  speaks second can just repeat what they received.
Having a different hash can be achieved in many  ways, such as by concatenating a constant to the quantities being hashed.
9.17.3 276 A UTHENTI CATION OF PEOPLE  stores “Alice”, p, 2W mod pchoose a, compute  W and p from password 2a mod p Alice 2b mod p,  hash(2ab mod p, 2bW mod p)  hash′(2ab mod p, 2bW mod p) Steve choose b  Protocol 9 -4.
 Augmented Form of PDM  9.17.3 SRP (Secure Remote Password)  SRP was invented by Tom Wu [WU98 ] and is a popular choice by the IETF for strong password  protocols.
It is documented in RFC 2945.
It is harder to understand than the others, but we describe  it here because it does appear in many IETF protocols.
Unlike EKE, SPEKE, and PDM, there is no  basic form of SRP.
The augmented property is an intrinsic part of the protocol.
 SRP is shown in Protocol 9-5.
Steve stores gW mod p, where W is a function of Alice’s pass - word.
Alice calculates W from the password.
The tricky part is how Alice and Steve each manage to  compute the session key K (see Homework Problem 14).
 stores “Alice”, gW mod p choose a “Alice”, ga mod p  choose b, challenge c1,  b and 32-bit number u g + gWmod p, u, c1  compute W from password  Alice Steve b(a+uW )K = g mod p  {c1}K, c2  {c2}K  Protocol 9-5.
The Secure Remote Password Protocol (SRP)
9.18 CREDENTIA LS DOWN LOAD PROTOC OLS 277  9.18 CREDENTIALS DOWNLO AD PROTOC OLS  A credential is something that can be used to prove who you are or prove that you are authorized to  do something.
In this case, it is easiest to think of it as a private key.
It might be nice to assume that  Alice has a smart card, that she has remembered to bring her smart card to work, that it is still func - tional after she accidentally ran it through the wash, and that the workstation she is accessing the  Internet from has a way to communicate with her smart card.
 But suppose Alice does not have a smart card.
All she knows is her name and password.
If  Alice walks up to a workstation that has trusted software but no user-specific configuration, then if  she can somehow obtain her private key, she can obtain all the other information necessary to recre - ate her environment by downloading it from the cloud.
Any information she has stored in the cloud, such as cookies or browser bookmarks that need to be kept private, can be stored in the cloud  encrypted with her private key.
So, all she really needs to do is securely retrieve her private key  from the cloud.
 We can use strong password protocols to do this.
The private key is kept in the directory,  encrypted with the user’s password.
Call that quantity Y. You don’t want to make Y world-readable,  since someone that has Y can test passwords against it.
And you can’t use traditional access control  since Alice can’t prove she’s Alice until she obtains Y (and decrypts it with her password).
Strong  password protocols are ideal for downloading credentials.
 For credentials download, the augmented protocols provide no added security.
The only pur - pose of a credentials download protocol is to download Y. If someone has stolen Steve’s database,  then they already know Y.  [PERL99A] includes an analysis of credential download protocols, along with a two-message  version that can be built upon any basic strong password protocol.
Other properties are explored,  such as the ability for Steve to save computation by reusing his Diffie-Hellman exponent b. In  Protocol 9-6 we show a simple two-message credentials download protocol built upon EKE.
 choose a, compute  W from password  AliceSteve “Alice”, { g a mod p}W  g b mod p, {Y}(g ab mod p) stores “Alice”, W,  Y = {Alice’s private key} W′  choose b  Protocol 9 -6.
 Two-Message Credential Download Protocol  Steve cannot tell whether Alice really knew the password, but Alice can only guess one pass - word in each on-line query, since once she encrypts by W, she is committing to a single password.
 She only knows the a for the quantity she encrypted with the chosen W. So Steve can audit
278 A UTHENTI CATION OF PEOPLE 9.19  download requests and get suspicious if the credentials for the same user were requested too many  times.
Note that the key W′ used to encrypt Alice’s private key must be different from W, or else  someone stealing Steve’s database (not to mention Steve itself) would know Alice’s private key.
 9.19 HOMEWO RK  1.
Given that the Lamport hash (see §9.12 Lamport’s Hash ) value is sent in the clear over the  network, why is it more secure than a password?
 2.
Is the Lamport hash protocol vulnerable to dictionary attack by an eavesdropper (assuming communication is not using an encrypted channel such as TLS)?
Can someone impersonating  Steve do a dictionary attack?
 3.
Design a variant of Lamport’s hash using k times more storage at the server but needing only  1/k as much processing, on average, at the client.
 4.
Suppose we are using Lamport’s hash, and Steve crashes before receiving Alice’s reply.
Sup - pose an intruder, Trudy, can eavesdrop and detect that Steve crashed (maybe Trudy can even  cause Steve to crash).
Then Trudy has a quantity (whatever Alice replied that Steve did not  receive) that Trudy can use to impersonate Alice, if Trudy logs in before Alice attempts to log into Steve again.
How can we modify Steve’s behavior to prevent this threat? (
Exactly when does Steve overwrite its database, and with what)?
 5.
Suppose in Lamport’s hash, neither Alice’s name nor Steve’s name is part of the hashed quan - tity, so what is installed for Alice at Steve is 〈n, hashn(password )〉.
Suppose Alice uses the  same password at server Carol (also using Lamport’s hash).
Suppose an eavesdropper can lis - ten to Alice authenticating to Steve.
Why might this enable the eavesdropper to impersonate  Alice at Carol?
 6.
Suppose you want to use Lamport’s hash for authentication but want to allow Alice to reset  her password to the same password as before when n gets to 0.
Alice might not even remem - ber which passwords she used before.
How can the Lamport hash protocol be modified so that it will be secure even if Alice reuses her password?
 7.
If there is a limit to the number of bad password guesses a user is allowed before she gets  locked out of her account (say n), it is unreasonable to count it against her if she guesses the  same wrong password multiple times (perhaps because she isn’t sure whether it was the wrong password or she mistyped her password).
Suppose the authentication protocol involves the user sending her password to the server.
What can the server do to not count multiple  guesses of the same wrong password against a user?
Now suppose the authentication protocol
9.19 HOMEWORK 279  is a challenge/response protocol as in section §9.1.1.
Since the server cannot see which pass - word the user typed, what can the server do (without changing the authentication protocol) so  that the user’s account will only be locked out after n different wrong password attempts?
 8.
Suppose Trudy maliciously guesses n wrong passwords for Alice’s account at service S,  causing Alice to get locked out.
This can be annoying to Alice, even though Trudy will not be  able to impersonate her.
What behavior could S do so that Alice will not be locked out, even  though S still has a limit to the number of wrong password guesses?
Hint: Consider having S  use web cookies to differentiate which machine is trying to authenticate or having S remem - ber IP addresses from which it received bad password guesses.
 9.
Suppose there are 1000 users.
An attacker has stolen the database of hashed passwords and  has a dictionary of common passwords.
How much extra computation does the attacker  require to find all users that have passwords in the dictionary if salt is including in the pass - word hash (see §9.8)?
Suppose the hash is slow to compute, e.g., consists of the password  hashed 10000 times.
How much extra computation does the attacker require to find all users  that have passwords in the dictionary if both the expensive hash and salt are used?
 10.
In §9.8, we mention that it would be 10000 times as much work for the server if the password  hash were hash10000(password ).
Suppose the server still stores hash10000(password ), but  when Alice logs in, her client machine first computes hash9999(password ) and sends that to  the server.
Will there still be the advantage gained by storing hash10000(password )?
Will this  save the server computation?
 11.
Show protocols for doing augmented forms of EKE and SPEKE.
 12.
Show how Alice and Steve can each compute 2ab mod p and 2bW mod p in Protocol 9-4.
 13.
Show how in Protocol 9-4 Alice can be assured that it is Steve, i.e., that the other side has the  information stored at Steve.
Explain why someone who has stolen Steve’s database (but can - not find Alice’s actual password through a dictionary attack) cannot impersonate Alice to  Steve.
 14.
Explain how Alice and Steve each compute K in the SRP protocol (Protocol 9 -5).
 15.
Show two-message credentials download protocols built upon SPEKE, PDM, and SRP.
 16.
Why is the EKE-based protocol in Protocol 9-7 insecure? (
Hint: Someone impersonating  Steve can do a dictionary attack, but show how.)
How can you make it secure while still hav - ing Steve transmit gb mod p unencrypted?
 17.
Consider the protocol in Protocol 9-8.
How would Alice compute K?
How would Steve com - pute K. Why is it insecure? (
Hint: Someone impersonating Steve can do a dictionary attack,  but show how.)
280 A UTHENTI CATION OF PEOPLE 9.19  stores “Alice”, W  a choose random a “Alice”, { g mod p}W  choose random b and  challenge c1 gb mod p, c1  compute W from password Steve ab K = g mod p  {c1}K, c2Alice {c2}K  Protocol 9-7.
For Homework Problem 16  choose a a“Alice”, g mod p  b g mod p, c1 stores “Alice”, g W mod p  choose b and challenge c1  Steve AliceK = hash( g ab mod p, g Wb mod p)  {c1}K, c2  {c2}K choose challenge c2  compute W from password  choose challenge c2  Protocol 9-8.
For Homework Problem 17
10TRUSTED  INTER MEDIAR IES  10.1 INTRODUC TION  If nodes Alice and Bob want to be able to communicate securely, they need to know keys for each  other.
Configuring each node with keys for every other node will not scale beyond a small number,  so a trusted third party (someone that Alice and Bob trust) is used for introducing Alice and Bob  to each other.
 In this chapter we describe different types of systems based on trusted third parties.
One is a  system that uses only secret keys, in which case the trusted third party is usually known as a KDC  (Key Distribution Center ).
In a public key system, the trusted third party is usually known as a  certification authority (CA), and it signs certificate s, which assert things such as the mapping  between the name of something and its public key.
In this chapter, we describe the concepts behind Kerberos (a KDC-based system) and PKI (Public Key Infrastructure ).
A PKI includes compo - nents such as certificates, CAs, revocation mechanisms, and directories.
We also describe DNSSEC (Domain Name System Security Extensions) that can be considered a form of PKI.
 In a KDC-based system, the KDC has a database consisting of a secret key for each princi - pal.
A principal is any human or service for which the KDC facilitates secure communication.
Each  principal just needs to initially know the single secret that it shares with the KDC.
 So the KDC knows keys KA (Alice’s secret) and KB (Bob’s secret).
If Alice wants to talk to  Bob, she asks the KDC to help her talk to Bob by creating a new key KA-B for Alice and Bob to  share.
KA-B can then be securely sent to Alice by being encrypted with KA and securely sent to Bob  by being encrypted with KB.
 In contrast, with a PKI solution, the CA signs a certificate saying “Alice’s public key is  928…38021.”
When Alice and Bob wish to communicate they can simply send each other their  certificates, and then they can do mutual authentication and establish a shared secret key for  encrypted communication.
Note that with a PKI solution, the CA never sees the private key of any  node, so compromising the CA cannot leak any private keys of users or services.
But if a compro - mised CA creates a fraudulent certificate mapping Alice’s name to a key belonging to an attacker,  281
282 T RUSTED INTER MEDIARIES 10.2  that attacker can impersonate Alice.
There might be a service that keeps Alice’s private key, either  because it’s required by her employer or the government or in case she loses it and needs help  recovering it.
This might be a service provided by the CA, but logically it is a different function.
 This service might be called a key escrow service or a key recovery service .
 10.2 FUNCTI ONAL COMPARISON  The KDC solution requires an online server (the KDC).
This had several disadvantages:  • If the KDC is broken into, the attacker learns the secret keys of all the resources.
This enables  the attacker to impersonate all the resources, and it also enables the attacker to eavesdrop on  conversations between Alice and Bob.
 • The fact that Alice is talking to Bob will be known by the KDC.
The KDC can also remember  the key that it created for Alice and Bob to communicate and so can decrypt all their commu - nication.
 • If the KDC is down, no new connections can be made between any resources.
It is possible to have multiple KDCs, both for robustness and for load splitting, but the more places where the  database of secrets is stored, the more difficult it will be to protect them.
 These properties might actually be viewed as advantages over a PKI solution in some situations  (e.g., within a company).
The KDC is a central place that can keep a record of which resources have  communicated.
It is also easy to remove a resource, such as when the company finally decides to  fire Trudy after all her eavesdropping and impersonation of other employees.
Removing her from  the KDC database will prevent Trudy from starting new conversations, but it will not immediately  stop any ongoing conversations involving Trudy.
 In contrast, in a public key design, the CA need not be online.
This makes it easy to protect it  from network-based attacks.
It also makes it easier to physically protect it ( e.g., it can be kept in a  vault).
Bob or Alice might keep an audit log of who they have communicated with, but there is  nothing like a KDC that will know they have communicated.
 Although in a PKI system, the CA need not be online, revocation of either a key (Alice’s key  was compromised, but Alice is still a welcome member of the network) or a resource (Alice is no  longer welcome on the network) is usually implemented with an online revocation service that advertises the list of revoked certificates or the list of valid certificates, or can be queried about a specific certificate.
The revocation service is not as security-sensitive as a CA or KDC.
If the revo - cation service is compromised, it might fail to accurately reflect the validity status of a certificate,  but it will not be able to grant certificates to bogus keys or reveal users’ private keys.
10.3 KERBER OS 283  10.3 KERBEROS  Kerberos is a KDC-based design.
It was originally designed at MIT [MILL87] based on a design by  Needham and Schroeder [NEED78].
Because of patents on public key cryptography, Kerberos was  designed to avoid any use of public key cryptography.
Public key cryptography is a more natural  solution, but even though patents have long expired, Kerberos (and designs that use it such as  Microsoft Active Directory) are still in widespread use because of a large installed base.
 For trust and scalability reasons, there would not be a single KDC for the world.
Instead, a  KDC serves some set of nodes.
The KDC and the nodes it serves are known as a realm .
 10.3.1 KDC Introduces Alice to Bob  The simplest way that Kerberos can be used is for Alice to directly request that the KDC help her talk to Bob ( Protocol 10-1).
In §10.3.3 Ticket Granting Ticket (TGT) we will discuss how it is usu - ally done.
Note that if Alice is a human, her secret will be derived from her password.
This means  that human Alice’s secret is likely to be cryptographically weak, and, as we’ll see in §10.3.5 Mak- ing Password-Guessing Attacks Difficult , the Kerberos design attempts to compensate for that.
 invents key KA-B  Protocol 10-1.
Alice Requests an Introduction to Bob Alice Alice,password  Workstat ionAlice wants to talk to Bob KDC  {“Bob”, KA-B}KA  {“Alice”, KA-B}KB  Alice types her password, and her workstation converts it into her secret, KA.
We’ll show that step  in Protocol 10 -1, but for simplicity we will usually refer to Alice’s workstation as “ Alice”.
The  KDC invents a new key, KA-B, for Alice and Bob to share, and sends two items to Alice:  • {“Bob”, KA-B}KA  • {“Alice”, KA-B}KB  Note that when we refer to encryption, we mean some sort of mode (see Chapter 4 Modes of Oper - ation ) that provides both encryption and integrity protection.
The quantity {“Bob”, KA-B}KA can be  decrypted by Alice, since it is encrypted with KA.
That quantity informs Alice that if she wants to  talk to Bob, she should use the key KA-B. The quantity {“Alice”, KA-B}KB cannot be decrypted by  Alice, but it can be decrypted by Bob.
That message tells Bob that to communicate with Alice, he
10.3.2 284 T RUSTED INTER MEDIARIES  should use the key KA-B. In theory, the KDC could have sent {“Alice”, KA-B}KB to Bob, but looking  up Bob’s IP address and opening a connection to Bob is a hassle for the KDC.
Alice is going to be  talking to Bob soon anyway, so the KDC sends the message intended for Bob to Alice.
This mes - sage ({“Alice”, KA-B}KB) is known as a ticket to Bob.
When Alice starts the conversation with Bob,  she sends the ticket, which informs Bob that if someone who contacts him can prove they know the  key KA-B, they are “Alice”.
 For Kerberos purists, the entire response from the KDC is actually encrypted with Alice’s key  KA.
There is no security reason for the ticket to Bob to be encrypted with Alice’s key—any eaves - dropper can see it in the clear when Alice contacts Bob (see §10.3.2).
 10.3.2 Alice Contacts Bob  In Protocol 10-2, Alice starts a connection with Bob.
Notice that this protocol does mutual authen - tication.
Bob knows, based on the information in the ticket, that if the thing talking to him knows  KA-B, it is “ Alice”.
When Bob decrypts the authenticator with KA-B, if the timestamp is close  enough to what Bob thinks is the current time, he assumes the thing he is talking to knows KA-B. Alice’ s Workstati onticket to Bob = {“Alice”, KA-B}KB  authenticator = {timestamp} KA-B Bob  {timestamp+1} KA-B decrypts ticket to find  “Alice” and KA-B  decrypts authenticator  verifies timestamp  Protocol 10-2.
 Logging in to Bob from Alice’s Workstation  The timestamp in the authenticator field has to be reasonably close to the time on Bob’s clock (say  within five minutes or so), and Bob needs to remember all the timestamps it has received from  Alice within that time window.
This field prevents someone who eavesdropped on the conversation  from replaying the conversation.
For example, if the result of the Alice-Bob conversation were that  Alice tells her bank (Bob) to transfer money to Carol, then it would be bad if Carol could replay the  entire conversation to Bob, even if Carol could not decrypt the conversation.
 To authenticate to Alice, Bob needs to prove he also knows KA-B. It wouldn’t be secure to  have Bob just send the same encrypted timestamp back, so in Kerberos v4, Bob increments the time  that Alice sent (which is why we incremented the timestamp in Protocol 10-2).
In Kerberos v5, Bob  does send the same timestamp, but it’s part of a message that has other information inside (such as  that this is a response to Alice’s message), so the encrypted value sent by Bob will be different from  what Alice sent.
After the mutual authentication, it is optional whether Alice and Bob want to
10.3.3 KERBER OS 285  encrypt and/or integrity-protect the remainder of their conversation.
If they do want to cryptograph - ically protect their conversation in Kerberos v4, they’d use the key KA-B. In Kerberos v5, Alice  chooses a new secret key SA-B for this conversation and includes SA-B with the timestamp in the  authenticator.
 10.3.3 Ticket Granting Ticket (TGT)  It is good security practice for Alice’s workstation not to remember Alice’s password or master  secret KA for any longer than necessary (so as to minimize exposure to malware).
For that reason,  Alice’s workstation requests the KDC to give it a session key SA and a TGT.
The TGT is a ticket to  the KDC that informs the KDC that Alice’s current session key is SA. (
See Protocol 10-3.)
Alice Alice,password  Workstati onAlice needs a TGT KDC {SA,TGT} KA  Protocol 10-3.
Obtaining a TGT invents session key SA  finds Alice’s master key KA  TGT = {“Alice”, SA}KKDC  Why isn’t it just as much of a problem for someone to steal Alice’s session key SA and TGT as it  would be if they stole her long-term secret KA?
The reason is that the TGT expires, usually in a few  hours, and usually includes limits on how it can be used.
For instance, the TGT might include the IP  address from which the KDC received the TGT request.
This means that someone who stole SA and  TGT would not be able to use it to impersonate Alice unless they could also impersonate her IP  address.
 Note for purists: The KDC is usually described as being implemented by two different ser - vices—one that gives out TGTs and one that gives out tickets.
We find that detail just makes the  description more complicated, so we will use the name KDC for both services (see Homework  Problem 4).
So, Alice’s workstation requests a TGT from the KDC.
The KDC invents a session key  SA for Alice and sends two items to Alice’s workstation, both encrypted with Alice’s master key  KA—the session key SA and the TGT. (
Note that there is no reason for the TGT to be encrypted  with Alice’s key when transmitted to Alice, but Kerberos happens to do that.)
 Alice can’t decrypt the TGT.
Only the KDC can read the information in the TGT, because it is  encrypted with KKDC (the KDC’s key).
After receiving the session key and TGT, Alice’s worksta - tion then forgets Alice’s password and her master key KA.
The KDC does not remember the session
10.3.4 286 T RUSTED INTER MEDIARIES  key it gave Alice, but when Alice needs a ticket to Bob, she sends the TGT along with the request  for a ticket to Bob, and the KDC will be able to figure out Alice’s session key by decrypting the  TGT (see Protocol 10-4).
Note that the ticket to Bob is encrypted with Bob’s master key KB.
Sup- pose Bob, like Alice, wants to convert his master key into a session key and TGT and then forget  KB.
How could that work? (
See Homework Problem 8.)
 Alice wants to talk to Bob KDC TGT = {“Alice”, SA}KKDC  authenticator = {timestamp} SA  {“Bob”, KA-B, ticket to Bob }SA Workstat ion Alice Want Bob  invents key KA-B  decrypts TGT to get SA  decrypts authenticator  verifies timestamp  finds Bob’s master key KB  ticket to Bob = {“Alice”, KA-B}KB  Protocol 10-4.
Alice Uses a TGT to Get a Ticket to Bob  10.3.4 Interrealm Authentication  Suppose the world is partitioned into n different Kerberos realms.
It might be the case that princi - pals in one realm need to securely communicate with principals in another realm.
This is supported  by Kerberos.
The way it works is that the KDC in realm B can be registered as a principal in realm  A. This allows users in realm A to access realm B’s KDC as if it were any other resource in realm A,  and once a user in realm A can access realm B’s KDC, the user can ask for tickets to principals in  realm B.  Suppose Alice, in realm Wonderland , wishes to communicate securely to Dorothy in realm  Oz.
Alice’s workstation notices that Dorothy is in a different realm (because Dorothy will have a  name like Dorothy@Oz ).
Alice asks her KDC for a ticket to the KDC in realm Oz (see Protocol  10-5).
If the managers of Wonderland and Oz have decided to allow this, the KDC in Oz will be  registered as a principal in Wonderland.
Now Alice can communicate with Oz’s KDC, and Alice  can then ask the Oz KDC to give her a ticket to Dorothy.
The Oz KDC then issues a ticket for Alice  to talk to Dorothy (see Protocol 10-5).
 After Alice connects to Dorothy using this ticket, Alice and Dorothy will know they are talk - ing to each other, and they will share a key KA-D, the same as if they were in the same realm.
The  ticket to Dorothy will include the list of realms that were traversed to get from Alice to Dorothy.
10.3.5 KERBER OS 287 Alice Alice@Wonderland requests Oz@Wonderland Wonder land KDCOz KDCDoroth y {KAlice-Oz , ticket to Oz KDC} KAlice  ticket to Oz KDC, Alice@Wonderland requests Dorothy@Oz  {KAlice-Dorothy , ticket to Dorothy} KAlice-Oz  ticket to Dorothy  Protocol 10-5.
Interrealm Authentication  The Oz KDC will have a different secret for each realm that it connects to.
There will be a key  KOz-Wonderland stored for the principal “Oz KDC” in Wonderland.
And if the realm Oz cooperates  with the realm Mordor , the key for the Oz KDC in realm Mordor would be KOz-Mordor .
In the third  message in Protocol 10-5 , the Oz KDC will know it should decrypt the ticket with KOz-Wonderland ,  because the requestor name is Alice@Wonderland .
If there were several intermediate realms  Alice traversed to get to Oz, the Oz KDC will use its key in the last realm specified in the list of  realms.
 10.3.5 Making Password-Guessing Attacks Difficult  If Alice is a human, her master key KA is derived from a password.
There is no way in Kerberos to  prevent Eve (an eavesdropper) from doing a password guessing attack, based on observing the pro - tocol (Protocol 10 -3) in which Alice asks for a TGT.
The protocol involves the KDC sending a TGT  encrypted with KA.
The TGT will have recognizable fields so Eve will be able to recognize a likely  password.
 But it’s even easier to merely ask for a TGT for Alice than to try to eavesdrop during the  exchange where Alice asks for a TGT.
If the protocol for requesting a TGT required no authentica - tion of the requester, Trudy could easily say “Hey, I’m Alice; send me a TGT” and the KDC would  send Trudy the TGT encrypted with KA, and then Trudy could do an offline password guessing  attack.
 To make it harder for Trudy to just ask for a TGT, Kerberos added (in version 5) a mechanism  called preauthentication to prove that whoever is asking for a TGT for Alice knows her password.
10.3.6 288 T RUSTED INTER MEDIARIES  This is done by having Alice include a timestamp, encrypted with Alice’s master key KA, in the  TGT request.
The KDC will not issue a TGT unless the preauthentication field is valid (the time - stamp decrypted with KA is close enough to the current time).
 Another potential opportunity for password guessing is for Alice to request a ticket to human  Bob.
Bob’s master key, KB, is likely to be derived from a password, because Bob is a human.
Ker - beros prevents this attack by allowing principals in the database to be marked as do not issue tickets  to this principal .
 10.3.6 Double TGT Protocol  There might be cases where it would make sense for human Alice to request a ticket to human Bob,  for instance, so she could send him encrypted email, where she could include the ticket in the  header of the encrypted email.
There also might be cases where non-human principals might wish  to exchange their master secret for a TGT and session key.
For example, a server might have its  super-secret master key stored in protected hardware, so only accessible through the hardware and  thus slow to use.
Therefore, the server might want to obtain a time-limited, easily accessible session  key that it feels safe storing in software to use for most of its operations.
 Suppose Bob no longer knows his master key, because he forgot it after he used it to obtain a  TGT and session key.
Since the KDC does not keep track of session keys, if Alice asks for a ticket  to Bob, the KDC will give her a ticket encrypted with Bob’s master key.
But Bob will not be able to  decrypt the ticket, since he no longer knows his master key.
If Bob is a user at a workstation, the  workstation could at this point prompt him to retype his password, but this would be inconvenient for him.
 Kerberos assumes Alice knows that Bob is the kind of thing that is likely to have exchanged  his master key for a session key.
In a method unspecified in Kerberos, Alice is supposed to ask Bob  for his TGT (see Homework Problem 6 ).
Alice then sends Bob’s TGT as well as her own TGT to  the KDC (hence the name double TGT authentication ).
Since Bob’s TGT is encrypted under a  key that is private to the KDC, the KDC can decrypt it.
It then issues a ticket to Bob for Alice that is  encrypted with Bob’s session key rather than Bob’s master key.
 Another way to do this is for Bob to send the KDC the ticket Alice sent him and his TGT and  have the KDC reissue the ticket encrypted with Bob’s session key (see Homework Problem 10).
 10.3.7 Authorization Information  There is a field in Kerberos tickets and TGTs named AUT HORIZ ATION -DAT A. This can contain  Alice’s groups and roles.
In Microsoft’s version of Kerberos, this information is configured into the KDC under principal Alice’s information.
By default, the KDC puts this information into TGTs and
10.3.8 PKI 289  tickets.
Alice can ask for a subset of this information, which will limit her rights (principle of least  privilege), or she can ask for a subset of this information when delegating to Bob (see §10.3.8 Del- egation ).
 10.3.8 Delegation  Suppose Alice wishes for Bob to be able to act on her behalf.
For instance, Bob might be a backup  service that will scan her file system for any files that have changed since the last backup.
She  might want to give Bob only read permission on her files so he can’t corrupt them.
She could send  Bob her master secret.
Or she could send Bob her session key and TGT.
Security people generally  frown on giving out your secrets and letting someone impersonate you.
 To make tickets and TGTs more secure so only the requester can use them, Kerberos (version  4) always includes the IP address of the requester in the ticket or TGT.
This means that even if  Trudy were to somehow obtain Alice’s TGT and session key, Trudy would not be able to use these  unless she could impersonate Alice’s IP address.
In version 5, the IP address in a ticket or TGT is  optional, because requiring a specific IP address is too restrictive in cases where something had  multiple IP addresses or has moved.
 However, suppose Alice really wants Bob to be able to act on her behalf.
Kerberos allows  Alice to send her TGT to the KDC and request a new TGT with the name “Alice” but with Bob’s IP  address or without an IP address.
Also, Alice can limit what Bob can do on her behalf by requesting  that the list of groups and roles in the ticket’s AUT HORIZ ATION -DAT A be shrunk from the total list of  groups and roles Alice belongs to.
 10.4 PKI  A public key infrastructure (PKI) consists of the components necessary to securely distribute  public keys—certificates, a method to revoke certificates, and a method to evaluate a chain of certif - icates from public keys that are known and trusted in advance ( trust anchors ) to the target name.
 There have been some public key systems deployed that leave out components such as revocation,  or even certificates.
Instead, Alice might be configured with Bob’s public key, or Bob might send  his public key to Alice (rather than sending a certificate), and Alice will store it, taking it on faith  that she was talking to the real Bob during the interaction where he sent his public key.
The industry terminology for this strategy is either trust on first use , or leap of faith .
If an active attacker  noticed the initial message from Bob to Alice, the attacker could replace Bob’s public key with the
10.4.1 290 T RUSTED INTER MEDIARIES  attacker’s public key and then be able to impersonate Bob to Alice.
But in many cases, having Bob  just send Alice his public key is reasonably secure in practice.
 10.4.1 Some Terminology  A certificate is a signed message vouching that a particular name goes with a particular public key,  as in [ Bob’s public key is 829348 ]Carol.
If Carol signs a certificate vouching for Bob’s name and  key, then Carol is the issuer and Bob is the subject .
If Alice wants to discover Bob’s key, then  Bob’s name is the target .
A trust anchor is a public key that the verifier has decided is trusted to  sign certificates.
A chain of certificates is a sequence of certificates where each certificate is  signed with the key that is certified in the previous certificate.
If Alice is to believe a chain, the first  certificate would be one of Alice’s trust anchors.
If Alice is evaluating a certificate or a chain of cer - tificates, she is the verifier , sometimes called the relying party .
Anything that has a public key is  known as a principal .
 There is often other information in the certificate that might determine whether Alice consid - ers the certificate valid.
This information includes an expiration date, where to look for revocation information about this certificate, whether the subject should be trusted to act as a CA, policies  such as how carefully the subject was checked ( e.g., multifactor authentication, security clearance),  and name constraints.
 Sometimes public keys are distributed in the form of self-signed certificates .
A self-signed  certificate says [ Bob’s public key is 829348 ]829348 .
There is no security difference between a  self-signed certificate asserting what Bob’s public key is (signed by the key asserted as being Bob’s key) and an unsigned message claiming the same information.
The reason information is some - times exchanged in this form (self-signed certificates) is that there is already code to parse a certifi - cate.
But it’s important to realize that there is no security gained from the signature, and the only  reason to verify the signature is if your computer is bored and needs something to do.
Some people  claim that a self-signed certificate should be considered invalid if it is signed using an out-of-favor  cryptographic algorithm such as MD5.
If someone says that to you, just smile and nod.
 10.4.2 Names in Certificates  A certificate mapping a name to a key is an incredibly simple concept.
The most widely deployed  standard for certificates used on the Internet is PKIX (Public Key Infrastructure using X.509) (RFC  5280).
X.509 was jointly published by ITU, as ITU-T X.509, and by ISO, as ISO/IEC 9594-8.
 X.509 certificates map an X.500 name to a public key, rather than mapping a DNS name to a key.
 X.500 names are perfectly reasonable hierarchical names, similar in spirit to the DNS names used  on the Internet, but they have different syntax and are administered by a different organization.
This
10.5 WEBSI TE GETS A DNS N AME AND CERTIFICATE 291  makes things unnecessarily complicated when used on the Internet, since X.500 names are not used  for Internet applications.
 Some people were unhappy with basing Internet certificates on X.509 certificates, and they  started a working group within the IETF called SPKI (Simple Public Key Infrastructure).
If SPKI  certificates simply mapped DNS names to public keys, that would have been a nice simple form of  certificate for the Internet, but instead the SPKI group tried to be really innovative and have certifi - cates not be based on names at all, or have the names be relative to whoever is using the name ( e.g.,  Nicki’s cousin’s friend).
So SPKI did not catch on, and the Internet is using PKIX certificates.
 In PKIX, both the ISSUE R field and the SUBJE CT field in certificates are X.500 names.
Internet  applications do not care about X.500 names.
When a CA certifies a DNS name, PKIX uses an  extension known as the SUBJE CT ALTE RNAT IVE NAME field.
Usually, the party purchasing a certifi - cate creates the unsigned certificate and sends it to the CA to be signed.
So, to get the name  example.org certified, example.org  puts example.org into the SUBJEC T ALTERNAT IVE NAME field.
 There were other places one could put the DNS name, but they have been deprecated.
It was also  legal for the DNS name to be encoded in the CN field, which is a component of the X.500 name  known as “common name”).
There was another encoding where the components of the X.500 name  were labeled as DC (domain component), and a DNS name such as labs.example.com could be  encoded as DC= com , DC=example , DC=labs.
Having multiple places where the DNS name  could be encoded is a potential security vulnerability.
Before the CA signs a certificate, it needs to  verify that all the information in the certificate is accurate.
For instance, the CA should make sure that Bob really owns all names encoded anywhere in the certificate.
 Even more complicated, it would be nice to have certificates for human users.
Users have lots  of different types of names, e.g., email addresses, social media handles, and legal names.
In theory,  any of these could be stored in the SUBJEC T ALTERNATIVE NAM E of a PKIX certificate.
 Some people get excited about the syntax of certificates and complain about PKIX because  its syntax is ASN.1, which is somewhat verbose and a bit computationally difficult for a computer  to parse.
We don’t care about syntax, as long as anything necessary can be stated in the syntax and  the result is unambiguous.
But, interestingly, the people who tend to get upset about ASN.1 seem  not to complain about XML, which is even more verbose.
 10.5 WEBSITE GETS A DNS N AME AND CERTIFICATE  A website needs a DNS name.
It chooses a top-level domain from which to purchase the name ( e.g.,  .com , .org), contacts the registrar associated with that domain, and requests a name that it likes.
If  the registrar says the name is already purchased, the aspiring website needs to choose another  name.
When the website finds a name that it can purchase, say, example.org , it purchases the
292 T RUSTED INTER MEDIARIES 10.6  name.
The DNS registrar for .org adds an entry in its domain for example.org and puts in informa - tion for it such as its IP address.
 It would make a lot of sense if the same registrar from which the website is purchasing a DNS  name also issued a certificate.
While the website is purchasing the name, it has a secure connection  to the registrar so that it can send information such as a credit card number.
However, that is not  how it is done.
Instead, there are other organizations, unaffiliated with DNS registrars, that issue  certificates.
So, after purchasing the DNS name, example.org contacts a CA and says “My DNS  name is example.org and I’d like you to certify that my key is 947289143.”
How is the CA sup - posed to know this is the legitimate owner of the name example.org ?
 There is no standard for doing this, but one method is for the CA to look up the name exam - ple.org in DNS and find the associated IP address.
The CA sends a secret number to that IP address  (similar to when a bank sends a PIN to your cellphone).
If whoever is requesting a certificate for  example.org can then tell the CA what the secret number is, then the CA assumes that they can  receive at the IP address listed in DNS, and then the CA is willing to issue a certificate to the name  example.org .
 Note that if being able to receive at a particular IP address were secure, there would be no  need for certificates.
This is an example of a leap of faith for the CA.
Again, what would make the  most sense is for the DNS registrar to also be the CA associated with names that it issued, but the CA organizations want to have standards for how securely a CA stores its key, how often the CA  operators are drug tested, and so on, and the DNS organizations might not find these rules accept - able.
 10.6 PKI T RUST MODELS  In this section we will explore various strategies for getting trust anchors and finding chains of cer - tificates that lead to a target name.
This section is about how a verifier would operate under various  models.
 10.6.1 Monopoly Model  In this model, the world chooses one organization, universally trusted by all countries, companies, and organizations, to be the single CA for the world.
That organization’s public key is embedded in  all software and hardware as the single PKI trust anchor.
Everyone must get certificates from that  one organization.
10.6.2 PKI T RUST MODEL S 293  This is a wonderfully simple model, mathematically.
This is probably the model favored by  organizations hoping to be chosen as the monopolist.
However, there are problems with it:  • There is no single universally trusted organization.
 • Given that all software and hardware would come preconfigured with the monopoly organi - zation’s key, it would be infeasible to ever change that key in case it were compromised, since  that would involve reconfiguration of every piece of equipment and software.
 • It would be expensive and insecure to have a remote organization certify your key.
How  would they know it was you?
How would you be able to securely send them your public key?
 • Once enough software and hardware were deployed so that it would be difficult for the world  to switch organizations, the organization would have monopoly control and could charge  whatever it wanted for granting certificates.
 • The entire security of the world rests on that one organization never having an incompetent or  corrupt employee who might be bribed or tricked into issuing bogus certificates or divulging  the CA’s private key, since that one CA can impersonate the entire world.
 10.6.2 Monopoly plus Registration Authorities (RAs)  This model is just like §10.6.1 Monopoly Model , except that the single CA trusts other entities,  known as registration authorities (RAs), to securely check identities of certain principals and  obtain and vouch for their public keys.
For instance, an RA might be run by the IT department of a  company for vouching for the keys of that company’s employees.
The RA then securely communi - cates with the CA (because the CA and RA have a relationship and know how to authenticate each other), and the CA can then issue a certificate.
 This model’s advantage over the monopoly model is that it is more convenient and secure to  obtain certificates, since there are more places to go to get certified.
However, all the other disad - vantages of the monopoly model apply.
 RAs can be added to any of the models we’ll talk about.
Some organizations have been con - vinced that it is better for their organization to run an RA and pay a CA organization to create cer - tificates.
They believe the CA organization will be more expert at what it takes to be a CA ( e.g.,  protecting the CA private key and maintaining a revocation infrastructure).
However, in practice,  the CA just rubber-stamps whatever information is verified by the RAs.
It is the RA that has to do  the security-sensitive operations of ensuring the proper mapping of name to key.
The CA might be  better able to provide a tamper-proof audit trail of certificates it has signed, though.
 RAs are invisible to a verifier.
Certificates would still be signed by the CA, so verifiers only  see certificates issued by CAs.
10.6.3 294 T RUSTED INTER MEDIARIES  10.6.3 Delegated CAs  In this enhancement to other models, a trust anchor CA can issue certificates to other CAs, vouch - ing for their keys and trustworthiness as CAs.
Principals can then obtain certificates from one of the  delegated CAs instead of having to get a certificate directly from the verifier’s trust anchor CA.
 The difference between a delegated CA and an RA comes down to whether a verifier sees a  chain of certificates from a trust anchor to Bob’s name or sees a single certificate.
 10.6.4 Oligarchy  This is the model commonly used in browsers.
In this model, instead of having products preconfig - ured with a single trust anchor key, the products come configured with hundreds of trust anchors.
A  certificate issued by any of the trust anchors, or a chain originating with one of them, is accepted by  the browser.
Sometimes in such a model it is possible for the user to add or delete trust anchors.
The  oligarchy model has the advantage over the monopoly model that the organizations chosen as trust anchors will be in competition with each other, so the world might be spared monopoly pricing.
 However, it is likely to be even less secure than the monopoly model:  • In the monopoly model, if the single organization ever has a corrupt or incompetent  employee, the entire security of the world is at risk.
In the oligarchy model, any of the trust  anchor organizations getting compromised will put the security of the world at risk.
 • The trust anchor organizations are trusted by the product vendor, not necessarily by the user.
 Why should the vendor decide which organizations the user should trust?
 • It might be easy to trick a user into adding a bogus trust anchor into the set.
This depends on  the browser implementation.
One implementation that was common, but no longer behaves  this way, displayed a pop-up box if the server the browser was talking to presented a certifi - cate signed by a public key that was not in the browser’s list of trust anchors.
The language in  the pop-up box was much more confusing than our rewording of the questions:  Warning.
This was signed by an unknown CA.
Would you like to accept the certificate any - way? (
The user will almost certainly say OK.)
 Would you like to always accept this certificate without being asked in the future? (
OK.)
 Would you like to always accept certificates from the CA that issued that certificate? (
OK.)
 The first OK says the user is happy to go to that site anyway.
The second OK says the user is  willing to always trust that certificate for that one site.
The third OK installs the unknown  CA’s public key into the set of trust anchors.
It would be an interesting psychology exercise to  see how outrageous you can be before the user stops clicking OK.
Would you like to always  accept certificates from any CA? (
OK.)
Since you’re willing to trust anyone for anything,
10.6.5 PKI T RUST MODEL S 295  would you like me to make random edits to the files on your hard drive without bothering you  with a pop-up box? (
OK.)
 Note that if a user is sufficiently sophisticated and careful, she can ask for information about  the certificate before clicking OK to accept it.
She will be informed of the name of the signer,  say, Mother Teresa (the most trustworthy imaginable signer).
But this does not necessarily  mean it was really signed by Mother Teresa.
It just means that whoever signed it (for exam - ple, SleazeInc ) put the string Mother Teresa into the ISSUE R NAM E field.
 • Users will not understand the concept of trust anchors.
If they have been assured that the  application they are using does encryption, they will assume that it will be secure even if  they’re using a public workstation, perhaps in a hotel room or at an airport.
Although it will  always be an issue if a user can be tricked into using a public workstation with malicious  code, it would be easier for the previous user of the workstation to modify the set of trust  anchors (probably not a privileged operation) than to change the software.
 • There is no practical way for even a knowledgeable user to be able to examine the set of trust  anchors and tell if someone has modified the set.
Browsers today come shipped with hun - dreds of trust anchors.
A user can look at the set of trust anchors.
Each entry has a name and  a key, but someone could delete the key of TrustworthyInc and put in a new key claiming that  it belongs to TrustworthyInc .
You might even be able to look at the public keys, but what user  will be sufficiently paranoid to have printed out all the certificate hashes displayed in the list  of trust anchors in order to compare them with the set currently displayed?
 Today, most browser implementations make it difficult or impossible for a user to modify the set of  trust anchors, and users just have to trust that the browser vendor is making sure the list only has  trustworthy trust anchors.
Many company IT departments manage the list of trust anchors for their  employees’ devices.
 10.6.5 Anarchy Model  This model is also sometimes called the web of trust .
Each user, say Alice, is responsible for con - figuring some trust anchors, for instance, public keys of people she has met and who seem trustwor - thy and who send her their public key in some reasonably secure way.
 In this model, anyone can sign certificates for anyone else.
In many gatherings of nerds, there  is a PGP key-signing party with some sort of ritual where keys are exchanged in email first, and  then people get up and state their name and a hash of their key, and other people can vouch for that  person having that name.
People at the party can then sign certificates for the person who has just  announced his name and key.
Some organizations volunteer to keep a certificate database into  which anyone can deposit certificates.
These databases can be read by anyone.
To get the key of  someone whose key is not in user Alice’s set of trust anchors, she can search through the public
10.6.6 296 T RUSTED INTER MEDIARIES  database of certificates to try to find a path from one of her trust anchors to the target name.
This  eliminates the monopoly pricing, but it is really unworkable on a large scale:  • The database would get unworkably large if it were deployed on Internet scale.
If every user  donated, say, ten certificates, the database would consist of billions of certificates.
It would be  impractical to search through the database and construct paths.
 • Assuming somehow Alice could piece together a chain from one of her trust anchors to the  name Bob, how would she know whether to trust the chain?
Say Carol (her trust anchor)  vouches for Ted’s key.
Ted vouches for Gail’s key.
Gail vouches for Ken’s key.
Ken vouches  for Bob’s key.
Are all these individuals trustworthy?
 As long as this model is used within a small community where all the users are trustworthy, it will  appear to work.
However, on the Internet scale, when there are individuals who will purposely add  bogus certificates and naive users who will be tricked into signing bogus certificates, it would be  impossible to know whether to trust a path.
Some people have suggested that if you can build mul - tiple chains to the target name you can be more assured of the trustworthiness.
But once someone  decides to add bogus certificates, they can create arbitrary numbers of fictitious identities and arbi - trary numbers of certificates signed by those entities.
So, sheer numbers will not be any assurance  of trustworthiness.
 10.6.6 Name Constraints  The concept of name constraints is that the trustworthiness of a CA is not a binary value where a  CA would either be completely untrusted or trusted for everything.
Instead, a CA should only be  trusted for certifying some subset of the users, and in particular, the name by which I know you  implies whom I trust to certify the key for that name.
If I want the key for the name  roadrunner@socialnetworksite.com , I would trust a CA associated with  socialnetworksite.com to certify the key.
If I want the key for Bob.Smith@example.com , I  would trust a CA associated with example.com to certify the key for that name.
If I want to know  the key for creditcardnumber4928749287@bigbank.com , I would trust a CA associated with  BigBank.com to certify the key for that name.
These names might all refer to the same human, but  that is irrelevant.
User Alice might use different public keys for each of her names, or she might use  the same public key for some of her names.
But that does not affect whom I should trust to certify  the binding of a particular name to a key.
 Note: Usually the DNS name of the registrar that can vouch for a name ( e.g., a social media  handle) will somehow be derivable from the name the human is using, even if the human name is not the same syntax as an email address.
10.6.7 PKI T RUST MODEL S 297  10.6.7 Top-Down with Name Constraints  This model (top-down) is similar to the monopoly model in that everyone must be configured with  a single trust anchor—the key of the one chosen root.
That root CA delegates to other CAs, mean - ing that the root CA might sign a certificate to some other CA, but the certificate indicates that this CA is only trusted for issuing certificates in a portion of the namespace, e.g., names of the form  *.com , or *.edu .
 In a hierarchical namespace such as DNS, there would be a CA associated with each node in  the namespace tree.
The CA associated with a parent node would certify the key of the CA associ - ated with the child node and also indicate that this child CA is only trusted to issue certificates in the tree rooted at the name of that child node.
 With the top-down name-constraint model, it allows the organization associated with a  namespace to have policies that CAs in their organization should follow.
For example, the organiza - tion associated with *.cia.gov is likely to want to have different policies for CA operators than the  organization associated with the namespace of *.mit.edu , which will likely be managed by playful  undergraduates unlikely to pass a drug test.
 In this model, it is easy to find the path to a name (just follow the namespace from the root  down).
But it has the other problems of the monopoly model, in that everyone has to agree upon a  root organization, the security of the entire world depends on that one organization never being  compromised, and that organization and its key would be prohibitively expensive to ever replace.
 10.6.8 Multiple CAs for Any Namespace Node  In any of these models (the ones we have described so far and the ones we will describe), it is pos - sible to have multiple CAs representing any node in the namespace.
For example, in the top-down  model, if there are two nodes in competition to provide the service of being the root CA, then veri - fiers would either be configured with both CAs as trust anchors, or principals certified by a CA  associated with that namespace will need to be certified by both CAs, and the principal will need to  ask the verifier which CA is its trust anchor.
 For links in a certificate chain other than the first link, the verification will work with which - ever CA has been delegated to.
 10.6.9 Bottom-Up with Name Constraints  This is the model we recommend.
It was originally proposed for Digital’s security architecture in  the late 1980s by Charlie Kaufman.
This model is created with two enhancements to the model in  §10.6.7 Top-Down with Name Constraints .
The two enhancements are:
10.6.9.1 298 T RUSTED INTER MEDIARIES  • up-links , where a CA associated with a child node in the name space hierarchy certifies the  key of the CA associated with the parent node  • cross-links , where any node in the namespace can certify the key of the CA associated with  any other node in the namespace  In this model, Alice’s trust anchor can be any node in the namespace.
For example, if she works at  a company, the IT department is likely to configure her device with the CA associated with her  company’s name as her trust anchor.
If Alice isn’t associated with a company, she might copy a list  of trust anchors from a place she trusts or not even notice when her browser comes pre-configured.
 The rule for finding the key of a target name is that Alice starts at her trust anchor and, if that CA  represents a node that is an ancestor (in the namespace) of the target name, she follows down-links.
 Otherwise, she tries to find a cross-link to an ancestor of the target name.
If so, she follows that  cross-link to the ancestor node in the namespace and then follows down-links to the target name.
Or  if there is no cross-link, she follows the up-link to go up one level in the namespace.
If she’s at an ancestor of the target name, she follows down-links.
If not, she looks for a cross-link to an ancestor  of the target name.
 10.6.9.1 Functionality of Up-Links .
 Up-links are shown in Figure 10 -6.
 .com  xyz.com a.com  labs.xyz.com sales.xyz.com .org root  .tv  Figure 10-6.
Up-links Allow Starting with Any Node as Your Trust Anchor  There are two advantages up-links provide over the top-down model:  • A root CA is no longer hard to replace.
If the root CA misbehaves or its key gets compro - mised, the root key can be replaced by a different key reasonably painlessly, since it is only  the child nodes ( e.g., the CAs associated with the top level domains in DNS) that need to  revoke the old key and issue a new certificate.
The vast majority of principals on the Internet  would not have the root CA as their trust anchor, so replacing the root organization, or chang - ing the key for that root, does not affect their key or their traversal rules.
 • If the target name and trust anchor are in the same organization’s namespace subtree, a com- promised CA outside that namespace subtree will not be on the path of certificates between
10.6.9.2 PKI T RUST MODEL S 299  principals in that namespace.
For example, for principals in xyz.com (assuming they are con - figured with a CA associated with a node in the namespace xyz.com ), if .com or the root  were malicious, it could not impersonate principals in xyz.com to each other.
 10.6.9.2 Functionality of Cross-Links  There are two advantages gained by cross-links:  • There is no need to wait for the entire PKI of the world to be connected.
Organization a.com  can deploy its own internal PKI, and xyz.com can deploy its own internal PKI.
If the two  organizations want the principals in their namespace to be able to find keys for principals in  the other organization, they just need to cross-certify each other’s keys. (
See Figure 10-7.)
 a.com xyz.com  Figure 10-7.
Cross-links Connect Two Organizations  • Another advantage of cross-links is that they allow bypassing CAs that are not trusted. (
See  Figure 10-8.)
If the organization associated with the name tree under sales.xyz.com creates  a cross-link to a.org , then the path of CAs will go directly from sales.xyz.com to a.org .
For  a principal whose trust anchor is sales.xyz.com or below, compromise of any of the CAs  associated with xyz.com , .com , root, or .org will not affect the security of the chain from the  namespace sales.xyz.com to a.org , since those CAs will not be in the chain.
Note that the  cross-link created by sales.xyz.com will not create a bi-directional cross-link unless a.org  creates its own cross-link to sales.xyz.com .
Principals in the namespace under a.org will  have to follow the complete chain of certificates from a.org to its parent .org, to its parent  root, and then down the namespace (through .com and xyz.com ) to get to sales.xyz.com .
 .com  xyz.com a.org  labs.xyz.com sales.xyz.com .org root  Figure 10-8.
Cross-links for Added Security
10.6.10 300 T RUSTED INTER MEDIARIES  To review, the advantages of the bottom-up model are:  • It is easy to find out if a path exists.
 • The policy of assuming that the name by which something is known implies which CA you’d  trust to certify the name is something people can understand, and it is sufficiently flexible and  simple that it might actually work.
 • PKI can be deployed in any organization independently of the rest of the world.
There is no  reason to pay a commercial CA to build a PKI for your own organization.
There is no reason  to wait for the entire world-encompassing PKI to be in place before you can use PKI in your  own organization or between cooperating organizations.
 • Since CA certificate chains between principals in your own organization never go outside of  your own organization, security of what is presumably the most security-sensitive opera - tion— authenticating users in your own organization—is entirely in your own hands.
Com - promise of any CA outside of your own organization will not allow anyone to impersonate one of your own principals to another in your name space.
 • Replacing any key is reasonably easy.
If a root service’s key gets compromised, then it only  affects the top CA of each of the root service’s customers.
Each such CA has to revoke the old  certificate it issued to the root service and issue a new certificate containing the new key, and  automatically all the users in the CA’s subtree are using the new key in place of the old key.
 • No organization gets so entrenched that it can start charging monopolistic prices.
It is easy to  replace any key, and competition is always possible (see §10.6.8 Multiple CAs for Any  Namespace Node ).
 10.6.10 Name Constraints in PKIX Certificates  PKIX has a field called NAM E CONST RAINT S, which allows the issuer CA to specify which names  the subject CA is trusted to certify.
The name constraints can specify names of various forms, such  as X.500 names, DNS names, or email addresses.
For simplicity, let’s just assume that the name constraints are DNS names, and we will assume the CA is associated with a name in the DNS hier - archy.
The NAM E CONST RAINTS field can contain allowed names with wildcards ( e.g.,  *.example.com , which means any name that ends in .example.com ) and disallowed names, also  with wildcards.
 Any of the models we’ve mentioned can be enforced with name constraints.
To build the  anarchy model or the oligarchy model, there would be no name constraints.
In the top-down model,  a certificate in which a CA certifies the key of the child CA in the DNS hierarchy would state only  trusted for this name and everything below that name .
In the bottom-up model, a child or  cross-certificate would specify only trusted to certify names in the subtree below the DNS name
10.7 BUILDING CERTIFICATE CHAINS 301  that the subject CA represents .
A parent certificate in the bottom-up model (an up-link) would con - tain the constraint any names except the DNS name I represent and below, or any other names I  have issued a cross-link to .
 We’d still recommend mostly building the bottom-up model, but there is some amount of  flexibility that the strict up*–cross once–down* algorithm might not give.
For instance, an organi - zation might have a cross-link to other-org.com , but realizing that other-org.com also keeps  cross-certificates to yet-another.com and still-another.com , the name constraint in the  cross-certificate might say that the subject would be trusted to certify names in the namespaces of  any of { other-org.com , yet-another.com , still-another.com }.
Or there might be several root  organizations that all cross-certify each other, with each having certified some subset of the organi - zations.
Since two organizations might not have been certified by the same root, it might be neces - sary to go up to the root, then find a path across a mesh of roots to the target’s root, and then go  down.
This could be accomplished by having roots cross-certify each other using the name con - straint trusted for all names .
The further one gets from the bottom-up model, and the closer one  gets to the anarchy model, the more complex it will be to search all valid paths.
 Although PKIX certificates can contain name constraints, they are rarely used.
If a CA’s cer - tificate contains name constraints, verifiers are supposed to check whether any certificates that CA  has issued follows the name constraints, but not all verifiers do this.
 10.7 BUILD ING CERTIFICATE CHAINS  In all these models, there is the problem of how Alice, the verifier, obtains the relevant certificates for the target name Bob.
There are various strategies.
In email, a strategy was for a signed email  message from Bob to contain a certificate chain to his key, hopefully including one of Alice’s trust anchors.
When Alice receives this, she can verify the chain and perhaps cache his public key (assuming Bob’s chain includes one of Alice’s trust anchors).
 If there were directories associated with each CA, Alice could follow the path from her trust  anchor(s) asking each CA along the path for up, down, or cross certificates.
Note the actual CA  need not be online.
A directory just needs to store information signed by the offline CA.
The direc - tory would not be able to give a verifier an incorrect key because the directory would not be able to  forge the CA’s signature.
 In protocols such as IPsec and TLS, the assumption is a top-down/oligarchy model.
During  the initial handshake, Alice informs Bob what her trust anchors are, and Bob sends her a chain of  certificates from one of her trust anchors.
302 T RUSTED INTER MEDIARIES 10.8  10.8 REVOCAT ION  If someone realizes their key has been stolen, or if someone gets fired from an organization, it is  important to be able to revoke their certificate.
A certificate typically has an expiration date, but  since it is a lot of trouble to issue a certificate (especially if the CA is off-line), certificate lifetime is  typically years, which is too long to wait for a certificate to expire if it needs to be revoked.
 This is similar to what happens with credit cards.
They, too, have an expiration date.
They are  usually issued to be good for a year or more.
However, if a credit card is stolen, it is important to be  able to revoke its validity quickly.
Originally, the credit card companies published books of bad credit card numbers and distributed these books to all the merchants.
Before accepting the card, the  merchant would check to make sure the credit card number wasn’t listed in the book.
This mecha - nism is similar to a CRL (certificate revocation list) mechanism (see §10.8.1).
 Today, the usual mechanism for credit cards is that for each transaction, the merchant calls an  organization that has access to a database of invalid credit card numbers (or valid credit card num - bers), and the merchant is told whether the credit card is valid (and perhaps, if there is sufficient  credit for the purchase).
This is similar to an OLRS (on-line revocation service) mechanism (see  §10.8.2 ).
The IETF standard protocol for requesting revocation status of a certificate is called  OCSP (on-line certificate status protocol) and is documented in RFC 6960.
 Why do certificates have expiration dates at all?
Assuming there is a method of revoking  them, the only security reason to have them expire is to make the revocation mechanism more effi - cient, for instance, by avoiding the CRL becoming overly large.
Cynics might think the reason for  designing certificates with expiration dates is so that companies that want to collect revenue from  issuing certificates can collect multiple times for the same certificate.
The PKIX certificate format  does allow very long lifetimes, such as until 31 December 9999, which is, in practice, the same as not having an expiration date.
 There might be cases where it would make sense for a certificate to have a very short lifetime,  such as if you know this is only to be used for a temporary period.
For example, the certificate  might be a visitor badge valid for a week or a one-day parking permit.
It is simpler to issue a  short-lived certificate than to have to revoke the certificate.
 10.8.1 CRL (Certificate Revocation list)  The basic idea of a CRL is that the CA periodically issues a signed list of all the revoked certifi - cates.
This list must be issued periodically, even if no certificates have been revoked since the last CRL, since otherwise an attacker could post an old CRL (from before his certificate was revoked).
 If a timestamped CRL is issued periodically, then the verifier can refuse to honor any certificates if
10.8.1 REVOCAT ION 303  it cannot find a sufficiently recent CRL.
Each CRL contains a complete list of all the unexpired,  revoked certificates.
 Delta CRLs are intended to make CRL distribution more efficient.
Let’s say you want to  have revocation take effect within one hour.
With a CRL, that would mean that every hour the CA  would have to post a new CRL, and every verifier would have to download the latest CRL.
Suppose  the CRL was very large, perhaps because the company just laid off ten thousand people.
Every  hour, every verifier would have to download a huge CRL, even though very few certificates had  been revoked after that layoff.
 A delta CRL lists changes from some full CRL.
The delta CRL would say “ These are all the  certificates that have been revoked since 10 AM 7 February”, which is the timestamp of the full  CRL that this delta CRL is referencing.
The delta CRL would hopefully be very short, often con - taining no certificates.
Only when the delta CRL gets large would it be useful to issue a new full  CRL.
 An idea we1,2 designed for making the CRL small again after it has become too large is what  we1,2 call first valid certificate .
This scheme also allows certificates to not have a predetermined  expiration date when issued.
Instead, they are only marked with a serial number, which increases  every time a certificate is issued (or the issue time could be used instead of a serial number).
A CRL  would have one additional field that is not included in X.509.
The CRL would contain a FIRST  VALID CERT IFIC ATE field.
Any certificates with lower serial numbers (or issue times) are invalid.
 Certificates in our scheme would have no predetermined expiration date.
As long as the CRL  is of manageable size, there is no reason to reissue any certificates.
If it looks like the CRL is get - ting too large, the CA organization issues a memo warning everyone with certificate serial numbers  less than some number n that they’ll need new certificates by, say, a week from the date of the  memo.
The number n is chosen so that few of the serial numbers in the current CRL are greater than  n. Revoked certificates with serial numbers greater than n must continue to appear in the new CRL,  while valid certificates with numbers greater than n do not have to be reissued.
Some time later, say,  two weeks after the memo is sent, the CA issues a new CRL with n in the FIRST VAL ID CERT IFICATE  field.
Affected users (those with serial numbers less than n) who ignored the memo will thenceforth  not be able to access the network until they get new certificates, since their certificates are now  invalid.
 There are cases when even with this scheme it might be reasonable to have expiration dates in  certificates.
For example, at a university, students might be given certificates for use of the system on a per-semester basis, with a certificate that expires after the semester.
Upon paying tuition for  the next semester, the student is given a new certificate.
But even in those cases, it may still be rea - sonable to combine expiration dates in some certificates with our scheme, since our scheme would  allow an emergency mass-revocation of certificates.
10.8.2 304 T RUSTED INTER MEDIARIES  10.8.2 Online Certificate Status Protocol (OCSP)  OCSP (RFC 6960) is a protocol for querying an OLRS (on-line revocation server) about the valid - ity of individual certificates.
If Bob is verifying Alice’s certificate, Bob would ask the OLRS if  Alice’s certificate is valid.
You might think that introducing an on-line server into a PKI eliminates  an important security advantage of public keys because you now have an on-line trusted service.
But the OLRS is not as security sensitive as a CA (or KDC).
The worst the OLRS can do is claim  that revoked certificates are still valid, so the damage is limited.
It does not have a vulnerable data - base of user secrets (like a KDC does).
Its key should be different from the CA’s key, so if its key is  stolen, the CA’s key would not be compromised.
Surprisingly, it is not uncommon to have the  OLRS key be the same as the CA key.
 An OLRS variant is for Alice to query the OLRS server about her own certificate.
The OLRS  response will be signed (by the OLRS server) and timestamped, so Alice can send the OLRS response along with her certificates to Bob.
Assuming Alice will be visiting many resources, this  saves the OLRS the work of talking to multiple verifiers, saves the verifiers the work of querying  the OLRS, and saves the network from the bandwidth used by having multiple verifiers query the  OLRS.
 Bob can decide how quickly revocation should take effect.
If he wants revocation to take  place within, say, one hour, then he can insist that Alice’s OLRS response be time-stamped within the last hour.
If he complains Alice’s OLRS response isn’t sufficiently recent, then Alice can obtain  a new one, or Bob could query the OLRS himself.
 Alice can proactively refresh her OLRS response, knowing that most servers would want one  that is, say, less than an hour old.
Then the round-trip querying of the OLRS does not need to be  done at the time of a transaction.
 Even with Bob (instead of Alice) querying the OLRS, it is possible to do caching and refresh - ing.
Bob can keep track of the certificates in the chain for users that tend to use his resource and  proactively check with the OLRS to see if any of them have been revoked.
 Note that there’s even a bigger performance gain if the server uses this strategy and collects  an OLRS response to use with multiple clients.
 10.8.3 Good-Lists vs. Bad-Lists  The standards assume that the CRL will contain all the serial numbers of bad certificates, or that the OLRS would have a database of revoked certificates.
This sort of scheme is known as a bad-list  scheme, since it keeps track of the bad certificates.
 A scheme that keeps track of the good certificates is more secure, however.
Suppose a CA  operator is bribed to issue a certificate, using a serial number from a valid certificate, and that no  audit log indicates that this bogus certificate has been issued.
Nobody will know this certificate
10.9 OTHER INFORMA TION IN A PKIX C ERTIFICATE 305  needs to be revoked, since no legitimate person knows it was ever issued.
It will not be contained in  the CRL.
 Suppose instead that the CRL contains a list of all the valid certificates (and not just serial  numbers, but hashes of the certificate for each serial number).
Then the bogus certificate would not  be honored, because it would not appear in the list of good certificates.
 There are two interesting issues with good-lists:  • The good-list is likely to be much larger than the bad-list and might change more frequently,  so performance might be worse than with a bad-list.
 • An organization might not want to make the list of its valid certificates public.
This is easily  answered by having the published good-list contain only hashes of valid certificates, rather  than any other identifying information.
 Note that usually the good-list or bad-list, especially if publicly readable, will contain only serial numbers and hashes of the certificates rather than any other identifiable information.
Then the only  information divulged is the number of valid certificates (in the good-list case) or invalid certificates  (in the bad-list case).
There is no reason to believe that the count of good certificates is more secu - rity sensitive than the count of bad certificates.
If for some reason the count was security sensitive, the revocation service could claim additional fictitious certificates as being valid or invalid.
 The X.509 standard says that it is not permitted to issue two certificates with the same serial  number and that all certificates issued must be logged.
But we shouldn’t assume that a bad guy  would be hindered from issuing bogus, unaudited certificates just because it would violate the spec - ification!
The standard assumes that a CA will be run in such a way that nobody would be able to  sneak in and have it create a certificate with a duplicate serial number.
This could be enforced to a  high probability with hardware.
 10.9 OTHE R INFORMATION IN A PKIX C ERTIFICATE  Some of the fields in a certificate vouching for Bob’s key are what you’d expect.
SUBJECT NAME is  where you’d expect to see Bob’s name.
The other obvious field in a certificate is the CA’s signature.
 Given that there might be a lot of different signature algorithms, a signature needs to include both a  SIGNATUR E TYPE that would specify the signature algorithm, and the actual SIGNATUR E. However,  there are other fields in the PKIX certificate.
Given that the PKIX format is extensible, new fields  can always be added.
 • SUBJECT PUBLIC KEY INFO.
This is an important field.
It specifies two things: the type of key  being certified, e.g., RSA key or ECDSA key, and the value of the key.
10.10 306 T RUSTED INTER MEDIARIES  • VALIDIT Y INTE RVAL .
This specifies both a NotBefore and a NotAfter time in units of sec - onds.
There is no IssuedTime.
You might think that the NOTBEFORE field would be when the  certificate was signed, but the PKIX format allows post-dating a certificate (signing it on  Monday, but saying it will not be legal until Friday).
Also, interestingly, there are two differ - ent representations of time, both represented as ASCII strings.
The first representation is  what is used for any dates up until 2050—YYMMDDHHMMSSZ, where YY is the last two  digits of the year, MM is the month represented as two digits, DD is the day represented as  two digits, HH is the hour in 24-hour time, MM is the minute, SS is the second, and Z is a  constant, short for Zulu, which means what used to be called Greenwich Mean Time.
Since  there are only two digits in the year, this format originally would have had a Y2K problem in  2000.
The time committee bought themselves an extra 50 years by saying that if the value of  the two digits was greater than or equal to 50, the year would be 19YY, and if the digits were  less than 50, the year would be 20YY.
The time representation committee people could have  assumed they’d all be retired by 2050, and left it up to some future generation to notice the problem and do something.
Instead, the forward-looking visionaries came up with a different  representation for dates after 2050.
That representation is YYYYMMDDHHMMSSZ, where the year is four digits.
So, the next panic for time representation won’t come until the year  9999, when all us of will be retired, so we don’t need to worry about it.
 • USAGE RESTRIC TIONS .
This contains name constraints (see §10.6.6 Name Constraints ).
It  also contains restrictions such as whether the key should only be used for encryption, for  signing (and if so, what types of information it should be trusted to sign), or for authentica - tion.
There is a bit in usage restrictions that indicates whether the subject is allowed to be a  CA.
If it is allowed, then a certificate signed by the subject name can be trusted to be a link in  a certificate chain.
And the usage restrictions also allow specifying how long the chain from  the subject is allowed to be.
 • WHERE TO FIND REVOCA TION INFORM ATION .
This indicates where revocation information for  this certificate can be found.
 10.10 ISSUES WIT H EXPIRED CERTI FICATES  When a public key is used for real-time authentication, the only thing that matters is that the certif - icate is valid at the time it is being used.
But suppose a public key is signing something.
Should the  signature remain valid if the certificate has expired?
What if the key has been revoked?
 Assuming a signature includes a date, it might be tempting to say that the signature should be  valid if the key was valid at the time the document was signed.
However, suppose the document is
10.11 DNSSEC (DNS S ECURITY EXTENSIONS ) 307  one in which Alice signs over the deed to her house to Bob.
At the time she signed it, her certificate  was current, and it was not revoked.
So Bob trusts Alice’s signature on the deed.
But then suppose  ten years later, Alice says to Bob, “Why are you living in my house?”
Bob shows her the deed that  she signed ten years ago.
Alice can then say, “I reported my key stolen last week.
Whoever stole the  key must have created that document and backdated it to ten years ago.”
 A solution is to have a third party, usually referred to as a notary , sign the document any time  while Alice’s key is still valid.
The notary is attesting to the fact that Alice’s key was valid at the  time the notary signed the document.
But the notary’s key might also expire or get revoked.
So,  multiple notaries should sign the document.
A notary can revalidate the document even after Alice’s  key has been revoked or has expired, provided that the new notary trusts a previous notary (and its  key) that has signed the document.
Even after Alice’s key expires, or some of the notary keys have expired or been revoked, the signatures on the document can still be validated provided that at least one notary who has signed it still has a valid key.
 10.11 DNSSEC (DNS S ECUR ITY EXTEN SIONS)  DNS names are hierarchical.
The details of DNS are somewhat arcane, due to the history of getting it deployed or optimizing its performance.
We will simplify the concepts in DNS (for instance, we  will assume each name has one IP address, and there is one DNS server for each domain in the  DNS hierarchy) and focus instead on the conceptual aspects of DNSSEC.
 There is an online server associated with each DNS domain.
The DNS infrastructure allows  someone to look up things about a DNS name, such as its IP address.
For example, information  about the name dell.com would be found by querying the server responsible for storing informa - tion and answering queries about the domain consisting of names of the form *.com .
There are lots  of things DNS stores about a particular name in addition to the IP address, for example, the DNS information about the name  dell.com would probably include the name of the email server that  handles mail for email addresses of the form user@dell.com .
 If Alice does not authenticate the DNS server when she looks up information about a name,  something impersonating a DNS server can give her false information.
Even if Alice were some - how to authenticate the DNS server she is querying, it is still better to minimize the amount of trust  in an online server and have the information the online server sends you be digitally signed by  something that is better protected and offline.
For performance you would want many online serv - ers that you could query about names of the form *.com , and it would be better not to require them  all to be physically secured.
 The main security features of DNSSEC are that it adds a public key to each name’s informa - tion, and it enables information in DNS to be digitally signed, with those digital signatures stored in
10.11 308 T RUSTED INTER MEDIARIES  DNS.
The digital signature is preferably created by a physically secured offline entity, and the DNS  server storing the information and answering queries about names in a domain will not know the  private key for the domain.
 DNSSEC is a bit more complicated than you’d expect because  • People want to allow information about some names in a domain to be signed, and others not  to be signed.
The probable motivation for this is so that the owner of a name could be charged  by the domain manager for requesting that its information be digitally signed.
 • People have decided that it should be difficult to figure out all the names in a domain.
You are  not allowed to ask for a list of names in the domain.
You can ask for information about a spe- cific name, and the reply will either be information for that name or that the specific name does not exist in the domain.
 With the simplest imaginable design, if the information about a name was signed, the DNS server  would return the signed information, and otherwise, return information and say “This was not  signed.”
If that were the design, a dishonest DNS server could give you false information even if  the information for the name was signed, because the DNS server can say “Here is the information,  and it wasn’t signed.”
Likewise, a dishonest DNS server could claim a name does not exist in the  domain even if it does.
 There are three cases that DNSSEC must accommodate:  • The name does not exist in the domain.
 • The name exists, but the information for that name is not signed.
 • The information for the name is signed.
 DNSSEC has a clever mechanism for preventing a dishonest DNS server from lying.
Associated  with a domain (in which at least some of the entries are signed) is a list of hashes of names for  which information is signed.
This list of hashes is sorted numerically, e.g., h1, h2, h3,…, h , where n  h1< h2< h3 <…< h .
The DNS manager (which signs information for that domain) signs each n  adjacent pair of hashes, e.g., it will sign h1|h2, and sign h2|h3, ….
If a query is made for a particular  name, and the information is signed, the DNS server returns the signed information.
However, if  that name does not exist or the information for that name is unsigned, the DNS server hashes the  name, determines which hash range the name belongs in ( e.g., the hash of the name might fit  between h7 and h8), and returns the signature on h7|h8.
This proves that any name whose hash fits in  that range either does not exist in the zone or is unsigned.
A dishonest DNS server cannot trick you  into believing incorrect information about an entry that was signed, but for entries that are not  signed, a dishonest DNS server can claim the name doesn’t exist, even if it does, or it can give you  false information for the name, whether the name exists or not.
 DNSSEC could work as a PKI.
It would allow Alice to find Bob’s key through querying  online servers (unlike the way TLS and IPsec work today, which depend on Bob sending Alice its  certificates when Alice tries to talk to Bob).
DNSSEC is a top-down model, in that everyone is
10.12 HOMEWORK 309  configured with the key for the root domain, and by traversing through each child domain, you dis - cover the key for that domain.
In theory, it could implement the bottom-up model if a domain  allowed an entry for the key for its parent (an up-link) or allowed an entry for a cross-link.
The orig - inal version of DNSSEC allowed storing keys for server names, and it would be conceptually easy  to have email and authentication keys for users at an organization to be stored in a DNS directory  managed by that organization.
Unfortunately, the DNSSEC committee removed the specification  for how to represent any keys stored in DNS other than DNS signing keys for child directories.
There are ongoing efforts to add them back in, at least in certain cases.
DANE (DNS-based Authen - tication of Named Entities), RFC 6698, is one such effort.
 10.12 HOMEWORK  1.
What could a malicious CA do compared with a malicious KDC?
Consider scenarios such as  decrypting conversations between Alice and Bob or impersonating Bob to Alice.
 2.
In Protocol 10-1, the first message to the KDC “Alice wants to talk to Bob” is not crypto - graphically protected, so an active attacker could change the message to “ Alice wants to talk  to Trudy.”
How does the protocol ensure that Alice will not be tricked into thinking Trudy is  Bob?
 3.
Design a variant of Kerberos in which the workstation generates a TGT rather than asking the  KDC for a TGT.
Hint: The TGT will be encrypted with the user’s master key rather than the  KDC’s master key.
How does this compare with standard Kerberos in terms of efficiency,  security,…?
 4.
Suppose there were two different services: TGT-server, that only gives out TGTs, and  Ticket-server, that gives out tickets.
Assume only human users get TGTs, other principals  keep their master key and never get TGTs, and tickets are never granted to human principals.
 What information needs to be configured into the TGT-server?
What information needs to be configured into Ticket-server?
 5.
Suppose all principals get TGTs and forget their master key, and the double-ticket protocol as described in §10.3.6 Double TGT Protocol is used.
What information needs to be configured  into the TGT-server?
What information needs to be configured into the Ticket-server?
 6.
In §10.3.6 Double TGT Protocol , we say that Alice asks Bob to send her his TGT.
If Alice  knows Bob’s TGT, what prevents her from impersonating Bob?
10.12 310 T RUSTED INTER MEDIARIES  7.
Suppose to avoid disruption in case the KDC failed, a realm had several redundant KDCs.
Do  the KDC databases need to be synchronized when a new principal is added to or deleted from  the realm?
What about when a KDC creates a session key and TGT?
 8.
Suppose all Kerberos principals (not just humans) obtained session keys and TGTs so they  could avoid keeping their master keys around.
Design a system that allows Alice to get a  ticket to Bob even though Bob has forgotten his master key and only remembers his session key and TGT, and the KDC does not keep track of session keys.
 9.
Why is the authenticator field not of security benefit when asking the KDC for a ticket for  Bob but useful when logging into Bob?
 10.
Suppose Alice gets a ticket to Bob (which will be encrypted with Bob’s master secret KB).
 However, Bob has requested a session key and TGT and has forgotten KB.
What mechanism  could a system like Kerberos use to enable Bob to find out the name of the principal contact - ing him (“Alice”) and the shared secret KA-B for communicating with Alice ( KA-B)?
 11.
How could you use Kerberos for securing electronic mail?
The obvious way is for Alice, when sending a message to Bob, to obtain a ticket for Bob and include that in the email mes - sage, and encrypt and/or integrity-protect the email message using the key in the ticket.
The problem with this is that then the KDC would give Alice a quantity encrypted with Bob’s  password-derived master key, and then Alice could do off-line password guessing.
How  could you modify Kerberos to support email without allowing off-line password guessing? (
Hint: Issue human users an extra, unguessable master key for use with mail, and extend the  Kerberos protocol to allow Bob to safely obtain his unguessable master key from the KDC.)
 12.
Suppose human Alice is a principal in two different realms, say, Wonderland and Oz, and she  wants to use the same password in each realm.
How can she ensure that her master key in each realm is different?
 13.
To build the anarchy model with PKIX, what name constraints should go into a certificate?
In  a top-down model (where the only certificates are signed by a parent for a child), what name  constraint should go into the certificate?
To build the bottom-up model, what name constraint should go into the up-link certificate (where a child signs a certificate for its parent)?
What  name constraint should go into a cross-link?
 14.
In Figure 10-7 there are no up-links.
What trust anchor would principals in a.com need to be  configured with in order to reach all the principals in the figure?
 15.
In Figure 10-8, which CAs need to be trustworthy for the certificate path from principals in  sales.xyz.com to principals in a.org to be secure?
How about the path from principals in  a.org to principals in sales.xyz.com ?
 16.
Why must a CRL be reissued periodically, even when no new certificates have been revoked?
 17.
If there is a revocation mechanism, why do certificates need an expiration date?
10.12 HOMEWORK 311  18.
Compare performance ( e.g., bandwidth, latency, timeliness of revocation) of various revoca - tion schemes: relying on the expiration date in certificates, verifiers downloading complete  CRLs, verifiers downloading delta CRLs, verifiers querying an online revocation server,  principals obtaining “my certificate is still valid” certificates, and “first valid certificate” in a  CRL.
Consider factors such as a company suddenly laying off thousands of employees, long  periods of time in which no certificates have been revoked, a verifier serving huge numbers of  principals, or a principal visiting many services.
 19.
Why is it important in a good-list revocation scheme to keep hashes of the valid certificates,  rather than just their serial numbers?
11 COMMUNICATION  SESSION  ESTABLIS HMENT  Knock Knock!
 Who’s there?
 Alice.
 Alice who?
 …and you’ll have to read on to find secure ways of continuing…  This chapter analyzes various considerations when designing real-time communication hand - shakes.
We start with very simple example handshakes that do authentication only (rather than also  creating a session key and cryptographically protecting the data).
These types of protocols are use - ful for simple scenarios, such as opening a door, and were common when people just wanted to  replace sending a password in the clear with the least amount of effort.
Even though most Internet  communication today is done with TLS, it is still instructive to start with analysis of very simple  handshakes.
The second half of this chapter explains a lot of design considerations that went into  the design of the handshakes for IPsec (Chapter 12 IPsec ), TLS, and SSH (Chapter 13 SSL/TLS and  SSH), widely deployed standards for doing authentication, establishing secret keys for cryptograph - ically protecting a session, and sending cryptographically protected data.
 Reminder of our notation: If Bob and Alice share a secret key we’ll call that key KA-B. The  notation means that some function f cryptographically transforms two inputs—the f(KA-B,R)  shared secret KA-B and a challenge R. When we explicitly mean encryption, we’ll write { R}KA-B.  When we explicitly mean a hash, we’ll write hash( KA-B,R).
KA-B might be a high-quality key con - figured into both Alice and Bob, or, if Alice is a human, it is likely to be a low-quality secret  derived from a password typed into her device.
 Also, as described in §9.8 Off-Line Password Guessing , a dictionary attack is where an  attacker can capture some data that will allow them to verify whether a password is the user’s pass - word.
Since it is offline, the number of guesses the attacker tries cannot be audited and is limited  only by the compute power of the attacker and the time they have.
 313
314 C OMMUNICATI ON SESSION ESTABLI SHMENT 11.1  11.1 ONE-WAY AUTHENTI CATION OF ALICE  A lot of older protocols were designed in an environment where eavesdropping was not a concern  (rightly or wrongly), and bad guys were (rightly or wrongly) not expected to be very sophisticated.
 The authentication in such protocols generally consists of  • Alice (the initiator) sends her name and password (in the clear, i.e., with no cryptographic  protection) across the network to Bob.
 • Bob (who has a password database consisting of usernames and passwords) verifies the name  and password, and then communication occurs with no further attention to security—no  encryption, no cryptographic integrity protection.
 A common enhancement to such a protocol is to replace the transmission of the cleartext password  with a cryptographic challenge/response.
Consider Protocol 11-1.
Alice I’m Alice  a challenge R  f(KA-B,R) Bob  Protocol 11 -1.
Bob Authenticates Alice Based on a Shared Secret KA-B  An eavesdropper will see both R and f(KA-B,R).
This protocol is a big improvement over passwords  in the clear.
However, there are some weaknesses to this protocol:  • Authentication is not mutual.
Bob authenticates Alice, but Alice does not authenticate Bob.
If  Trudy can receive packets transmitted to Bob’s network address and respond with Bob’s net - work address (or through other means convince Alice that Trudy’s address is Bob’s), then  Alice will be fooled into assuming Trudy is Bob.
This is especially convenient for Trudy if  she is on the path between Alice and Bob, e.g., Trudy is malware on a router along the path.
 Trudy doesn’t need to know Alice’s secret in order to impersonate Bob—she just needs to  send any old number R to Alice and ignore Alice’s response.
 • If this is the entire protocol ( i.e., the remainder of the conversation is transmitted without  cryptographic protection), then Trudy can hijack the conversation after the initial exchange,  assuming she can impersonate Alice’s IP address, and Bob will assume he’s talking to Alice.
 Hijacking the conversation means that Trudy steps in and starts communicating with Bob  after Alice has completed the authentication to Bob.
This is analogous to Trudy waiting for  Alice to insert her ATM card and type her PIN at an ATM machine, and then pushing Alice  away and withdrawing money for herself from Alice’s account.
In a network, assuming Trudy  can impersonate the IP address from which Alice authenticated, Trudy can send some TCP
11.1 ONE-WAY AUTHE NTICATI ON OF ALICE 315  packets before Alice does, and then when Alice starts trying to continue her conversation,  Bob will ignore Alice’s packets because the sequence numbers Alice is using are smaller than  what Trudy is using.
 • An eavesdropper, seeing R and f(KA-B,R), can mount an off-line password-guessing attack  (assuming KA-B is derived from a password).
 • Someone who steals the password database at Bob can impersonate Alice.
 Note that if Mallory (he/him) has stolen Bob’s password database, he will know KA-B. If Mallory is  able to modify the client software to bypass the step where the human types a password and the cli - ent device converts that into a key, then Mallory can directly impersonate Alice without needing to  do a dictionary attack to find Alice’s password.
However, if Mallory cannot modify the client soft - ware, then he will need to do a dictionary attack using Bob’s password database in order to find a  password that will translate to KA-B.  A minor variant on Protocol 11-1 is Protocol 11 -2: Alice I’m Alice  {R}KA-B  R Bob  Protocol 11 -2.
Bob Authenticates Alice Based on a Shared Secret Key KA-B  In this protocol Bob chooses a random challenge R, encrypts it, and transmits the result.
Alice then  decrypts the received quantity, using the secret key KA-B to get R, and sends R to Bob.
This protocol  has only minor security differences from Protocol 11-1 :  • Protocol 11-1 can be done using a hash function.
But in Protocol 11-2, Alice has to be able to  reverse what Bob has done to R in order to retrieve R.  • Suppose KA-B is derived from a password and therefore vulnerable to a dictionary attack.
If R  is a recognizable quantity, for instance, a 96-bit random number padded with 32 zero bits to  fill out an encryption block, then Trudy can, without eavesdropping, mount a dictionary  attack by merely sending the message “ I am Alice” and obtaining { R}KA-B from Bob.
If  Trudy is eavesdropping, however, and sees both R and { R}KA-B, she can mount a dictionary  attack with either protocol.
It is often the case that eavesdropping is more difficult than send - ing a message claiming to be Alice.
Kerberos V4 is an example of a protocol that allows  someone to send a message claiming to be Alice and obtain a quantity that can be used to  mount an offline dictionary attack.
Kerberos V5 made it so that only an eavesdropper would  be able to do an off-line dictionary attack.
11.1.1 316 C OMMUNICATI ON SESSION ESTABLI SHMENT  11.1.1 Timestamps vs. Challenges  If R is a recognizable quantity with a limited lifetime, such as a random number concatenated with  a timestamp, Alice can somewhat authenticate Bob because generating { R}KA-B requires knowing  KA-B. However, if Alice attempts a new connection to Bob (within the acceptable lifetime of the  timestamp), eavesdropper Trudy, (impersonating Bob’s address, tricking Alice into connecting to  Trudy), could replay { R}KA-B and trick Alice into thinking she is connecting (again) to Bob.
A way  to fix this weakness is to have Alice send a challenge c in her first message and have the quantity  (R|c|timestamp ) be what Bob encrypts, as in Protocol 11-3.
 I’m Alice, c  {R | c | timestamp} KA-B  R AliceBob  Protocol 11-3.
Mutual Authentication with Alice Sending a Challenge  Another variant on Protocol 11-1 is to shorten the handshake to a single message by having Alice  use a timestamp (instead of an R that Bob supplies), as in Protocol 11-4: Alice I’m Alice, { timestamp} KA-B Bob  Protocol 11-4.
Timestamp-based Authentication of Alice, with a Shared Secret KA-B  Using timestamps requires that Bob and Alice have reasonably synchronized clocks.
Alice encrypts  the current time.
Bob decrypts the result and makes sure the result is acceptable ( i.e., within an  acceptable clock skew, e.g., ten minutes).
The implications of this modification are  • Protocol 11-4 can be added very easily to a protocol designed for sending cleartext pass - words, since it does not add any additional messages—it merely replaces the cleartext pass - word field with the encrypted timestamp in the first message transmitted by Alice to Bob.
 • The protocol is now more efficient.
It goes beyond saving two messages.
It means that a  server, Bob, does not need to keep any volatile state (such as R in Protocol 11-1) regarding  Alice (but see next bullet).
This protocol can be added to a request/response protocol (such as  RPC) by having Alice merely insert the encrypted timestamp into her request.
Bob can  authenticate the request, generate a reply, and forget the whole thing ever happened.
 • Someone eavesdropping can use Alice’s transmitted { timestamp }KA-B to impersonate Alice,  if done within the acceptable clock skew.
This threat can be foiled if Bob remembers all  timestamps sent by Alice until they expire ( i.e., they are old enough that the clock skew check  would consider them invalid).
However, if there are multiple servers for which Alice uses the
11.1.1 ONE-WAY AUTHE NTICATI ON OF ALICE 317  same secret KA-B, an eavesdropper who acts quickly can use the encrypted timestamp field  Alice transmitted, and (if still within the acceptable time skew) impersonate Alice to a differ - ent server.
This vulnerability can be fixed by concatenating the server name in with the time - stamp, e.g., Alice sends { Bob | timestamp }KA-B. However, in the case where there are  multiple instances of what appears to Alice to be a single server Bob, putting the name Bob  into the encrypted timestamp will not help.
Having all the instances of Bob coordinate the  database of timestamps used by Alice within the acceptable timestamp window would help  but would be very expensive, and it would be essential that the instances of Bob learn of a  used timestamp faster than an eavesdropper could replay Alice’s encrypted timestamp.
Alter - natively, there could be one main Bob instance that keeps track of used timestamps, and  before an instance of Bob accepts an encrypted timestamp from Alice, he checks with the  main Bob to ask if that timestamp has yet been used.
 • Assuming Bob only bothers remembering used timestamps within the validity window ( e.g.,  ten minutes), if our bad guy Trudy can convince Bob to set his clock backwards, she can  reuse encrypted timestamps she had overheard more than ten minutes ago (since Bob would  have forgotten those).
In practice there are systems that are vulnerable to an intruder resetting  the clock.
If the security protocols are not completely understood, it might not be obvious that  clock-setting could be a serious security vulnerability.
 • If security relies on time, then setting the time will be an operation that requires a security  handshake.
A handshake based on time will fail if the clocks are far apart.
If there’s a system  with an incorrect time, then it may be difficult to log in to the system in order to manage it  (perhaps to correct its clock).
A plausible solution to this is to have a different authentication  handshake based on challenge/response ( i.e., not dependent on time) for managing clock set - ting.
 In Protocol 11 -1, computing f(KA-B,R) may be done by encrypting R using KA-B as a key or by  concatenating KA-B with R and doing a hash.
When we’re using timestamps the same is true, except  for a minor complication.
How does Bob verify that hash( KA-B,R) is acceptable?
Suppose the time - stamp is in units of minutes, and the believable clock skew is ten minutes.
Then Bob would have to  compute hash( KA-B, timestamp ) for each of the twenty possible valid timestamps to verify the value  Alice sends (though he could stop as soon as he found a match).
With a reversible encryption func - tion, all he had to do was decrypt the quantity received and see if the result was acceptable.
While  checking twenty values might have acceptable performance, this approach would become intolera - bly inefficient if the clock granularity allows a lot more legal values within the clock skew.
For  instance, the timestamp might be in units of microseconds, in which case there would be 1.2 billion  valid timestamps within a ten-minute clock skew.
This would be unacceptably inefficient for Bob to  verify.
The solution (assuming you wanted to use a microsecond clock and a hash function rather
11.1.2 318 C OMMUNICATI ON SESSION ESTABLI SHMENT  than a reversible encryption scheme) is to have Alice transmit the actual timestamp unencrypted, in  addition to transmitting the hashed value, as in Protocol 11-5: AliceI’m Alice, timestamp , hash( KA-B,timestamp) Bob  Protocol 11 -5.
Alice Sends the Timestamp in the Clear, As Well As hash( KA-B,timestamp )  11.1.2 One-Way Authentication of Alice using a Public Key  With protocols in the previous section, which are based on shared secrets, Trudy can impersonate  Alice if she can read Bob’s password database.
If the protocols are based on public keys instead  (where Bob’s user database consists of public keys for each user), this can be avoided, as in Proto - col 11 -6.
 Alice Bob I’m Alice  R  [R]Alice  Protocol 11 -6.
Bob Authenticates Alice Based on Her Public Key Signature  The notation [ R]Alice means that Alice signs R using her private key.
Bob will verify Alice’s signa - ture [ R]Alice using Alice’s public key.
This is very similar to Protocol 11 -1.
The advantage of this  protocol is that the public key database at Bob is no longer security-sensitive to an attacker reading  it.
Bob’s public key database must be protected from unauthorized modification, but not from unau - thorized disclosure.
And, as before, the same minor variant works, as in Protocol 11-7: Alice I’m Alice  {R}Alice  R Bob  Protocol 11 -7.
Bob Authenticates Alice if She Can Decrypt a Message Encrypted with Her Public Key  In this variant, Bob chooses R, encrypts it using Alice’s public key, and Alice proves she knows her  private key by decrypting the received quantity to retrieve R. Note that some public key schemes
11.1.2 ONE-WAY AUTHE NTICATI ON OF ALICE 319  can only do signatures, not reversible encryption.
This variant requires a public key scheme that can  do reversible encryption.
 In both Protocol 11-6 and Protocol 11-7, there is a potential serious problem.
In Protocol  11-6, you can trick someone into signing something.
Suppose Trudy has a quantity on which she’d  like to forge Alice’s signature ( e.g., the hash of a message that says “I agree to pay Trudy a million  dollars”).
If Trudy can impersonate Bob’s network address and wait for Alice to try to connect,  Trudy can give Alice that quantity as the challenge.
Alice will sign it, and now Trudy knows Alice’s  signature on that quantity.
Protocol 11-7 has Alice decrypting something.
So, if there’s a quantity  encrypted with Alice’s public key, such as the AES encryption key of a message, encrypted with  Alice’s public key, and Trudy wants to decrypt the message, Trudy can again impersonate Bob’s  address, wait for Alice to connect, and then give the encrypted quantity to Alice as the challenge.
 How can we avoid getting in trouble?
The general rule is that you should not use the same  key for two different purposes unless the designs for all uses of the key are coordinated so that an  attacker can’t use one protocol to help break another.
An example method of coordination is to  ensure that R has some structure.
For instance, if you sign different types of things (say an R in a  challenge/response protocol versus an electronic mail message), each type of thing should have a  structure so that it cannot be mistaken for another type of thing.
For example, there might be a type  field concatenated to the front of the quantity before signing, with different values for authentica - tion challenge and mail message .
PKCS #1 (see §6.3.6) defines enough structure to distinguish  between using an RSA key for signing and for encryption.
Note that merely having a specification  doesn’t magically cause all implementations to be careful.
There are software implementations that  do not do all the checks and might be able to be tricked into signing or decrypting something that  does not have the appropriate structure.
Also, PKCS #1, unfortunately, does not distinguish  between different uses of signing, e.g., signing as part of an authentication handshake vs. signing a  message, or different uses of encryption.
The basic idea of PKCS #1 is that typically an RSA key is used to sign the hash of something.
A hash is likely to be at most 512 bits, and an RSA key is likely  to be about 4096 bits, so there is plenty of room to encode extra information in the RSA-keysized  block to be signed.
And, likewise, when an RSA key is used for encryption, it is likely to just be  encrypting a secret key that is at most 512 bits, again, leaving plenty of room in the padding of the  block to encode extra information.
320 C OMMUNICATI ON SESSION ESTABLI SHMENT 11.2  11.2 MUTUA L AUTHEN TICA TION  Suppose we want to do mutual authentication so both Alice and Bob will know who they are talk - ing to.
We could just do an authentication exchange in each direction, as in Protocol 11-8:  I’m Alice  R1 Alice f(KA-B,R1) Bob  R2  f(KA-B,R2)  Protocol 11-8.
Mutual Authentication Based on a Shared Secret KA-B  11.2.1 Reflection Attack  “I can’t explain myself, I’m afraid sir,” said Alice, “because I’m not  myself, you see.”
 —Alice in Wonderland  The first thing we might notice is that Protocol 11-8 is inefficient.
We can eliminate two messages  by putting more than one item of information into each message, as in Protocol 11 -9: Alice I’m Alice, R 2  R1, f(KA-B,R2) Bob  f(KA-B,R1)  Protocol 11 -9.
Optimized Mutual Authentication Based on a Shared Secret KA-B  Protocol 11-9 has a security pitfall known as the reflection attack .
Suppose Trudy wants to imper - sonate Alice to Bob.
First Trudy starts Protocol 11 -9, but when she receives the challenge from Bob  in the second message, she cannot proceed further, because she can’t encrypt R1:
11.2.1 MUTUA L AUTHENTICAT ION 321  I’m Alice, R2 TrudyBob  However, note that Trudy has managed to get Bob to encrypt R2.
So at this point, Trudy opens a  second session to Bob.
This time she uses R1 as the challenge to Bob: R1, f(KA-B,R2)Trudy I’m Alice, R1 Bob  R3, f(KA-B,R1)  Trudy can’t go any further with this session, because she can’t encrypt R3.
But now she knows  f(KA-B,R1), so she can continue the first session to complete Protocol 11-9 and impersonate Alice.
 This is a serious security flaw, and there are deployed protocols that contain this flaw.
In  many environments it is easy to exploit this, since it might be possible to open multiple simulta - neous connections to the same server, or there might be multiple servers with the same secret for  Alice (so Trudy can get a different server to compute f(KA-B,R1) so that she can impersonate Alice  to Bob).
 We can foil the reflection attack if we are careful and understand the pitfalls.
Here are two  methods of fixing the protocol, both of which are derived from the general principle don’t have  Alice and Bob do exactly the same thing :  • different keys—Have the key used to authenticate Alice be different from the key used to  authenticate Bob.
We could use two totally different keys shared by Alice and Bob at the cost  of additional configuration and storage.
Alternatively, we could derive the key used for  authenticating Bob from the key used to authenticate Alice.
For instance, Bob’s key might be  KA-B+1, or −KA-B, or KA-B⊕ F0F0F0F0F0F0F0F0 16.
Any of these would foil Trudy in her  attempt to impersonate Alice to Bob since she would not be able to get Bob to encrypt any - thing using Alice’s unmodified key, and we mention these examples because these choices  have been used in protocols such as PEM (an early secure email standard).
But some crypto - graphic algorithms have what is known as related key weaknesses , where if an attacker can  see ciphertext encrypted with keys with simple known mathematical relationships (as in our  examples), the workfactor for cryptanalyzing the key is less expensive than brute force.
So, a  preferable scheme for modifying KA-B is hash( constant , KA-B).
 • different challenges—Insist that the challenge from the initiator (Alice) look different from  the challenge from the responder.
For instance, we might require that the initiator challenge  be an odd number and the responder challenge be an even number.
Or the party encrypting a  challenge might concatenate their name to the challenge before encryption, e.g., if the chal - lenge from Alice to Bob was R, Bob would encrypt Bob|R.
11.2.2 322 C OMMUNICATI ON SESSION ESTABLI SHMENT  Notice that Protocol 11-8 did not suffer from the reflection attack.
The reason is that it follows  another good general principle of security protocol design: The initiator should be the first to prove  its identity .
Ideally, you shouldn’t prove your identity until the other side does, but since that  wouldn’t work, the assumption is that the initiator is more likely to be the bad guy.
 …if you only spoke when you were spoken to, and the other person always  waited for you to begin, you see nobody would ever say anything…  —Alice (in Through the Looking Glass )  11.2.2 Timestamps for Mutual Authentication  We can reduce the mutual authentication down to two messages by using timestamps instead of ran - dom numbers for challenges, as in Protocol 11 -10: Alice I’m Alice, {timestamp }KA-B, Bob  {timestamp+1} KA-B  Protocol 11-10.
Mutual Authentication Based on Synchronized Clocks and a Shared Secret KA-B  This two-message variant is very useful because it is easy to add onto existing protocols (such as  request/response protocols), since it does not add any additional messages.
But it has to be done  carefully.
In Protocol 11-10 we have Bob encrypt a timestamp larger than Alice’s timestamp.
Obvi - ously, Bob can’t send the same encrypted timestamp back to Alice, since that would hardly be  mutual authentication. (
Alice would be assured that she was either talking to Bob or someone smart  enough to copy a field out of her request!)
So in the exchange, Alice and Bob have to encrypt differ - ent timestamps, use different keys for encrypting the timestamp, concatenate their name to the  timestamp before encrypting it, or use any other scheme that will cause them to be sending different  things.
And the issues involved with the one-way authentication done with timestamps apply here  as well (time must not go backwards, they must remember values used within the clock skew, and  so on).
 Note that any modification to the timestamp would do.
We use timestamp +1 in our example  because that’s what Kerberos V4 uses, but timestamp +1 is probably not the best choice.
Increment - ing timestamp has the potential problem that Trudy eavesdropping could use { timestamp +1}KA-B  to impersonate Alice (unless Bob remembers both timestamp and timestamp +1 as having been  used.
A better choice would be concatenating a value with the timestamp indicating whether the  initiator or responder is transmitting.
11.3 INTEGRITY /ENCRYP TION FOR DATA 323  Yet another variant that has the advantages of Protocol 11-10, but is simpler, has Alice con - catenating a challenge c with the timestamp and Bob returning c, as in Protocol 11 -11: Alice I’m Alice, {timestamp, c }KA-B Bob  c  Protocol 11-11.
Alice Encrypts a Timestamp and a Challenge; Bob Decrypts and Returns the Challenge  We haven’t found a deployed protocol that uses this variant.
 11.3 INTEG RITY /ENCRY PTION FOR DATA  Even if Alice and Bob have been configured with a long-term shared secret KA-B, it is better to  encrypt each conversation with a different secret key rather than cryptographically protecting the  data with KA-B. This per-conversation key is known as a session key .
So we’ll want to enhance the  authentication exchange so that after the initial handshake, both Alice and Bob will share a session  key.
 In a communication session, a sequence number is typically used to prevent replay or reor - dering of packets.
A new session key should be established for each new session as well as during a  session if the sequence number might be reused ( i.e., wrap around).
Changing to a new key, because  the previous key has been used for too much time or because too much data has been encrypted  with the old key, is known as key rollover .
 A good security rule is that both communicating parties should contribute to the session key.
 One way to do this is by having each side send a value encrypted with the other side’s public key,  and then using a hash of the two values as the session key.
This rule makes it less likely that the pro - tocol will have flaws in which someone will be able to impersonate one side in a replay attack.
For  instance, in a protocol in which Alice sends the session key to Bob, encrypted with his public key,  someone impersonating Alice can simply replay all of Alice’s messages from a previously recorded  session.
 With our example of using a hash of the values sent by the two sides, if either side has a good  random number generator, then the session key will be sufficiently random.
Note that this is not  true with every scheme in which both sides contribute.
For example, in a Diffie-Hellman exchange,  if one side sends g 1 mod p as their Diffie-Hellman value, the resulting Diffie-Hellman key will not  be at all secure, no matter how great the other side’s random number generator is.
11.3.1 324 C OMMUNICATI ON SESSION ESTABLI SHMENT  11.3.1 Session Key Based on Shared Secret Credentials  Alice and Bob have a shared secret key KA-B. The authentication exchange is shown in Protocol  11-12.
Perhaps mutual authentication was done, in which case there are two challenges: R1 and R2.
Alice Bob I’m Alice  R  {R}KA-B  Protocol 11-12.
Authentication with Shared Secret  At any rate, there is sufficient information in this protocol so that Alice and Bob can establish a  shared session key at this point in the conversation.
They can, for example, use hash( R,KA-B).
 There are some protocols that modify KA-B and use that to encrypt R, for instance, using  {R}(KA-B+ 1) as the session key.
Why would such protocols need to modify KA-B?
Why shouldn’t  they use { R}KA-B as the key?
The reason they shouldn’t use { R}KA-B is that { R}KA-B is transmit - ted by Alice as the third message in the authentication handshake, so an eavesdropper would see  that value, and it certainly would not be secure as a session key.
 How about using { R+1}KA-B as the session key?
There’s a more subtle reason why that isn’t  secure.
Suppose Alice and Bob have started a conversation in which Bob used R as the challenge.
 Perhaps Trudy recorded the entire Alice-Bob conversation, encrypted using { R+ 1}KA-B. Later,  Trudy can impersonate Bob’s network layer address to Alice, thereby tricking Alice into attempting  to communicate with Trudy instead of Bob, and Trudy (pretending to be Bob) can send R+1 as a  challenge, to which Alice will respond with { R+ 1}KA-B: Alice I’m Alice  R+1 Trudy as Bob  {R+1}KA-B  Then Trudy will be able to decrypt the previous Alice-Bob conversation because she will see  {R+1}KA-B. She won’t be able to continue the current conversation with Alice (because Trudy will  not know {R+2}KA-B), but Trudy will know {R+1}KA-B, which is the session key for the Alice-Bob  conversation.
 So, Alice and Bob, after the authentication exchange, know KA-B and R, and there are many  combinations of the two quantities that would be perfectly acceptable as a session key, but there are  also some that are not acceptable.
What makes a good session key?
It must be different for each
11.3.2 INTEGRITY /ENCRYP TION FOR DATA 325  session, unguessable by an eavesdropper, and not be a quantity X encrypted with KA-B, where X is a  value that can be predicted or extracted by an intruder (as just discussed for X = R+1).
See  Homework Problem 1.
 11.3.2 Session Key Based on Public Key Credentials  Suppose we are doing two-way authentication using public key technology, so that Alice and Bob  know their own private keys and know each other’s public keys.
How can they establish a session  key?
We’ll discuss various possibilities, with their relative security and performance strengths and  weaknesses.
 1.
One side, say, Alice, could choose a random number R1 to be used as the session key, encrypt  R1 with Bob’s public key, and send { R1}Bob to Bob as an extra field in one of the messages in  the authentication exchange.
This scheme has a security flaw.
Since we are assuming there is  no integrity protection on the initial authentication exchange, our intruder, Trudy, could  hijack the conversation by picking her own random number RT, encrypt it with Bob’s public  key, and replace Alice’s { R1}Bob with { RT}Bob in the message to Bob.
Then Trudy (acting as  a MITM; see Figure 1-1 ) will be able to communicate with Bob using RT as the session key.
 Bob will think he’s talking to Alice.
Alice won’t be able to talk to Bob after the initial  exchange, but she’ll assume the network or Bob is flaky and try again.
 2.
Alice could, in addition to encrypting R1 with Bob’s public key, sign the result.
So she’d send  [{R1}Bob]Alice to Bob.
Bob would take the received quantity, first verify Alice’s signature  using Alice’s public key, and then use his private key on the result to obtain R1.
If Trudy were  to attempt the same trick as in item 1, namely choosing her own RT and sending it to Bob, she  wouldn’t be able to forge Alice’s signature on the encrypted RT.
 This scheme has a minor security weakness that can be fixed partially (in 3, below) or com - pletely (in 4, below).
The flaw is that if Trudy records the entire Alice-Bob conversation, and then later breaks into Bob and learns Bob’s private key, she will be able to decrypt the conver - sation she’d recorded.
 3.
This is like 2, above, but Alice picks R1 and Bob picks R2.
Alice sends { R1}Bob to Bob.
Bob  sends { R2}Alice to Alice.
The session key will be R1⊕R2.
Breaking into Alice and stealing  Alice’s private key will enable Trudy to retrieve R2.
Breaking into Bob and stealing Bob’s pri - vate key will enable Trudy to retrieve R1.
But in order to retrieve R1⊕ R2, she’ll need to break  into both of them. (
This is assuming that Trudy only manages to break into Alice and Bob  after their conversation has terminated, and Alice and Bob forget R1, R2, and R1⊕ R2 when  the conversation is over.)
Note that in item 2 we had Alice sign her quantity ( i.e., she sent  [{R1}Bob]Alice instead of merely { R1}Bob).
Why isn’t it necessary for Bob and Alice to sign  their quantities here?
See Homework Problem 6.
11.3.3 326 C OMMUNICATI ON SESSION ESTABLI SHMENT  4.
Alice and Bob can do a Diffie-Hellman key exchange where each signs the Diffie-Hellman  value they are sending (Alice transmits [ gRA mod p]Alice; Bob transmits [ gRB mod p]Bob).
In  this scheme, even if Trudy breaks into both Alice and Bob, she won’t be able to decrypt  recorded conversations because she won’t be able to deduce either RA or RB.
This property is  known as perfect forward secrecy (PFS) and will be discussed further in §11.8 Perfect For - ward Secrecy .
Note this is not the only way to achieve PFS.
 11.3.3 Session Key Based on One-Party Public Keys  In some cases, only one of the parties in the conversation has a public/private key pair.
Commonly,  as in the case of TLS, it is assumed that servers will have public keys, and clients will not bother  obtaining keys and certificates.
Cryptographic authentication is one way.
The protocol assures the  client that she is talking to the right server Bob, but if Bob wants to authenticate Alice, after the  cryptographic session is established, Alice will send a name and password.
Here are some ways of  establishing a shared session key in this case.
 1.
Alice could choose a random number R, encrypt it with Bob’s public key, send { R}Bob to  Bob, and R could be the session key.
A weakness in this scheme is that if Trudy records the  conversation and later breaks into Bob and steals his private key, she can decrypt the conver - sation.
 2.
Bob and Alice could do a Diffie-Hellman exchange where Bob signs his Diffie-Hellman  quantity.
Alice can’t sign hers because she doesn’t have a public key.
This achieves perfect  forward secrecy even though only Bob signs his Diffie-Hellman value.
 Note that neither of these schemes assures Bob he’s really talking to Alice, but in either scheme  Bob is assured that the entire conversation is with a single party.
Bob can identify that single party  as Alice by, for instance, having Alice send Bob a password or other shared secret encrypted with  the session key.
 11.4 NONCE TYPES  A nonce is a quantity any given user of a protocol uses only once.
Many protocols use nonces, and  there are different types of nonces with different sorts of properties.
It is possible to introduce secu - rity weaknesses by using a nonce with the wrong properties.
Various forms of nonce are a time - stamp, a large random number, or a sequence number.
What’s different about these quantities?
A  large random number tends to make the best nonce, because it cannot be guessed or predicted (as
11.4 NONCE TYPES 327  can sequence numbers and timestamps).
This is somewhat unintuitive, since non-reuse is only  probabilistic.
But a random number of 128 bits or more has a negligible chance of being reused.
A  timestamp requires reasonably synchronized clocks.
A sequence number requires nonvolatile state  (so that a node can be sure it doesn’t use the same number twice even if it crashes and restarts  between attempts).
When are these properties important?
 Protocol 11-13 is a protocol in which the unpredictability of the challenge is important.
Alice Bob I’m Alice  {R}KA-B  R  Protocol 11 -13.
Protocol in Which R Must Be Unpredictable  Suppose Bob is using sequence numbers, so when Alice attempts to log in, Bob encrypts the next  sequence number and transmits it to Alice who decrypts the challenge and transmits it to Bob.
Sup - pose our eavesdropper, Eve, watches Alice’s authentication exchange and sees Alice return an R of  7482.
If Eve knows Bob is using sequence numbers for the challenge, she can then claim to be  Alice, get an undecipherable pile of bits from Bob (the encrypted challenge), and return 7483.
Bob  will be suitably impressed and assume he’s talking to Alice.
So it is obvious in this protocol that  Bob’s challenge has to be unpredictable.
 How about if we do it the other way, i.e., make Alice compute a function of R and the shared  key, as in Protocol 11 -14?
Must the challenge then be unpredictable?
Alice I’m Alice  R Bob  hash(R |KA-B)  Protocol 11-14.
Another Protocol in Which R Must Be Unpredictable  Suppose again that Bob is using sequence numbers.
Eve watches Alice’s authentication exchange  and sees that R is 7482.
Then Eve lies in wait, impersonating Bob’s network address, hoping to  entrap Alice into authenticating herself to Eve.
When she does, Eve sends her the challenge 7483,  and Alice will return the function of 7483 and the shared key.
Now Eve can impersonate Alice to  Bob, since Bob’s challenge will be 7483, and Eve will know the answer to that challenge.
 These protocols are also insecure if timestamps are used.
Eve has to guess the timestamp Bob  will use, and she might be off by a minute or two.
If the timestamp has coarse granularity, say sec -
328 C OMMUNICATI ON SESSION ESTABLI SHMENT 11.5  onds, Eve has a good chance of being able to impersonate Alice.
But if the timestamp has nanosec - ond granularity, then it really does become just like a random number and the protocol is secure.
 Here’s Protocol 11-15 , in which it would be perfectly secure to use a predictable nonce for R:  I’m Alice  {R}KA-B  hash( R|KA-B) AliceBob  Protocol 11 -15.
Protocol in Which R Need Not Be Unpredictable  Even if Eve could predict what R would be, she can’t predict either the value sent by Bob or the  appropriate response from Alice.
 11.5 INTEN TIONA L MITM  We introduced MITM attacks in §1.6 Active and Passive Attacks and §6.4.1 MITM (Meddler-in-the- Middle) Attack .
Sometimes MITM is not an attack, but it is intentionally deployed.
Some compa - nies intentionally deploy a proxy (let’s call it Fred) that acts as a MITM for communication  between their employees and external websites.
There are many ways Fred might work, but an  example is that the corporate IT department configures all the employee devices with a trust anchor  (see §10.4 PKI) with a public key belonging to Fred.
When Alice (inside the corporate network)  tries to connect to Bob (an external site), Fred intercepts the request from Alice, creates a certificate  for “Bob”, signed by Fred, with a key known to Fred and completes the TLS session with Alice.
 Alice will believe that the certificate newly created by Fred is valid because Fred is one of her trust  anchors.
Fred will open a second TLS connection to Bob and relay messages between Alice and  Bob.
If the only thing Fred does is forward messages between Alice and Bob, that wouldn’t be very  useful.
Fred can offer services such as having a deny-list of dangerous sites to prevent employees  from connecting to them, save bandwidth and lower latency for Alice by caching web images that  have been fetched by other users and send cached images to Alice, scan the data stream for viruses,  or check whether employees are sending confidential information.
 Note this form of Fred works when only Bob has a certificate.
If Alice also has a certificate  (client authentication), this will not work.
All the devices managed by the company can be config - ured by the IT department to have Fred as a trust anchor (so that Fred can impersonate Bob), but the
11.6 DETECTING MITM 329  world is not going to allow a company to install their trust anchor at all the servers, which would  enable Fred to create certificates to impersonate all users.
 There might be some external websites for which Fred is configured not to act as a MITM  and to instead allow direct connections to those websites.
For example, if Alice is doing business at  a bank, Alice’s company would prefer not to see her banking information for liability reasons.
And  the bank would be a site that the company trusts not to be doing anything malicious.
So for those  sites, Fred is configured to not impersonate Bob.
Instead, Fred just forwards messages from Alice  to Bob without decrypting and re-encrypting.
He does not create a certificate for Bob.
He does not  complete the TLS session with Alice and really just acts like a router.
There will only be a single  TLS connection in this case—from Alice to Bob—and Fred will not be able to see what Alice and  Bob are talking about.
 11.6 DETEC TING MITM  In some cases, there is a way for Alice and Bob to detect a MITM.
Suppose Alice and Bob are  humans, communicating using special telephones.
The telephones start a call by doing an anony - mous Diffie-Hellman exchange.
They also have a display that displays a small hash of the session  key.
Alice and Bob read aloud the value they see displayed.
If there were a MITM, Alice’s session  key would be different from Bob’s key.
This assumes that the humans Alice and Bob recognize  each other’s voices.
This technique—providing channel users with a channel identifier that would  be different at the two endpoints if there were a MITM—is known as channel binding .
 Channel binding can be used without depending on humans recognizing voices.
Suppose  there is a layer ( e.g., IPsec or TLS) that does anonymous Diffie-Hellman to create an encrypted tun - nel, but perhaps with a MITM.
Assume that layer has an interface in which it can inform an upper  layer of some sort of session identifier, such as a hash of the session key.
So, after an anonymous  Diffie-Hellman with Trudy in the middle, the application at Alice’s computer will be informed that  the channel identifier is hash( gat mod p), and the application at Bob’s computer will be informed that  the channel identifier is hash( gtb mod p).
If Alice merely sent a message saying “I think the channel  identifier is gat mod p”, Trudy could replace that field in the message with gtb mod p.  However, suppose that layer has some sort of credential for Alice, for instance, a password  that Bob has stored for Alice.
In that case, that credential, along with the channel identifier, can be  used cryptographically in a way that Trudy cannot impersonate.
For example, see Protocol 11-16,  which foils a MITM.
330 C OMMUNICATI ON SESSION ESTABLI SHMENT 11.7  I’m Alice  a challenge R  hash(channel ID, Alice’s password, R) AliceBob  Protocol 11-16.
Detecting a MITM Using Channel Binding  11.7 WHAT LAYER ?
 Definition of a layer n protocol: anything defined by a committee whose  charter is to define a layer n protocol. —
Radia Perlman (in Interconnec - tions: Bridges, Routers, Switches, and Internetworking Protocols )  The concept of layers for network protocols (see §1.8.1 Network Layers ) comes from the OSI  (Open Systems Interconnection) model.
Although the OSI 7-layer model is a useful way of learning  about network protocols, implementations seldom conform to that model.
TLS and SSH (see  Figure 11-17) are said to be “implemented at layer 4”, whereas IPsec is said to be “implemented at  layer 3”.
What does it mean that a real-time communication security protocol is implemented at  layer 3 vs. layer 4, and what are the implications?
 Many IP stacks are implemented so that layer 4 ( e.g., TCP) and below are implemented in the  operating system, and anything above is implemented in a user process.
The philosophy of TLS is  that it is easier to deploy something if you don’t have to change the operating system, so these pro - tocols act “above TCP”.
It requires the applications to interface to TLS instead of TCP and requires  no changes to TCP.
TLS is really a new version of SSL, or Secure Sockets Layer, whose name  comes from the most popular API (Application Programming Interface) to TCP, which is called  “sockets”.
The API to TLS is a superset of the API to TCP sockets.
Modifying an application to  work on top of TLS requires minimal changes, but the security benefits are limited if the richer API  is not used.
 Although with TLS (and SSH) the applications have to be modified (albeit minimally), the  operating system, which includes TCP and the layers below it, does not need to be modified.
 “Transport Layer Security” is somewhat of a misnomer because rather than being at layer 4, these  protocols tend to act like a layer on top of layer 4.
 In contrast, the philosophy behind IPsec is that implementing security within the operating  system automatically causes all applications to be protected without the applications having to be  modified.
11.7 WHAT LAYER?
331  applications applications  TLS or SSH  TCP TCP  OS IPsec  IP IP  lower layers lower layers  TLS or SSH operate above TCP.
IPsec is within the OS.
OS changes.
 OS doesn’t change.
Applications do.
Applications and API to TCP are unchanged.
 Figure 11-17.
Implementing at “Layer 3” vs. “Layer 4”  There is a problem in operating above TCP.
Since TCP will not be participating in the cryptogra - phy, it will have no way of noticing if malicious data is inserted into the packet stream, as long as  the bogus data passes the (noncryptographic) TCP checksum and has the correct TCP sequence  numbers (which is likely to require some eavesdropping by the attacker).
TCP will acknowledge  such data and send it up to TLS.
The bogus data will fail the TLS integrity check, so TLS will dis - card the data, but there is no way for TLS to tell TCP to accept the real data at this point.
When the  real data arrives, TCP will assume it is duplicate data and discard it, since it will have the same TCP  sequence numbers as the bogus data.
TLS has no choice but to close the connection, since it can no  longer provide the service that the API claims TLS offers, namely a lossless stream of cryptograph - ically protected data.
An attacker can launch a successful denial-of-service attack by inserting a sin - gle packet into the data stream.
IPsec’s approach of cryptographically protecting each packet  independently can better protect against such an attack.
 In contrast, IPsec’s constraint of not modifying the applications winds up with its own serious  problem.
With the current commonly used API, IP tells the application only what IP address it is  talking to, not what user is on the other end.
So IPsec (with the current API) cannot tell the applica - tion anything more than which IP address is on the other end, even though IPsec is capable of  authenticating an individual user.
Most applications need to distinguish between users.
If IPsec has  authenticated the user, it could in theory tell the application the user’s name, but that would require  changing the API and the application.
 Implementing IPsec without changing the application has the same effect as putting firewalls  between the two systems and implementing IPsec between the firewalls.
It accomplishes the fol - lowing:
332 C OMMUNICATI ON SESSION ESTABLI SHMENT 11.7  • It causes the traffic on the path between the communicating parties to be encrypted, hiding it  from eavesdroppers.
 • As with firewalls, IPsec can access a policy database similar to what a firewall can access.
For  instance, it can specify which IP addresses are allowed to talk to which other IP addresses, or  which TCP ports should be allowed or excluded.
This is true whether the endpoint of the  IPsec connection is a firewall or an endnode.
 • Some applications do authentication based on IP addresses (see §9.2 Address-based Authenti - cation ).
The API informs the application of the IP address from which information was  received.
Before IPsec, the source IP address was assumed based solely on the value of the  SOURC E ADDRE SS field in the IP header.
With IPsec, address-based authentication becomes  much more secure because one of the types of endpoint identifiers IPsec can authenticate is  an IP address.
 What IPsec (with the current API and an unmodified application) does not accomplish for the  application is authentication of anything other than IP addresses.
Most principals would have some  identity, such as a name, and be allowed to access the network from a variety of IP addresses.
In  these cases, the most likely scenario for using IPsec is that IPsec would do its highly secure and  expensive authentication, authenticating based on the user’s public key and establishing a secure  session to the user’s name, but would have no way of telling the application who is on the other  side.
The application would have to depend on existing mechanisms, most likely a name and pass - word, to determine which user it is talking to.
IPsec is still of value in this scenario where the  (unmodified) application authenticates the user based on name and password, since the name and  password will now be encrypted when transmitted.
 To take full advantage of IPsec, applications will have to change.
The API has to change in  order to pass identities other than IP addresses, and the applications have to change to make use of  this information.
So the best solution is one where both the operating system and the applications  are modified.
This illustrates why security would best be done by being designed in from the start,  rather than being added in with the least modification to an existing implementation.
 One other advantage of the packet-by-packet cryptographic handling of IPsec is that it is eas - ier to build a network adapter that does IPsec processing.
To implement a TLS-type protocol that  operates over TCP in such a device, it would have to implement TCP, including buffering out-of - order packets.
 DTLS (Datagram Transport Layer Security, RFC 6347) does not require TCP.
Rather it will  work over a datagram protocol such as UDP.
This gives DTLS the best of both worlds—IPsec’s  invulnerability to connections breaking due to an attacker injecting bogus packets, and TLS’s abil - ity to assure an application of the authenticated identity of the endpoint.
IPsec could, in theory, be  modified to have the best of both worlds as well, by modifying the API so it could pass up the  authenticated identities.
11.8 PERFE CT FORW ARD SECRE CY 333  11.8 PERFEC T FORWARD SECR ECY  A protocol is said to have perfect forward secrecy (PFS) if it is impossible for an eavesdropper  Mallory to decrypt a conversation between Alice and Bob even if Mallory records the entire  encrypted session, and then subsequently breaks into both Alice and Bob and steals their long-term  secrets.
The trick to achieving perfect forward secrecy is to generate a temporary session key, not  derivable from information stored at the node after the session concludes, and then forget it after the  session concludes.
If the session will last for a long time, it is common to generate and forget keys  periodically so that even if Mallory seizes Alice’s and Bob’s computers while the conversation is  still going on, he will not be able to decrypt messages received before the last key rollover.
Protocol  11-18 is an example of a protocol with perfect forward secrecy.
It uses Diffie-Hellman to agree on a  session key, which achieves perfect forward secrecy assuming both sides generate an unpredictable  Diffie-Hellman private number and forget both the private number and the agreed-upon session key  after the session ends.
 Alice [“Alice”, g a mod p]Alice Bob message 1  [“Bob”, g b mod p]Bob  message 2  hash( g ab mod p)  message 3  hash(1, g ab mod p)  message 4  Protocol 11-18.
Diffie-Hellman for Perfect Forward Secrecy Using Signature Keys  In the first two messages, each side identifies itself and supplies a Diffie-Hellman value  signed by its private key.
In the next two messages, each side proves knowledge of the agreed-upon  Diffie-Hellman value gab mod p by sending a hash of it, with each side sending a different hash.
If  each side forgets g ab mod p and its private Diffie-Hellman number ( a or b) after the session, there is  no way for anyone to reconstruct g ab mod p from knowledge of both long-term private keys and the  entire recorded conversation.
 What kind of protocol would not have perfect forward secrecy?
Examples include  • Alice sends all messages for Bob encrypted with Bob’s public key, and Bob sends all mes - sages for Alice encrypted with Alice’s public key.
334 C OMMUNICATI ON SESSION ESTABLI SHMENT 11.8  • Kerberos (since the session key is inside the ticket to Bob and is encrypted with Bob’s long - term secret).
 • Alice chooses the session key, and sends it to Bob, encrypted with Bob’s public key.
 Perfect forward secrecy might seem like it only protects against a fairly obscure threat.
However,  protocols designed with perfect forward secrecy usually have another property, which is particu - larly popular with the IETF crowd, which we’ll call escrow-foilage .
This means that even if the  forces of darkness (and we make no value judgments here) have required Alice and Bob to give  their long-term private keys to some benign, completely trustworthy organization, the conversation  between Alice and Bob will still be secret between only Alice and Bob.
In other words, even with  prior knowledge of Alice and Bob’s long-term keys, a passive eavesdropper cannot decrypt the  conversation.
 Of course, if Mallory has prior knowledge of all of Alice’s and Bob’s secrets, then he can  impersonate Alice or Bob and perhaps trick them into divulging what they would have divulged in  the conversation.
Maybe you’d think Alice and Bob could start off the conversation by asking each  other a few personal questions like, “What café did we meet at in Paris?”
but Mallory could be act - ing as a MITM, decrypting and re-encrypting the traffic, relaying the personal questions and  answers, and this would be very difficult to detect.
 Anything with perfect forward secrecy will also have escrow-foilage against a passive attack,  since anything you can do by having recorded the conversation and learned the secret later you can  also do knowing the secrets in advance and eavesdropping in real-time.
But often with escrowed  systems a user has two public key pairs, one for encryption and one for signatures.
And in those  cases, only the encryption key is escrowed, since law enforcement would like to decrypt data but  does not need the ability to forge signatures.
It would be counterproductive for them to have a  user’s private signature key, because then the user can repudiate his signature, claiming someone  else with access to his key signed the message.
So assuming the signature keys are not escrowed,  then Protocol 11-18 will have escrow-foilage even against active attacks.
 There are other ways of achieving PFS.
An ephemeral public key is a public key created for  short-term use, and the private key is then forgotten.
For example, Alice or Bob could create an  ephemeral RSA public key pair, sign the ephemeral public key with their long-term RSA key, and  the session key could be sent encrypted with the ephemeral public key.
This strategy was actually  used in SSL for exportability reasons.
At the time, an implementation was not exportable if it used  a long RSA key for encryption, but it was okay to have a long (and therefore secure) RSA key for  signing.
So SSL allowed creating a shorter ephemeral RSA key pair, signing the ephemeral public  key with the long RSA key, and using the ephemeral RSA key for encrypting session secrets.
But  RSA key generation is very expensive.
Some of the post-quantum algorithms (see Chapter 8 Post- Quantum Cryptography ) create ephemeral keys inexpensively and achieve PFS.
11.9 PREVENTING FORGED SOUR CE ADDRESSE S 335  11.9 PREVEN TING FORGED SOURC E ADDR ESSES  In a denial-of-service attack, Trudy does not successfully impersonate Alice, but she might success - fully use up enough of server Bob’s resources to lock out legitimate users.
If Trudy sends malicious  packets from her own IP address, she is likely to get caught, or people might notice the IP address  of malicious traffic and have firewalls delete anything from her address.
It is much easier to send a  packet with a forged source address than to be able to receive a packet to that address when the  other side replies.
 If Trudy can inject packets into the Alice-Bob TCP connection that will be accepted by Bob,  her data will be accepted by Bob, and since Bob’s TCP only looks at the sequence number to deter - mine whether the next portion of the data from Alice is new or retransmission, Alice’s real data will  be ignored by Bob.
Even if there is a cryptographic protocol such as TLS running on top of TCP,  TCP will have no idea that Trudy’s packets are fraudulent, and there is no way for TLS to ask TCP  to ignore part of the data and let the real data (with the same sequence numbers) to go through.
So,  this will cause Alice’s connection to Bob to break.
 After many denial-of-service attacks on TCP, TCP implementations have been modified to  prevent someone from sending TCP packets from a forged IP address unless they can receive traffic  sent to that address.
TCP could have been modified with extra fields, but instead the industry made  clever use of the TCP sequence number, so that new implementations still followed the specifica - tion and could interwork with unmodified implementation.
TCP sequence numbers are 32 bits long  and refer to octets in the stream (as opposed to numbering packets).
A TCP packet contains two  sequence numbers—one numbers the first octet the sender is sending in this packet, and the other is  the last octet that the receiver is acknowledging.
If Trudy attempts to inject a TCP packet from  Alice’s address, Bob will not accept it if the sequence number from “Alice” is not close to what he  expects.
So TCP implementations attempt to make it hard for Trudy to guess the sequence numbers  in the Alice-Bob conversation.
Instead of starting each connection with sequence number 0, an  implementation may choose a random sequence number as the initial sequence number.
RFC 6528  discusses various techniques.
 11.9.1 Allowing Bob to Be Stateless in TCP  There is another TCP denial-of-service attack, where Trudy opens lots of connections to Bob, with  forged IP addresses, and we assume she can’t see Bob’s replies.
TCP connections start with a three- way handshake, with the first message called a SYN.
Alice sends a SYN announcing her initial  sequence number.
Bob replies with a SYN-ACK, announcing his initial sequence number and  acknowledging Alice’s sequence number.
Alice responds with an ACK, which contains both her  sequence number and Bob’s sequence number.
11.9.2 336 C OMMUNICATI ON SESSION ESTABLI SHMENT  The simplest implementation would be for Bob to keep track, after a SYN is received from  Alice, of Alice’s sequence number and what Bob had randomly chosen as his initial sequence num - ber.
However, if Trudy just sends lots of SYNs from lots of forged IP addresses, she might exhaust  all the space Bob’s TCP implementation has set aside for remembering partially completed TCP  connections.
 The defense is to let Bob be stateless , meaning that Bob does not need to devote any space to  remembering anything about a TCP connection unless the three-way handshake completes, which  it will only do if the client is sending from an IP address on which it can receive traffic.
 TCP implementations had to be clever in order to accomplish allowing Bob to be stateless  without changing the protocol.
Other protocols ( e.g., IPsec) have different techniques.
But TCP can  only use the sequence number.
If Bob chose a random number for each connection, he’d have to  remember what sequence number he chose.
But instead, Bob can compute his initial sequence  number so that when he receives the third message in a TCP handshake, he can decide “is that the  sequence number I would have chosen?”
 Bob can’t use the same initial sequence number on all connections, so it needs to be secret  and (with reasonable probability, given that the sequence number is only 32 bits) different for each  connection.
An example technique is for Bob to have a secret Si that he changes every minute.
If  Bob’s current secret is S , he will not know whether he generated his sequence number using S or n n  Sn−1 (because he might have switched secrets between the first message and the third message).
 When he receives a SYN from IP address x, Bob chooses as his sequence number hash( x,S ), sends n  his SYN-ACK, and then can forget about this connection.
If Bob receives the third message of a  connection attempt (the ACK) from IP address x, the ACK might say that Alice’s sequence number  is p, and Bob’s is q. Bob is willing to believe anything Alice wants for her own sequence number,  but Bob will sanity check q by computing what he would have chosen as his initial sequence num - ber.
Does q=hash( x,S )?
If not, maybe Bob had rolled over his secret, so does q =hash( x,Sn−1)?
If n  there are few enough possibilities for Bob’s secret within conceivable network delays, Bob could  try all potential secrets he might have used when calculating his initial sequence number.
Or Bob  could use a few bits within the sequence number to encode which of the recent secrets he used.
For  instance, if there might at any point be eight different secrets Bob might have used, he could use  three of the 32 bits in his sequence number to specify to himself which secret to use.
 None of this is cryptographically secure, of course, but people did launch denial-of-service  attacks of this sort, and TCP implementations deployed defenses similar to this.
 11.9.2 Allowing Bob to Be Stateless in IPsec  The designers of Photuris (RFC 2522, an early key-management protocol for IPsec) provided for  denial-of-service protection by adding a feature the designers called cookies .
These have nothing to  do with web cookies, so that is a confusing name.
But the best way to get the attention of IETF peo -
11.10 ENDPOINT IDENTIFIER HIDING 337  ple and have them like your protocol is to mention “cookies”.
This feature is similar to the initial  sequence number in TCP.
 The cookie is a number chosen by Bob, reproducible by Bob, and unpredictable to the side  initiating communication with Bob.
When Bob receives a connection initiation from IP source  address S, Bob computes a cookie that is a function of a secret Bob knows and the IP address from  which he received a connection attempt.
Bob keeps no state and does no significant computation  until he receives a connection attempt with a valid cookie value.
 This feature makes it somewhat harder for Trudy, using a forged IP address, to use up  resources at Bob.
The cookie feature doesn’t hurt, other than making the protocol slightly more  complex.
In theory, cookies could be used only when a node is getting swamped, saving the round- trip delay in the usual case.
The cookie protocol might look like Protocol 11-19:  I want to talk InitiatorBob c = hash( IP address , secret )  c  c, start of rest of protocol  Does c = hash( IP address , secret )?
 If so, continue with protocol.
 Protocol 11-19.
Stateless Cookie Protocol  11.10 ENDPOIN T IDENT IFIER HIDING  Another feature in some of these protocols is the ability to hide the identities of the two communi - cating parties from eavesdroppers.
The IP addresses will still be visible, but this feature hides their  names.
Assume Alice wants to talk to Bob without letting an eavesdropper know that Alice wants  to talk to Bob.
A mechanism for accomplishing this is to first do an “anonymous” Diffie-Hellman  exchange, which establishes an encrypted tunnel, but to an unknown endpoint.
The tunnel might  have a MITM.
 The next step will be for Alice to divulge her identity to Bob (but within the encrypted tun - nel).
Eavesdroppers won’t see her name, but a MITM would see it.
Then Alice and Bob can do
11.10 338 C OMMUNICATI ON SESSION ESTABLI SHMENT  mutual authentication.
A MITM will not be able to successfully impersonate Alice to Bob or Bob  to Alice.
So all the MITM will have accomplished is learning that Alice wanted to talk to Bob.
 Note that by carefully designing the protocol, you can arrange for the MITM to only be able  to learn one of the two identities before being discovered by the other side as an impostor.
Which  identity is better to hide from an active attacker?
One argument says that it is better to hide the initi - ator’s identity (Alice) than the responder’s identity (Bob), because Bob’s identity is probably  already known.
He has to be sitting at a fixed IP address waiting to be contacted, whereas Alice  might be dialing in from anywhere, and her identity could not be guessed from her IP address.
 But a different argument says that it is better to hide Bob’s identity.
If Bob divulges his iden - tity first, then anyone can initiate a connection to Bob and get him to divulge his identity.
Unless  there is a strict client/server model in which clients never accept connections and only initiate them,  having a protocol in which the responder divulges his identity first makes it trivial to find out who is  at a given IP address.
In contrast, for an active attacker to trick Alice into revealing her identity, it  requires impersonating Bob’s address and waiting for Alice to initiate a conversation.
 An example protocol, assuming the two sides have public signature keys, might be Protocol  11-20:  aI want to talk, g mod p  OK, gb mod p  {“Alice”, [ ga mod p]Alice} gab mod p  {“Bob”, [ gb mod p]Bob} gab mod p AliceBob  Protocol 11-20.
Identity Hiding  In this protocol, an active attacker will be able to discover Alice’s identity, but not Bob’s.
It is easy  to arrange instead to hide Alice’s identity instead (see Homework Problem 10).
 If Alice and Bob know in advance to whom they will be talking (perhaps they are two spies  who will be contacting each other at a specific time), then a protocol based on a shared secret key  will hide both identities.
This is accomplished by authenticating based on the secret key and not  sending identities at all (see Homework Problem 11).
 If Alice already knows Bob’s public encryption key, it is possible to hide both identities from  active attackers (see Homework Problem 12).
11.11 LIVE PARTNER REASSURA NCE 339  There are cases where Alice needs to tell Bob his own name.
Suppose Bob is one of many  services provided at a web server.
For example, all the services (like book seller, dictionary, hire-a - hitman) reside at the same address and each has a different certificate.
When Alice makes a connec - tion to that address, Alice needs to inform Bob to which service she wants to connect.
This feature  is sometimes called You-Tarzan-Me-Jane , because Alice is telling the other side who she wants  him to be.
TLS has this feature (called SNI, for Server Name Indicator).
IPsec, unfortunately, does  not.
 11.11 LIVE PARTNER REASSURANCE  If Trudy can replay messages from previous conversation negotiations, she might be able to get Bob  to waste space on a connection, or worse yet, she might be able to replay the subsequent data mes - sages and, even if she can’t decrypt the conversation, she might be able to cause Bob to repeat  actions.
For instance, when Bob (an ATM machine) talked to Alice, she might have requested Bob  to put $100 into the money tray, as in Protocol 11-21 :  a“Alice”, [ g mod p]Alice  [gb mod p]Bob  gab mod p {“please give me $100”} AliceBob  Protocol 11-21.
A Protocol Vulnerable to Replay if Bob Reuses b  An hour later, if Trudy replays Alice’s messages, it is important that Bob realize that this is not a  conversation with the live Alice.
If Bob chose a different b in each Diffie-Hellman exchange, then  bthere wouldn’t be a problem, but it is computation intensive to compute g , so it might be nice for  Bob to be able to reuse b.  A way to allow Bob to reuse b and still avoid replay attacks is for Bob to choose a nonce for  each connection attempt and have the session key be a function of the nonce as well as the Diffie- Hellman key.
So the protocol might be modified to look like Protocol 11-22.
The session key is a  function of the nonce N as well as the Diffie-Hellman value.
This seems similar to a cookie, but it is  desirable for a cookie to be stateless so that Bob does not have to keep state until he’s at least sure
11.12 340 C OMMUNICATI ON SESSION ESTABLI SHMENT  the other end can listen at the IP address from which it claims to be sending.
With the most straight - forward implementation of a stateless cookie, the cookie will be reused, so wouldn’t work as a  nonce.
It is possible to design a protocol that will allow something to work both as a nonce and as a  stateless cookie.
Bob a“Alice”, [ g mod p]Alice  [gb mod p]Bob, N  K = hash( N, gab mod p)  {“please give me $100”}K Alice Protocol 11 -22.
Using a Nonce so Bob Knows It’s Not Replayed Messages from Alice  Note that we’ve only ensured that Bob knows it’s the live Alice (and not replayed messages).
How  would Alice know it’s the real Bob?
If Alice chooses a different a each time, and if Alice receives  proof from the other side that it knows K (for instance, by acting on her request, which was  encrypted with K), then she knows it’s the real Bob.
But suppose Alice, like Bob, would like to  reuse a to save herself from computing ga mod p (see Homework Problem 14 ).
 11.12 ARRAN GIN G FOR PARA LLEL COMPUTATION  A lot of protocols require both Alice and Bob to compute a shared Diffie-Hellman key.
This might  take a long time.
It can speed up the total elapsed time for an exchange if Alice and Bob can be  computing at the same time, as in Protocol 11-23.
 This exchange might seem silly.
Why not combine message 2 with message 3?
The reason is  ab that telling Alice gb mod p gives Alice a head start on computing g .
She can be computing gab at  the same time Bob is computing it.
Al Eldridge probably was the first to invent this trick of sending  an extra message in order to allow the computation-intensive calculations to be done in parallel.
He  implemented it in Lotus Notes (an early public key messaging system that we discuss because it  had some interesting innovations).
In Lotus Notes, Bob sent something encrypted with Alice’s pub - lic key in message k and then later sent his signature on that message in message k+1.
This let Alice  do the expensive private key decryption while Bob was doing the expensive signature.
11.13 SESSION RESUMPT ION/MULTI PLE SESSIONS 341 Alice [g a mod p]Alice Bob message 1  [g b mod p]Bob  message 2  {Bob’s message}g ab mod p  message 3  {Alice’s message} g ab mod p  message 4  Protocol 11-23.
Parallel Computation  Notice that although this adds a message it doesn’t add any round-trip times, so it can be faster even  if Alice and Bob are very far apart, talking to each other via, say, carrier pigeons (see RFC 1149 or  RFC 2549).
 11.13 SESSIO N RESU MPTIO N/MULT IPLE SESSIONS  Protocols such as IPsec and TLS use public key operations for the initial handshake in which Alice  and Bob authenticate and establish a secure session.
Because this initial step is expensive, a trick  some protocols use is to leverage the initial expensive handshake to cheaply resume a secret session  that has been idle or spawn multiple secure sessions in parallel.
For example, a common use of TLS  is for Alice to be a browser and Bob a server with a web page that has many images.
Alice will cre - ate multiple secure sessions to Bob to retrieve the images in parallel.
 In IPsec (where they refer to secure sessions as SAs), Alice and Bob might want to spawn  multiple SAs for different types of traffic (§12.1.3 IKE-SAs and Child-SAs ).
A resumed SA or a  new SA, leveraging the handshake of the original SA, will use different secrets computed by hash - ing in new nonces exchanged during the SA resumption or SA creation handshake.
 A simple mechanism for resuming a secure session or cheaply spawning a new secure session  is for Bob, during the initial expensive handshake, to send Alice an ID for the session that he  chooses.
If Bob remembers the information for that session, he can inexpensively resume the ses - sion or create a new session.
If he has forgotten the state associated with the session, then he tells  Alice they’ll have to start over, and they establish a new session.
11.13 342 C OMMUNICATI ON SESSION ESTABLI SHMENT  But what if there are many instances of Bob?
If Alice attempts to resume a session with a dif - ferent instance of Bob, then the other instance will not remember the session, unless all the Bob  instances do some sort of synchronization so that they all know the IDs, secrets, and other informa - tion (such as crypto algorithms negotiated) for all sessions.
 An elegant alternative is for Bob to encrypt all the state associated with the session with a  secret S that all the Bob instances know (and nobody else knows, hopefully), and use that quantity  as the ID.
Then when Alice sends the “ID”, Bob decrypts it and knows everything he needs to know  in order to resume the session.
This mechanism works even if there are multiple instances of Bob.
 Alice does not need to know whether the session ID Bob sends her is the ID of a database  entry where the session state is kept or is the encrypted session state itself.
The ID is just a large  random-looking number to Alice.
The only issue is that the protocol must allow the ID to be large  enough.
If the ID were only a database entry ID, then a few octets would be long enough.
To encode  all the state associated with a session would require a much larger ID.
 There were other interesting mechanisms used in older protocols.
In Lotus Notes, Bob (the  server) had a secret S that he shared with nobody and changed periodically ( e.g., once a month).
 After Bob authenticated Alice, he sent her a secret, SA-B, which was a hash of her name and S. (He  sent SA-B encrypted with Alice’s public key.)
Until Bob changed S, he’d give Alice the same SA-B  each time she successfully authenticated.
The actual session key (used for encryption and integrity  protection) was a function of SA-B and nonces sent by each side.
If Alice told Bob her name “Alice”  and showed knowledge of the SA-B that would result from hashing S and “Alice”, then Bob assumed  he’d authenticated her in the recent past, and they’d skip the expensive public key operations and  create secrets for the session by exchanging nonces.
The actual session key (used for encryption  and integrity protection) was a function of SA-B and the nonces.
If Bob had changed S, then Alice’s  attempt to bypass the expensive authentication step would fail, and Alice and Bob would start from  the beginning, exchanging certificates, signing things, etc.
The Lotus Notes scheme did not require  Bob to keep state, and it therefore worked with multiple instances of Bob.
 Digital’s DASS scheme (RFC 1507) has a different interesting method of bypassing the pub - lic key cryptography.
During the handshake, Alice sends Bob the session secret S, encrypted with  Bob’s public key and signed with Alice’s private key, i.e., If Bob remembers [{S}Bob]Alice.
 S from a recent Alice-Bob session, then he merely compares the received [{S}Bob]Alice and  [{S}Bob]Alice from Alice with the stored [{ S}Bob]Alice.
If they match, he doesn’t have to bother  decrypting it to extract S. But if he doesn’t remember [{ S}Bob]Alice, or if Alice has decided to create  a session with a different secret S, then Bob verifies Alice’s signature on [{ S}Bob]Alice and decrypts  {S}Bob with his private key to obtain the new S. If Bob doesn’t remember the previous session, but  Alice does, then Alice still saves time.
If they both remember the previous session, then they both  save time.
What’s interesting about this protocol is that the protocol messages look the same  whether state has been kept or not.
11.14 PLAUSIB LE DENIABI LITY 343  11.14 PLAUSIBLE DENIABILITY  If a protocol involves having Alice sign something containing Bob’s name, then it offers proof that  Alice intentionally talked to Bob (though it still gives no indication of what they talked about).
In  some cases Alice would like to assure Bob that it is her but not provide proof that she talked to Bob.
 If Alice and Bob are authenticating each other based on a shared secret, then there is no way to  prove to a third party that Alice and Bob communicated with each other, because the entire conver - sation could have been constructed by Alice or Bob.
If Alice and Bob authenticate each other using  public encryption keys, anyone could create an entire conversation that looks like a conversation  between Alice and Bob. (
For example, consider Protocol 11-23 and change the first two messages  from being signed by the sender to being encrypted with the recipient’s public key.
No knowledge  of either side’s private key is required to create such messages.)
If Alice and Bob authenticate each  other using public signature keys, then it is possible to create a protocol in which each signs infor - mation including the other’s identity, in which case there is no plausible deniability.
But it is also  possible to avoid signing the other side’s identity, and therefore preserving plausible deniability  (see Protocol 11-24).
 Alice [g a mod p]Alice Bob message 1  [g b mod p]Bob  message 2  {Alice’s message} g ab mod p  message 3  {Bob’s message}g ab mod p  message 4  Protocol 11-24.
Plausible Deniability with Signature Keys  11.15 NEGO TIATING CRYPTO PARA METE RS  It’s fashionable today for security protocols to negotiate cryptographic algorithms, rather than  simply having the algorithms be part of the specification.
It certainly makes the protocols more
11.15.1 344 C OMMUNICATI ON SESSION ESTABLI SHMENT  complex.
Examples of things to negotiate are the algorithm for encryption, the size of the key, the  algorithm for integrity protection, the algorithm for hashing, the algorithm for key expansion, and  the group to use in a Diffie-Hellman exchange.
 One reason for allowing choice of cryptographic algorithms is so that over time, systems can  migrate to stronger but slower crypto as attackers’ and defenders’ devices get faster.
Also, it allows  migration from a cryptographic algorithm that gets broken.
In order to allow for a smooth migration  to new algorithms when endpoints are being updated independently, support is added for the new  algorithms while the old ones continue to be supported until all the endpoints have been updated.
 Only then can support for the old algorithms be removed.
Typically, algorithm negotiation takes  place by having one side announce all the algorithms it supports while the other side chooses the  best ones that it also supports.
 11.15.1 Suites vs. à la Carte  There are several types of cryptographic algorithms a protocol might use, e.g., encryption, integrity  protection, hashing.
Suppose there are four types of cryptographic algorithms to be negotiated, and  for each of those, there are five choices.
It is feasible for Alice to say, “Here are my acceptable  encryption algorithms.
Here are my acceptable encryption keysizes.
Here are my acceptable hash  algorithms.
Here are my acceptable algorithms for integrity protection.”
Four types of algorithms  times five choices for each is twenty items to list when negotiating.
 But what if not all the algorithms work with each other?
AES-GCM, for example, provides  both authentication and encryption, so it would not make sense to choose AES-GCM for one of  those functions and some other algorithm for the other.
TLS before TLS 1.3 solved this by propos - ing suites of cryptographic algorithms, where each suite identifier implied all the algorithms that  would be used.
The problem there was that if an endpoint wanted to support all five choices for four  types of algorithms, it would need to offer 625 different suites!
And as people wanted to standard - ize new cryptographic algorithms, they wanted to be able to allow it to work with all other algo - rithms, so the list of all possible suites became unworkably large.
IPsec’s IKE took a different  approach.
It offered lists of each type of algorithm and allowed its peer to mix and match.
But when  there were objections that an endpoint might not support all combinations, they introduced a com - plicated additional mechanism that allowed alternate whole sets where mix and match was allowed  within them.
This evolved into a very complex solution to what seemed like a very simple problem.
 TLS 1.3 greatly reduced the number of algorithms it would support and went to more of an à la  carte approach where endpoints were required to support arbitrary combinations of the algorithms  they offered.
At the time of this writing, it is yet to be seen whether they will be able to make that  simple approach work or whether people wanting to support different variations will cause the offer  structure to balloon in complexity again.
11.15.2 HOMEWORK 345  11.15.2 Downgrade Attack  One potential security flaw, if the negotiation isn’t done correctly, is that an active attacker Trudy  might be able to trick Alice and Bob into using weaker cryptography by removing from Alice’s  suggestions the ones that Trudy can’t break.
Alice and Bob would like to agree on the strongest  possible cryptography that they both support.
If Alice is willing when necessary to use weak crypto  and a weak algorithm is among the choices, and Bob is willing when necessary to use weak crypto,  removing the strong choices from Alice’s list may cause them to agree on weak crypto if the proto - col is not carefully designed.
 The reason this is a common vulnerability is that while Alice and Bob are negotiating crypto - graphic algorithms, they probably do not yet have a shared secret with which to do integrity protec - tion of the packets.
The solution is to wait until they have established a shared secret and then detect  the tampering by having Alice reiterate (in a cryptographically protected way) what proposals she  had made.
 The way both IPsec and TLS defend against this attack is to accumulate a hash of all the mes - sages sent during connection setup before a key is established and then send that hash (encrypted  and integrity protected) once the key is established.
That is effective unless Trudy managed to get  Alice and Bob to agree to such weak algorithms that Trudy can defeat the session authentication  mechanism in real time.
But the only way Trudy will succeed is if both Bob and Alice support a  choice that is absurdly weak.
It is easy for either Alice or Bob to defend against that case—just  don’t support such a weak cryptographic algorithm.
 11.16 HOMEWORK  1.
Consider Protocol 11 -12.
R is the challenge sent by Bob to Alice, and S is the secret Alice and  Bob share. (
S is shorthand in this problem for KA-B.) Which of the following are secure for a  session key?
 S ⊕ R {R+S}S {R}R+S {S}S {R}S {S}R  2.
Suppose we are using a three-message mutual authentication protocol, and Alice initiates  contact with Bob.
Suppose we wish Bob to be a stateless server, and therefore it is inconve - nient to require him to remember the challenge he sent to Alice.
Let’s modify the exchange so  that Alice sends the challenge back to Bob, along with the encrypted challenge.
So the
11.16 346 C OMMUNICATI ON SESSION ESTABLI SHMENT  protocol is:  Alice I’m Alice  R  R, {R}KA-B Bob  Is this protocol secure?
 3.
In the discussion of Protocol 11-4, Bob remembers all the timestamps he’s seen within the  last ten minutes.
Why is it sufficient for him to remember only ten minutes’ worth of times - tamps?
 4.
In Protocol 11-4, assume there are multiple instances of Bob, say, Bob1, Bob2, Bob3.
Let’s  assume one of the instances, the main Bob, that we’ll call BobM, is informed by the other  Bobs when timestamps have been used.
When Bobi receives an encrypted timestamp from  Alice, Bobi informs Bob M of the used timestamp and asks Bob M if the timestamp has been  used already.
Bob M tells Bobi whether the timestamp should be accepted or not.
Suppose  Trudy eavesdrops on Alice’s message to Bobi and immediately repeats that encrypted time - stamp to a different Bob instance, Bobj .
Is it possible that Bobj’s query to BobM will arrive at  BobM before Bobi’s query?
If so, Trudy will be able to impersonate Alice, and Alice’s  authentication will fail.
What kind of mechanism might be used to lower the probability that  Trudy will successfully impersonate Alice?
Is there any way to make it impossible for this  scenario to occur? (
Hint: Consider Bobi crashing before informing Bob M.)  5.
Design a two-message authentication protocol, assuming that Alice and Bob know each  other’s public keys, which accomplishes both mutual authentication and establishment of a  session key.
 6.
In §11.3.2 Session Key Based on Public Key Credentials , in item 2, Alice signed her quantity  (i.e., she sent [{ R1}Bob]Alice instead of merely { R1}Bob) to foil Trudy from doing a MITM  attack and to avoid Trudy being able to impersonate Alice.
In item 3, she doesn’t sign her  quantity, i.e., she sends { R1}Bob to Bob.
Bob chooses R2 and sends { R2}Alice to Alice.
They  use R1⊕ R2 as a session key.
Why isn’t it necessary for Bob and Alice to sign their quantities  in item 3 when they are using R1⊕ R2 as the session key?
 7.
There is a product that consists of a fancy telephone that, when talking to a compatible fancy  telephone, does a Diffie-Hellman key exchange in order to establish a secret key, and the  remainder of the conversation is encrypted.
Suppose you are an active wiretapper capable of  modifying messages before relaying them.
How can you listen to a conversation between two  such telephones?
11.16 HOMEWORK 347  8.
Suppose the telephones in Homework Problem 7 display (to their users) a hash of the session  secret.
How can Alice and Bob, using these telephones to talk, detect whether someone is  eavesdropping?
 9.
Consider this protocol and assume R fits into a single block and Alice encrypts R in CBC  mode, so { R}KA-B is computed (using CBC mode) by having Alice choose an IV and sending  IV, {IV⊕R}KA-B. Suppose Eve sees an exchange between Alice and Bob, where the chal - lenge is R1.
How can Eve then impersonate Alice, if Bob sends a different challenge R2?
 I’m Alice  R  {R}KA-B = IV | {IV ⊕ R}KA-B AliceBob  Protocol 11-25.
Another Protocol in Which R Must Be Unpredictable  10.
Referring to §11.10 Endpoint Identifier Hiding , modify Protocol 11-20 to hide the initiator’s  identity rather than the target’s identity.
 11.
As mentioned in §11.10 Endpoint Identifier Hiding , it is possible to hide both identities from  active attackers if Alice and Bob share a secret key and there is a small set of entities that  might initiate a connection to Bob.
Show such a protocol.
 12.
As mentioned in §11.10 Endpoint Identifier Hiding , it is possible to design a protocol that  will hide both identifiers from an active attacker, assuming that Alice (the initiator) already  knows Bob’s public key.
Show such a protocol.
 13.
Assuming private key operations are very slow, show a protocol that runs faster with an extra  message.
Suppose transmission delay is longer than the time it takes to do a private key oper - ation.
Would the protocol still complete faster with an extra message?
 14.
In Protocol 11 -22, explain why Bob knows that Alice is the real Alice and not someone  replaying Alice’s messages.
How does Alice know that it’s the real Bob if she uses a different  a each time?
Modify the protocol to allow both Alice and Bob to reuse their a and b values  and yet have both sides be able to know they are talking to a live partner.
 15.
Assume a stateless session resumption scheme as described in §11.13 Session Resumption/  Multiple Sessions .
Suppose Bob changes his S every ten minutes, and yet you’d like to be able  to resume sessions that have been idle for longer than that, say, two hours.
How might that  work?
 16.
In the DASS session resumption protocol described in §11.13 Session Resumption/Multiple  Sessions , under what circumstances does Bob save computation?
Under what circumstances  does Alice save computation?
Is there a case where Bob saves computation but Alice does  not?
12 IPSEC  As we said in Chapter 11 Communication Session Establishment , IPsec is a secure session protocol  that runs on top of network layer 3 (see§11.7 What Layer? ).
The implication of running directly on  layer 3 ( e.g., IP) is that each packet is independently cryptographically protected.
IPsec does not  guarantee that all packets will arrive or that those that do arrive will be delivered in the order they  were sent.
IPsec only guarantees that packets that do not meet the integrity check will be discarded, and packets that are duplicates will be discarded.
This design makes it easy to implement in net - work adapters.
IPsec does not need to buffer packets.
IPsec can process and deliver packets inde - pendently, even if they arrive out of order.
IPsec could in theory work on top of any layer 3 protocol, and, indeed, works with both IPv4 and IPv6.
 IPsec consists of several pieces.
One is the initial authentication handshake, which is speci - fied by IKE (Internet Key Exchange, RFC 7296).
Another is the encoding of data packets, which is  in one of two formats: ESP (Encapsulating Security Payload, RFC 4303) or AH (Authentication  Header, RFC 4302).
IPsec was originally envisioned to also be able to protect multicast traffic, and  ESP and AH are designed to handle both unicast and multicast.
But IKE can only establish keys for a connection between two endpoints, so there is no standard mechanism for setting up multicast IP  streams (other than perhaps manual configuration).
 12.1 IPSEC SECUR ITY ASSO CIATIONS  As we mentioned in §1.5 Cryptographically Protected Sessions , IPsec refers to a cryptographically  protected session between Alice and Bob as a security association or SA.
 Associated with each end of an SA is a cryptographic key and other information such as the  identity of the other end, the sequence number currently being used, and the cryptographic services  being used ( e.g., integrity only, or encryption +integrity, and which cryptographic algorithms should  be used).
An IKE-SA (the SA Alice and Bob use to do mutual authentication, signaling, and creat - ing child-SAs) is bidirectional.
All the expensive public key operations are performed in the  IKE-SA.
The IKE-SA is then leveraged to create what IPsec refers to as child-SA s, on which actual  349
350 IPSEC 12.1.1  application data is sent.
Child-SAs between Alice and Bob are unidirectional and always created in  pairs (Alice-to-Bob traffic and Bob-to-Alice traffic).
A child-SA will either use ESP or AH format.
 ESP (encapsulating security payload) has optional integrity protection and optional encryption.
AH  (authentication header) only has integrity protection.
 The IPsec data header (ESP or AH) includes a field known as the SPI (SECURIT Y PAR AMETER  INDEX ) that identifies the security association, allowing Bob to look up the necessary information  (such as the cryptographic key) in his SA database for a received packet.
The SPI value is chosen by  the destination (Bob).
It would seem as though the SPI alone should allow Bob to know the SA, since Bob can ensure that the SPI for the SA is unique with respect to all the sources that Bob has SAs with.
But the IPsec design was intended to also allow IPsec to protect multicast data (where the  network delivers a packet to multiple recipients).
If Bob is receiving multicast data, Bob would not  have chosen the SPI, and the SPI value for the multicast group might be equal to an SPI value that  Bob already assigned for an SA between him and a single other principal.
Or two different multi - cast groups might choose the same SPI.
Therefore, the SA is identified by both the SPI and the des - tination address. (
The destination IP address of a packet received by Bob will be Bob’s own IP  address for unicast or a group address if it’s multicast.)
Furthermore, IPsec allows the same SPI val- ues to be assigned to different SAs if one SA is using AH and one is using ESP, so the SA is defined  by the triple 〈SPI, destination address, flag for whether it’s AH or ESP 〉.
 IKE-SAs are also distinguished by SPIs, but the SPIs are handled a little differently than in  child-SAs.
In IKE, both Alice’s chosen SPI (SPI A) and Bob’s chosen SPI (SPI B) are in each IKE  packet.
It would have been simpler to have the pair be 〈SPIA,SPIB〉 when Bob is sending to Alice,  and 〈SPIB,SPIA〉 when Alice is sending to Bob.
If the recipient’s SPI value were always first, then  the recipient could just look at the first SPI value in the pair and find it in his table (since he assigned the value).
However, IKE chose to always have Alice’s (the original initiator of the IKE)  SPI first and Bob’s (the responder’s) second.
There is a flag in the IKE header indicating whether  this packet is from the original SA initiator or the original SA responder. (
This was how it was done  in IKEv1.
It would have been nice to have fixed this in IKEv2, because then IKEv2 would have  been a bit simpler.)
 12.1.1 Security Association Database  A system implementing IPsec keeps a security association database.
When transmitting to IP desti - nation X, the transmitter looks up X in the security association database, and that entry will tell it  how to transmit to X, i.e., it will provide the SPI, the key, the algorithms, the sequence number,  which IP addresses are acceptable to send on this SA, and so on.
When receiving an IPsec packet, the SPI of the received packet is used to find the entry in the security association database that will  tell the receiver which key, which algorithms, which sequence numbers within a range have already  been used, and so on to use to process the packet.
12.1.2 IPSEC SECURI TY ASSOCIAT IONS 351  12.1.2 Security Policy Database  Firewalls are configured with tables telling them what type of traffic to allow, based on information  such as the IP header source and destination addresses and TCP ports.
IPsec is assumed to have  access to a similar database specifying which types of packets should be dropped completely,  which should be forwarded or accepted without IPsec protection, and which should be protected by  IPsec, and if protected, whether they should be encrypted and/or integrity-protected.
Decisions  could, in theory, be based on any fields in the packet, e.g., source IP address, destination IP address,  protocol type in the IP header, and layer 4 (TCP or UDP) ports.
When Alice and Bob negotiate  parameters for a child-SA, they also negotiate which ranges of IP addresses and ports are accept - able to send on that SA.
If Bob is configured to only accept a certain set of IP addresses, and Alice  sends packets to a different IP address over the SA, Bob will drop the packet.
But it would be more  polite to warn Alice which IP addresses he is willing to handle packets for on that SA.
Being polite  in this case saves bandwidth, but more importantly, if Alice has an alternative path (perhaps a dif- ferent SA to Bob) that will not drop the traffic, it is certainly better for Alice to know, so she is not  sending traffic just to be dropped.
 12.1.3 IKE-SAs and Child-SAs  Alice and Bob use the IKE protocol to do mutual authentication and establish a session secret.
But  the SA created by IKE is not actually used for sending application data.
Instead, the IKE-SA is used  to inexpensively create child-SAs, and it is over a child-SA that application data will be transmitted.
 The session key for a child-SA spawned from an IKE-SA is a function of the secret created by the  IKE handshake, along with other information such as nonces that are exchanged during the hand - shake to create a child-SA.
 Why would Alice and Bob want more than a single SA to communicate with each other?
 Wouldn’t it be simpler to create a single SA and send data on that, rather than to first create an  IKE-SA and then create a child-SA for sending data?
The rationale for this feature is that perhaps  Alice and Bob would want to create many different SAs for sending data between them.
Each  child-SA has separately negotiated cryptographic algorithms.
Alice and Bob might create one  child-SA with only integrity protection because the data is not security-sensitive, and they don’t  want to spend extra computation encrypting the data.
They might create another child-SA for nor - mal security-sensitive data and use 128-bit AES.
And for super-sensitive data, they might create another child-SA and use 256-bit AES.
They can also roll over the session key on a child-SA by  creating a new child-SA with a new key and closing the old child-SA.
And some customers feel  more secure if their data-in-transit is sent with a different encryption key than some other cus - tomer’s data, so they would like separate SAs.
352 IPSEC 12.1.3  Also, IPsec puts a sequence number on data packets and is supposed to discard duplicate  packets.
Note that is the only purpose of the IPsec sequence number.
IPsec does not put packets  back in order or ask for retransmission of lost packets.
But IPsec is not supposed to deliver a replay  of a packet to the application.
So Bob (the receiver) needs to remember all the used sequence num - bers, but only for sequence numbers that are close enough to the most recent one received that net - work delays could account for receiving a packet with that sequence number.
Bob can discard  packets with sequence numbers older than that.
 The scenario the IPsec designers envisioned that would benefit from many child-SAs  between Alice and Bob is that Alice is sorting data into streams that she sends on different paths.
Alice might do this because one application requires a different quality of service than another, and  so the paths that different applications are transmitted on might have very different delay character - istics.
Another scenario is if Alice and Bob are firewalls multiplexing traffic for many customers  between themselves, and some customers might have paid for higher quality of service.
If streams  multiplexed over the same IPsec SA are sent on very different paths, the delays on the different  paths might be very different, and Bob would have to remember sequence numbers from a larger  range.
If all the packets on a single child-SA are handled by the network similarly, Bob does not  need to remember a very large range of used sequence numbers (see Figure 12-1).
 Bob Alice  Figure 12-1.
Multiple Paths between Alice and Bob  Instead of IPsec’s design of creating lots of child-SAs, Alice and Bob could have indepen - dently created separate SAs for each class of traffic.
However, creating child-SAs is very inexpen - sive once the IKE-SA has been established.
All the expensive computation ( e.g., private key  operations, Diffie-Hellman) is done during the IKE-SA (although, optionally, Alice and Bob can do  a Diffie-Hellman handshake when creating a child-SA).
Without the optional Diffie-Hellman hand - shake, creating the IKE-SA and a single child-SA is not more expensive than doing a single hand - shake and establishing a single SA, because the creation of the first child-SA is piggybacked on the  IKE exchange.
The concept of two different SAs (IKE and child) makes it a bit confusing for  understanding IPsec, but even in the case where Alice and Bob just want to have a single SA for  sending data, the IPsec design doesn’t require additional round trips or significantly more computa - tion.
12.2 IKE (I NTERNET KEY EXCHANGE PROTOCOL ) 353  12.2 IKE (I NTERN ET KEY EXCHANG E PROTOC OL)  The original version of IKE was overly complex, documented in several different documents (RFCs  2407, 2408, and 2409) with conflicting terminology and technical details, and vague about things  like which end of the connection should be responsible for lost handshake messages.
It was while  trying to document IKE for the second edition of this book that we realized how baroque, and in some cases, broke, the first version of IKE was.
We 1,2 suggested to the working group various ways  of simplifying IKE and found out from them which features they really wanted to retain.
I1 was the  chief designer of IKE version 2, which was much simpler, more efficient, and retained all the fea - tures that people had plausible reasons to want.
The goal of preserving as much of the flavor of the design of IKEv1 as possible made IKEv2 not quite as clean as it might have been if it had been  designed from scratch.
We will only describe IKE version 2 here.
 IKE runs over UDP (see §1.8.2 TCP and UDP Ports ).
Unlike TCP, UDP does not retransmit  messages or give sequence numbers to messages.
The only things it really provides are ports to  enable the receiver of a UDP message to know which process to give the message to.
UDP packets  can be very large, and IP will, if necessary, fragment a large packet into chunks that can traverse the  network.
The IP implementation at the other end will reassemble all the pieces of the UDP packet  before sending the UDP packet to the destination process.
IKE uses UDP port 500 and/or 4500.
 All messages in IKE consist of a request and a response.
The usual case of creating the  IKE-SA between Alice and Bob consists of two request/response pairs ( e.g., four messages total;  see Protocol 12-2 ).
Additionally, the first child-SA can be negotiated in the same set of messages.
 Alice SPIA/0: IKE crypto offered, g A , NA Bob message 1  SPIA/SPI B: IKE crypto selected, g B , NB, preferred CAs  message 2  SPIA/SPI B:{“Alice”, certs, preferred CAs, “Bob”, signature on  previous messages, child-SA crypto offered, addresses offered}  message 3  SPIA/SPI B:{“Bob”, certs, signature on previous messages,  child-SA crypto selected, addresses selected}  message 4  Protocol 12-2.
IKEv2 Initial Exchange
354 IPSEC 12.2  There might be more messages, for instance, if Alice uses a Diffie-Hellman group that is unaccept - able to Bob (in which case she has to restart the negotiation), or if Bob wants Alice to prove she can  receive at the IP address she is sending from (see §11.9.2 Allowing Bob to Be Stateless in IPsec ), or  if messages are lost and the requester times out and retransmits the request.
 In the first two messages, Alice and Bob accomplish the following:  • Negotiate cryptographic algorithms for cryptographic protection of the IKE-SA.
Alice sug - gests which algorithms she’s willing to use in message 1, and Bob chooses from that set and informs Alice of his choice in message 2.
Note that she also sends a Diffie-Hellman value  (g A).
If Bob doesn’t like that choice of Diffie-Hellman parameters, Bob might reject the first  message.
Alice will start over, using a Diffie-Hellman group that Bob will accept.
This rare  case will result in an extra two messages.
 • Alice chooses her SPI value SPI A (for the IKE-SA) in message 1.
Since she doesn’t yet know  Bob’s chosen SPI value, she sets that field to 0.
Note that SPI B works as a stateless cookie for  Bob (see §11.9.2 Allowing Bob to Be Stateless in IPsec ).
Suppose Bob is under attack  (swamped with garbage requests for IKE-SA creation from bogus IP addresses).
Bob can avoid doing any significant computation or keeping any state until he confirms that the initia - tor can receive at the IP address in the source address of the IP packet.
Bob confirms that the  initiator can receive at that IP address by responding to a create IKE-SA request containing a  zero SPI B with a message that says “repeat your request, but include this nonzero SPI B when  you repeat your request.”
Bob will have chosen a value of SPI B that he can reconstruct but  that nobody else will be able to predict.
Then Bob can forget he received that request.
 • Negotiate a master session secret SM.
SM is a function of the Diffie-Hellman exchange and  the randomly chosen nonces ( NA and NB).
SM is used, along with the same nonces ( NA and  NB) and the SPIs to generate five more keys: one encryption key for Alice-to-Bob IKE traffic,  another for Bob-to-Alice IKE traffic, an integrity key for Alice-to-Bob IKE traffic, a different  integrity key for Bob-to-Alice IKE traffic, and another master secret we’ll call SC that will be  used to generate session keys for child-SAs.
The keys for a child-SA (two encryption keys  and two integrity keys) are derived from SC, the nonces exchanged during creation of the  child-SA, and the Diffie-Hellman value (if an optional Diffie-Hellman was done during cre - ation of the child-SA).
 • Alice and Bob let the other side know which CAs they trust, so that the other side knows  which certificates to send.
Note that IPsec allows for other ways to authenticate, e.g., with a  configured pre-shared secret key, but usually authentication is done with certificates.
 • Alice and Bob each show knowledge of their private key and detect whether a MITM med - dled with messages 1 and 2 (where there was no integrity check because Alice and Bob had  not yet established a session key) by signing fields from the previous messages.
12.3 CREAT ING A CHILD-SA 355  • Alice’s and Bob’s identities are hidden from an eavesdropper because messages 3 and 4 are  encrypted.
 • They create the first child-SA (in messages 3 and 4), along with negotiating which IP  addresses are acceptable to send on this child-SA.
When creating a child-SA, each side  chooses the SPI they would like.
 12.3 CREATING A CHILD-SA  Once the IKE-SA is established, either side of the IKE-SA can request establishing an additional  pair of child-SAs (the first pair of child-SAs is piggybacked on the initial IKE-SA establishment).
 Protocol 12-3 illustrates creating a pair of ESP-SAs, including doing the optional Diffie-Hellman.
Alice SPIA/SPIB:{ESP crypto offered, g A , NA,  addresses offered} Bob message 1  SPIA/SPI B:{ESP crypto selected, g B , NB,  addresses selected}  message 2  Protocol 12-3.
IKEv2 Rekey or Extra Child SA  SPIA/SPIB are the pair of SAs from the IKE-SA.
Alice offers a set of cryptographic algorithms she  is willing to support.
She also suggests the list of IP addresses she would like to send over this SA.
 Bob chooses from the sets suggested by Alice.
There is no issue with a MITM because these mes - sages are cryptographically protected by the IKE-SA.
The SPIs chosen for the child-SAs are  included in the CRYPT O OFFE RED and CRYPT O SELECTE D fields.
 The handshake in Protocol 12-3 can also be used to roll over the IKE-SA key by creating a  new IKE-SA and then deleting the old one.
356 IPSEC 12.4  12.4 AH AND ESP  AH and ESP are the two types of IPsec headers for data packets.
AH provides integrity protection  only.
ESP provides encryption and/or integrity protection.
To understand why there are two headers  defined, and why they look so different, it is important to understand the history.
The AH commit - tee had a lot of IPv6 enthusiasts.
The ESP committee was composed of security people who didn’t  care what kind of layer 3 protocol they ran over.
Mostly the two committees ignored each other.
 Given that ESP optionally provides integrity protection (in addition to optional encryption),  it’s natural to wonder why AH is needed.
In fact many people argue (and we concur) that AH is not  necessary.
 ESP and AH provide integrity protection differently.
Both provide integrity protection of  everything beyond the IP header, but AH provides integrity protection for some of the fields inside the IP header as well.
We explain why it is not necessary to protect the IP header, and the real moti - vation for AH doing so, in §12.4.2 Why Protect the IP Header?
 12.4.1 ESP Integrity Protection  Originally, AH was intended to provide integrity protection, and ESP was intended to provide  encryption only.
The original IPsec vision was that if people wanted both encryption and integrity  protection, they would use both headers.
But at some point the ESP people (correctly) decided it  was dangerous to provide encryption without integrity protection, so they added optional integrity protection.
The AH people didn’t notice, or at least, they didn’t object.
They could have argued that  anyone who wanted to get integrity protection should also use the AH header.
 So, given no objections, ESP added optional integrity protection.
But later, the ESP people  said, “People might want integrity only, perhaps for performance reasons.”
So they wanted to make encryption optional in ESP.
But this time, the AH people were paying attention and objected, say - ing that if you want only integrity protection, you should use AH.
They insisted that ESP must pro - vide encryption.
 So the ESP people said, “Okay.
Encryption is mandatory.
We can live with that.
But we can  use any encryption algorithms we want, right?”
The AH people said, “Of course.”
This led to my 2  favorite specification ever, RFC 2410, “The NULL Encryption Algorithm and Its Use With IPsec”.
 This defines a new encryption algorithm (null encryption).
The RFC brags about how efficient and  flexible it is.
It will work with any key size.
There is no reason even for the two ends to agree on a key.
There is even some test data to test that your implementation is implementing null encryption properly.
 Since null encryption is one of the algorithms that can be negotiated, this means that encryp - tion is effectively also optional.
12.4.2 AH AND ESP 357  12.4.2 Why Protect the IP Header?
 AH advocates claim AH is needed because it protects the IP header.
But nobody has proposed any  plausible reason why it’s important to protect the IP header.
Some people claimed to have really  good reasons, but couldn’t divulge the reasons because they were secret.
Routers along the path  cannot enforce AH’s integrity protection, because they would not know the session key for the Alice-Bob security association.
And once Bob receives the packet, all he needs to know is that the  sender knew the session key and that the packet’s integrity check verifies properly.
 AH’s desire to protect the IP header makes the protocol complicated to implement and com - pute-intensive.
Some fields in the IP header get modified by routers, so they can’t be included in  AH’s end-to-end integrity check.
For example, the TTL field (which counts how many hops remain  before the packet should be dropped) must be decremented by every router.
So, AH specifies which  fields in the IP header are mutable ( e.g., TTL), and therefore not included in the AH integrity check;  which ones are immutable (and therefore included in the integrity check); and which fields are  mutable but predictable (such as fragmentation information, which might change along the path,  but what the packet will look like when reassembled is predictable).
This means that an implemen - tation can’t simply compute an integrity check based on the packet but must zero out some fields (the mutable fields) before computing the integrity check.
 A motivation for some of the AH people to want to protect the IP header was that they (being  IPv6 enthusiasts) were frustrated that the world didn’t immediately deploy IPv6.
Note that if they  really wanted the Internet to have bigger layer 3 addresses, they should have adopted CLNP, as rec - ommended in 1992 by the IAB (Internet Architecture Board).
CLNP had 20-octet addresses, was  widely deployed at the time, and, in fact, is technically superior to IPv6 in many ways.
But espe - cially switching the Internet to bigger addresses in 1992, when the Internet was much smaller, would have been very easy.
 So the IPv6 proponents hoped that IPsec (among other features) would be the motivator for  moving to IPv6.
Some IPv6 advocates proposed making it illegal to make any improvements  (including IPsec) to IPv4, so that if the world wanted any of the stuff IETF designed (since the  mid-1990s or so) it would have to move to IPv6.
And they especially viewed NAT (Network Address Translation) as one of the reasons people continued to use IPv4 (because, indeed, NAT  greatly expands the size of network that can be supported with IPv4).
NATs allow networks to have  addresses that are only locally meaningful, and the same block of locally meaningful addresses can  be used in multiple networks.
A node Alice inside such a network cannot be spoken to from outside  unless Alice first initiates communication with an external node.
If Alice does send a packet to Bob,  where Bob’s address is globally meaningful, the NAT box between Alice’s network and the rest of the Internet will assign Alice a temporary globally reachable address.
The NAT box will translate  the source address on packets from Alice to Bob to Alice’s temporary globally reachable address  and translate the destination address on packets from Bob to Alice to Alice’s internal address.
358 IPSEC 12.4.3  So, the AH designers saw an opportunity to “break NATs”.
NATs need to modify the  addresses in the IP header.
If an AH-protected packet traversed a NAT box, the integrity check  would fail.
The vision of the AH designers was that as soon as AH was deployed, people would say,  “NAT boxes make AH not work.
Get rid of the NAT boxes.”
But what people did say, after their network was working fine with NAT, and then AH got deployed and things stopped working, was  “Turn off this AH stuff.
It’s making things break.”
 Note that there is some complication involved in making ESP and IKE work through NATs.
 The simplest strategy is to have IKE and ESP run over UDP port 4500 instead of directly over IP.
 Then both IKE and ESP will work through the NAT.
The IKE spec specifies some elaborate mech - anisms for Alice and Bob to figure out whether there is a NAT box between them, and if not, Alice  and Bob can save the eight octets of the UDP header.
 12.4.3 Tunnel, Transport Mode  The IPsec specification talks about two modes of applying IPsec protection to a packet.
Transport  mode refers to adding the IPsec information between the IP header and the remainder of the packet.
 Tunnel mode refers to keeping the original IP packet intact and adding a new IP header and IPsec  information (ESP or AH) outside. (
See Figure 12 -4.)
 Transport Mode Tunnel Mode  IP header rest of packet original packet IP header rest of packet  IP header IPsec rest of packet new IP hdr IPsec IP header rest of packet  Figure 12-4.
Transport Mode and Tunnel Mode  Transport mode is most logical when IPsec is being applied end-to-end.
A common use of  tunnel mode is firewall-to-firewall, or endnode-to-firewall, where the data is only protected along  part of the path between the endpoints.
Suppose two firewalls establish an encrypted tunnel to each  other across the Internet (see Figure 12 -5).
They treat the tunnel as if it is an ordinary, trusted link.
 In order to forward packets across that link, F1 adds an IP header with destination =F2.
When A  launches an IP packet to destination B, it will have, in the IP header, source = A and destination =B.  When F1 forwards the packet to F2 across the encrypted tunnel, it will use IPsec tunnel  mode.
F1 will not modify the inner header, other than doing what any router would do when for - warding a packet, such as decrementing the hop count.
The outer IP header added by F1 will have  source =F1 and destination = F2.
The inner header will be unmodified by the routers along the path  between F1 and F2.
Those routers will only look at the outer IP header.
12.4.3 AH AND ESP 359  Internet F1 F2  A B  added by firewall F1 original packet  IP: src= F1, dst= F2 ESP IP: src= A, dst=B  Figure 12-5.
IPsec,Tunnel Mode, Between Firewalls  Transport mode is not strictly necessary, since tunnel mode could be used instead.
Tunnel  mode just uses more header space since there will be two IP headers.
 The same packet might have multiple layers of IPsec (ESP and/or AH) headers and might be  multiply encrypted (see Figure 12-6).
Suppose A and B are talking with an encrypted end-to-end  connection.
Their packets will contain an ESP header.
When F1 forwards it across the tunnel to F2,  F1 takes the entire packet (including the IP+ESP header) and adds its own IP +ESP header.
F1  encrypts the entire packet it received, including the IP header, with the key that F1 shares with F2.
 original packet as launched by A, encrypted with the F1–F2 key  IP: src= F1, dst= F2 ESP IP: src= A, dst= B ESP  Figure 12 -6.
Multiply Encrypted IP Packet  Tunnel mode is essential between firewalls in order to preserve the original source and desti - nation addresses; and as we said earlier, tunnel mode can be used instead of transport mode at the  expense of adding a new IP header.
Given that IPsec is too complex, many have argued that getting  rid of transport mode would be one way of simplifying IPsec.
But transport mode is such a small  piece of the complexity of IPsec that we don’t feel it’s worth worrying about.
Far more useful  would be to get rid of AH.
360 IPSEC 12.4.4  12.4.4 IPv4 Header  The IPv4 header is defined in RFC 791.
Its fields are  size  4 bits  4 bits  1 octet  2 octets 2 octets  3 bits  13 bits 1 octet 1 octet version  header length (in 4-octet units)  type of service  length of header plus data in this fragment  packet identification  flags (don’t fragment and last fragment)  fragment offset  hops remaining, known as TTL (time to live)  protocol  header checksum  source address  destination address  options 50= ESP, 51=AH  2 octets  4 octets  4 octets  variable  For the purposes of this chapter, the most important field is the PROTO COL field, which indicates  what follows the IP header.
Common values are TCP (6), UDP (17), and IP (4).
 IPsec defines two new values for the PROT OCOL field in the IP header: ESP = 50 and AH =51.
 For example, if TCP is on top of IP without IPsec, the PROTOCOL field in the IP header will be 6.
If  TCP is used with IP using AH, for instance, then the PROTOCOL field in the IP header will equal 51,  and the PROTOCO L field in the AH header will be 6 to indicate that TCP follows the AH header.
If  the packet is encrypted using ESP, then the PROT OCOL field in the IP header will be 50 but the actual  PROT OCOL field, the one that would have appeared in the IP header if encryption with ESP was not  being used, will be encrypted and therefore not visible until the packet is decrypted.
 12.4.5 IPv6 Header  The IPv6 header is defined in RFC 2460, and its fields are  # octets  4  2  1  1  16  16 version (4 bits) | type of service | flow label  payload length  next header  hops remaining  source address  destination address
12.5 AH (A UTHENTI CATION HEADER ) 361  In IPv6, the equivalent field to IPv4’s PROT OCOL is NEXT HEADER .
It has the same values  defined as the IPv4 PROT OCOL field, so ESP =50 and AH =51.
IPv6-style extension headers  (roughly equivalent to OPT IONS in the IPv4 header) are encoded as  # octets  1  1  variable next header  length of this header  data for this header  LENGT H OF THIS HEADER is in units of 8-octet chunks, not counting the first 8-octet chunk.
 AH looks like an IPv6 extension header, but its PAYLOAD LENGTH is in units of 4-octet chunks  instead of 8-octet chunks (and, like other IPv6 extension headers, doesn’t count the first eight  octets).
This violates one of the protocol folklore rules described in Chapter 17 Folklore , which is  that the LENGT H field should always be defined the same way for all options, so that it is easy to  skip over unknown options.
 DATA FOR THIS HEAD ER is a sequence of options, each one TLV-encoded , which means a  TYPE field, a LENGT H field, and a VALUE field.
The TYP E field is one octet long, and one of the bits  in the TYPE field for options that appear in some extension headers indicates whether the option is  mutable (might change along the path) or immutable (relevant for AH; see §12.5).
The mutable  flag is only useful for AH, and if AH ever goes away, the flag in IPv6 will be very mysterious.
 12.5 AH (A UTHEN TICATI ON HEADER)  The AH header provides authentication only (not encryption) and is defined in RFC 2402.
Its for - mat is patterned after IPv6 extension headers, which all start with NEXT HEADER and PAYLOAD  LENGT H (which gives the length of the AH header), except, as we said in the previous section, AH’s  PAYLOAD LENGT H is in different units than the equivalent field in an IPv6 extension header.
AH is  intended not only to protect the data but the IP header as well.
In IPv4, the AH header must be a multiple of 32 bits, and in IPv6 it must be a multiple of 64 bits.
So the  AUTHENT ICAT ION DATA field  must be an appropriate size to make the header size be the right length.
 Some integrity checks require the data to be a multiple of some blocksize.
If the data is not a  multiple of the blocksize, then AH is computed as if the data were padded to the proper length with  0s, but the 0s are not transmitted.
The AH header looks like
362 IPSEC 12.6  # octets  1  1  2  4  4  variable next header  payload length  unused  SPI (Security Parameter Index)  sequence number  authentication data  The fields in AH are  • NEXT HEADER .
Same as PROT OCOL field in IPv4.
For example, if TCP follows the AH header,  then NEXT HEADE R is 6.
 • PAYLOAD LENGTH .
The size of the AH header in 32-bit chunks, not counting the first eight  octets.
 • SPI.
Discussed in §12.1 IPsec Security Associations .
 • SEQUE NCE NUMB ER.
The sequence number has nothing to do with TCP’s sequence number.
 This sequence number is assigned by AH and used so that AH can recognize replayed pack - ets and discard them.
So, for example, if TCP retransmits a packet, AH will just treat it like a  new packet and give it the next sequence number.
AH will not know (or care) that this is a  retransmitted TCP packet.
 • AUTHENT ICATI ON DAT A. This is the cryptographic integrity check on the data.
 12.6 ESP (E NCAP SULATI NG SECUR ITY PAYLOAD )  ESP allows for encryption and/or integrity protection.
If you want encryption, you must use ESP.
If  you want integrity protection only, you could use ESP or AH.
If you want both encryption and  integrity protection, you could use both ESP and AH, or you could do both integrity protection and  encryption with ESP.
The security association database would tell you what to use when transmit - ting to a particular IP address.
It’s a little odd to call ESP a “header” because it puts information  both before and after the encrypted data, but everyone seems to call it a header so we will, too.
 Technically, ESP always does encryption, but if you don’t want encryption, you use the spe - cial “null encryption” algorithm.
 The presence of the ESP header is indicated by having the PROTOCO L field in IPv4 or the  NEXT HEADE R field in IPv6 equal to 50.
The ESP envelope itself consists of
12.6 ESP (E NCAPSULA TING SECURI TY PAYLOAD ) 363  # octets  4  4  variable variable variable  1  1  variable SPI (Security Parameters Index)  sequence number  IV (initialization vector)  data  padding  padding length (in units of octets)  next header/protocol type  authentication data  The fields are  • SPI.
Same as for AH, discussed in §12.1 IPsec Security Associations .
 • SEQUE NCE NUM BER.
Same as for AH, explained in §12.5 AH (Authentication Header) .
 • INITIAL IZATI ON VE CTOR .
An IV is required by some cryptographic algorithms, such as  encryption with CBC mode.
Although the IV is variable-length (it may even be zero-length),  it’s fixed-length for a particular cryptographic algorithm.
Once the SA is established, the  cryptographic algorithm is known, and therefore the length of the IV field is fixed for the  duration of the SA.
This is also true of the field AUTHE NTICAT ION DAT A.  • DATA .
This is the protected data, probably encrypted.
If it is a tunnel-mode packet, then the  beginning of the data is an IP header.
If it is a TCP packet, and ESP is being used in Transport  mode, then the beginning of the data is the TCP header.
 • PADDING .
Padding is used for several reasons: to make the data be a multiple of a blocksize  for cryptographic algorithms that require it; to make the encrypted data be a different size  than the plaintext so as to somewhat disguise the size of the data (limited because PADDING  LENGT H is only one octet); and to ensure that the combination of the fields DATA , PADDING ,  PADDING LENGTH , and NEXT HEADER is a multiple of four octets.
 • PADDING LENGTH .
Number of octets of padding.
 • NEXT HEADER .
Same as PROTOCOL field in IPv4 or NEXT HEADE R in IPv6, or NEXT HEADER in  AH.
 • AUTHENT ICATI ON DATA .
The cryptographic integrity check.
Its length is determined by the  authentication function selected for the SA; it is zero-length if ESP is providing encryption only.
 If encryption is used, the fields DATA , PADDING , PADDING LENGTH , and NEXT HEADE R are  encrypted.
The AUTHEN TICAT ION DATA appears only if the security association requests integrity  protection with ESP.
If ESP integrity protection is used, all fields in the ESP (starting with SPI and  ending with NEXT HEADE R) are included in the ESP integrity check.
364 IPSEC 12.7  12.7 COMPARISON OF ENCODINGS  AH was designed by IPv6 fans and looks similar to IPv6 extension headers. (
The only difference is  that its LENGT H field is expressed in different units.)
The ESP designers didn’t really care about  IPv6 and designed ESP to be as technically good as it could be, unconstrained with having to look  like something else.
 There are two wasted octets in AH (the “unused” octets) in order for all the fields to be on  4-octet boundaries.
But AH can cleverly avoid padding the data.
The integrity check is calculated as if the data were padded with zeroes to a multiple of a blocksize, but the padding is not transmitted.
 Therefore, if the data needed more than two octets of padding, AH winds up being smaller.
Of  course, this is assuming that IPsec is being used for integrity only, which as we said, we think will  be rare.
If the data is encrypted, then it requires much more overhead to use AH and ESP simulta - neously than just using ESP for both integrity protection and encryption.
 Having the MAC (the integrity check) appear before the data (as it is in AH) means that the  data needs to be buffered and the integrity check computed before the packet can be transmitted.
In contrast, in ESP, the MAC appears after the data.
 Attempting to protect the IP header and needing to classify every field and every option  according to whether it’s mutable or immutable makes AH very complicated.
 At one of the final IETF meetings before AH and ESP were finalized, someone from  Microsoft got up and gave an impassioned speech about how AH was useless given the existence of ESP, cluttered up the spec, and couldn’t be implemented efficiently (because of the MAC in front of  the data).
Our 1,2 impression of what happened next was that everyone in the room looked around at  each other and said, “Hmm.
He’s right, and we also hate AH, but if it annoys Microsoft, let’s leave it in.”
 12.8 HOMEWO RK  1.
Why isn’t the SPI value sufficient for the receiver to know which SA the packet belongs to?
 2.
How is the integrity check processing more efficient with ESP than with AH?
 3.
Suppose Alice is sending packets to Bob using IPsec.
Suppose Bob’s TCP acknowledgment  gets lost, and Alice’s TCP, assuming the packet was lost, retransmits the packet.
Will Bob’s  IPsec implementation notice that the packet is a duplicate and discard it?
 4.
Suppose you wanted the transmitter to assign the SPI rather than the receiver.
What problems  might this cause?
Can it be made to work?
12.8 HOMEWORK 365  5.
When sending encrypted traffic from firewall to firewall, why does there need to be an extra  IP header?
Why can’t the firewall simply encrypt the packet, leaving the source and destina - tion as the original source and destination?
 6.
Referring to Figure 12-5 , assume that A and B are using IPsec in transport mode, and F1 and  F2 have established an encrypted tunnel using IPsec.
Assume A sends a TCP packet to B.  Show the relevant fields of the IP header(s) as given to A ’s IPsec layer, as transmitted by A, as  transmitted by F1, and as received by B.
13 SSL/TLS AND SSH  The concepts in TLS (Transport Layer Security) have been covered in Chapter 11 Communication  Session Establishment , and the concepts are similar to IPsec.
Alice and Bob authenticate and estab - lish cryptographic keys for the session.
 TLS grew out of Netscape’s SSL (Secure Sockets Layer) protocol.
When the IETF took it  over to improve and standardize it, they renamed it TLS (Transport Layer Security).
Since being  called TLS, it has gone through three revisions, so the latest version is 1.3 (RFC 8446).
There is no  real logic to why the TLS versions were named TLS 1.0, TLS 1.1, TLS 1.2, and TLS 1.3 rather than TLS version 1, TLS version 2, TLS version 3, and TLS version 4 or even why TLS 1.0 was not  named SSL version 4.
Most of the revisions were quite minor, but the changes from version TLS  1.2 to 1.3 are interesting, and we’ll describe them here.
We will mainly discuss TLS versions 1.2  and 1.3.
 Credentials in TLS are usually PKIX certificates (§10.4.2 Names in Certificates ), but because  the first protocol to use TLS was HTTP (web browsing) and users in general don’t have public keys  or certificates, TLS allows for only the server to have a certificate.
In most deployed scenarios, if  the user needs to authenticate, she will authenticate herself using something like a username/pass - word after the TLS session is established.
As with IPsec, the TLS specification also allows for cre - dentials between Alice and Bob to be a pre-shared secret key rather than certificates, but in TLS, this is a rarely used scenario.
However, because authentication based on pre-shared keys was in the  specification, it was a convenient way to efficiently resume a session (using the previous session  secret as a pre-shared key).
 13.1 USING TCP  TLS is designed to run in a user-level process and runs on top of TCP.
As discussed in §11.7 What  Layer? ,
running on top of layer 4 allows deployment of TLS in a user-level process rather than  requiring operating system changes.
Using TCP (the reliable layer 4 protocol) rather than UDP (the  datagram layer 4 protocol) makes TLS simpler, because TLS doesn’t have to worry about issues  367
368 SSL/TLS AND SSH 13.2  such as timing out and retransmitting lost data, frame size limitations, and congestion avoidance.
 TLS could have retained the advantage of being easily deployable as a user-level process and still  avoided the rogue packet problem discussed in section §11.7 What Layer?
by running on top of  UDP and doing all the timeout/retransmission work of TCP within SSL/TLS, but the decision was  made to live with the rogue packet problem and keep SSL/TLS simpler.
In fact, there is a version of  TLS known as DTLS (Datagram Transport Layer Security, RFC 6347), which runs on top of UDP, and is based on TLS 1.2.
There will be a version of DTLS based on TLS 1.3.
 13.2 STAR TTLS  Some protocols (for instance, email) were designed and deployed long before anyone thought about  security.
New implementations can be upgraded to run over TLS, and the goal is for the new imple - mentations to talk over TLS if possible but still interoperate with legacy implementations.
There are  various solutions one could imagine:  1.
Get a new port assigned to the protocol that TLS-capable implementations would listen on.
 However, getting new well-known ports is sometimes difficult.
 2.
Advertise in DNS whether a service is TLS capable.
 3.
Use a single port, but after the TCP connection is established, send a message, carefully  crafted for that protocol that will be ignored by old implementations but recognized by new  implementations as a desire to speak over TLS.
 Some protocols ( e.g., SMTP) have deployed option 3.
In SMTP, the message is known as StartTLS .
 Since the StartTLS command is sent before there is any cryptographic protection, an attacker could remove that message and cause two TLS-capable nodes to speak without TLS.
 Note that TLS itself can protect itself from a downgrade attack, where an attacker removes  the strong cryptographic choices from the choices Alice offers (assuming Alice has not offered a set  of cryptographic algorithms so weak that an attacker can break them before Alice and Bob time out  the handshake).
But there is no similar defense against an attacker stripping the StartTLS command (see Homework Problem 2 ).
 Note that option 1 also has the problem of having an attacker prevent two TLS-capable  implementations from using TLS.
The attacker, on the path between the two nodes, can throw away  packets sent to the new port, which will cause the initiator to time out the attempt to speak over TLS  and, instead, revert to non-TLS communication.
13.3 FUNCTIONS IN THE TLS H ANDSHAK E 369  13.3 FUNCTI ONS IN THE TLS H AND SHA KE  The handshake accomplishes a variety of functions, some of which are optional:  • Negotiating version numbers (the client gives a list, the server chooses).
 • Negotiating which cipher suite to use (the client gives a list, the server chooses).
 • Having the client specify which DNS name the client wishes to speak to (if multiple services  are sharing the same IP address).
This field is known as SNI (Server Name Indication).
 • Sending Diffie-Hellman values for perfect forward secrecy.
 • Having the server send its certificates (either a single certificate signed by a CA that the  server assumes the client has as a trust anchor or a chain of certificates from (hopefully) a cli- ent’s trust anchor).
 • Having the server send a session ID so the client can resume a session, or create a parallel  session cheaply (similar in spirit to an IPsec child-SA).
 • Having each side send nonces so the session key will be different each time the session is resumed.
 • Optionally autheticating the client.
The server specifies whether it accepts client authentica - tion, and, optionally, what trust anchors the server recognizes and which extensions the server would like to see in the client certificate.
 • Having the server prove knowledge of its private key (by decrypting something or signing something, depending on the TLS version number and cipher suite) and cryptographically  protecting all messages in the exchange to avoid a downgrade attack (having an attacker  remove offered cipher suites or version numbers).
 13.4 TLS 1.2 ( AND EARLIER ) BASIC PROTOCO L  Using the reliable octet stream service provided by TCP, SSL/TLS partitions this octet stream into records, with headers and cryptographic protection, to provide a reliable, encrypted, and integrity- protected stream of octets to the application.
There are four types of records: user data, handshake messages, alerts (error messages or notification of connection closure), and change cipher spec  (which should be a handshake message, but they chose to make it a separate record type).
 In the basic protocol, the client (Alice) initiates contact with the server (Bob).
Then Bob  sends Alice his certificate.
Alice verifies the certificate, extracts Bob’s public key, picks a random  number S from which the session keys will be computed, and sends S to Bob, encrypted with Bob’s
370 SSL/TLS AND SSH 13.4  public key.
Then the remainder of the session is encrypted and integrity protected with those ses - sion keys.
If Bob correctly computes the session key, Alice can know that Bob knows the private  key associated with his certificate.
Note that depending on the cipher suite selected and the TLS  version, there might be many secrets derived from S. For instance, there might be encryption keys  and integrity keys in each direction (for CBC mode).
 First, we’ll present a simplified form of the protocol (Protocol 13-1 ), then discuss various  issues in the full protocol, and finally discuss the details.
 I want to talk, ciphers I support, RAlice, SNI  1  session_id, certificate, cipher I choose, RBob  2 choose secret S,  compute Bob  compute  K = f(S,RAlice,RBob)keyed hash of handshake msgs  4  data protected with keys derived from K K = f(S,RAlice,RBob)  Alice {S}Bob, keyed hash of handshake msgs  3  Protocol 13 -1. (
simplified) SSLv3 through TLS 1.2  The simplified protocol consists of four messages that establish a shared master key:  • Message 1.
Alice says she would like to talk (but doesn’t identify herself) and gives a list of  cryptographic algorithms she supports, along with a random nonce RAlice, that will be com - bined with the S in message 3 to form the various keys.
 • Message 2.
Bob sends a session ID, his certificate, a nonce RBob that will also contribute to  the keys, and Bob’s choice of cipher suites from among those listed by Alice in message 1.
 • Message 3.
Alice chooses a random number S (known as the pre-master secret ) and sends it,  encrypted with Bob’s public key.
She computes the master secret K, which is a function of S  and the two nonces RAlice and RBob.
She sends a keyed hash of the previous messages (using  key K) both to prove she knows the same key K as Bob will compute and to ensure that tam - pering of the previous handshake messages will be detected.
To ensure that the keyed hash  Alice sends is different from the keyed hash Bob sends, each side includes a constant ASCII  string in the hash, with a different string for the client than the server.
Surprisingly (and
13.5 TLS 1.3 371  unnecessarily), the keyed hash is sent encrypted and integrity protected.
The keys used for  encrypting the keyed hash and the rest of the data in the session are derived from hashing K,  RAlice, and RBob.
The keys used for transmission are known as write keys, and the keys used  for receipt are known as read keys.
So, for instance, Bob’s write-encryption key is Alice’s  read-encryption key.
 • Message 4.
Bob proves he knows the session keys and ensures that the early messages arrived  intact, by sending a keyed hash of all the handshake messages, encrypted with his write- encryption key and integrity protected with his write-integrity key.
Since the session keys are  derived from S, this proves he knows Bob’s private key because he needed it in order to  extract S.  At this point, Alice has authenticated Bob, but Bob does not know Alice’s identity.
As deployed today, authentication is seldom mutual—the client authenticates the server, but the server does not  authenticate the client.
The protocol allows optional authentication of the client if the client has a  certificate.
But in the most common case today, if the application on the server wishes to authenti - cate the user, authentication information (such as a username/password) is sent as data over the  established TLS session.
 13.5 TLS 1.3  Unlike the version changes from SSLv3 to TLS 1.0 to TLS 1.1 to TLS 1.2, where the differences  were largely incremental in response to some subtle security vulnerability, the changes in TLS 1.3  were more dramatic and aimed at improving performance.
TLS 1.3 rearranges the fields within  messages in order to reduce the total number of round trips required to complete a handshake and  begin sending application data. (
See Protocol 13-2 .)
TLS 1.2 introduced cipher choices that  ciphers, RAlice, gA  cipher, RBob, gB , {keyed hash, cert, sig, session-id} , {data}  {keyed hash of msgs} , {data}  {data} AliceBob  Protocol 13-2.
TLS 1.3 Session Initiation If No Previous State
372 SSL/TLS AND SSH 13.6  allowed perfect forward secrecy by having a Diffie-Hellman exchange at the beginning of the ses - sion using ephemeral keys and signing them instead of having Alice encrypt the pre-master secret  using Bob’s long term public key.
TLS 1.3 eliminates all cipher suites without perfect forward  secrecy.
That allows encryption of messages to begin before the handshake has completed, keeping  more information away from the prying eyes of at least passive eavesdroppers.
There are several  different Diffie-Hellman groups that can be negotiated for use with TLS.
The most common and  best performing choice is one of the ECDH curves [§6.7.1 Elliptic Curve Diffie-Hellman (ECDH) ].
 For the best performance, Alice has to correctly guess one that will be acceptable to Bob and send a  gA value on speculation.
If it is not acceptable to Bob, he will tell Alice what group she should use  at the cost of an extra pair of messages.
Bob will send his chosen cipher in message 2, and Alice  will send a new message 1, using a Diffie-Hellman group acceptable to Bob.
 13.6 SESSION RESU MPTIO N  SSL/TLS assumes that a session is relatively long-lived, from which many connections can be  cheaply derived.
This is because it was designed to work with HTTP 1.0, which had a tendency to  open lots of TCP connections between the same client and server (one per item on the web page).
 The per-session master secret is established using expensive public key cryptography.
Multi - ple connections can be cheaply derived from that master secret by doing a handshake that involves  sending nonces (so that session keys will be unique) but avoids public key operations.
 SSL/TLS calls this session resumption , though it does not require that one connection end  before the next is started.
It is common for many connections to run in parallel.
During the initial (expensive) handshake, Bob sends Alice a session_id that can be used later if Alice wants to set up  a second connection to Bob with the same cryptographic algorithms but with fresh keys and  bypassing the public key operations.
 In TLS 1.2 and earlier, resumption requires that both Alice and Bob remember the session_id  and master key from the previous connection. (
See Protocol 13-3 .)
In TLS 1.3, the session_id pro - vided by Bob is large enough to carry all the state that Bob needs.
The session_id is sent in the  clear, so it would be poor security practice if things like the session key were transmitted in the  clear.
So Bob should encrypt all the state he needs with a key only he knows and use that quantity  as the session_id.
It is not necessary for Bob to use the session_id this way, but TLS 1.3 allows it.
 If Bob has forgotten the session_id (perhaps because he crashed or because it has timed out),  he ignores the session_id that Alice presents and proceeds with the full handshake and likely pre - sents a new session_id for Alice to use in the future.
 It might seem odd for Alice, when resuming the session, to send a set of cipher suites rather  than just the suite used in the session.
Even if Bob has lost state about the session, wouldn’t he
13.7 PKI AS DEPLOYED BY TLS 373  session-id, ciphers, RAlice  session-id, cipher, RBob, {keyed hash of msgs} Alice Bob  {keyed hash of msgs}  {data}  Protocol 13-3.
Session Resumption for TLS 1.2 and Prior  make the same choice of cipher suite, given the same choices, as he made when the session was  created?
Not necessarily, because perhaps his policy has changed.
So Alice is allowed, when  resuming a session, to send a set of choices, but the set must at least contain the cipher suite that  had been chosen previously for that session.
 Session resumption for TLS 1.3 looks like Protocol 13-4:  session-id, ciphers, RAlice, {data}  cipher, RBob, {keyed hash of msgs}, {data} Alice Bob  {keyed hash of msgs}, {data}  {data}  Protocol 13-4.
TLS 1.3 Session Resumption  13.7 PKI AS DEPLOYED BY TLS  As deployed today, the client typically comes configured with public keys of various “trusted”  organizations (trusted by the browser vendor, not necessarily by the user). (
See §10.6.4 Oligarchy .)
 In some implementations, the user at the client machine can modify this list, adding or deleting  keys.
The server sends a certificate (or certificate chain) to the client, and if it’s signed by one of the  CAs on the client’s list, the client will accept the certificate.
If the server presents a certificate  signed by someone not on the list (such as a self-signed certificate), the user is typically presented  with a pop-up box informing him that the certificate couldn’t be verified because it was signed by
374 SSL/TLS AND SSH 13.8  an unknown authority, and would the user like to go to the site anyway?
We discuss this model of  PKI along with others in §10.6 PKI Trust Models .
 If the server wishes to authenticate the client using certificates, it sends a certificate request in  which it specifies the X.500 names of the CAs it trusts and the types of keys it can handle ( e.g.,  RSA or DSS).
This was a strange piece of asymmetry in TLS 1.2 and earlier, since the client did not  get to specify to the server what sort of certificate chain or key type it wanted.
TLS 1.3 does allow the client to specify this, although most implementations don’t.
 13.8 SSH (S ECUR E SHELL)  SSH has a lot in common with SSL/TLS, and since they are both standardized by the same stan - dards body (IETF), there has been a converging of their characteristics over time.
When the IETF was choosing an algorithm to be TLS, SSL and SSH were two of the candidates.
In the end, TLS  was based almost entirely on SSL.
It is possible that all use of SSH will convert to using TLS even - tually, but there are some important differences that allow SSH to maintain its ecological niche, so  it probably will not go away.
 While SSL was originally designed to protect HTTP traffic carried between web browsers  and web servers, SSH was originally designed to protect remote login sessions.
In Unix, the com - mand line processing application is known as the shell, and SSH secured connections to the shell, hence the name Secure SHell.
 Like SSL/TLS, SSH runs over TCP/IP in order to get its reliability through retransmission.
 As with SSL/TLS, and unlike IPsec, an attacker can disrupt an SSH connection by injecting a single  packet that confuses the TCP connection state.
The cryptographic integrity check will ensure that  the invalid data is not processed, but the two ends of the conversation do not preserve enough state  to recover, so the connection will close.
 Unlike SSL/TLS, which carries a single TCP connection as its payload, SSH is capable of  initiating multiple TCP connections that are carried over a single SSH session.
That means that all  the clever things SSL/TLS does to efficiently create multiple parallel connections between a single  pair of nodes are unnecessary with SSH.
SSH can create additional connections within the crypto - graphic envelope of an existing SSH session.
They will all be encrypted and integrity-protected with the same keys, but they are labeled to keep the connections from getting confused with each  other, and there are no security weaknesses from cryptographically protecting them all with the  same keys.
 When SSL was first developed, a second TCP port was allocated for HTTP connections that  were made using SSL.
By convention, HTTP connections were made to port 80, and HTTP over  SSL connections were made to port 443.
When people wanted to use SSL to protect connection
13.8 SSH (S ECURE SHELL ) 375  types other than HTTP, they needed to allocate a second port to each protocol that wanted to option - ally use SSL, and about 60 ports have been so assigned.
Seeing that this was going to allocate a lot  of ports from a scarce resource pool, many more protocols start up unencrypted and negotiate use  of SSL/TLS.
They then switch over to an SSL/TLS initial exchange on the same port.
 The message sequence (Protocol 13-5) is a lot like that of TLS, but with some differences.
 ciphers , gA  ciphers, gB , {[certs or public key], sig}  {[certs or public key], sig, <connect me to cmd shell>}  {data} AliceBob  Protocol 13-5.
SSH Session Initiation  Alice begins by sending the set of cipher suites she supports, and (assuming the cipher suite uses  Diffie-Hellman for PFS) speculatively can send gA .
These lists include not just cryptographic algo - rithms, but also the forms of authentication she can use to prove her identity and the forms of  authentication she will accept from Bob.
If Alice guessed wrong as to what group to use, then when  she sees Bob’s cipher suite list, she will figure that out and immediately send a gA from the correct  group.
So there is no need for an error message and a retry.
Once Bob sees Alice’s lists and gA , he  can not only send gB but can start encrypting and send his certificates or public key and a signature  on a collection of fields from previous messages.
Since SSH does not engage in session resume,  there is no need for nonces.
If Alice or Bob want to rekey in the middle of a connection, they  exchange new Diffie-Hellman values (or whatever PFS mechanism is used in the chosen cipher  suite).
 SSH can multiplex many connections over a single SSH session.
After Alice and Bob create  the SSH tunnel, Alice and Bob can negotiate connections that will be transmitted over the tunnel.
 SSH has a header comprising a 1-octet command followed by a 4-octet connection identifier, fol - lowed by a 4-octet length.
As with IPsec, Alice and Bob will tell the other side what to use as a con- nection identifier.
For example, assume Bob assigned two connections to be identifier C 1 and C 2,  respectively.
If Alice is sending data from C 1, the octet stream she will send will consist of an octet  identifying what follows is data, followed by C 1, followed by the length of the data, followed by the  data.
Then if she wants to send data for C2, she will send the octet identifying this as data, followed  by C2, followed by the length of data to send on C2.
 Another type of command is “close a connection”, followed by the connection identifier.
13.8.1 376 SSL/TLS AND SSH  13.8.1 SSH Authentication  While some implementations of SSH support authentication using X.509 certificates (as with TLS),  that is not its most common configuration.
SSH does not depend on any sort of PKI, and can  instead authenticate using raw public/private key pairs (rather than certificates).
This is useful in the  original use of SSH, where a user connects to a remote machine and starts a terminal session with a  command prompt.
When Alice connects to Bob for the first time, the software on Alice’s machine  has no idea what Bob’s public key should be, nor is Alice configured with trusted CA keys.
So  when Bob presents his public key, Alice gets a prompt “Bob says his public key is 2832734 …3623.
 Do you want to trust it?”
Alice could look up the public key from some trusted source, but in prac - tice she just says “Yes”, taking a leap of faith.
She doesn’t really know whether she’s connected to  Bob or some attacker impersonating Bob, so she will be suspicious.
But the software on her  machine will remember Bob’s public key and make sure it matches the next time she connects.
In  theory, this exchange is not very secure.
But a Bob impersonator would have to impersonate Bob  every time Alice tries to connect and would have to respond enough like Bob so as to not make  Alice suspicious.
So in practice, this mechanism is quite secure unless Alice is always using fresh  clients so that Bob’s public key is never remembered.
It is also common for administrators to con - figure public keys of well-known hosts, and that mechanism is even more secure.
 This prompting assumes that Alice is a person and not some automated process.
If an auto - mated process is going to be routinely connecting to Bob, it will have to be configured with Bob’s public key.
This can be done by having the adminstrator configure the key or by initially opening a  connection to Bob and answering “Yes” to “Do you want to trust the key?”
 There are many options for Alice to authenticate to Bob.
If Alice is an automated process (or  a human needing enhanced security), she will probably be configured with a public/private key pair  where her public key is configured at Bob.
If she is a human, she will likely use a username and  password or some single sign-on mechanism [see §9.15 Identity Providers (IDPs) ].
If she has a pri- vate key and certificate, Bob can also be configured to accept that (though this is uncommon).
 13.8.2 SSH Port Forwarding  Another important application of SSH is called port forwarding .
It can be used with any applica - tion that uses TCP or UDP ports, transparently adding encryption and integrity protection while  tunneling the application data over an insecure network and frequently tunneling traffic through  corporate firewalls that would block it if they knew what was being tunneled.
This makes SSH port  forwarding a favorite tool of users and attackers alike!
 It works as follows.
Alice opens an SSH session to Bob.
To allow a legacy process (that is not  SSH capable) to benefit from SSH protection when traversing the network between Alice and Bob, the legacy process is configured to speak to Alice’s IP address and a port on Alice’s machine allo -
13.9 HOMEWORK 377  cated for this purpose.
Alice tunnels the packet over the SSH connection to Bob.
Bob’s SSH pro - cess strips off the cryptography and forwards the tunneled legacy packet to the appropriate  destination process.
 This functionality is built into the SSH protocol.
This functionality could be implemented on  top of TLS by defining a TLS client demon that listened on selected ports and forwarded connec - tions over the TLS connection to a TLS-based service that opened outbound connections and for - warded the traffic.
But that would be a lot of work.
The port forwarding functionality is part of the  SSH base specification, which is one reason SSH maintains its ecological niche.
 Firewall adminstrators don’t want to block SSH because it is useful for reaching out to  remote nodes and opening terminal services, but once the connection is open, the firewall can’t tell  what it is being used for.
In general, anyone can tunnel anything over anything ( e.g., HTTP or DNS  lookups), but allowing SSH through makes it easy.
 13.9 HOMEWO RK  1.
Some certificates specify that a public key should only be used for signatures or only for  encryption.
Some public key algorithms only work for signatures or only work for encryp - tion.
In TLS 1.2, does Bob’s public key have to work for encryption, for signatures, or both?
 How about TLS 1.3?
 2.
How does TLS defend against a downgrade attack (where an attacker removes the most  secure cryptographic algorithm from Alice’s suggestions, causing Alice and Bob to commu - nicate using a less secure cryptographic algorithm)?
Why would this defense not work for  StartTLS (see §13.2 StartTLS )?
 3.
In TLS 1.3 (Protocol 13-2), if Alice guesses a Diffie-Hellman group that Bob does not sup - port, Alice and Bob start from the beginning, this time with Alice knowing (based on Bob’s message) a Diffie-Hellman group that Bob will support.
However, note that Bob knows after  receiving Alice’s first message which Diffie-Hellman group he will choose, so he could send  his Diffie-Hellman value in his reply, even though he will ignore Alice’s Diffie-Hellman  value.
Redesign the protocol to take advantage of having Bob immediately send his Diffie-Hellman value in this case.
14 ELEC TRON IC MAIL  SECURITY  The first thing that springs to mind with the phrase email security is a message from Alice to Bob  signed by Alice with her private key and encrypted with Bob’s public key.
Lotus Notes had a pro- prietary, widely deployed implementation of encrypted email in 1989.
PGP was created and  released as open source by Phil Zimmermann in 1991.
Various email standards for user-to-user  signed and encrypted email were developed over twenty years ago, including  • PEM [RFC 1421—Feb 1993]  • PGP/GPG [RFC 2015—October 1996]  • S/MIME [RFC 2311—March 1998]  In the early 1990s, there were deployment barriers such as export controls and patents, but these  have been resolved.
So it is somewhat astonishing that none of these, or anything implementing the  model signed-by-Alice and encrypted-for-Bob, are widely used today.
Encrypted and signed email  is used within some organizations such as the U.S. Government, which has deployed a  hardware-based PKI system using S/MIME encryption, but end-to-end encrypted email is not used by the vast majority of Internet users.
Why didn’t it catch on as the default mechanism for sending  mail?
Perhaps it was too difficult for user Alice to find Bob’s public key or to maintain her own pri - vate key across multiple devices.
Perhaps Bob was unhappy with losing all of his received email  when he forgot his password.
Perhaps large companies discouraged their employees from using  end-to-end encrypted email.
Perhaps the effort of using end-to-end encrypted email was never  worth it, because most people Alice might send email to didn’t support receiving encrypted email.
Or perhaps users just didn’t care.
 There are other email-specific security challenges that are not addressed by end-to-end  encryption and signing.
Without defenses, our inboxes would be buried in spam.
Buggy email pro - cessing software allows malicious email to carry malware that could infect the user’s device.
Often,  blame is placed on the user for having “opened a dangerous email” or “clicked on a suspicious link  in an email”.
However, it should be possible (and the world should aspire) to have safe  email-handling software.
Opening an email or clicking on a link in an email should not infect user Alice’s device or do other horrible things such as emailing something to everyone in her address  379
380 E LECTRONIC MAIL SECURITY 14.1  book.
We suspect that it’s just easier to blame the user than to develop safe email client software.
 The industry is aware of the problem, and there is ongoing research, including DARPA’s SafeDocs  initiative.
 Another impediment to deploying end-to-end encrypted email is that companies require  access to the plaintext of all email from or to the organization, perhaps for legal reasons, or perhaps  to attempt to prevent company confidential information from being sent out of the company.
Theo - retically, this could be accomplished by having the company keep copies of the private keys of all  their employees, but that would be inconvenient for the IT department and distract them from what  they really like to do, such as forcing users to change their passwords every sixty days.
Some secu - rity features interfere with each other.
For example, end-to-end encryption interferes with a middle - box’s filtering spam before it gets to the client device.
 This chapter deals with the conceptual challenges and potential solutions to various email  security issues.
Although there are products that address some of the issues we discuss, we are not intending to describe specifics of any particular product.
Also, the exact deployed mechanisms and  issues depend on current implementations (including their bugs).
So we will concentrate instead on broad conceptual issues.
 14.1 DISTR IBUTIO N LISTS  Electronic mail allows a user to send a message to one or more recipients.
The simplest form of  electronic mail message is where Alice sends a message to Bob.
 To: Bob  From: Alice  Care to meet me in my apartment tonight?
 Usually, a mail system allows a message to be sent to multiple recipients, for example:  To: Bob, Carol, Ted From: Alice  Care to meet me in my apartment tonight?
14.1 DISTRIBUTION LISTS 381  Sometimes it is impossible or inconvenient to list all recipients.
For this reason, mail is often sent to  a distribution list , a name that stands for a set of recipients.
There are two ways of implementing  distribution lists:  The first way ( Figure 14-1) involves sending the message to a site at which the list is main - tained, and that site then sends a copy of the message to each recipient on the list.
We’ll call that the  remote exploder method .
Note that a more commonly used term is a mail reflector , but that term  sounds to us like a service that will send any messages you transmit back to you.
 Distribution List Maintainer  Figure 14-1.
Remote Exploder Recipient 1  Sender  Recipient 2  Recipient 3 msg msg  msgmsg  The second method (Figure 14-2) is for the sender to retrieve the list from the site where it is  kept and then send a copy of the message to each recipient on the list.
We’ll call that the local  exploder method .
 Sender  Recipient 2 msg  msg  msg getlist  list Distribution List Maintainer  Recipient 1  Recipient 3  Figure 14-2.
Local Exploder  Sometimes a member of a distribution list can be another distribution list.
For instance, the  mailing list Security Customers , used to advertise security products, might include law enforc - ers, bankers , locksmiths , and members of organized crime .
It is possible to construct a distri - bution list with an infinite loop.
Suppose someone is maintaining a mailing list for cryptographers.
 Someone else is maintaining one for cryptanalysts.
The cryptanalysts point out that they also want  to hear the information sent to the cryptographers mailing list, so the distribution list Cryptana - lysts is added to the Cryptographers mailing list.
And for similar reasons, Cryptographers is  added to Cryptanalysts .
The mail system must handle infinite loops in distribution lists in a  reasonable manner, i.e., it must send each recipient at least one copy of each message but not an
382 E LECTRONIC MAIL SECURITY 14.2  unreasonable number of copies of any.
Loops like this effectively merge the mailing lists. (
See  Homework Problem 1.)
 The advantages of the local exploder method are:  • It is easier to prevent mail forwarding loops.
 • If there are multiple distribution lists, it is possible for the sender to prevent duplicate copies  being sent to individuals on multiple lists.
 • It is easier for the sender to know in advance how much bandwidth will be consumed to trans - mit the message.
 The advantages of the remote exploder method are:  • It allows you to send to a list whose membership you are not allowed to know. (
To U.S. spies  living abroad from the IRS: friendly reminder—the tax deadline is April 15.
Being caught or  killed is not one of the grounds for automatic extension. )
 • If distribution lists are organized geographically, you need send only one copy of a message  over an expensive link to a remote area. (
To Citizens of France from the U.S. government:  Thanks for the big statue. )
 • When distribution lists are included on distribution lists, it would be time-consuming to track  down the whole tree to get to all the individuals.
Instead, the message can be making progress  as it is sent to the various exploders.
Parallelism is exploited.
 • When the distribution list is longer than the message, it is more efficient to send the message  from the sender to the distribution list site than to send the distribution list to the sender. (
To  people of planet earth: Greeting.
Unless you stop transmitting reruns of I Love Lucy to us we  will be forced to destroy your planet. )
 14.2 STORE AND FORWARD  It might seem simplest for electronic mail from Alice to Bob to be sent directly from Alice’s per - sonal device to Bob’s personal device.
However, in order for a message to be successfully delivered  under that scenario, it is necessary for both Alice’s and Bob’s devices to be running and reachable  from each other on the network at the time Alice wants to send the message.
This might be espe - cially inconvenient if the user devices are only occasionally connected to the network, e.g., laptops  that the users only occasionally plug into the network.
Also, most client devices do not have stable globally addressable IP addresses.
A client device’s address is often only valid within its local net - work, and so it is not addressable from outside that network.
Communication with that client from  outside its network is only possible if the client device initiates a connection to the external node, in
14.3 DISGUISI NG BINARY AS TEXT 383  which case NAT (Network Address Translation) assigns the client a temporary globally reachable  address (IP address plus port) so that it can receive return traffic.
If both source and destination have  only local addresses in different networks, neither side would be able to initiate a connection to the  other unless they have both registered with some sort of relay service with a global address.
Instant  messaging services deploy such infrastructure, but typically email does not.
Even if IPv6 were uni - versally deployed, firewalls will most likely block most incoming connections from the Internet.
 Mail forwarders known as Message Transfer Agent s, or MTA s solve these issues.
Instead of  user Alice sending mail directly to user Bob’s client device, the mail is instead sent to an MTA that  is more or less permanently on the network.
When user Bob’s device attaches to the network, it  retrieves his mail from the server that handles mail for his account.
The mail processing at the  source and destination devices is done by a program known as the User Agent or UA.
Mail gets  forwarded from UA to MTA to MTA to … to MTA to UA.
 MTAs hold email for their users and send the email to the user when the user connects.
But an  MTA might provide additional services.
It might provide spam filtering or virus checking or might  examine email sent to addresses outside the company to detect and perhaps delete emails contain - ing company confidential information.
 14.3 DISGU ISIN G BINARY A S TEXT  When email was originally designed, the assumption was that its sole purpose was for  human-to-human text using the English alphabet.
Although most email infrastructure has been  modernized to allow sending more than 7-bit ASCII text, it still can’t reliably send arbitrary data  (such as images or encrypted data) without performing some encoding of the data.
 Even with English ASCII text messages, different platforms used somewhat different text for - mats.
For instance, different platforms used different line delimiters.
They might use <LF> (line  feed, ASCII 10) or <CR> (carriage return, ASCII 13) or <CR><LF> (<CR> followed by <LF> ).
 Or they might have line-length limitations, and, if there were too many characters between delimit - ers, a platform might add line delimiters or truncate.
Or if there was white space at the end of a line  (spaces or tabs), a platform might delete the white space.
Some systems expected parity on the  high-order bit of each octet.
Others wanted the high-order bit to always be 0, so if they saw an  eighth bit set to 1, they would change it to 0.
 These sorts of transformations are mostly harmless when trying to send simple English char - acter text messages from one user to another across diverse systems, but it causes problems when  trying to send arbitrary data between systems.
Certainly, if you’re trying to send arbitrary binary  data, having all your high-order bits cleared or having <CR><LF> inserted periodically will  hopelessly mangle the data.
But even if you are sending unencrypted text, security features can get
384 E LECTRONIC MAIL SECURITY 14.4  broken if the data is modified.
For instance, a digital signature covering the contents of an email  will no longer verify if any of the message is modified.
It’s difficult to predict exactly what kinds of  modifications the email infrastructure, designed around the third day of Creation, could make to  your file in its attempt to be helpful.
 To send data other than simple text ( e.g., diagrams, voice, or pictures), there are various  encoding standards such as BASE64 [RFC 4648] that will pass through all known mailers.
These  schemes treat the input as an arbitrary-length string of octets, partition the string into 6-bit chunks, and encode each group of six bits into an octet that represents only “safe” ASCII characters, with an occasional end-of-line delimiter to keep the mail infrastructure happy.
There are obviously fewer  than 256 safe values for an octet (otherwise, arbitrary binary data would work just fine).
There are  at least 64 safe characters to choose from ( A–Z, a–z, 0–9, +, /, ,, -, _,…).
The encoding packs six  bits of data into each 8-bit character by translating each of the possible 6-bit values into one of the  64 chosen safe characters, adding line delimiters sufficiently often.
This encoding expands the data  by about a third.
Note that it is possible to be more efficient.
By using more safe characters,  BASE85 expands the data by a quarter, using five characters to encode four octets of data rather  than BASE64, which uses four characters to represent three octets.
This overhead is often mitigated by doing compression, and sometimes the BASE64-encoded compressed data is smaller than the  original.
 Most email infrastructure today supports UTF8, which enables sending most of the 256 octet  values without modification.
This became absolutely necessary once the Internet community real - ized that not everything is written in English.
There are over a hundred thousand international char - acters that need to be expressed in order to say things in any of the currently supported languages.
 That takes more than eight bits per character, and there are various encoding schemes for using  multiple octets to express a character.
However, even if the email infrastructure could support all  256 octet values, mail gateways might insert end-of-line characters or remove whitespace, so BASE64 encoding is commonly used when transmitting arbitrary binary data such as images or digital signatures.
 14.4 HTML-F ORMATTED EMAIL  Email can be encoded in HTML, which means that the email has much of the power of a web page.
 It can embed URLs to fetch images, links to be clicked on, and JavaScript code that will execute.
 Suppose Alice is sending a message to user Bob.
Most email clients will ask Bob if he wants to  download pictures.
If Bob does not say yes, legitimate email may be unintelligible, or at least look  ugly, since many legitimate senders are using HTML to create “user friendly” email.
If Bob does
14.5 ATTAC HME NTS 385  agree to download pictures, and if Alice had cleverly embedded a recipient-specific URL contain - ing Bob’s email address, Alice will be able to know if and when Bob opened the message.
 If Bob clicks on a link in a received email, Bob’s device will perform whatever action the link  specifies, and Bob will be exposed to any evil that a web page can dish out, including bugs that  might infect his device or malicious content that might trick him into entering security-sensitive  information.
 The very rich functionality of HTML also means that it is likely there will be bugs in user  Bob’s email client that can be exploited by malicious email messages.
 14.5 ATTACH MENT S  Email messages can contain attachments, and although it ought to be safe to open something like a Microsoft Word or PowerPoint document, it unfortunately is not.
Clicking on an attachment may  run the application associated with that document type.
Some types are dangerous because they  contain arbitrary code, such as .exe or .cmd .
Others ought to be safe—a Word or PowerPoint doc - ument should merely display stuff, but, because of bugs or super fancy features that enable won - drous things like having the recipient digitally sign a document, it is not safe.
 Many email clients or MTAs are configured to delete attachments with certain file name  extensions ( e.g., .exe).
Word and PowerPoint documents now have different file types depending  on whether they should be harmless ( i.e., no dangerous features) or scary.
Should-be-harmless  Word and Powerpoint files have the type .docx and .pptx , whereas scary ones are of type .docm  and .pptm , where m stands for macro (or, if you prefer, malicious ).
 14.6 NON-CRY PTOGRA PHIC SECURI TY FEATURES  14.6.1 Spam Defenses  Spam is unwanted, unsolicited email, and there is so much of it that, without something identifying and deleting most of it before it gets to you, all resources involved in handling email for you ( e.g.,  your own time, your inbox storage, your MTA, the network delivering traffic) would become satu - rated with spam.
Example reasons why a sender would want to send spam are to cheaply advertise  a product or service to a large audience, trick people into divulging personal information (such as  credit card numbers), trick people into sending money (a “small processing fee” in order to receive
14.6.1 386 E LECTRONIC MAIL SECURITY  their prize of millions of dollars), or to distribute malware by tricking a recipient into running mali - cious software.
Even if only one in ten thousand recipients can be tricked by an email saying  I am the widow of a prince in Elbonia and need to find someone  trustworthy to move my money out of the country.
I will pay you 10%  of my fortune of $315 million if you help me.
 if a sender targets a hundred million recipients, they will successfully reach many gullible people  who will respond.
 It is very easy and inexpensive to send email to huge numbers of recipients.
If spammers  needed to send individual copies of email from their own accounts, they would be limited in how  much they could send.
They can magnify the bandwidth they have available by using distribution  lists, renting a name and address on a cloud, or using a bot army from which to send spam.
 Spammers, including supposedly legitimate commercial and political organizations that want  to advertise inexpensively to large numbers of potential customers, would love to have a huge data - base of live email addresses.
Better yet would be email lists that are sorted for specific types of products or political affiliation.
 How would a spammer know email addresses to send spam to?
Email addresses can be fairly  easily collected on the Internet ( e.g., in documents or posts on mailing lists), or they can be pur - chased from sites that collect email addresses (“for a discount coupon, sign up with your email  address”).
Or, since it is so inexpensive to send, a spammer can just try many combinations of  potential username strings at some email domain ( aaaa1@hotmail.com , aaaa2@hotmail.com ).
 If someone wants to sell lists of email addresses, the list will be more valuable if the majority of  addresses are legitimate.
Verification of email addresses can be done by sending an email to a wide  set of guessed email addresses and giving recipients the option to “unsubscribe” and saying what  their email address is.
Many MTAs will not send an explicit error message “username not found” to  avoid giving hints about which email addresses at their domain are legitimate.
 What can be done to combat spam?
Some people attempt to make it slightly harder for pro - grams that search the Internet to find their email address but still allow humans to find the email  address by posting their email address as something like radia at alum dot mit dot edu .
 Unfortunately, any service that attempts to identify spam will have some false negatives ( i.e.,  spam that gets through the filter) and false positives.
Often, instead of deleting email that an auto - matic filter thinks might be spam, the email system puts it into a special spam folder.
If legitimate  email gets misclassified, a user could (and should) scan the spam folder to see if any legitimate  email was misclassified as spam.
 What techniques would an anti-spam service look for to try to classify an email as spam?
Any  technique will become known to spammers shortly after it is deployed, and they can tailor their  emails to fool the filter.
A filter might guess something is spam if the same email is sent to a very  large set of recipients.
Or it might look for keywords like free or prize or apply various AI tech - niques.
If spammers sent email from their actual email addresses, those addresses could be easily
14.7 MALICIOUS LINKS IN EMAIL 387  added to a deny list, but, unfortunately, spammers usually do not send from legitimate email  addresses (or they forge legitimate addresses that are not theirs).
 Or, if a lot of spam is detected from some IP address, that address might get deny-listed.
 There is a problem with that, though.
If a spammer rents space on a public cloud, they might even - tually get evicted for misbehavior (such as sending spam), but then the innocent next tenant will  have their IP address deny-listed.
Or perhaps the spam filter might deny-list an entire block of IP  addresses, in which case all tenants of a particular public cloud will be punished for the misbehav - ior of some by having their IP addresses included in the deny-listed set.
 14.7 MALICIO US LINKS IN EMAIL  Spam email wastes users’ time, storage, and network bandwidth and can trick gullible users into  purchasing bogus products, sending money, or divulging personal information.
But another issue is  that an email can contain a malicious link that, if clicked by the user, can install malware on the  user’s device.
 Some advice given to users is don’t click on links in email , but that is really not practical.
 Legitimate email messages contain links.
A user might attempt to access the content pointed to by  the link without clicking the link, e.g., by typing the DNS name of the legitimate entity, starting at  their home page, and navigating their website.
This would be time consuming and sometimes impossible.
 There is at least one company that provides a service to help a company protect its employees  from malicious links.
Let’s call the service phishingprotection.org (intentionally not a real name).
 To use phishingprotection.org , a company has their MTA look for links in email.
The MTA  prepends phishingprotection.org / to the URL of each link.
Then, when the email user clicks on  the link, the request goes to phishingprotection.org .
Since the URL contains the original link,  phishingprotection.org can retrieve the page specified by the original URL and scan it for viruses,  or perhaps merely check the DNS name of the original URL against a deny-list or allow-list of  known bad or good sites.
 14.8 DATA LOSS PREVENTI ON (DLP)  The term data loss prevention is, in our opinion, a very confusing term.
It sounds like the data actu - ally gets lost, and the company should have made backups.
What the industry means by DLP is pre -
388 E LECTRONIC MAIL SECURITY 14.9  venting a company’s intellectual property or other confidential information from being sent to  unauthorized recipients.
 A company’s MTA could check messages that are destined for addresses outside of the com - pany and scan for anything marked company confidential.
Or it could look for keywords such as product names or credit card information.
It could delete messages that might leak information or  hold them for inspection by a human who can decide whether the messages are consistent with  company policy.
 Note there are other ways of stealing company confidential information besides emailing it.
 For example, the information could be loaded onto a USB stick or sent using a file transfer service.
 14.9 KNOWING BOB’S EMAIL ADDRESS  Alice wants to send email to user Bob, and for a simple example, let’s assume they both work at the  same company.
The first step in ensuring that only Bob sees the message is to enable Alice to learn  Bob’s email address.
At a large company, say company.com , there will likely be many people  named Bob Smith.
The first one hired might get the email address bob@company.com .
The next  one might be bob.smith@company.com .
The next one might be robert.q.smith@com - pany.com , and so forth.
When someone looks up Bob Smith in the company directory, they get  several choices, and they have to guess.
Invariably email gets sent to the wrong Bob Smith, and the unintended recipient has to read these carefully in order to guess which email address the sender  should have used.
In some cases, this can place more of a burden on the unfortunate Bob Smith than  receiving obvious spam.
He can quickly delete a message claiming that Bill Gates has chosen him  to receive a million dollars.
However, an email saying “the company CEO really wants you to  attend this meeting in Minneapolis in January” will take more thought to decide if the message was  actually intended for one of the other seven Bob Smiths at the company.
The original recipient  could forward the message to all seven of the other Bob Smiths, and then they could all have a great  time together in Minneapolis in January, even if there were no actual meeting.
 14.10 SELF-DESTRUC T, DO-NOT-FORW ARD, …  A sender might want a message to be destroyed shortly after the recipient reads it or prevent the  recipient from saving or forwarding the message.
This can be implemented by marking the message with the sender’s desires and having the destination email system follow the instructions.
However,
14.11 PREVE NTING SPOOF ING OF FROM FIELD 389  as with all DRM (digital rights management), enforcement in a software-only system can usually  be circumvented by modified versions of the client email application or taking a photo of the  screen.
 14.11 PREVEN TING SPOOF ING OF FROM FIELD  It is easy to place any string into the FROM field of an email.
Some email clients allow you to set  what you want in the FRO M field of all outgoing email.
 If Alice (the sender) were to digitally sign the email she sends to Bob, and Bob expects all  email from Alice to be signed by Alice, he couldn’t be fooled by someone else putting Alice’s name  into the FRO M field.
However, client-signed email is not widely deployed, perhaps because software  to verify signatures is not widely deployed.
So there are various proposals for making it harder to spoof the  FRO M field.
 SPF (Sender Policy Framework) [RFC 7208] allows an email domain to declare a limited set  of IP addresses (or IP address ranges) authorized to send mail from that domain. (
An email domain  is the DNS name after the @ in an email address.)
The declaration is made by placing data in the  DNS, which can be retrieved securely through use of DNSSEC.
A receiving MTA that supports  SPF will refuse mail claiming to be from a domain if it is not coming from one of the listed IP  addresses for that domain.
It is in practice difficult, but certainly not impossible, to impersonate a  connection from someone else's IP address, so although SPF adds security, it is not as strong as  cryptographic authentication.
Also, the security depends on the sending MTA authenticating the  user transmitting the email and checking that the authenticated user really does own the email  address in the FROM field.
 SPF causes some problems for mail forwarders.
For example, if Alice sent email to a distri - bution list exploder, it would forward copies to each recipient.
The IP address from which the for - warded email would arrive at a destination MTA would be the distribution list’s IP address and not  Alice’s MTA.
This sort of deployment challenge discourages deployment of SPF.
 DKIM (DomainKeys Identified Mail) [RFC 5585] allows an email domain to digitally sign  outbound email messages associated with its domain and to declare (in DNS) that mail coming  from that domain will be signed with one of a list of signing keys.
Most MTAs will respect such  declarations and refuse mail claiming to be from such a domain if it is not digitally signed with one  of the listed public keys.
Since mail forwarders often modify email messages in various ways, this will cause the DKIM signature to fail.
An example is that a forwarder might add a “forwarded by”  to the subject line or the body.
14.12 390 E LECTRONIC MAIL SECURITY  14.12 IN-FLIGHT ENCRYP TION  Instead of having Alice encrypt an email with Bob’s public key, she might send the email to her  MTA using an authenticated and encrypted channel such as TLS.
The MTA might authenticate to  Alice using a certificate.
Alice might authenticate using username and password, or there might be  a Kerberos ticket stored on her device.
Furthermore, the MTAs might communicate with each other  using TLS.
And Bob might log into his MTA to retrieve his email, and that connection might use  TLS.
This is often marketed as “email encryption”, but it is not end-to-end.
It requires trusting all  the MTAs along the path and requires Bob to store the email in some protected way.
 If an MTA is capable of communicating using TLS, it has to know which other MTAs can  communicate using TLS and assumes the standard Internet PKI with X.509 certificates for learning  each other’s public keys.
Otherwise, anyone could spoof an MTA by saying “I am the MTA for this  email domain, but I don’t do TLS, so you should just believe me.”
DANE (DNS-Based Authentica - tion of Named Entities) [RFC 7672] specifies how to announce in DNS (hopefully using DNSSEC) that you (an MTA) accept TLS connections.
MTA-STS (MTA Strict Transport Security) [RFC  8461] is similar to DANE but announces at the mail domain level, so the announced policy applies  to all MTAs serving that mail domain.
There is nothing wrong with using both mechanisms.
 STARTTLS [RFC 3207] allows an MTA to tell another MTA with which it is in the process of forming a connection that it supports TLS.
Note that without DANE or MTA-STS, an active  attacker can remove the “I can speak TLS” from the message and trick two TLS-capable MTAs into communicating without cryptographic protection.
 Sometimes this hop-by-hop in-flight encryption is referred to as “encrypted email”, even  though it is not end-to-end encrypted.
 14.13 END-TO-END SIGN ED AND ENCRY PTED EMAIL  Although, as we said, end-to-end cryptographically protected email (such as envisioned by PGP, PEM, and S/MIME) is not widely used, the concept is interesting.
If Alice wants to send a signed  message to Bob, she can just sign the message and send it, hoping that if Bob actually wants to val - idate her signature, he’ll already have her certificate or can obtain it if necessary.
Alternatively, Alice can be pessimistic and assume she needs to send her certificate to Bob, in which case she can  include her certificate in the email message.
 A signed email together with Alice’s certificate will work whether Alice is sending to a spe- cific individual or sending to a distribution list (provided that none of the fields included in the sig -
14.13 END-TO-END SIGNED AND ENCRYP TED EMAIL 391  nature is modified by a distribution list exploder).
The signature will also work if the signed email  is forwarded (in its entirely) by Bob.
 However, if Alice wants to send an encrypted message to Bob, she needs to know his public  key.
There are various methods by which she might discover Bob’s public key:  • She might have received Bob’s public key through some secure out-of-band mechanism and  installed it on her workstation.
 • She might obtain it through a PKI.
 • The email system could allow piggybacking of certificates on email messages.
For instance, if Bob sends an email to Alice, he could include his certificates so Alice will know his public  key.
Or if Alice has not received prior email from Bob, she can send an email to Bob request - ing a return message containing Bob’s certificates.
 How would Alice send an encrypted email to several recipients, say, to Bob and Carol?
Each recip - ient has a different public key.
To encrypt such an email, Alice chooses a random secret key S to be  used only for encrypting that one message.
She encrypts the message with S. Alice need only  encrypt the message once.
But she encrypts S once for each recipient, with the appropriate key, and  includes each encrypted S with the encrypted message.
She might send a single message to all three  recipients, in which case the header would contain { S}Bob, {S}Carol, and { S}Ted, or each copy might  be customized for each recipient (Carol’s copy will only contain { S}Carol, for instance).
 How would Alice send an encrypted email to a distribution list or to a group?
Note that very  few, if any, email systems implement this, but here is a description of how it could be done.
Sup - pose Alice is sending a message to a distribution list that will be remotely exploded, and Bob is  only one of the recipients.
Assume the distribution list is stored at some node remote from Alice, and Alice does not even know the individuals on the distribution list.
So she won’t know the public  keys for all the members of the distribution list.
Instead, she has a key only for the distribution list  exploder.
Alice does not need to treat the distribution list exploder any differently than any other  recipient of a message.
It is merely a recipient with a public key.
The distribution list exploder will  need keys for all the individuals on that distribution list.
It will decrypt S (which was encrypted with  the distribution list exploder’s public key) and encrypt S with each recipient’s public key as it for - wards the encrypted email to each member of the distribution list.
Note that the distribution list  exploder could see the plaintext of the message, but it need not decrypt the message; it only needs  to decrypt and re-encrypt S for each recipient.
 Another possible way of sending email (or sharing a file) with a group of recipients is to  encrypt the message with a key for the group.
The distribution list exploder might not be able to  decrypt the message and will just forward the ciphertext to all the recipients.
Again, the message would be encrypted with a secret key S just for that message.
The encrypted message would be  {message }S, and associated with the encrypted message would be { S} group-key .
Perhaps all mem - bers of the group have obtained and stored group-key in advance, and they can immediately decrypt messages for the group.
Alternatively, there might be a group server to which they authenticate, and
14.14 392 E LECTRONIC MAIL SECURITY  the member could send { S}group-key to the group server, which would authenticate the member,  decrypt { S}group-key , and send the member S.  14.14 ENCRYP TION BY A SERVER  There are some products that will encrypt email for a user.
There are various ways this could work.
 It is usually awkward if the receiver has not implemented the proprietary mechanism and receives  encrypted email.
Such email will usually come with instructions for how to sign up for the service  or download software.
 One mechanism might be for a server to create public key pairs for all the users it supports.
If  user Alice wants to send to user Bob, and the service does not have a public key for Bob, it creates  one and stores the key pair.
Later, when Bob, following the instructions in the received encrypted email, goes to the service and authenticates somehow as owning that email address, the service can decrypt the email for him or give him the opportunity to download software and his private key so  that he can decrypt future emails.
 Another mechanism is for Alice to tell the service that she will want to send an encrypted  email.
The service can create a secret key and a key ID and tell Alice to put the key ID in the header  and encrypt with that secret key.
When Bob receives the email, the service can decrypt the email for  him or send Bob the secret key (assuming Bob has downloaded the proprietary software for that service).
 Another mechanism is to use identity-based encryption (IBE).
This is a form of public key  cryptography introduced as a concept by Shamir in [SHAM84].
A practical solution was invented  by Boneh and Franklin and published in [BONE01].
Rather than using a certificate that maps a  name to a public key, the public key for a name in a domain can be derived from the name.
 • All the users in the domain trust a server known as a Private Key Generator (PKG ).
 • Associated with the domain are domain-specific parameters that are known by all the nodes  in the domain and will enable them to convert a name into a public key.
 • The PKG knows a domain secret that allows it to convert a public key into a private key.
 If a domain (for instance, a company) uses a product based on IBE, the server will know the domain  secret, and all users will be configured with the domain-specific parameters.
When Alice wants to send email to Bob, she derives his public key from his name and the domain-specific parameters.
 When Bob receives the email, if he already has authenticated to the server (somehow) and received  his private key, he can decrypt the email.
Otherwise, his email client needs to contact the server and  receive his private key.
Both Alice and Bob need to have special software for the encryption and decryption.
14.15 MESSAGE INTEGRITY 393  With all of these systems, although the email is encrypted by Alice with a key known to Bob,  the server will be able to decrypt all email.
Therefore, this sort of server-mediated email encryption  is usable for corporate email when a corporation wants to be able to access plaintext of all email  that goes to or from an email address in the corporate domain.
 14.15 MESSAG E INTEGR ITY  When Bob receives a message from Alice, how does he know that Carol did not intercept the mes - sage and modify it?
For instance, she might have changed the message “Fire Carol immediately” to  “Promote Carol immediately”.
It makes little sense to provide for authentication of the source with - out also guaranteeing integrity of the contents of the message.
Who would care that the message  originated with Alice if the contents might have been modified en route?
 For those used to cryptography, the solution is simple.
Have Alice digitally sign the content  of the message.
However, there are complications.
Suppose Bob would like to forward to Carol a  subset of an email he got from Alice.
Once Bob modifies Alice’s email in any way, her signature  will no longer verify.
 Another complication with having a message digitally signed by the source (Alice) is that it  would interfere with services that the MTA might provide that involve modifying the message.
For  instance, the MTA might look for fields that look like credit card numbers and delete or encrypt  them.
Or it might modify URLs in links in order to have a service that inspects links in emails (see  phishing protection in §14.7).
 Yet another complication is what fields Alice’s signature would cover.
Email systems can’t  encrypt most of the header because that would interfere with delivering the email to the right desti - nation.
The secure email standards PEM and PGP only provided integrity protection or privacy on the contents of a message.
S/MIME allows either behavior—where the header ( e.g.,  SUB JECT, TO,  FROM, or TIMESTAM P) is not protected or where a copy of the header is included with the protected  message and some other header, possibly a copy of what was in those fields before, would appear  outside the cryptographic envelope.
Most humans would expect, knowing that they were using email that used encryption and digital signatures, that the entire email message would be protected.
 There are interesting security flaws that can be exploited if the signature does not protect the  SUBJE CT, TO, FROM , or TIMESTAMP field.
A naive user might put private information into the subject  line, and expose the information to an eavesdropper (for instance, a subject line of “Our company is  getting acquired at twice the market price!”
with a forwarded email about the acquisition).
Or sup - pose Alice sees a message from Fred, in which Fred suggests some outrageous idea.
Alice forwards  Fred’s message, integrity protected, with the subject line “Fire this Bozo immediately”, but some - one modifies the unprotected subject line to “Great idea!
Implement this suggestion immediately”.
14.16 394 E LECTRONIC MAIL SECURITY  If the subject line is not protected, the message will arrive signed by Alice, but the modified subject  line completely changes what she intended to say.
 The header fields cannot be encrypted because the mail infrastructure needs to see them.
The - oretically they could be included in the integrity protection even though they couldn’t be privacy-protected.
 Note that even if email protects all the header fields, it would be possible to misuse electronic  mail.
For instance, if Alice sends Bob the signed message “I approve”, Bob can’t use the message  later to prove that Alice okayed his action, since the message isn’t explicit about what Alice was  approving.
 Perhaps if digitally signed email were widely deployed, users would get used to understand - ing how to use the feature securely.
 14.16 NON-REPUDIA TION  Repudiation is the act of denying that you sent a message.
If the message system provides for  non-repudiation , it means that if Alice sends a message to Bob, Alice cannot later deny that she  sent the message.
Bob can prove to a third party that Alice did, indeed, send the message.
For  instance, if Alice sends a message to the bank that she wishes to transfer a million dollars to Bob’s  account, the bank should not honor the transaction unless it is sent in such a way that not only can  the bank know the message was from Alice, but they can prove it to a court if necessary.
 The traditional method of having Alice sign the message with her private key provides  non-repudiation.
Not only does the recipient know that the message came from Alice, but they can  prove to anyone else that Alice signed the message.
 Note that non-repudiation sounds like it has a legal meaning.
It does not.
Even if something is  signed by Alice’s private key, she can claim that malware on her device signed on her behalf with - out her permission or that her private key was stolen.
 14.17 PLAUSIBLE DENIABILITY  It is not always desirable to have non-repudiation.
For instance, Alice might be the head of some  large organization and want to give the go-ahead to her underlings for some scheme like selling  arms to Iran to raise money to fund the Contras.
The underlings must be absolutely certain the  orders came from Alice, so it is necessary to have source authentication of the message.
But Alice
14.18 MESSAGE FLOW CONF IDENTIA LITY 395  wants plausible deniability (what a great phrase!),
so that if any of her underlings are caught or  killed, she can disavow any knowledge of their actions.
How can Alice send a message to Bob in  such a way that Bob knows it came from Alice, but Bob can’t prove to anyone else that it came  from Alice?
 First, we’ll review our notation.
We use curly braces {} for encrypting something with a pub- lic key, with a subscript specifying the name of the individual whose public key we are using.
We  use square brackets [] for signing something with a private key, with a subscript specifying the  name of the individual whose private key is being used.
 1.
Alice picks a secret key S, which she will use just for m.  2.
She encrypts S with Bob’s public key, getting { S}Bob.
 3.
She signs { S}Bob with her private key, getting [{ S}Bob]Alice.
 4.
She uses S to compute a MAC for m.  5.
She sends the MAC, [{ S}Bob]Alice, and m to Bob.
 Bob will know that the message came from Alice, because Alice signed the encrypted S. But Bob  can’t prove to anyone else that Alice sent him m. He can prove that at some point Alice sent some  message using key S, but it might not have been m. Once Bob has the quantity [{ S}Bob]Alice, he can  construct any message he likes and construct a MAC using S.  14.18 MESSAG E FLOW CONFIDENT IALITY  This feature allows Alice to send Bob a message in such a way that no eavesdropper can find out  that Alice sent Bob a message.
This would be useful when the mere fact that Alice sent Bob a mes- sage is useful information, even if the contents were encrypted so that only Bob could read the con - tents. (
For instance, Bob might be a headhunter.
Or Bob might be a reporter, and Alice might be a  member of a secret congressional committee who is leaking information to the press.)
 If Alice knows that intruder Carol is monitoring her outgoing mail to look to see if she is  sending messages to Bob, Alice can utilize a friend, Fred, as an intermediary.
Alice can send an  encrypted message to Fred with the message she wants to send to Bob embedded in the contents of the message to Fred.
So the message Fred would read (after he decrypts it) is “This is Alice.
Please forward the following message to Bob.
Thanks a lot.”
followed by Alice’s message to Bob.
If Alice  were sufficiently paranoid, she might prefer a service that is constantly sending encrypted dummy  messages to random recipients.
If Fred forwards Alice’s message after a random delay, then even if  Carol knows Fred provides this service, Carol cannot know to which recipient Alice was asking  Fred to forward a message.
14.18 396 E LECTRONIC MAIL SECURITY  To be even more paranoid, a path through several intermediaries could be used.
Suppose Bob  and all the intermediaries have public keys.
When we say encrypt a message with a public key , we  mean to encrypt the message with a randomly chosen key K and include K encrypted with the  recipient’s public key along with the encrypted message.
To send a message to Bob without divulg - ing that she is communicating with Bob, Alice chooses a path of intermediaries, say, R1, R5, R2 (so  the message will travel from Alice to R1, then from R1 to R5, from R5 to R2, and then from R2 to  Bob).
R1 will know that the message came from Alice, but not who the destination is.
R2 will know  the destination is Bob but won’t know who the source is.
Alice does the following:  • She encrypts the message with Bob’s public key.
 • She takes the result and encrypts that (plus instructions for R2 to forward to Bob) with R2’s public key.
 • She takes the result and encrypts that (plus instructions for R5 to forward to R2) with R5’s public key.
 • She takes the result and encrypts that (plus instructions for R1 to forward to R5) with R1’s public key.
 • She takes the multiply encrypted result and sends it to R1.
 R1 decrypts what it receives, and the result is an encrypted message and instructions to forward it to  R5.
R5 decrypts the result and gets an encrypted message and instructions to forward it to R2.
R2  decrypts the result and gets an encrypted message and instructions to forward to Bob.
Bob now  receives a message that he can decrypt, and Alice can, if she chooses, divulge her identity to Bob.
 This design, in which each hop uses public key encryption and includes in each encrypted  message the IP address of the next hop, makes the message longer for each hop, and private key  decryption for each intermediary for each message is expensive.
 It is possible to instead use secret key encryption by having Alice set up a path through a cho- sen series of intermediaries between Alice and Bob.
There are several advantages of using secret key encryption for data forwarding:  • Public keys will be used for connection setup, but for data forwarding, only secret keys will be needed.
So data can be fixed-size, and the computation burden on forwarding data is  reduced.
This is especially advantageous if a lot of traffic will be sent between Alice and Bob  (for instance, a web browsing session).
 • With this mechanism, Bob can return traffic to Alice even if he does not know who he is com - municating with.
 Again, assume the path Alice chooses uses intermediaries R1, R5, and R2.
Alice can establish a  connection to R1 and agree on a shared secret key, say, KA-R1, for the Alice–R1 connection.
She  then can use that connection to secretly establish a connection to R5 and agree upon a shared secret  key KA-R5 with R5.
Then she can leverage her secret connection with R5 to create a secret
14.19 ANON YMITY 397  connection with R2 and agree upon a shared secret key KA-R2.
Since the intermediaries keep state  about ongoing connections, there is no need to include forwarding instructions.
Each intermediary  keeps a connection table that tells them, for each connection, what key to use to decrypt with and  which connection to forward the resulting message on.
 To send a message to Bob, Alice encrypts the message with a secret key she has established  for this connection for Bob.
She takes the result and encrypts it with KA-R2, encrypts that with  KA-R5 , and encrypts that with KA-R1.
When she sends this quadruply encrypted message to R1, R1  recognizes the connection ( e.g., from the TCP port), does a table lookup for the connection, and  finds that it should decrypt with KA-R1 and forward the packet on a connection to R5.
The next  intermediary (R5) looks up information that it stored about this connection during connection setup  and finds that it should decrypt with KA-R5 and forward on a connection to R2.
And so on.
 Bob can send a message on the return path without setting up a new path to Alice.
He  encrypts the message with a key he shares with Alice, and forwards the result on the connection to  R2.
R2 looks up the information about the connection, which tells R2 to encrypt the message with  KA-R2 and forward to R5.
And so on.
 With this approach, not only do we avoid expensive private key decryption operations, but  the message does not increase in size at each hop.
 Using the multiple intermediary technique, even if someone bribed one of the intermediaries  to remember the directions from which it received and sent messages, Alice could not be known to  be communicating with Bob.
To discover that Alice had sent a message to Bob would require the  cooperation of all the intermediaries.
 There are several deployed systems using approaches such as this.
An email-specific system  was conceived by Chaum in 1981 [CHAU81].
There are two deployed systems inspired by  Chaum’s work— Mixmaster and Mixminion.
Another system, aimed mostly at anonymous web  browsing, is onion routing (TOR ).
Onion routing was published by Goldschlag, Reed, and Syver - son in [GOLD99], and a system is deployed and maintained by The Tor Project, Inc. ( tor- project.org ).
 14.19 ANONYM ITY  There are times when Alice might want to send Bob a message but not have Bob be able to tell who  sent the message.
One might think that would be easy.
Alice could merely not sign the message.
However, most mail systems automatically include the sender’s name in the mail message.
One  could imagine modifying one’s mailer to leave out that information, but a clever recipient can get  clues.
For instance, the network-layer address of the node from which it was sent might be sent with  the message.
Often the node from which the recipient receives the message is not the actual source
14.20 398 E LECTRONIC MAIL SECURITY  but instead is an intermediate node that stores and forwards mail.
However, most mail transports  include a record of the path the message took, and this information might be available to the recipi - ent.
 If Alice really wants to ensure anonymity, she can use the same technique as for message  flow confidentiality.
She can give the message to a third party, Fred, and have Fred send the  unsigned message on to Bob.
 If Fred does not have enough clients, he can’t really provide anonymity.
For instance, if Alice  is the only one who has sent a message to Fred, then Bob can guess the message came from her,  even if Fred is sending out lots of messages to random people.
Fred can have lots of clients by pro - viding other, innocuous services, such as weather reports, bookmaking, or pornographic screen sav - ers (where you are required to transmit a fixed-length, encrypted message in order to get a  response), or by having the user community cooperate by sending dummy messages to Fred at ran - dom intervals to provide cover for someone who might really require the service.
We specified fixed-length messages to Fred so that people could not correlate the lengths of messages into and  out of Fred.
Someone wanting to send a longer message would have to break it into multiple mes - sages.
Short messages would have to be padded to the fixed length.
This extra traffic is known as  cover traffic .
 Another concept is pseudonymity .
That means that although Bob will not know who sent a  message, he can know that multiple messages all came from the same entity.
This is done by creat - ing a pseudonym associated with a public key and always signing messages associated with that  pseudonym with that same public key.
 If you send to Bob with a pseudonym, Bob won’t be able to reply to that pseudonym through  normal DNS lookups, so if replies are required, the remailer intermediaries all have to remember  the previous hop from which they received email for a pseudonym.
Only if someone breaks into all  the remailers will they be able to trace who owns that pseudonym.
Anonymous email would pre - sumably not allow Bob to reply.
 14.20 HOMEWORK  1.
Outline a scheme for sending a message to a distribution list, where distribution lists can be  nested.
Attempt to avoid duplicate copies to recipients who are members of multiple lists.
 Discuss how it could be done in both the local exploder and remote exploder methods of dis - tribution list expansion.
 2.
Suppose Alice sends an encrypted, signed message to Bob via the mechanism suggested in  §14.17 Plausible Deniability .
Why can’t Bob prove to third party Fred that Alice sent the
14.20 HOMEWORK 399  message?
Why are both cryptographic operations on S necessary? (
Alice both encrypts it  with Bob’s public key and signs it with her private key.)
 3.
Using the authentication without non-repudiation described in §14.17 Plausible Deniability ,  Bob can forge Alice’s signature on a message to himself.
Why can’t he forge Alice’s signa - ture on a message to someone else using the same technique?
 4.
Suppose you changed the protocol in §14.17 Plausible Deniability so that Alice first signs S  and then encrypts with Bob’s public key.
So instead of sending [{ S}Bob]Alice to Bob, she  sends {[ S]Alice}Bob.
Does this work? (
Can Bob be sure that the message came from Alice but  not be able to prove it to a third party?)
 5.
Which security features (privacy, integrity protection, repudiability, non-repudiability, source  authentication, anonymity) would be desired, and which ones would definitely not be used, in  the following cases:  • submitting an expense report  • inviting a friend to lunch  • sending a mission description to the Mission Impossible team  • sending a purchase order  • sending a tip to the IRS to audit a neighbor you don’t like but are afraid of  • sending a blackmail letter  6.
Suppose Alice wants to use remailers (§14.18) to hide that she is communicating with Bob.
 The path of remailers she chooses to get to Bob is R1, R5, R11, R2.
Suppose R1, R11, and R2  all collude with each other to try to discover which parties are communicating, but R5 is hon - est.
Can the colluding set discover that Alice is communicating with Bob? (
Assume there are  lots of messages passing through the system, they are all the same size, and there are random  delays introduced at each hop).
15 ELECTRONIC MONEY  Money has a long and storied history as a medium of exchange and a mechanism for measuring and  storing wealth.
Nations control the minting of coins, the printing of paper currenc y, and the quantity  of money in circulatio n, trying hard through physical means to avoid counterfeit bills and coins.
 Nowadays, almost all but the smallest payments are made using credit cards, checks, and electronic  funds transfers .
Large cash transactions (suitcases full of money) are usually used only for illicit or  illegal transactions .
 Forms of electronic money have been common for many years.
The Internet is used to ma n- age bank accounts, make purchases with credit cards, do wire transfers, and pay bills .
 New protocols for dealing with money, such as the ones described in this chapter, typically  aim at achieving things that are not easily done with current banking systems, such as cheaply and  quickly transferring money across international borders, hiding assets, and anonymously receiving  money for services (such as restoring a victim’s system after attacking it with ransomware).
We  will discuss two very different electronic money protocols :  • eCas h, an elegant design by David Chaum [CHAU8 2], was designed to allow spending  money electronically with the anonymity of cash.
It is a centralized design, in that a particular  bank allows users to purchase anonymous coins.
Many banks could independently create  their own anonymous currency using this scheme.
eCash was launched as a company (Digi- Cash) and was implemented by a handful of banks between 1995 and 1998.
However, it was  not commercially successful and is no longer deployed .
 • Bitcoin is currently deployed, along with hundreds of similar cryptocurrencies.
Bitcoin does  not involve any central bank or country.
Instead, money is created and spent using a network  of thousands of nodes that prevent double-spending.
The nodes can be anonymous.
New  nodes can start participating, existing nodes can depart, and the total number of nodes can  only be estimated.
Users can also be reasonably anonymous but not quite as anonymous as  with eCash .
 We will concentrate on the interesting security concepts in the designs, rather than on the specific  details of these as deployed .
 401
402 ELECTRONIC MONEY 15.1  15.1 ECAS H  The goal of eCash is to electronically create a form of money that, like physical cash, can be spent  anonymously.
Let’s call that an eCash coi n. A financial institution (which we’ll call “TheBank”)  converts traditional money into eCash coins that can be paid by a user (“Alice ”) to a payee (“Bob ”).
 Unlike traditional cash, which uses esoteric materials and printing techniques to make it hard  to counterfei t, an electronic message (such as an eCash coin) can be easily duplicated.
Therefore,  eCash has mechanisms to assure that an eCash coin only gets spent once .
 The eCash scheme uses blind signatures, invented by Chaum for this purpose (see §16.2  Blind Signatur e).
 To create an eCash coin, Alice chooses a unique identifier (UID)—a large random number  that will, with high probability, distinguish that coin from all other eCash coins.
She constructs a  message that contains that UID along with other formatting information.
The extra formatting pr e- vents a random number from looking like a valid signature.
Then Alice pays TheBank to blindly  sign what she has created.
Alice then unblinds the result, and the signed message is an eCash coin.
 Although TheBank will know how many eCash coins Alice has obtained, it will not know the UIDs  of the coins that belong to Alice .
 Alice pays Bob by sending him an eCash coin.
Although Bob can immediately verify that the  formatting of the coin is legitimate, and that TheBank’s signature is authentic, Bob must commun i- cate with TheBank to be assured that the coin has not already been spent.
TheBank must remember  all the UIDs it has seen in spent coins in order to detect double-spending.
Once Bob forwards the  coin to TheBank, TheBank verifies that nobody has already deposited a coin with that UID, and  then adds the value of the coin to Bob’s account (or allows Bob to create a new coin) .
 Suppose Alice uses the same eCash coin to pay both Bob and Carol.
Whichever of Bob or  Carol turns it into TheBank first will be paid, and the other will be told the coin is not valid because  it will have the same UID as a coin that has already been spent .
 Note that the cryptographic aspects of this scheme do not require Bob to know Alice’s ide n- tity (although attributes such as the IP address or the ship-to address might allow Bob to know  something about Alice).
Bob should probably not deliver the services to Alice until he has verified  with TheBank that the coin has not already been spent .
 To actually have eCash users be anonymous requires having many users purchasing coins.
 TheBank knows which users have purchased coins, so if there are only a few that have purchased  coins, the identity of the spender of a coin would be limited to a small set .
 Note also that Bob need not have an account with TheBank.
When Bob verifies with Th e- Bank that he holds a valid coin, TheBank could pay Bob with physical currency or allow Bob to  create a new coin that he could spend later .
 If all eCash coins were worth the same amount, say, a penny, then it would require a thousand  coins for Alice to pay Bob ten dollars, which would be inefficient.
Therefore, it is desirable that
15.2 OFFLINE ECASH 403  eCash coins be issued in multiple denominations.
To accomplish this, TheBank needs to use a di f- ferent public key for each denomination (see Homework Problem 3).
For instance, if Alice pays  TheBank ten dollars, then TheBank will blindly sign her coin with the public key associated with  ten-dollar coins .
 15.2 OFFLINE ECASH  The offline eCash scheme, published by Chaum, Fiat, and Naor in [CHAU8 8], has properties  somewhat different from the online eCash scheme and is more complicated.
This variant of eCash  is intended to allow Bob to somewhat safely accept an anonymous payment of eCash, even if he  temporarily cannot communicate with TheBank to check for double-spending.
If Alice cheats by  paying Bob a coin she had already paid to Carol, TheBank will tell Bob that the coin has already  been spent and will reveal Alice’s identity.
Bob can then presumably sue her.
If Alice has not  cheated, she remains anonymous .
 Although the added practical benefit over the online scheme is small at best (see §15.2.
1),  some of the cryptographic tricks to achieve this variant are interesting enough that we will give an  overview of it .
 • To create a single coin, Alice creates many (for concreteness, let’s say 256) coinlet s with a  particular format, each of which embeds her identity.
However, Alice’s identity will not be  readable from any of the coinlets unless Alice has cheated and double-spent a coin.
Alice  blinds the 256 coinlets and sends them all to TheBank .
 • A coin consists of 128 coinlets, and the collection is blindly signed by TheBank.
Note that  Alice has created more coinlets than needed for a coin (in our example, she created 256, but  the coin will only consist of 128).
Before blindly signing the set of 128 coinlets in a coin,  TheBank will ensure that (with high probability) Alice has generated the coinlets honestly.
 This is done by having TheBank choose a subset, say, 128 out of the 256 blinded coinlets that  Alice presented to TheBank and having Alice unblind and reveal those coinlets.
If the 128  coinlets that Alice is asked to reveal are, indeed, in the correct format, TheBank will trust that  the coinlets it did not ask Alice to reveal are also honestly generated, and then TheBank will  sign the remaining hidden coinlets.
This technique of having someone other than Alice  choose a subset of the items and forcing Alice to prove she is following the rules on all those  chosen items is known as cut and choos e.  • Each coinlet has a number associated with it, computed according to information that Alice  must remember about that coinlet until she has spent the coin.
TheBank’s signature on a
404 ELECTRONIC MONEY 15.2  collection of coinlets consists of a signature on the product of the 128 numbers associated  with each of the coinlets in the collection.
 • The number for each coinlet can be computed using either of two different subsets of the  information Alice is required to remember about that coinlet.
We will refer to the two subsets  as left-info and right-inf o. During the cut and choose step, Alice reveals both left-info and  right-info about the chosen coinlets to TheBank.
Although the number associated with the  coinlet can be computed using either left-info or right-info, Alice’s identity is only revealed if  someone knows both left-info and right-info for a coinlet .
 • To pay Bob, Alice sends TheBank’s signature on the product of the 128 coinlets to Bob.
 However, in order to verify TheBank’s signature, Bob needs more information (left-info or  right-info for each coinle t) that he must obtain from Alice in order to calculate the number for  each of the 128 coinlets.
He can then multiply the 128 coinlet numbers together and verify  TheBanks’s signature on that product.
 • Bob chooses a random 128-bit challenge and sends it to Alice.
Each of the bits in the cha l- lenge corresponds to one of the 128 coinlets.
If the bit in the challenge corresponding to a  coinlet is a 0, then Alice must disclose left-info for that coinlet to Bo b. If the bit is 1, Alice  must disclose right-info for that coinlet to Bo b. Note that Bob will receive exactly one of left- info or right-info for each of the 128 coinlets.
Knowledge of either right-info or left-info will  allow Bob to calculate the number for a coinlet.
However, knowledge of both left-info and  right-info for any coinlet will reveal Alice’s identity.
 • In order to be paid, Bob sends the right-info or left-info he got from Alice about each of the  coinlets to TheBank.
TheBank calculates the number for each coinlet based on this inform a- tion and remembers the number for each coinlet, along with the right-info or left-info that  someone turning in a coin has told TheBank.
TheBank then checks its database to see if any  of the coinlets have already been spent.
If TheBank has no record of any of those coinlets  having been spent, it tells Bob the coin is valid .
 • If Alice had already paid the coin to someone else, say Carol, the combination of what Th e- Bank got from Bob and what TheBank got from Carol will allow TheBank to calculate  Alice’s identity.
The reason is that with high probability, when Alice paid Carol, Carol’s 128 - bit challenge will differ from Bob’s challenge.
For any bit where Carol’s and Bob’s challenge  differs, one of them will have collected left-info for a coinlet, and the other will have co l- lected right-info for the same coinlet.
When both have sent their information about the coin to  TheBank, TheBank will have both left-info and right-info for at least one coinlet, which will  reveal Alice’s identity .
 The actual math for accomplishing this is arcane and not of practical importance.
However, if you  are curious, you can read [CHAU8 8] or do Homework Problem 4, which specifies all the details .
15.2.
1 OFFLINE ECASH 405  15.2.
1 Practical Attack s  The offline scheme has some security issues.
If someone can create a bank account in a fictitious  name (and, in practice, people succeed in doing this at real banks), there is no limit to the amount of  multiple-spending they can do, since tracing the fraudulent activity to a bank account will not help  catch the actual culprit.
As a defense, TheBank could publish a list of coinlet numbers that have  been found to be from such fraudulent activity, assuming the list is not prohibitively large.
But once  a criminal starts multiple-spending, they can steal a huge amount until TheBank alerts all eCash - accepting entities about these bad coinlet numbers.
The online eCash model is far more efficient  than requiring TheBank to promptly alert all entities that might accept offline eCash .
 Another attack is possible if someone were to break into Alice’s computer and steal the info r- mation (left-info and right-info) for all the coinlets in any of her coins (spent or unspent).
They  could then spend these coins as many times as they want, and it would only be Alice who would  look guilty.
Presumably once she reports the theft, she wouldn’t be prosecuted for the coins being  spent.
However, to protect merchants from being cheated with multiply spent stolen coins, me r- chants must immediately check with TheBank to make sure the coins are valid before shipping the  merchandise , which is the same use model as for the online scheme .
 With the online scheme, if Alice’s information were stolen from her computer, all her unspent  coins could be spent by the attacker, once, but no further damage could be done .
 Another downside of the offline scheme (as compared with the online scheme) is that Bob  cannot be anonymous to TheBank and simply exchange a coin he receives for a fresh coin he could  spend elsewhere.
That’s because he must prove his identity to TheBank to ensure the coinlets he  creates correctly record his identity .
 Yet another downside is that if TheBank were malicious, it could easily frame Alice .
 The original paper suggests fixes to some of these issues .
 So, given how complex the offline scheme is, and given the questionable added benefit over  the online scheme, we described it only because of its interesting design.
The interesting ideas are :  • cut and choos e  • having Alice’s identity embedded in the coinlet in a way that is revealed only if both left-info  and right-info are known for that coinle t  • having a coin consist of many coinlets, and having a recipient send a challenge, whose bits  specify which of left-info or right-info Alice should reveal for each coinle t
406 ELECTRONIC MONEY 15.3  15.3 BITCOIN  The Bitcoin concept was introduced in 2008 in a paper [NAKA0 9] published under the pseudonym  Satoshi Nakamoto.
The goal is to have a version of electronic cash in which parties can pay each  other without having the transaction mediated through a financial institution.
Double-spending is  prevented by having a ledger of all transactions recorded in a publicly readable data structure  known as a blockchai n, maintained by an anonymous group of nodes called miner s.  In contrast with eCash, the value of a bitcoin is not tied to the value of any other currency.
 Moreover, unlike national currencies, there is not (and by design there cannot be) any institution  like the Federal Reserve or European Central Bank tasked with keeping the value of a bitcoin st a- ble, so its price can and does fluctuate wildly.
While government-issued currencies can also fluct u- ate, they rarely fluctuate as much as Bitcoin (or other cryptocurrencies), and large fluctuations of  national currencies generally only occur in cases of severe political instability.
Bitcoin currency  might be purchased in the belief that someone else will accept it later for similar (or higher) value.
 Or a transaction might be done with bitcoins rather than traditional currency because it is more co n- venient (e.g., no need to incur the overhead of currency exchange).
Or a criminal might want to be  paid in bitcoins because it is fast, hard to reverse, and mostly untraceable .
 In the following description, we will concentrate on the intuition behind the mechanisms  rather than the implementation details.
Currentl y, Bitcoin is an open source project, and the co m- munity can change the details .
 Some Bitcoin terminology :  • addres s—the hash of a public key of the sender or receiver of an amount of bitcoi n  • transactio n—a signed message from one address, transferring bitcoins received previously,  to another addres s  • ledge r—a record of all transactions that have ever occurre d  • block—a data structure consisting of a list of valid transactions, the hash of the previous  block in the chain, and other informatio n  • blockchai n—the ledger, formatted as a sequence of block s  • mine r—a node that gathers transactions and attempts to create the next block in the chai n  We will now explain various interesting aspects of the design: why the ledger is difficult to forge;  how the algorithm can adjust the difficulty of adding a new block so that on average, the comm u- nity of miners adds a block about every ten minutes; why Bitcoin consumes so much power; how  new bitcoins are created; and how the design ensures there is a maximum number of bitcoins that  will ever be created .
15.3.
1 BITCOIN 407  15.3.
1 Transaction s  A transaction specifies that the owner of a public key that received some amount b of bitcoin in a  previous transaction is transferring that amount to one or more payee addresses.
Bitcoin received in  a transaction must be spent all at once.
A transaction is identified by the hash of the information in  the transaction .
 If Alice needs to pay Bob some amount, say b bitcoins, she needs to find a transaction x in  which a public key she owns was paid at least b bitcoins (that she has not yet spent).
If she received  more than b bitcoins in transaction x, then she can list two payees for the new transaction—an  address that Bob tells her to pay to, and a public key that she owns (to receive the difference  between what she had received in transaction x and what she is paying Bob—like receiving change  at a store).
That is an example of a transaction that has multiple outputs.
It is also possible for a  transaction to have multiple inputs, if the amount being paid needs to combine smaller amounts  received in previous transactions, or if several individuals pool resources to purchase something.
 So, a transaction includes :  • a set of inputs, each including a hash value of a previous transaction together with a sequence  number indicating which output of that previous transaction is being spent in this transactio n  • a set of outputs, each including an amount of bitcoin and an address of the recipient.
The bi t- coin amounts in the outputs will add up to no more than the value of the inputs, and any di f- ference is a transaction fe e  • a public key and a signature for each distinct address among the input s  All this complexity is hidden behind the user interface of software a user would download.
The  software is called a walle t.  15.3.
2 Bitcoin Addresse s  Unlike eCash, Bitcoin doesn’t provide complete anonymity.
Identities in Bitcoin are public keys.
 Users are encouraged to create a new public key for each transaction to make it difficult to link  transactions to the same individual.
If a user reuses her public key for multiple transactions, som e- one that knew her identity (for instance, because she purchased something that needed to be  shipped to her home address) might be able to figure out what else she was purchasing.
Also, if  someone stole the private key associated with one of her addresses, they would be able to spend all  the unspent coins under that address .
 Using a different public key for each transaction still doesn’t guarantee anonymity.
The le d- ger will be world-readabl e, and a lot of information can be mined by following the trail of coins  (e.g., “X1 pays X2; X2 pays X3” or “X1, X2, and X3 all pay X4 the same amount on the first of  each month ”).
408 ELECTRONIC MONEY 15.3.
3  The address to which a payer pays is the hash of a public key, rather than the actual public  key of the recipient.
This makes the address shorter.
It also may make Bitcoin more secure if the  public key algorithm is weak, assuming that a user, say Bob, changes his public key on every tran s- action so only needs to sign once with each public key.
If Carol tried to steal bitcoin paid to address  hash (B) by computing the private key associated with B, she’d have to first find a public key that  hashed to hash (B) before she could start trying to break the public key scheme by finding the pr i- vate key for B. However, Bob must reveal his actual public key when he spends bitcoin so that his  signature on the spending transaction can be verified.
There might be a very long time (e.g.,  months) between the time Bob received the coins and when he spends them.
If the transaction in  which Bob received the coins specified Bob’s actual public key, the attacker would have a very  long time to try to break the key and spend the money.
But if Bob’s actual public key is only  divulged when Bob spends the coins, an attacker would have to break the key between the time the  attacker sees the transaction and when the transaction appears in the ledger, which would likely be  a few minutes, rather than months .
 15.3.
3 Blockchai n  The ledger is in the format of a chain of blocks (the blockchai n—Figure 1 5-1), where each block  contains the hash of the previous block (in addition to other information, such as new transactions) .
 0  data hash of previous block  data hash of previous block  data  Figure 1 5-1.
Blockchain  This format makes it impossible to replace a block in the middle, say block n, without replacing all  the blocks following that block.
If block n were replaced with a different block, the hash of block n  would be different, which would cause block n+1 to have to be changed (since block n +1 contains  the hash of block n).
Likewise for all subsequent blocks .
 15.3.
4 The Ledge r  The ledger records all transactions that have ever occurred.
Having all transactions visible allows  nodes to prevent double-spending.
A transaction is identified by its hash.
Suppose A pays B some  number of bitcoins in a transaction T1, and then, perhaps years later, B pays C what B received in  transaction T1.
To verify that the B to C transaction is valid, the ledger must contain transaction T  (which must show that B indeed received that amount of bitcoin), and also there must be no other  transaction later in the ledger in which B paid T1 to anyone else .
1
15.3.
4 BITCOIN 409  To save time when validating a transaction, most nodes keep a separate, internal database  (that is not shared with other nodes) that keeps track of all the unspent transactions in the ledger.
 This database is usually called the list of UTXOs (Unspent Transaction Outputs).
If a valid transa c- tion Ti has input Tj , then Tj is removed from the unspent transaction database (because it is being  spent), and Ti is added to the UTXO database.
If a transaction T has multiple outputs, say three ou t- x  puts, then three entries will be placed in the UTXO database: 〈T ,1〉, 〈T ,2〉, and 〈T ,3〉.
If transa c- x x x  tion T has multiple inputs, all those inputs will be removed from the UTXO database.
Ifx  transaction T1 is in the UTXO list, and T1 specifies that B received the necessary amount of bitcoin,  then the node does not need to look through the ledger to see if B had paid T1 already, because if B  had spent T1, that transaction would have been removed from the list of UTXOs .
 Each block in the blockchain contains some number of transactions plus other information.
 The list of transactions includes this information :  input payees/amount s signe r transaction has h  transaction xi (where P was paid )  transaction x1  transaction x2, output # 2  transaction x2, output # 1  transaction x4  transaction x5, output # 2  transaction x6, output # 2 A/13 3  B/78, C/5 1  Q/50.8 8  Z/77.9 5  M/49, K/28.
6  D/15, K/1 3  N/12.
8 P  A  C  B  Z  K  K x1  x2  x3  x4  x5  x6  x7  The first row of the table says that P had paid A 133 coins from a previous transaction with  hash xi.
If P had received less than 133 coins in transaction xi, this transaction would not be valid.
If  P received more than 133 coins in transaction xi, then the remainder is a transaction fee paid to the  miner that includes this transaction in a block.
In the second row, A signs over the proceeds of the  transaction with hash x1 to B and C. Of the 133 coins A receives in transaction x1, A pays 78 to B,  51 to C, and the remainder goes to the miner.
In the third row, the input says “transaction x2, output  #2”, meaning that the second payee in transaction x2 (namely C, who is seen receiving 51 coins in  the second row) pays Q 50.88 coins .
 To prevent double-spending, it is essential that the community agrees on an order for transa c- tions.
In other words, if A attempts to double-spend what it received in transaction x1, there will be  two transactions—one where A signs the output of x1 to B, and one where A signs the output of x1  to C. Once one of those transactions is recorded in the ledger, the other transaction is considered  invalid, and a miner will not record the other transaction in the ledger because any block that co n- tained an invalid transaction would be ignored by the other miners .
 It is possible for the transaction that occurred later to be the one that is recorded in the ledger,  because which transactions to include in a block is completely at the discretion of the miner that  creates the next block in the chain.
As long as all the transactions that are included are valid, the
410 ELECTRONIC MONEY 15.3.
5  block will be accepted by the other miners.
For instance, since a block has a fixed maximum length,  if not all pending transactions will fit into a block, a miner might choose to record transactions with  the highest transaction fee .
 15.3.
5 Minin g  A miner that successfully creates the next block in the chain is rewarded with bitcoins.
The rule is  that a block includes a set of new valid transactions, the address of the miner that found this block,  the hash of the previous block, and a random number known as a nonce.
The hash of the new block  must be smaller than some value.
For example, if the hash must have 77 leading zeroes (the appro x- imate required size of the hash in 2022), a miner has only a 1 in 277 chance that a candidate block  (including the random number the miner chose) will have a hash that is sufficiently small.
If the  candidate block has a hash bigger than the maximum value (which it almost certainly will), then the  miner chooses a new random number and tries again .
 Coming up with a block with a sufficiently small hash is like winning the lottery.
The more  hashes the miner can compute, the more likely it is to find a block that will be accepted by the other  miners before any other miner finds one.
In a money lottery, the more tickets you buy, the more  likely it is you will win.
In Bitcoin, the more computation you can devote to calculating hashes, the  more likely you will find the next block in the chain .
 If a miner finds a block with such a hash, that miner distributes the new block to the peer-to - peer network of miners, and the rest of the miners now start building on that block.
Since the new  block takes some time to propagate, it is possible for multiple miners to find a new block with a  sufficiently small hash, and those blocks might contain different transactions .
If a miner sees two  valid blockchains, if they are of equal size, the miner keeps trying to build on the one it saw first.
 However, if one is longer, the miner ignores the shorter blockchain and attempts to build on the  longer blockchain.
The miner that finds a block is not rewarded unless the mining community  agrees on the chain that includes that miner’s block, so it would not make sense for a miner to build  on a shorter blockchain.
Note that having multiple valid chains is known as a fork (see §15.3.6  Blockchain Fork s).
 Bitcoins are created and awarded to the miner that is lucky enough to find a block that is  included in the blockchain.
The lucky miner is rewarded not only with the newly minted bitcoins,  but also with all the transaction fees in transactions included in that block.
Some interesting details :  • In 2009, the miner of a block was awarded 50 bitcoins, in addition to any transaction fees.
 The award for a block decreases by half about every four years, because the designers wanted  there to be a finite amount of bitcoin that can ever be generated.
In 2022, the award for a  block was 6.25 bitcoins.
Eventually, after the amount is halved many more times, the reward
15.3.
6 BITCOIN 411  of newly minted bitcoins per block will be negligible, and miners will only be rewarded  based on transaction fees of the transactions in the block .
 • The desire is that a hashed block is found approximately every ten minutes.
Every 2016  blocks (nominally two weeks, since 2016= 6×24×7×2 ), the time it took to create those  blocks is computed.
If it took too little time (i.e., it took less than ten minutes, on average, for  each block), the difficulty of finding a hash is increased by making the maximum hash size  smaller.
If too much time has elapsed, the difficulty of finding a hash is decreased.
For  instance, to increase the difficulty by a factor of two, the maximum hash value is divided by  two.
 It might seem problematic to halve the reward for finding a hash.
Wouldn’t there at some point be  no incentive for a miner to spend all that electricity to compute hashes?
The hope is that people will  add enough transaction fees to their transactions to provide continuing compensation .
 15.3.
6 Blockchain Fork s  It is possible to have two valid blockchains that start with the same first n blocks, but diverge afte r- wards (see Figure 1 5-2).
 0  data prev hash  data prev hash  data prev hash  data  prev hash  data prev hash  data prev hash  data  prev hash  data  Figure 1 5-2.
Forked Blockchai n  This can happen due to several reasons.
For instance, two miners might find a next block at appro x- imately the same time, and it takes some time for the different chains to propagate through the peer- to-peer network.
Also, if the network of miners were to partition into subgroups, where there is no  communication between the subgroups, each subgroup will continue adding blocks to their bloc k- chain.
When the partition is repaired, whichever subgroup has the longest chain at that point will  have their chain accepted, and the shorter blockchains will be forgotten .
 The consequence of forking is that transactions in the shorter chain are no longer recorded,  and coins paid in those transactions can be spent again.
It also means that miners that thought  they’d won the lottery by finding blocks in the shorter chain no longer own the bitcoins they  thought they’d earned .
412 ELECTRONIC MONEY 15.3.
7  15.3.
7 Why Is Bitcoin So Energy-Intensive ?
 This design, computing all those hashes (known as proof of wor k), is very expensive.
Estimates (in  2022) are that the energy consumed by mining bitcoins is about twice the output of the largest U.S.  nuclear power plant.
The energy use is despite the fact that Bitcoin is a very low-volume applic a- tion (in number of transactions per second) compared to credit cards .
 If the maximum hash size has 77 leading zeroes, it means that the total mining community  needs to compute, on average, 277 hashes every ten minutes.
That is a huge number, but that very  expense is what Bitcoin relies on for security.
If an attacker could amass more compute power than  the Bitcoin mining community, the attacker could, for example, erase a transaction in block n by  creating an alternate block n, and then compute enough subsequent blocks in a chain to be longer  than the chain the rest of the community had computed.
Once the attacker introduces the longer  chain, the shorter chain is replaced.
It could very well be that the Bitcoin mining community is so  large that no nation-state or well-funded criminal enterprise could amass more compute power.
 However, there are many cryptocurrencies being created, and most of them will not have a very  large mining base .
 15.3.
8 Integrity Checks: Proof of Work vs. Digital Signature s  In traditional cryptography, someone who is authorized to create an integrity check on data (a dig i- tal signature) knows a secret (a private key).
With the magic of cryptography, there is an enormous  gap between the computation needed to create a signature (by an authorized party that knows the  private key) and the computation needed to forge a signature (create a valid-looking signature wit h- out knowing the private key).
Furthermore, increasing the size of the key increases that gap .
 In contrast, as a consequence of the Bitcoin design goal of having no known authorized pa r- ties, the integrity check (blocks that hash to a small number) requires an equa l amount of compute  power to create as it would take for someone to maliciously create an alternate chain that looked  valid .
 What do we mean by “large gap” between signing and forging for traditional cryptography,  and what do we mean by increasing the gap by increasing the key size?
For example, with a 1024 - bit RSA key, forging a signature is about 263 times harder than creating a signature.
With a 2048-bit  RSA key, forging a signature is about 294 times harder.
It is hard to appreciate these numbers, so a  tangible way of saying this is that with a 1024-bit RSA key, signing takes about one millisecond on  a typical CPU, whereas forging the signature would require the compute power of today’s entire  Bitcoin miner community for about an hour.
This is certainly an enormous gap, but by doubling the  key size to 2048-bit RSA, signing increases from one millisecond to six milliseconds, but forging  would require the compute power of today’s Bitcoin miner community for the next million years !
15.3.
9 BITCOIN 413  In contrast, as a consequence of the Bitcoin design goal of having no known authorized pa r- ties, the integrity check (constructing blocks that hash to a small number) requires an equa l amount  of compute power to create as to forge !
 15.3.
9 Concern s  The Bitcoin infrastructure is incredibly expensive in terms of computation, and moderately expe n- sive in terms of storage and network bandwidth.
Its rules (e.g., halving the reward to miners every  four years) seem arbitrary and likely to cause problems.
Given the fixed blocksize, and blocks  being created at a fixed interval (ten minutes), the number of transactions per unit time is limited.
 One of the supposed benefits of Bitcoin is low transaction fees, but when miners are only rewarded  through transaction fees, miners will only choose to include transactions with the largest transa c- tion fees.
How high are these fees likely to get ?
 To respond to these issues, people have deployed mechanisms such as the Lightning Network  [POON1 6] to allow multiple small transactions to occur off of the main blockchain and then have  multiple accumulated transactions be summarized with just a few big transactions listed on the  main blockchain .
 Another issue could be that if there are independent implementations of miners, implement a- tions might have incompatibilities such that the community partition s, ignoring each other’s bloc k- chains as invalid.
This actually occurred (in March 2013).
There was a block B that looked valid to  one version and looked invalid to the other.
All the nodes that thought B was valid built upon the  blockchain that included B, and the nodes that thought B was invalid built a chain that forked star t- ing at the block before B. This situation lasted six hours but could have lasted for much longer  before it was detected and fixed.
The rule of thumb is that if the blockchain contains six blocks  after the block containing a transaction, then the transaction can be assumed to be safely commi t- ted.
But this obviously would not be true if there were frequent incidents of subtly incompatible  miner implementations .
 Note that when “someone” makes the decision to choose one fork in a case like this, the bi t- coins earned by miners that thought they had won the lottery by having found blocks in the losing  chain lose those bitcoins.
And transactions that seemed to be safely recorded in the losing fork are  no longer recorded .
 As for the complaint about storage and network bandwidth, the Bitcoin proponents claim that  both storage and bandwidth capacity will increase faster than the growth of the use of Bitcoin .
414 ELECTRONIC MONEY 15.4  15.4 WALLETS FOR ELECTRONIC CURRENCY  Most people prefer keeping their (traditional) money in a bank rather than hidden in their house,  because they assume that the bank has better security measures and that the money is insured even  if the bank is robbed or goes out of business .
 Electronic money presents a challenge as to how a person, say, Alice, can keep it safe.
If a  thief steals the information associated with a coin, they can spend the coin, and Alice will have no  recourse.
Electronic money is not the same as physical coins—it is information stored in Alice’s  computer.
For instance, it might be a set of signed eCash coins, or it might be the private keys ass o- ciated with received bitcoins.
So, there are at least two things that can go wrong :  • Alice’s computer could get lost or broken, and if Alice did not properly back up the contents,  all the stored electronic money would be lost.
This is analogous to her money burning up in a  house fire .
 • Alice’s computer could get broken into, and the money could be stolen.
This is analogous to  her house being robbed .
 15.5 HOMEWORK  1.
If Bob receives an eCash coin from Alice, and Bob trusts Alice not to double-spend, could  Bob pay Carol with the same coin (without turning it into TheBank and converting it into a  new coin) ?
 2.
In the online eCash scheme, we say that a coin must contain not only a large random number,  but also some formatting information.
Suppose instead an eCash coin consisted solely of a  random number x, signed by TheBank’s RSA private key (meaning that the coin would co n- sist of xd mod n).
Why would this not be secure?
Why does the formatting make it secure ?
 3.
If TheBank issues eCash coins in different denominations, why does it need a separate public  key for each denomination?
For instance, why couldn’t the eCash coin contain both the UID  and the value of the coin ?
 4.
Here are (slightly simplified) details for the offline eCash scheme.
For each of the 256 coi n- lets, say coinlet Ti , Alice’s ID is Alice.
She chooses three random numbers: ai, ci, and di.
 Coinlet T is constructed out of the values Alice, a, c, and d as follows:  First Alice computes the hashes of two values: hash (a|c) and hash( (a ⊕ Alice )|d), which she  then concatenates and hashes to obtain T = hash(hash (a|c) | hash( (a ⊕ Alice )|d)).
Then  left-info for the coinlet is the triple 〈a,c,hash( (a ⊕ Alice)|d)〉, and righ t-info for the coinlet is
15.5 HOMEWORK 415  the triple 〈hash(a|c), a ⊕ Alice, d〉.
 (a) Show how to compute T from left-info.
 (b) Show how to compute T from right-info.
 (c) Show how knowing only left-info or right-info does not divulge Alice’s identity.
 (d) Show how knowing both left-info and right-info does divulge Alice’s identity .
 5.
In the offline eCash model, how could a malicious bank make it look like innocent Alice is  double-spending ?
 6.
In Bitcoin, assume Alice changes her public key on every transaction.
When she spends what  she received in a transaction, she has to list her public key (so that her signature can be ver i- fied).
Given that she has to divulge her public key when spending, why does it add security  for her to only reveal the hash of her key to receive money ?
 7.
Suppose someone replaced the hash algorithm currently used in Bitcoin with a hash alg o- rithm that took a tenth as much compute power per hash.
Would the Bitcoin community  therefore use less electricity ?
16 CRYPTOGRAPHIC  TRICKS  We’ve covered the major cryptographic building blocks, such as secret key encryption and integrity  checks, public key encryption and signatures, and hashes/message digests.
In this chapter we’ll talk  about some other functions.
We won’t go into great detail about the math, or proofs, but we will at  least demystify what these functions are attempting to do, give some intuition about how they accomplish it, and give examples of those that are used in actual real-world protocols.
 16.1 SECR ET SHAR ING  Secret sharing is a way for someone, say, Alice, to store a piece of information, say, S, that must be  kept secret from all except Alice but must also be retrievable by Alice.
There is a tradeoff between  robustly storing S so that it can always be retrieved or risking having S stolen.
 Alice will compute n numbers (known as shares ) such that any k of those numbers will  enable computing S, but any subset of size smaller than k will give no information about S. Alice  can store the n shares in different locations.
The parameters k and n should be chosen so that  • k is larger than the number of locations Alice thinks could be broken into, while  • n−k is at least as large as the number of locations Alice thinks could lose the information she  stores or be unavailable when needed.
 There are various schemes for accomplishing secret sharing.
Shamir’s scheme [ SHAM79 ] is easiest  to understand, although the concept of secret sharing, using a different mechanism, was also  invented by Blakley [BLAK79].
 First, let’s see how Shamir’s scheme handles k =2, meaning it will require two out of the n  shares to recover S. Alice chooses a random number b and creates an equation for a line y=bx+S.  Each share of S consists of a point 〈x,y〉 οn the line.
The secret S is the value of y when x=0.
If  Alice (or an attacker) were to be told any two points 〈x1,y1〉 and 〈x2,y2〉, they could compute the line  and therefore know S. But any single point will yield no information.
 417
418 C RYPTOGRAP HIC TRICKS 16.2  For general k and n (n shares, k of which can reconstruct S), the equation would be a polyno - mial of degree k−1.
Shares can be any n distinct points on the curve represented by the equation  (other than the point where x=0, of course).
 Usually when we think of equations and curves, we think of points on the curve as being  pairs of real numbers (the x and y values) and the equation having coefficients that are real num - bers.
That would be very messy—digital computers are notoriously imprecise at computing with  real numbers.
Instead, in a system used for secret sharing, an example implementation might have  the coefficients being integers mod p for some suitably large prime p, and points fitting the equation  would be tuples of integers mod p. This is possible because integers mod p form a field.
For n-bit  secrets, the obvious field of choice would be GF(2n) (see §2.7.1 Finite Fields ).
 16.2 BLIND SIGNATUR E  The concept of a blind signature was invented by David Chaum [CHAU82].
The idea is for Alice  to get Bob to sign a message m without Bob being able to see what he’s signing.
It is rather surpris - ing that such a protocol exists, that it would be useful for anything, and that anyone would have  thought of it!
 Alice chooses two functions blind and unblind that interact in a special way with Bob’s pub - lic key pair.
She applies the blind function to m, which disguises m so that Bob cannot know m.  Then Bob signs the blinded m. Alice then applies the unblind function, and the result is Bob’s sig - nature on m. (See Figure 16-1.)
 blind  m blinded( m)  sign sign  unblind signed( m) signed(blinded( m))  Figure 16 -1.
Blind Signature  An example application for blind signatures is anonymous electronic cash (see §15.1 ECASH ).
 Another is for one of the variants of group signatures (see §16.5.1.4 Blindly Signed Multiple Group  Membership Certificates ).
 Blind signature schemes exist for many public key signature algorithms, but the one that is  easiest to understand is based on RSA.
Let Bob’s RSA public key be 〈e,n〉 and private key be 〈d,n〉.
 Alice wants m signed by Bob’s private key 〈d,n〉, meaning she wants md mod n.
16.3 BLIND DECRYPTION 419  To blind m, Alice chooses a random number R and applies Bob’s public key to R (meaning  that she computes Remod n).
She then multiplies the message m by Remod n, and the result is  mRemod n, which is the blinded message.
 Next, she asks Bob to sign the blinded message, which means he will exponentiate  dmRe mod n to the power d, and reduce the result mod n. The result is m Red mod n. Since e and d  dare inverses, Red mod n is equal to R mod n, so the blinded signed message is m R mod n, which  Bob sends to Alice .
 dNow Alice unblinds by multiplying m R mod n by R−1mod n, and the result is md mod n—in  other words, m signed by Bob, even though Bob was not able to see what he was signing.
 16.3 BLIND DECRY PTION  Blind decryption is very similar to blind signatures, and for RSA, the same math works.
Alice has  {X}Bob and wants to retrieve X. She chooses functions (blind and unblind), applies the blind func - tion to { X}Bob, and sends the result to Bob.
Bob applies his private key, which reverses the encryp - tion with Bob’s public key, resulting in X encrypted with the blind function.
Alice can then decrypt  with the unblind function to retrieve X.  An example use of blind decryption is for a concept known as oblivious transfer , which was  introduced in [ RABI81].
The problem being solved is that Bob has several items that he can send to  Alice, but Alice does not want Bob (or an eavesdropper) to know which item she has retrieved.
 For instance, Bob might be a content provider, where Alice pays to download a movie but  does not want anyone to know which movie she has purchased, such as was described in [PERL10].
 Bob might post a bunch of encrypted movies, each encrypted with its own secret key K. Associated  with each movie is a header consisting of K encrypted with a public key that Bob created for that  purpose, say, Bob-movies.
 So, associated with each movie is { Ki}Bob-movies .
 Alice can download any encrypted movie.
However, to decrypt it, she will need the key Ki.
 She needs to pay Bob for each movie but does not want Bob to know which movie she has pur - chased.
 The solution is simple.
Alice selects { Ki}Bob-movies for the movie she wants and asks Bob to  blindly decrypt it.
 Of course, to build a complete solution and preserve Alice’s privacy requires some sort of  mechanism for Alice to obtain the encrypted movie without anyone noticing which movie she  downloaded.
One possibility is that the movies are broadcast, and Alice, knowing the decryption  key, can watch the movie when it is broadcast.
Another is that Alice might download the encrypted  movie through some sort of anonymizer such as TOR [ DING04].
420 C RYPTOGRAP HIC TRICKS 16.4  16.4 ZERO-KNOWLED GE PROOFS  The concept of zero-knowledge proofs (ZKP) was introduced in [GOLD85].
The basic goal is for  Alice to prove to Bob that Alice knows a secret without revealing the secret to Bob.
Furthermore,  the zero knowledge part means that Bob should gain no information from the interaction other than  that the party he is talking to knows the secret.
 Traditional ZKPs are interactive protocols, where there is a per-round probability (in most  examples, ½) that someone impersonating Alice would be able to fool Bob.
If Bob quizzes Alice n  times, the probability that an imposter could fool him all n times would be ½n .
 ZKP should also have the property that although Alice can convince Bob that she knows  Alice’s secret, Bob would not be able to show the transcript of the conversation to a third party to  prove that Bob, indeed, had talked to Alice, because Bob could have created the entire conversation himself.
 16.4.1 Graph Isomorphism ZKP  A graph is a set of vertices plus a set of links connecting pairs of vertices.
If we label the vertices  and represent each link as an unordered pair of vertex labels, then two graphs G 1 and G 2 are iso- morphic if there is a way of relabeling the vertices in G 1 to produce G 2.
It is assumed difficult to  determine in all cases, whether two graphs are isomorphic. (
See Figure 16-2 .)
The best-known  solution is harder than polynomial.
But if presented with a renaming of vertices in G1 to G2, it is  easy to show that they are isomorphic.
 Figure 16-2.
Isomorphic Graphs?
 A ZKP for graph isomorphism is for Alice to prove that she knows that G 1 and G 2 are iso - morphic, meaning she knows how to rename the vertices of G1 to produce G2.
She can produce two  such graphs by taking a large graph G1 and renaming the vertices to produce G2.
She will now  know how to map from G1 to G2, and her “public key” is the pair of graphs 〈G1, G2〉.
But it is diffi - cult for Bob to find a relabeling to be sure that G 1 is isomorphic to G 2.
 For Alice to prove to Bob that she knows that G 1 and G 2 are isomorphic, Alice chooses a ran - dom relabeling of one of G 1 or G 2 to produce a new graph G 3.
Alice presents G 3 to Bob.
This step  is known as Alice’s commitment .
16.4.2 ZERO-KNOWLE DGE PROOF S 421  Bob then challenges her by choosing either G 1 or G 2 and asking Alice to show how it maps to  G3.
If Alice indeed knows how to map G1 to G2, she can correctly answer his question, whether he  chose G1 or G2.
Suppose an imposter, say Trudy, attempts to impersonate Alice.
Trudy does not  know how to map G1 to G2.
If Trudy created G3 by renaming the vertices in G1, she will be able to  show the mapping to G 3 if Bob challenges her with G 1.
However, if Bob challenges her with G 2,  Trudy will not be able to answer correctly.
If Trudy were able to pick a G 3 and be able to correctly  answer whether Bob challenged her with G 1 or G 2, then Trudy could compose the mappings to get  a map from G1 to G2.
 This means that Trudy has a ½ chance per round of fooling Bob.
Given that Bob needs to be  more than 50% sure he’s talking to Alice, he can perform more rounds.
At each round, Alice must  choose another graph, Gi, and Bob will challenge her to show a mapping between Gi and his choice  of G 1 or G 2.
 16.4.2 Proving Knowledge of a Square Root  Another example of a ZKP protocol was published in [FIAT86].
The rules are as follows.
We  assume there is a composite modulus, n, but nobody (except possibly Alice) knows n’s factoriza - tion.
Alice creates a public key s 2 mod n by picking a random s and squaring it mod n. In order to  authenticate to Bob, Alice must prove to Bob that she knows s (her private key).
 • Message 1: (Alice’s commitment) Alice chooses a random r and sends r 2 mod n to Bob.
 • Message 2: (Bob’s challenge) Bob asks Alice either for r or for rs.
 • Message 3: If Alice really knows s, it will be easy for her to give either r or rs.
If Bob asked  for r, he squares it and verifies that the result equals Alice’s commitment.
If Bob asked for rs,  he squares that and verifies that the result is equal to the product of Alice’s commitment and  Alice’s public key.
 If Trudy is impersonating Alice, then she has a ½ chance of fooling Bob.
If she thinks Bob will ask  her for r, then she sends r 2 in message 1.
However, if she thinks Bob will ask for rs, then Trudy  picks a random number, say, t, and sends t2/s 2 mod n, in message 1.
When Bob asks for rs, Trudy  sends t. Bob will square t (to get t2) and verify that what he received from Trudy ( t2/s 2) multiplied  2by s 2 is indeed t , so Bob will be fooled.
However, if Trudy made the wrong prediction for whether  Bob will ask for r or for rs, she will not be able to answer correctly.
16.4.3 422 C RYPTOGRAP HIC TRICKS  16.4.3 Noninteractive ZKP  Any ZKP can be turned into a noninteractive ZKP by having the prover (Alice) simulate the chal - lenger (Bob) with challenges that she cannot control.
A typical way of doing this is by deriving the  challenges from a cryptographic hash of Alice’s commitments.
 So, for instance, let’s take the protocol in §16.4.2 Proving Knowledge of a Square Root .
Let’s  2 2say that Alice wants to simulate k rounds.
She creates k inputs ( r1, r2,…, rk 2).
Then she takes a  2hash of r12|r22|…|rk .
If the ith bit of the hash is 1, then she must provide ri s. If the ith bit of the  hash is 0, then she must provide ri.
 If someone, say, George, wanted to impersonate Alice without actually knowing s, George  2 2 2can choose r1, r2,…, rk , but he will only know one of ri or ri s, for each i. He could hash  2 r12|r22|…|rk , and if he doesn’t like the challenges selected by that hash, he can choose new ris,  rehash, and hope for the best.
The assumption is that the hash would be large enough that it would  be computationally infeasible for George to be lucky enough to find a set of ris that result in a set of  challenges that he can answer.
Note that twenty rounds might be sufficiently secure in an interactive  ZKP where a forger can answer each round half the time.
With a noninteractive ZKP you’d need  many more rounds than that. (
See Homework Problem 8.)
 This form of noninteractive ZKP can be used as a type of signature if, instead of simply hash - ing the set of commitments as described above, Alice hashes the concatenation of the message and  the commitments.
 The formal definition of zero knowledge requires that a third party, seeing a transcript  claimed to be an interaction between Bob and Alice, will not be convinced that Bob actually inter - acted with Alice, since Bob could have created that transcript without Alice.
This strict definition is useful in some security proofs, but it is not necessary for any plausibly practical application of ZKP  that we know of.
 The transformation of a zero-knowledge proof into a non-interactive ZKP is known as the  Fiat-Shamir construction .
Most proposed applications of ZKP use the Fiat-Shamir construction.
 These only require a weaker notion of ZKP ( honest verifier zero knowledge ) where it only needs  to be zero knowledge if Bob chooses his challenges randomly.
 Suppose an impersonator has a 50% chance of answering correctly on each simulated round  in a noninteractive ZKP.
Given that the input to each round (the commitment) is many bits long, the Fiat-Shamir construction would be highly inefficient.
Therefore, to make a practical non-interactive  ZKP, the challenges need to have a far smaller probability that an imposter can respond correctly.
 Here is an example protocol original published by Schnorr [SCHN89] with a small probabil - ity of an imposter creating a correct response.
In this protocol, Alice proves that she knows the dis - crete log, x, of a public key gx mod p.*  *This protocol is a secure honest verifier zero-knowledge proof system to the extent that the underlying discrete log problem  is hard.
This will depend on the factorization of p−1 in various ways that aren’t worth getting into here.
16.5 GROUP SIGNATURE S 423  1.
Alice picks a random number r and sends Bob gr mod p (Alice’s commitment).
 2.
Bob sends Alice a random number c (Bob’s challenge).
 3.
Alice sends Bob r−cx (mod p−1) (Alice’s response).
 4.
Bob verifies that ( gr−cx)(gx)c= gr mod p.  The above protocol (where Alice proves she knows the discrete log of gx mod p) and other such  proofs that have very small probabilities of an imposter guessing a single correct response are hon - est verifier zero-knowledge proofs (see Homework Problem 3 and Homework Problem 4).
 16.5 GROUP SIGNA TURES  The goal of group signatures is to have a member of a group of individuals sign something, such  that a verifier can know that it was one of the individuals in the group that signed it but not know  which member of the group generated the signature.
 One application of group signatures is to enable someone (known as a whistleblower) to  safely divulge some wrongdoing to a news outlet.
The news outlet needs to know that the whistle - blower is genuinely one of the people who would have accurate information (rather than some trou - blemaker fabricating a story).
But it is also important to keep the whistleblower’s identity secret.
 Another example application is for a device to prove it is a genuine device, while preserving  the privacy of the owner of the device.
An example use is for DRM (digital rights management).
A  corporation selling content might want to be assured that the device purchasing the content was one  of a set of devices designed to follow the DRM rules.
Users might be hesitant to purchase content if  the corporation could determine which device was purchasing the content.
 The concept of group signatures was introduced in [CHAU91].
It should be impossible to  forge a signature if you’re not part of the group.
Seeing a signature should not leak information, so  even if you see a lot of them, you shouldn’t be able to forge a signature.
Some group signature  schemes involve a group manager that creates the credentials for the group.
 There are various schemes that have been proposed and various potential properties they  might have, such as:  • unlinkable signatures—the verifier does not know whether two signatures were signed by the  same group member  • a choice between two properties—either nobody would be able to identify which member generated a given signature, or there is one entity (the group manager) that is able to identify  the member that generated a given signature
16.5.1 424 C RYPTOGRAP HIC TRICKS  • if the private key of a compromised member becomes known, it is possible to revoke that  member’s membership, so signatures from that member will no longer be verified as valid by  verifiers  • if a signature is done in bad faith ( e.g., someone agrees to purchase something but doesn’t  pay), even though the particular member who generated that signature cannot be identified (even by a group manager), that signature can be revoked, so that in the future a prover can  prove that they were not the one that generated a particular signature  16.5.1 Trivial Group Signature Schemes  In this section we will look at simple ways of achieving some of the features that are generally  desired of group signature schemes.
The sorts of schemes we discuss in this section might be con - sidered too trivial to be considered “true” group signature schemes.
We will also discuss the draw - backs of these simple approaches to see why (aside from getting papers published) cryptographers might want to design more complicated schemes for group signatures.
 16.5.1.1 Single Shared Key  In this trivial approach, all the members of the group share the same private key.
This simple scheme does have the property that a verifier would not be able to tell which member signed some - thing or to tell that two signatures were created by the same device.
 However, sharing a private key with many individuals is not very secure.
If many individuals  (or devices) know the same private key, it is likely someone outside the group would eventually fig - ure out how to extract the private key.
 To revoke a single member of the group, the group key would need to be revoked, a new  group key created, all verifiers notified that the group key has changed, and the new private key  would need to be shared by all the remaining group members.
 16.5.1.2 Group Membership Certificate  This simple approach to group signatures is to have each member create a new public key pair spe - cifically for signing group signatures for this group.
Member Bob creates a new public key P and  sends it (signed with his normal private key) to the group manager.
The group manager signs a cer - tificate for P, “P is part of this group”.
The group manager would know which public key is associ - ated with each member.
The verifier would not know which public key goes with each member, but  the verifier would know whether two items were signed by the same individual.
Revocation of a  member would be easy.
The group manager revokes the certificate for the public key associated  with that member.
16.5.1.3 GROUP SIGNATURE S 425  16.5.1.3 Multiple Group Membership Certificates  This variation provides unlinkability of signatures (except by the group manager).
Each member  creates lots of public key pairs and has the group manager certify each of those keys.
The group  manager will know which public keys are associated with each member.
However, a verifier will  not know which public keys are associated with each member.
Therefore, if a member uses a differ - ent private key for each signature, then a verifier will not be able to link two signatures as being  from the same member.
To revoke a member, the group manager will need to revoke all certificates for that member.
 16.5.1.4 Blindly Signed Multiple Group Membership Certificates  This variation makes it impossible for the group manager to know which member is associated with  a given public key.
It uses blind signatures ( §16.2).
Let’s say Bob is a member of the group.
Bob  will authenticate to the group manager, create a public key P, embed P in a blinded message that  says “public key P is a member of the group”.
The group manager signs the blinded message.
If  Bob uses a different public key pair and certificate (blindly signed by the group manager) each time he creates a signature, then nobody, including the group manager, will be able to know which mem - ber created a signature, nor be able to link two signatures.
 With this variant, if the group manager wants to revoke Bob’s membership, it can refuse to  blindly sign any new certificates for Bob, but will not be able to revoke the certificates the group  manager had blindly signed before Bob’s membership was revoked.
Instead, to revoke a member,  the group manager will need to change its own public key pair, declare all certificates signed by the  old key to be invalid, and issue new certificates for the still-valid members, blindly signed with the  group manager’s new private key.
 Alternatively, instead of only changing the group manager’s key when a member is revoked,  the group manager key could be changed frequently, for instance, every hour.
To avoid problems for a member who receives a group membership certificate right before the group manager changes  its key, there should be an overlap in time when verifiers accept certificates signed either by the cur - rent group manager key or the previous one.
Perhaps each member might request a single blindly  signed certificate at the point when it needs to sign something, in which case there is a risk that the  group manager can correlate when Bob asked for a certificate and when it was used for a signature.
 16.5.2 Ring Signatures  This elegant scheme was presented in [RIVE01].
It is particularly suited for the whistleblower sce - nario.
Each individual has its own public key pair, and the set of individuals in the group does not  have to be determined in advance.
Anyone, say, Bob, can choose a set of n−1 individuals and be
16.5.2 426 C RYPTOGRAP HIC TRICKS  able to hide his identity (as the signer) in the group of n individuals.
The verifier will know that one  of those n users signed the message but would not know which one.
 This scheme is described in the literature in a much more general way, and needs to be tuned  to the specific cryptographic algorithms used.
For simplicity of explanation, we’ll choose unpadded  RSA, SHA2-256, and AES-256 as the functions.
 Assume there are n individuals, M 1,M2,…,M .
Each individual in this group has a public key n  pair (say a 2048-bit RSA key).
The public keys of all the members are known to everyone.
We’ll  use the notation that a 2048-bit quantity, say R, encrypted with Mi’s public key is { R}i.
We’ll be  using unpadded RSA encryption so that any 2048-bit quantity smaller than the modulus can be encrypted or decrypted.
 Let’s say there is a message m that a member, say M3, would like to sign.
The SHA2-256  hash of m is h. This hash will be used as an AES key in some mode such as CBC with zero IV so  that the encryption of a 2048-bit quantity by h results in a 2048-bit quantity.
 The ring signature on message m (with n individuals in the group) includes n +1 seemingly  random 2048-bit numbers y1, R1,R2,…,R . (
See Figure 16-3.)
The signature could start with any of n  the y values, but it’s simpler to always start with y1.
 y1 R1  Encrypt with M 1 ’s public key  AES Encrypty2 R 2Encrypt with M 2’s public key AES Encrypt y3R3 Encrypt with M3’s public keyAES Encrypty4 R4Encrypt with M 4’s public key AES Encrypt y5 R5  Encrypt with M5’s public keyAES Encrypt                                                            Figure 16-3.
Ring Signature  To verify a signature, begin by computing the SHA2-256 hash h of m. h will be the key we  use for each of the AES encryption steps.
Then, starting at, say, i=1, compute around the ring for n  steps:  • Encrypt Ri with Mi’s public key to get { Ri}i.
Compute yi ⊕ {Ri}i  • Set yi+1 to the AES-encryption (using key h) of yi ⊕ {Ri}i
16.5.3 GROUP SIGNATURE S 427  After completing the above for each member, you will have computed y2,…,y6.
If y6=y1, then the  signature will be considered valid.
 Suppose Bob is member M3.
To compute a signature for message m with hash h, Bob picks a  random 2048-bit number for y4 and n−1 random numbers R1,…,R2,R4,…,R . (
Note that R3 will be n  computed later; see Figure 16-4.)
Bob encrypts R4 with member M 4’s public key and ⊕ s the result  with y4.
Then he AES-encrypts the result with h, and the result is y5.
He continues around the ring  computing each yi until he gets to y3.
Now he computes the AES-decryption of y4 using key h. Let’s  call that X. To complete the ring, Bob computes y3⊕X and decrypts the result using his own private  key.
If Bob is unlucky, and his calculation results in a number that is larger than his modulus, he  will have to change one of the random numbers and repeat the calculation.
The result will be the R3  that he supplies as part of the group signature.
 Figure 16-4.
Ring Signature Creation y1 R1  Encrypt with M 1 ’s public key  AES Encrypty2 R 2Encrypt with M 2’s public key AES Encrypt y3 R3  Decrypt with M3’s private key AES Decrypt y4 R4Encrypt with M 4’s public key AES Encrypt y5 R5  Encrypt with M5’s public keyAES Encrypt                                                 Someone who knows any one of the private keys can compute a ring of values that satisfies  the verification algorithm, but the verifier will have no way of knowing which private key was used.
 16.5.3 DAA (Direct Anonymous Attestation)  DAA [BRIC04] is a group signature scheme that has been adopted by the TCG (Trusted Computing  Group) for use by TPMs (Trusted Platform Modules).
An example application that would be suited  for DAA is DRM (Digital Rights Management).
Attestation is a signed statement of something  such as, in this case, the configuration of the hardware and software.
16.5.4 428 C RYPTOGRAP HIC TRICKS  An overview of DAA (without the math) is that there are three entities—a DAA issuer (that  issues DAA credentials), a platform (that is doing the attestation using the credentials it has  received), and a verifier.
 If the credential were simply a certificate, the platform would present the certificate to the  verifier, along with proof that the platform knows the private key associated with the public key in  the certificate.
However, DAA does not do this.
The dialog between the platform and the prover  consists of a (noninteractive) zero-knowledge proof that the platform has a valid credential issued  by the DAA issuer.
 If the private key of a platform becomes known (someone has broken into the platform and  obtained the key and posted it somewhere), then all previous attestation proofs using that private  key can be identified, and future attestations can be refused.
 If the private key is not known, even the DAA issuer would not be able to know which plat - form has issued a particular proof, and the only way to revoke it would be to rekey all valid devices.
 16.5.4 EPID (Enhanced Privacy ID)  EPID [BRIC10] is an enhancement to DAA that allows an additional form of revocation.
The only  form of revocation possible in DAA is revocation of a private key, and it requires that the private  key of the member to be revoked be known in order to do revocation of that member.
 With EPID, there is a way to revoke “whoever generated this signature”.
Suppose there were  a revocation list of “bad signatures”, where it’s not known who generated those signatures, but the  signatures were created by some platform in the past that was later found to have acted badly in some way.
With EPID, the verifier asks the platform to prove it is a valid member of the group (as  it would with DAA).
In addition, the verifier can present a list of bad signatures to the platform, and  the prover can prove (again, using a zero-knowledge non-interactive proof) that its signature was  produced by a different key than each of the signatures on the list of bad signatures.
 16.6 CIRCU IT MODEL  A concept that we’ll need for §16.7 Secure Multiparty Computation (MPC) and §16.8 Fully Homo - morphic Encryption (FHE) is that of a circuit .
A circuit is a type of algorithm for computing an  output from an input, consisting of a sequence of simple operations called gates.
The output of a  gate may either be the final output of the circuit, or it may be an intermediate value.
The gates can  act on the inputs of the circuit, intermediate values, or constants ( e.g., a value that is always 1).
One  way that circuits differ from more general kinds of programs is that the sequence of gates and
16.7 SECURE MULTI PARTY COMP UTATI ON (MPC) 429  which values they act on cannot depend on the input to the circuit, so circuits cannot have branches,  loops, or references to a memory location specified by the input.
Nonetheless, it is a theorem that  any program with a fixed-size input and a known maximum number of computation steps can be  converted into a circuit acting on 0s and 1s and consisting of only two types of gates:  • bitwise AND ( ∧) (mod 2 multiplication)  • bitwise XOR ( ⊕) (mod 2 addition)  Executing a program as a circuit can be expensive.
For instance, if there is a branch, you have to  append two circuits—one for computing based on the computation that would be done if the condi - tion were known to be satisfied and the other for the computation to be done if the condition were  known to be not satisfied.
If the program has a loop that executes a variable number of times, the  circuit has to execute the loop the maximum number of times.
Also, to look up something in a large database (assuming the index of the database cannot be predicted without knowing the input to the  program), the cost of looking up a single item in the database will be proportional to the total size  of the database.
 16.7 SECUR E MULT IPAR TY COMPUTATI ON (MPC)  Secure multiparty computation [BENO88] (sometimes abbreviated as MPC) consists of having a group of n participants, each with its own input, collectively compute a function of the n inputs  without divulging its own input to the other participants.
One simple way of accomplishing this is to have all participants send their inputs to someone, say, Tom, that all the participants trust, and have Tom compute the function of all the inputs.
But the cryptographers that have been devising  protocols for secure multiparty computation are assuming there is no trusted Tom, and so the partic - ipants have to somehow collectively compute the function.
One model is “honest but curious”,  where the participants do the computations honestly, but the only mischief the system is designed to  prevent is participants learning information about other participants’ inputs.
That is what we’ll  describe here.
There are additional research papers that discuss how to detect and protect against maliciously incorrect computation.
 An example function where participants might want to keep their inputs secret is an auction,  where the participant with the highest bid will win the auction (and have to pay what they have bid).
 If you knew all the other bids, you could bid just slightly higher than the highest bid, so the partici - pants do not want their bids known by the other participants, even after the auction is completed.
 There are several published protocols for computing a result based on inputs from several partici - pants, where no one can see any inputs other than their own.
Indeed, one of these protocols has been used in the Danish sugar beet auction [BOGE09].
It is an expensive protocol.
But it only hap - pens once a year, so the Danish sugar beet farmers are reportedly happy with the result.
430 C RYPTOGRAP HIC TRICKS 16.7  There are various schemes, but most secure multiparty computation is based on secret sharing  (see §16.1 Secret Sharing ).
The computation needs to be converted into a circuit (see §16.6 ).
Then  each participant’s input is divided into shares that are distributed to the other participants.
 After a process where the participants compute on the shares they have been given and com - municate with one another to produce shares of intermediate values, the final output can be con - structed from a quorum of the participants combining their shares of the output.
 Each participant Pi has an input pi that will be used in computation by the community of n  participants.
We’ll use the term t-share to refer to a share of an input based on a degree- t polyno - mial.
Participant Pi uses secret sharing, with a degree t polynomial, to create and distribute t-shares  of its input to each of the other participants.
Now each participant has n numbers, each of which is  a t-share of the input of each of the n participants.
Note that a group of t +1 participants could col - lude to recover the input of any of the participants.
 Suppose a computation that the group must perform is to add the inputs of Pk and Pj .
Addi - tion is easy.
Each participant takes its share of pk and adds it to its share of pj.
Everyone now has a  share of pk+pj.
 Multiplication is more complicated.
To compute pk pj, everyone multiplies their t-share of pk  to their t-share of pj.
This, unfortunately, doubles the degree of the polynomial, so they now all have  a 2t-share of pk pj .
It will now take 2 t +1 participants to recover pk pj .
It will be necessary to reduce  the degree of the polynomial for the item for which the participants have 2 t-shares.
This is done  with two rounds of communication, where every participant sends a message to every other partici - pant, followed by computation.
 • Round 1: Each Pi creates n t-shares of its 2 t-share of pk pj and then sends the corresponding t- share to each of the other participants.
After this step, each of the participants will have n val- ues, each of the n values being a t-share of one of the participants’ 2 t-share of pk pj.
Then  each participant takes the vector of these n values and multiplies that vector by a matrix that  turns it into a different set of n values, which are each participant’s t-share of their t-share of  the product pk pj.
In other words, the first of the n values is P1’s t-share of P1’s t-share of pk pj .
 The second of the n values is P1’s t-share of P2’s t-share of pk pj , and so on.
 • Round 2: Each participant distributes the first of their n numbers to P1, the second number to  P2, and so forth.
What each participant will receive in round 2 is n t-shares of its own t-share  of the product pk pj .
From these (actually only t +1 of these are needed), each participant can  now construct its own t-share of pk pj.
So now, instead of having a 2 t-share of pk pj, each par - ticipant has a t-share of pk pj .
 Note that if t were much smaller than n, perhaps several multiplications could be done before the  degree reduction step.
However, since most applications don’t involve very many participants, and  since small values of t would be insecure (too easy for t colluding participants to find each other),  most implementations set t to be the largest value for which 2 t +1 is no greater than n.
16.8 FULLY HOMOM ORPHIC ENCRYPTION (FHE) 431  16.8 FULLY HOMOMORPHI C ENCRYP TION (FHE)  With homomorphic encryption, the data can be encrypted (in a special way), computation can be  done on the encrypted data, and the result is the encrypted answer.
So, computing on the plaintext  data will yield the same answer as computing on the encrypted data and decrypting the result.
One  application of homomorphic encryption is to store your encrypted data in a public cloud and do  computations on the encrypted data, without needing to trust the cloud not to leak your data.
 Another reason for doing this might be if Alice has secret data, and Bob has a proprietary program  to do computation on the data.
Alice doesn’t want Bob to see her data, and Bob doesn’t want Alice  to learn about his program.*
 There are alternative approaches that get some or all of the advantages sought by homomor - phic encryption:  • Copy all your encrypted data to a location considered more secure, such as your private net - work, decrypt it, and then do your computation on the plaintext.
 • Use secure enclaves , which are special hardware features in chips, such as Intel’s SGX or  AMD’s SEV, designed to hide user-level data and code from potentially malicious hypervi - sors, operating systems, or cloud administrators with admin privileges.
Secure enclaves do  not significantly impact performance, and it is far easier to adapt a program to take advantage  of secure enclaves than to turn a program into a circuit of ∧ and ⊕ operations (which is what  computing on homomorphically encrypted data requires).
Secure enclaves may not give the  provable security that homomorphic encryption could, especially since, historically, there have been side-channel attacks from which data could leak.
However, this approach will  always be more efficient than homomorphic encryption.
 Homomorphic encryption has been a passion of the cryptography community for many years.
As  we said in §16.6, any computation can be done with a circuit consisting of ∧ (and/multiply) and ⊕  (xor/add).
So, all that is necessary to achieve a homomorphic encryption scheme is to figure out  some mathematics that allows both operations on encrypted data.
 There are schemes where it is possible to do one operation but not the other.
For example,  RSA encryption (without padding) can do multiplication homomorphically.
If you have two mes - esages m1 and m2, a public key 〈e,n〉, and private key 〈d,n〉, multiplying the encryption of m1(m1  emod n), by the encryption of m2(m2 mod n) will yield ( m1m2)e mod n. Then if you decrypt  (m1m2)e mod n (by mod n exponentiation to the power d), it will have the same result ( m1m2) as  having multiplied the plaintexts m1 and m2.
But that’s only one operation (multiplication).
 *Even if Alice cannot see Bob’s program, because she sends her encrypted data to Bob to run his computation, there is the  worry that the pattern of noise that Alice sees in the result might divulge something about Bob’s program.
To avoid this  potential issue, an extra property called circuit privacy is needed.
FHE schemes with circuit privacy have been proposed,  and they are not markedly less efficient than schemes that are not designed to have circuit privacy.
16.8.1 432 C RYPTOGRAP HIC TRICKS  There are also schemes where you can do one operation many times and the second operation  only a limited number of times.
These schemes are called “somewhat homomorphic encryption”.
 An example we’ve already seen that behaves similarly to somewhat homomorphic encryption is the  scheme in §16.7 (if you don’t do the degree reduction step when you multiply).
In most somewhat - homomorphic-encryption schemes, the number of additions and multiplications that can be per - formed on ciphertext is limited due to “noise” that must be combined with ciphertext in order to  make the homomorphic encryption secure.
Addition and more so multiplication increase the  amount of noise.
If there is too much noise, decryption is no longer possible.
 16.8.1 Bootstrapping  Craig Gentry [GENT09 ] came up with a trick that eliminates the issue of a homomorphic operation  that can only be done a limited number of times.
He calls the trick bootstrapping , which reduces the  amount of noise that ciphertext has accumulated after some computation steps.
 We assume the cloud, which is not allowed to see the plaintext or private key, needs to do the  bootstrapping.
In order to do this, the cloud needs to know  • the user’s public key, (which enables the cloud to convert plaintext into minimally noisy  ciphertext), and  • the user’s private key, encrypted with the user’s public key.
 If the cloud knew the actual private key, it could decrypt the noisy ciphertext C and re-encrypt it  with the public key.
But the cloud is not allowed to see the actual private key or the plaintext.
The  cloud is allowed to see the user’s private key homomorphically encrypted with the user’s public  key, so we will provide that.
 The cloud first encrypts the noisy ciphertext C with the user’s public key, resulting in doubly  encrypted data.
Typically, each bit of C will be independently encrypted, so the doubly encrypted  data will be vastly larger than C. The cloud then homomorphically evaluates the decryption circuit  using the encrypted private key and the encrypted ciphertext ( i.e., the doubly encrypted plaintext).
 This results in a new singly encrypted plaintext.
As long as the noise in the original singly  encrypted plaintext ( C) was sufficiently small that C was decryptable, the new singly encrypted  plaintext will be an encryption of the same plaintext, and it will have a fixed amount of noise that  will not depend on how much noise the ciphertext had accumulated.
 As long as the bootstrapping operation itself plus any single operation doesn’t introduce too  much noise, you can do arbitrarily many computations on the encrypted data, so long as you run the  bootstrapping operation often enough.
Because of the bootstrapping trick, many fully homomor - phic encryption schemes have been proposed.
16.8.2 FULLY HOMOM ORPHIC ENCRYPTION (FHE) 433  16.8.2 Easy-to-Understand Scheme  Most of the FHE schemes involve tedious math.
Just for intuition, we will present one scheme  [VAND10] that is surprisingly easy to understand.
It is definitely not practical ( e.g., the encrypted  data would expand by a factor of about a billion), but our purpose is to give the reader intuition.
 This scheme requires encryption to be done on each bit.
So it is necessary to know how to  encrypt 0 and how to encrypt 1.
The private key is a large odd integer n.* This scheme will do ordi - nary integer arithmetic and not modular arithmetic.
 To encrypt a bit if you know n:  • Choose some very large multiple of n.  • Add or subtract a relatively small even number (which we’ll call noise ).
We’ll call the value  at this point a noisy multiple of n. If you are encoding a 0, you are now done.
 • If the bit to be encrypted is a 1, add or subtract 1.
 In order to allow someone who does not know n to encrypt, we create a public key that is a list of  encryptions of 0.
To encrypt a 0, someone would add together a randomly chosen subset of the  encryptions of 0.
To encrypt a 1, they would do the same thing but add or subtract 1 at the end.
 Only someone who knows n will be able to decrypt.
To decrypt a value x, find the nearest  multiple of n and if the difference between x and the multiple of n is even, x decrypts to a 0.
If the  difference is odd, x decrypts to a 1.
 Adding two encoded bits results in the ⊕ (exclusive or) of the two plaintext bits.
Multiplying  two encoded bits results in the ∧ (and) of the two bits.
Adding or subtracting 1 to an encoded bit  computes the ¬ (not) of the encoded bit.
Note that the noise increases each time two encrypted bits  are added or multiplied.
If the noise ever gets bigger than n/2, decryption will no longer be guaran - teed to produce the right answer (see Homework Problem 10).
So, we can do additions and multi - plications, but we have to avoid letting the noise get bigger than n/2.
Another annoying issue is that  the size of the encrypted bit doubles each time a multiplication is performed.
If you multiply two  billion-bit numbers together, you get a two-billion-bit number.
It doesn’t take too many multiplica - tions before the numbers (which started out as ridiculously huge) to get super-absurdly ridiculously  huge.
 The paper has a very cute trick for reducing the size of an encrypted bit using a value similar  to the public key.
It is a value that is safe to give to the cloud, which will be doing the computation  and the number reductions.
The reduction list contains noisy multiples of n of different sizes, say  m1,m2,m3,….
 To reduce x, take the largest mj that is smaller than x and reduce x mod mj.
The result, say x′,  can be reduced further by looking for the largest mk that is smaller than x′ and reducing x′ mod mk,  *The paper uses p for the secret odd number, but since usually p is a prime, and it doesn’t need to be a prime in this case, we  use n.
434 C RYPTOGRAP HIC TRICKS 16.9  and so forth.
Although it’s nice that this operation makes the encoding of x smaller, modular reduc - tion increases the noise.
 Additions will increase the noise somewhat, but multiplications and modular reductions will  very quickly increase the noise.
It is essential to bootstrap before the noise gets bigger than n/2.
 Bootstrapping requires using the public key (the multiple encryptions of 0) to encrypt each  bit of the noisy value, resulting in about a billion billion bits).
Then this value needs to be homo - morphically decrypted, using a circuit that homomorphically decrypts the result with a circuit that  has access to a homomorphically encrypted version of the private key n.  As we said, we present this scheme only because it is easy to understand.
There are less  impractical homomorphic schemes proposed, and this is an area of active research.
 16.9 HOMEWO RK  1.
In Shamir’s secret sharing scheme ( §16.1 Secret Sharing ), if k =7 and n =37, what degree  polynomial would you need to create shares?
 2.
In the graph isomorphism ZKP, why must Alice choose a different graph each time? (
In other  words, what would happen if Bob asked Alice to show how to map G 3 to G 1 in one round and  asked Alice to map the same G3 to G2 in another round?)
 3.
Show how in the protocol in §16.4.3 Noninteractive ZKP , Bob could create a transcript with - out knowing x. (Hint: Have Bob choose random numbers for messages 2 and 3, and then  compute what Alice would have needed to send in message 1.)
 4.
Suppose in the protocol in §16.4.3 Noninteractive ZKP , Bob were a dishonest verifier.
How  could he convince a third party that he actually communicated with Alice? (
Hint: Instead of  choosing a random number for message 2, his message 2 could be, say, a hash of message 1.
 Once committed to message 1, Bob can no longer derive message 1 from messages 2 and 3 without knowing x.)  5.
In §16.5.2 Ring Signatures , we described the ring signature computation as working in the  forward (clockwise) direction.
How would you compute a signature working in the backward  (counterclockwise) direction?
Describe how you could start at any point in the ring and still  compute a signature.
 6.
For each of the simple group signatures schemes in §16.5.1 Trivial Group Signature Schemes ,  specify its properties (linkability of signatures, whether the group manager can determine  which member signed something, whether a verifier can know which member signed some - thing, whether it is possible to revoke a group member).
16.9 HOMEWORK 435  7.
Show how a ring signature scheme could be done with an irreversible function such as  HMAC instead of a reversible function such as AES.
 8.
Why would it be secure to use twenty rounds in an interactive ZKP where an impersonator  had a one-in-two chance of answering correctly on each round, but many more rounds would  be required for a noninteractive ZKP?
 9.
Section §16.6 Circuit Model asserts that the only gates that are required are ⊕ and ∧. How  would one create a ¬ gate (a NOT gate)?
 10.
Consider the homomorphic encryption scheme described in §16.8.2.
Why must n be odd?
 11.
In §16.8.2, why would an encrypted 0 look like an encrypted 1 if the noise ever got bigger  than n/2?
17 FOLKLORE  Whenever I made a roast, I always started off by cutting off the ends, just  like I’d seen my grandmother do.
Someone once asked me why I did it, and  I realized I had no idea.
It had never occurred to me to wonder.
It was just  the way it was done.
Eventually I remembered to ask my grandmother.
 “Why do you always cut off the ends of a roast?”
She answered “Because my pan is small, and otherwise the roasts would not fit.” —
anonymous  Many things have become accepted security practice.
Most of these are to avoid problems that  could be avoided in other ways if you really knew what you were doing.
It’s fine to get in the habit  of doing these things, but it would be nice to know at least why you’re doing them.
A lot of these  issues have been discussed throughout the book, but we summarize them here.
First, though, in  §17.1 Misconceptions we debunk some things we often hear.
The rest of the chapter covers things  that are considered good to do.
We explain why they came to be accepted practice and when they are actually important.
 17.1 MISCO NCEPTION S  We list here several common security-related beliefs and describe what’s really true:  • Any program would run zillions of times faster on a quantum computer than on a classical  computer.
In fact, classical programs would not run faster on a quantum computer.
It is only a  narrow set of problems that can benefit from algorithms that are designed for quantum com - putation, and these algorithms are very different from classical algorithms.
A program  designed for a classical computer would not run faster on a quantum computer.
 • Quantum computers would break all of cryptography.
In fact, hashes and secret key algo - rithms are not broken by any known quantum attack (aside from Grover’s algorithm, which, even in the most optimistic scenarios for quantum computation, can be counteracted by dou - bling keysizes and hashsizes).
It is only public key algorithms that are threatened, and they  437
438 F OLKLORE 17.2  will (hopefully) be long replaced by post-quantum algorithms before the world might have a  quantum computer capable of breaking our currently deployed public key algorithms.
Also,  hopefully, any encrypted data that used current public key algorithms will no longer be sensi - tive at that point.
 • Quantum computers can factor prime numbers.
Indeed they can, but so can classical comput - ers.
In fact, you can probably factor a prime number in your head.
We assume this oft- repeated sentence that quantum computers can factor prime numbers is just a misstatement,  but people really ought to get in the habit of saying “quantum computers can factor num - bers.”
 • It is good to make users change passwords every few months .
It depends on your goals.
If  your goal is to annoy users, this is a good strategy, but if your goal is to make things more  secure, periodic password changes make systems less secure.
If a bad guy stole a user’s pass - word, they can do enough damage in the weeks between forced password changes that having  the user change her password every few months will not enhance security.
Also, forcing users to remember even more passwords will lower security.
Given that users tend to be human,  they will resort to a simple strategy such as ending their password with a number and incre - menting that number at each cycle.
 • Any system using MD5 is insecure .
We are mentioning MD5 as an example, but we are really  referring to any cryptographic algorithm that cryptographers have shown to have vulnerabili - ties.
In fact, just because an algorithm is insecure in one usage does not mean that it will be  insecure in all cases.
If you are implementing a new system, you should definitely use algo - rithms that the cryptographic community believes in unless there is some really good reason  (such as backwards compatibility) to use a deprecated algorithm.
For an existing system,  even if the deprecated algorithm would actually be secure in that context, there is the possi - bility that when analyzing its security, you might have missed some subtle property.
Also,  once a customer realizes you are using an algorithm that they have read is insecure, it will be  time-consuming to explain to the customer why it is secure in that particular context.
 Explaining might be more trouble than modifying the working system to use more current algorithms.
 17.2 PERFEC T FORWARD SECR ECY  Perfect forward secrecy (PFS) (see §11.8) is a protocol property that prevents someone who  records an encrypted conversation between Alice and Bob from being able to later decrypt the con - versation, even if the attacker has since learned the long-term cryptographic secrets of either or both
17.3 CHANG E ENCRY PTION KEYS PERIODICALLY 439  sides.
A protocol designed with PFS cannot be deciphered by a passive attacker, even one with  knowledge of Alice and Bob’s long-term keys.
However, an active attacker with knowledge of, say,  Bob’s long-term key can impersonate Bob to Alice or act as a MITM.
And even if attacker Trudy  doesn’t know Bob’s long-term key, if she can convince Alice that a different key belongs to Bob,  Trudy can act as a MITM (see §11.5).
PFS is considered an important protocol property to keep the  conversation secret from  • an escrow agent who knows the long-term key  • a thief who has stolen the long-term key of one of the parties without this compromise being  detected  • someone who records the conversation and later manages to steal the long-term keys of one or both parties  • law enforcement that has recorded the conversation and subsequently, through a court order,  obtains the long-term key of one or both parties.
 17.3 CHANG E ENCRY PTION KEYS PERIO DICALLY  It is good to change keys (do key rollover ) before a key is used on more than some amount of data,  or for more than a certain amount of time.
When a key needs to be rolled over depends on the  encryption mode and the block size.
For example, in CBC mode, due to the birthday problem (see  §5.2), it is likely that after 2n/2 blocks have been encrypted with the same key, two ciphertext blocks  will be equal.
In that case (a collision of two ciphertext blocks) it will be possible to compute the ⊕  of two plaintext blocks, which might leak information.
If you’d like the probability of a ciphertext  collision to be much smaller than 1/2, then the key should be changed more frequently than every  2n/2 −32blocks.
For example, if you want the probability of a collision to be less than 2 , you should  change keys before you’ve encrypted 2n/2−16 blocks.
 In GCM mode, it is essential that the 96-bit per-message portion of the IV is not reused (with  the bottom 32-bits of the 128-bit counter incremented for blocks within a message).
If the applica - tion can definitely keep track of all IVs used, then it could keep the same key and encrypt 296 mes- sages with the same key.
If, however, the application chooses the 96-bit per-message portion of the IV at random, then the application should not encrypt more than 2 32 messages with the same key  (making the probability of reusing a randomly chosen IV less than one in a billion).
 Another reason to do key rollover (with perfect forward secrecy in the case of communicated  data, rather than encrypted data-at-rest) is in case the data encryption key were stolen without your  knowledge in the middle of the conversation.
This would limit the damage from a stolen key.
440 F OLKLORE 17.4  With encrypted data-at-rest, if the data is directly encrypted with the data encryption key, it  will be expensive, and probably not that helpful, to decrypt all the ciphertext and encrypt with a  new key, because it is likely that there will be backups of the old ciphertext.
However, if each mes - sage is encrypted with a randomly chosen data encryption key K, and associated with the data is K  encrypted with a long-term encryption key S1, it is inexpensive to rollover S1, since only the meta - data { K}S1 needs to be decrypted with the old S1 and encrypted with a new key, say S2.
 17.4 DON’T ENCRYP T WITHOU T INTEGR ITY PROTEC TION  It is common misconception that if something is encrypted, it would be impossible to modify the  ciphertext without it being detected.
There are some encryption modes that include integrity protec - tion, but if the mode does not include integrity protection, there are many attacks possible.
 • In a mode such as CTR mode, if the attacker knows or can guess the plaintext, it is trivial for  them to ⊕ the desired changes to the plaintext into the ciphertext.
Imagine a secret key net - work authentication system such as RADIUS (RFC 2058) in which an authentication server is configured with user secrets and shares a secret key with each server that a user might log  in to.
A server Bob will communicate with the authentication server over a secure session  (and we are assuming here, using a stream cipher).
Trudy connects to server Bob, claiming to  be Alice.
Bob sends Trudy a challenge and gets her response, then sends to the authentication  server, “Alice sent Z to challenge X.” The authentication server responds either success or  failure .
Without integrity protection, Trudy (who is impersonating Alice) can intercept the  message between the authentication server and Bob and ⊕ into the message ( success ⊕ fail - ure).
 • In other modes, modifying the ciphertext would corrupt the plaintext without the attacker being able to cause a predictable change to the plaintext.
Without integrity protection, the  system might accept the corrupted plaintext, with unknown consequences.
Who knows what  the nuclear power plant would do when asked to execute the command  &(Hk2’#zAzq*$fc_)2@c instead of reduce output by 10% ?
 • Integrity protection would prevent the vulnerabilities discussed in §17.5.
17.5 MULTIP LEXING FLOWS OVER ONE SECURE SESSION 441  17.5 MULT IPLEX ING FLOWS OVER ONE SECUR E SESSIO N  Folklore says that different conversations should not be multiplexed over the same secure session.
 For instance (Figure 17 -1), suppose two machines, F1 and F2, are talking with an IPsec SA and for - warding traffic between A and B and between C and D (where A and C are behind F1, and B and D  are behind F2).
F1 and F2 might be firewalls, with A, B, C, and D machines behind those firewalls.
 Or A and C might be processes on F1 and B and D might be processes on F2.
Some people would  advocate creating two secure sessions between F1 and F2, one for the A-B traffic and one for the C - D traffic, and using different keys.
 F1 F2  A BC D  Figure 17-1.
 Multiplexing A-B and C-D Traffic over One Secure Session  Creating multiple secure sessions is obviously more expensive in terms of state and computa - tion.
In §17.5.1 The Splicing Attack , §17.5.2 Service Classes , and §17.5.3 Different Cryptographic  Algorithms , we explain some cases where it is advantageous to create multiple secure sessions.
 17.5.1 The Splicing Attack  In a splicing attack, an attacker who can see multiple encrypted conversations replaces ciphertext  blocks from one encrypted conversation with ciphertext blocks from a different encryption conver - sation.
This sort of attack is foiled if the secure session is integrity protected as well as encrypted,  but let’s assume the F1-F2 secure session in Figure 17-1 is only doing encryption and assume F1 is  multiplexing A-B traffic and C-D traffic over a single secure session to F2.
 Suppose C is capable of eavesdropping on the F1-F2 link, as well as injecting packets.
It is  possible for C to do a splicing attack and see the decrypted data for the A-B conversation.
Assume  that the beginning of the plaintext, say, the first sixteen octets, identifies the conversation to F2, most likely by specifying the source and destination.
The splicing attack involves the following steps:  • Record an encrypted packet from the C-D conversation.
 • Record an encrypted packet from the A-B conversation.
 • Overwrite the first sixteen octets of ciphertext from the C-D packet onto the first sixteen  octets of the encrypted A-B packet.
 • Inject the spliced packet into the ciphertext stream.
442 F OLKLORE 17.5.2  When F2 decrypts the packet, it will observe from the first sixteen octets that the packet should be  delivered to D. The remainder of the packet is the data from the A-B conversation.
If it were  encrypted in ECB mode, all the data from the A-B plaintext packet will be delivered to D. But if it  were encrypted in, say, CBC mode, the first block of data will be garbled, but the remainder will be the plaintext from the A-B conversation.
 Note that this flaw is only relevant if the F1-F2 link does encryption only.
If it is using integ - rity protection, this attack is not possible.
However, since people found the flaw with encryption only, some people think it would be safer to use separate secure sessions between F1 and F2 for  each conversation, just in case a similar flaw is found when integrity is also used.
 Even if there is no security flaw due to multiplexing traffic from different conversations over  an encrypted and integrity protected tunnel, if F1 is a machine at an ISP (Internet Service Provider)  serving multiple customers, the customers often feel safer if their traffic is carried over its own SA.
 Since the ISP needs the customers, it is advantageous for it to make the customers feel safer.
 17.5.2 Service Classes  Suppose some types of traffic being forwarded between F1 and F2 get expedited service or some different routing that would make it likely for packets transmitted by F1 to get very much out of  order.
If F2 is protecting against replays based on a sequence number in the packet, a common  implementation to detect replays is for F2 to remember the highest sequence number seen so far,  say, n, and remember which sequence numbers in the range n−k to n−1 have already been seen.
If  F1 marks the packets with different classes of service, then packets with a lower priority might take  a lot longer to arrive at F2.
If more than k high-priority packets launched by F1 can arrive before a  lower priority packet launched earlier by F1, then the low-priority packet will be discarded by F2 as  out of the sequence number window.
 For this reason, some people advocate creating a different secure session for each class of ser - vice, so that each service class has sequence numbers from a different space.
 17.5.3 Different Cryptographic Algorithms  Some people advocate different secure sessions for different flows, because each flow might  require a different level of security.
Some flows might require integrity only (without encryption).
 Some encrypted traffic might require more security, and thus a longer key, than other traffic.
One  might think this could be solved by using the highest level of security for all traffic.
But the higher  security encryption might be a performance problem if it was used for all the traffic.
 Another reason to use different cryptographic algorithms is if some of the flows are for cus - tomers who would, for their own reasons, like to use particular cryptographic algorithms.
It might
17.6 USING DIFFERENT SECRET KEYS 443  be vanity crypto developed by their own company or country, they might feel safer using different  algorithms than what others use because they might have heard rumors about potential weaknesses,  or there might be legal reasons why particular algorithms might only be usable for traffic of certain  customers.
 17.6 USING DIFFERENT SECR ET KEYS  It is good practice to use different secret keys for establishing a secure session, as well as for pro - tecting data during the session.
 17.6.1 For Initiator and Responder in Handshake  Often a secure session between Alice and Bob is established using a shared key KA-B, but one must  be somewhat careful.
Using different keys for the initiator than the responder in an authentication  handshake avoids attacks such as in §11.2.1 Reflection Attack .
There are other ways of avoiding  reflection attacks, for instance:  • Have the initiator generate odd challenges and the responder even challenges.
 • Have the response to the challenge consist of a function of the challenged side’s name in  addition to the key and challenge, e.g., hash( name ,key,challenge ) or {name|challenge }key.
 17.6.2 For Encryption and Integrity  With CBC residue (see §4.3.1 CBC-MAC ) as an integrity check, there are problems when the same  key is used for encryption as well as integrity protection, as described in section §4.4 Ensuring Pri - vacy and Integrity Together .
But if the integrity protection were, say, a keyed hash, then there are no  known weaknesses of using the same key.
However, people are worried that since there was a prob - lem with using the same key for both purposes with CBC residue, perhaps someone will later find a  weakness with other schemes, and using two different keys avoids the issue.
 Another reason using two keys became popular in protocols was because of U.S. export laws.
 The allowed keysize for exportable encryption was 40 bits, which was definitely very weak.
But the  U.S. government did allow stronger integrity checks.
They only wanted to read the data, not tamper  with it, so it was okay with them if the integrity check were reasonably strong.
As a result, proto - cols often wound up with two keys: a 40-bit one for encryption and one of adequate size for integ - rity protection.
But today, strong encryption keys are allowed.
444 F OLKLORE 17.6.3  Today, there are cryptographic modes ( e.g., GCM and CCM) that are secure without requir - ing two different keys.
 17.6.3 In Each Direction of a Secure Session  If different keys are used for the two directions of a secure session, it prevents an attacker from  reflecting traffic to one of the parties to get them to interpret the message as if it came from the  other side.
 17.7 USING DIFFERENT PUBLIC KEYS  People often refer to “Bob’s public key”, but having Bob use a single public key can be problem - atic.
 17.7.1 Use Different Keys for Different Purposes  If the same key is used for, say, decrypting a challenge in an authentication protocol, and for  decrypting headers of encrypted messages, it is possible for an attacker (say, Trudy) to trick Bob  into decrypting something that will give Trudy information or signing something that will allow  Trudy to impersonate Bob.
An extreme example is a blind signature (see §16.2).
By definition, with  a blind signature, Bob has no idea what he’s signing.
So that key had better not be used for any  other purposes.
 But even when the application isn’t purposely designed to prevent Bob from knowing what  he’s doing, it is very likely that one application might not recognize an action that would have  meaning in another application.
For example, if the private decryption key is used to decrypt a chal - lenge in a challenge/response protocol, the “challenge” given could be an encrypted data encryp - tion key extracted from an email header.
 One method of preventing such cross-application confusion is to encode something  application-specific in the padding of the information to be signed or encrypted using public key cryptography.
So in a challenge/response protocol (let’s call the protocol QXV) in which Bob  would be asked to sign the challenge, the challenge could be a 128-bit number, and Bob would pad  the challenge, before signing, with an application-specific constant such as a hash of the string  authentication protocol QXV challenge-response .
If all applications using that same key were  carefully coordinated, no confusion would arise.
However, if any application didn’t insist on those
17.7.2 USING DIFFERENT PUBL IC KEYS 445  rules, there could be vulnerabilities.
Also, if any application were using blind signatures or blind  decryption, the padding would not be visible to the private key owner (Bob).
Therefore it would be  safest, in theory, to certify a key as only applicable for a given application.
The downside of this is  that maintenance of so many different keys could present its own vulnerabilities as well as perfor - mance issues.
 17.7.2 Different Keys for Signing and Encryption  There are are different consequences for forgetting a key and for having a key stolen.
The conse - quences for a signing key are different from the consequences for an encryption key.
This argues  that the same key should not be used for both purposes, so that different mechanisms can be used for storage and retrieval.
 If a signature private key is forgotten, it’s not a big deal.
You can just generate a new one.
But  if an encryption private key is lost, you’d lose all the data encrypted with it.
Therefore, it is common for Alice to make sure she doesn’t lose her private encryption key by keeping extra copies of the  key in lots of different places.
But this makes the key easier to steal, a trade-off considered accept - able in some cases for an encryption key, given the high cost of losing all the encrypted data.
 Note that if the signature key is stolen, the obvious solution is to revoke it.
But that would  cause all previous signatures to become invalid, even those that were validly signed before the sig - nature key was stolen.
You might think you could avoid invalidating all signatures by including a timestamp in the revocation saying “any signatures earlier than this date are still valid”.
However,  the attacker who stole the signature key can put any date he wants into the signature, so specifying  a date in the revocation will not help.
So, it’s dangerous to make copies of a signature private key,  because that makes it more likely to be stolen.
 If a signature key is only used for authentication, it is much simpler, since it isn’t important to  maintain its validity for previous authentications.
But if the signature key is used for signing docu - ments, you want to avoid invalidating all signatures.
 Another reason for separate keys for encryption and signatures is for law enforcement or for  a company that wants to be able to decrypt anything encrypted for any of its employees.
They  would like to make Alice’s private encryption key readily accessible to the IT department (or the  government).
But they have no need to be able to forge Alice’s signature, so they do not need to  have a copy of Alice’s signature private key.
In fact, it’s to their advantage not to have access to the signature key.
Then Alice cannot claim that messages signed by her key were an attempt by law  enforcement to frame her. (
Though she could still claim that an attacker or malware used her key  without her knowledge.)
 Yet another reason is that you’d like to treat old expired encryption keys differently than old  signature keys.
There’s no reason to archive old private signature keys.
Once the key is changed,  just throw away the old one.
Irretrievably discarding a private signature key once it isn’t supposed
446 F OLKLORE 17.8  to be used anymore safeguards against someone stealing the old key and backdating a document.
 But old encryption keys are likely to still be needed to decrypt old data.
 17.8 ESTABLI SHIN G SESSION KEYS  17.8.1 Have Both Sides Contribute to the Master Key  It is considered good cryptographic form for both sides of a communication to contribute to a key.
 For instance, consider the protocol in which Alice and Bob each choose a random number, send it  to the other side encrypted with the other side’s public key, and use a hash of the two values as the  shared secret.
Although this protocol does not have PFS, it has “better” forward secrecy than some - thing like SSL (where one side sends the secret encrypted with the other side’s public key).
By hav - ing both sides contribute to the key and hashing the two values, an attacker would need to learn  both side’s private keys to decrypt the recorded conversation.
Another reason for having both sides  contribute to the master key is that if either side has a good random number, the result will be a  good random number.
 17.8.2 Don’t Let One Side Determine the Key  In addition to ensuring that each side contributes to the key, folklore says it is a good idea to ensure  that neither side can force the key to be any particular value.
For instance, if each side sends a ran- dom number encrypted with the other side’s public key, if the shared key were the ⊕ of the two val - ues, then it would be easy for whichever side sent the last value to ensure that the result would be a  particular value.
If instead, the shared key were a cryptographic hash of the two values, this would  be impossible.
 Why is this an important property?
In most cases it wouldn’t matter.
However, here is a subtle  example where it would matter.
Suppose the application is doing an anonymous Diffie-Hellman exchange to establish a session key, because there are no credentials for Alice or Bob at that proto - col layer.
However, because an upper layer protocol has the ability to authenticate, Alice and Bob  can use channel binding (§11.6 Detecting MITM ), using the session key agreed-upon by the lower  layer.
If Trudy were acting as a MITM and could force the Alice-Trudy key to be the same as the Trudy-Bob key, then channel binding would not detect the MITM.
17.9 HASH IN A CONSTANT WHEN HASHI NG A PASSWOR D 447  17.9 HASH IN A CONSTAN T WHEN HASHI NG A PASSWORD  Users tend to use the same password in multiple places.
Consider a protocol in which a secret key is  derived from the user’s password and used in a challenge-response protocol (for example, see  Protocol 11-1).
In such a protocol, the server will contain a database of each user’s hashed pass - word, and the user’s hashed password can be used to directly impersonate the user.
Note we are not  talking about a protocol in which the client establishes a TLS session to Bob, Alice sends her pass - word over the secure session, and Bob hashes Alice’s password and compares it against Bob’s data - base of user passwords.
We are specifically referring to a protocol in which the client converts  Alice’s password into a shared secret key with Bob and uses that shared secret key in a challenge- response protocol.
 To prevent theft of one server’s database from allowing someone to impersonate users using  the same password on a different server, have the server hash a constant into the password hash,  such as the server name.
In that way, If Alice used the password albacruft on both server X and  server Y, her hashed password stored at X would be different from what would be stored at Y,  because at X, the value “X” would be hashed with her password, and at Y, the value “Y” would be  hashed with her password.
If someone were to steal server X’s database, they could impersonate  Alice on server X. If they did a dictionary attack on X’s stolen database and discovered Alice’s  actual password, they could impersonate her on server Y .
However, if Alice’s password were good  enough to withstand a dictionary attack, X’s stolen database would not allow the attacker to imper - sonate Alice on server Y .
 This strategy, of course, requires the client machine to hash in the same constant when creat - ing the user’s shared secret with that server.
Since Alice has requested to talk to X, the client will know what constant to hash with the password.
 This strategy solves the problem of Alice using the same password on multiple servers.
It  should be used in addition to approaches like using salt ( §9.8 Off-Line Password Guessing ) or mak - ing the hashing computation artificially expensive ( e.g., hashing the user’s password many times),  which are both solutions to a different problem, namely making off-line password guessing more  computationally expensive for an attacker.
 17.10 HMAC R ATHE R THAN SIMPLE KEYED HASH  Folklore today says that in order to do a keyed cryptographic hash, you should use HMAC (see  §5.4.11 ).
It is computationally slower than, say, doing hash( message |key).
But it is possible, as we  explain in §5.4.10 Computing a MAC with a Hash , to do the keyed hash incorrectly.
With some
448 F OLKLORE 17.11  hash algorithms, if you do hash( key|message ) and divulge the entire hash, then it is possible for  someone who sees the message and the entire keyed hash to append to the message and generate a  new keyed hash, even without knowing the key with which the hash is produced.
With HMAC, this  attack is impossible.
There are other ways of avoiding that attack that are simpler and faster and likely to be as secure.
However, HMAC comes with a proof, so it is popular despite its (minor) per - formance disadvantage.
Today there are encryption modes that include integrity protection such as AES-GCM that are also provably secure, and more efficient, and these are gradually replacing  HMAC.
 17.11 KEY DERIVATION  Key derivation is the technique of using a small number of random bits as a seed and from it deriv - ing lots of bits of keys.
For instance, from a 128-bit random seed, you might want to derive 128-bit  encryption keys and 128-bit integrity keys in each direction.
Or you might want to periodically do key rollover (without PFS) lots of times, using the same seed.
 The alternative to key derivation is obtaining independent random numbers for each key.
It  might be expensive to obtain that many random bits.
For instance, if the random number is sent  encrypted with the other side’s public RSA key, it is convenient to limit the size of the random  number to fit within an RSA block.
And if an unpredictably many future keys also need to be derived from the same keying material, you wouldn’t know how large a random number you’d  need.
 If the random number is sufficiently large (say, 128 bits or more), then it can be used as the  seed for an unlimited number of keys.
It is important to do this in such a way that divulging one key  does not compromise other keys.
So each key is usually derived from the random number seed and  some other information (such as a key version number or timestamp), using a one-way function.
 The random number from which all other keys are derived is often known as the master key  or key seed .
 In many protocols, creating and/or communicating the initial secret is expensive, because it  involves private key operations.
Reusing the original secret for generating new keys will be less  expensive than creating and sending a new random seed.
 In the cryptography community, companies with outrageous claims and bogus security prod - ucts are known as “snake-oil” companies.
A common claim made by the snake-oil marketeers is  that the company’s algorithm uses keys of some huge size, perhaps millions of bits, and therefore  their product is much more secure than, say, the 128-bit keys used by other companies. (
And they  usually have a “patented”, proprietary encryption scheme.)
There are many aspects of such a state - ment that should raise the suspicion that the claims are snake oil.
Often the million-bit key is gener -
17.12 USE OF NONCES IN PROTOC OLS 449  ated from a very small seed, perhaps 32 bits, which, through key derivation, turns into many bits.
So  even though the key used might be a million bits, if the seed from which it is computed is 32 bits,  then the key itself is equivalent to a 32-bit key.
 17.12 USE OF NONC ES IN PROTOC OLS  Nonces are values that should be unique to a particular running of the protocol.
Protocols use non - ces as challenges (Bob gives Alice a nonce as a challenge, and she proves she knows her secret by  returning a function of the challenge and her secret), or perhaps as one of the inputs into the session  key derived from the exchange.
Some protocols would be insecure if the nonces were predictable, whereas other protocols only require that the nonce be unique.
If you don’t want to bother analyz - ing whether your protocol requires unpredictable nonces, it is usually safest to generate them ran - domly.
 See §11.4 Nonce Types for examples of both types of protocol (those that require unpredict - able nonces and those that do not).
It has become fashionable to put lots of nonces into protocols,  and furthermore, to require them all to be randomly chosen.
However, in some cases, such as when  the nonce must not be reused, it is more secure to use a sequence number, because it requires a lot  less state to remember all the sequence numbers used recently, since the application can delete mes - sages with sequence numbers that are too old.
If the nonce were randomly chosen, the application  would need to remember all nonces ever used.
 17.13 CREATING AN UNPREDI CTA BLE NONC E  If it’s inconvenient to obtain large random numbers, a unique nonce such as a sequence number can  be turned into an unpredictable nonce by hashing it with a secret known only by the sender.
450 F OLKLORE 17.14  17.14 COMPRESSIO N  If compression is desired for encrypted data, compression must occur before the data is encrypted.
 This is because compression algorithms depend on the data being somewhat predictable.
With any  secure encryption algorithm, ciphertext looks random and therefore would not compress.
 Obviously, compression saves bandwidth for transmitted data and saves storage for stored  data.
This advantage might be offset by the disadvantage of additional computation to compress  before encryption and decompress after decryption.
On the other hand, compression might provide  a computation benefit because there are fewer octets to encrypt/decrypt after the data has been com - pressed.
On the third hand (we1,2,3,4 have eight hands between us), there is sometimes hardware  support for the encryption algorithm but not for the compression algorithm.
 In olden times when encryption algorithms were weaker, the folklore belief was that com - pression made things more secure.
The assumption was that when an attacker did brute-force  search of the keyspace, it would be harder to recognize valid compressed plaintext than valid  uncompressed plaintext.
However, often compressed data starts with a fixed easily recognizable  header, so it would be likely easier for a brute-force attacker to recognize valid compressed data.
 Recently, though, cryptographers have been recommending against doing compression.
That  might seem surprising.
What do they have against saving bandwidth and storage?
The reason that compression has fallen out of favor among cryptographers is that in many cases, compression leaks  information about the plaintext.
For example, in a block storage system, the plaintext is chopped  into fixed-size blocks.
With compression and encryption, although an attacker would not be able to  decrypt an encrypted block, the attacker would be able to see the size of the ciphertext block.
The  compression algorithm would be well-known, so based on the size of the resulting ciphertext block,  the attacker will be able to know which candidate plaintext blocks would compress to that size.
 Another example where compression leaks information was published in [ WRIG08].
The  natural way of encoding audio is to break it into fixed time-unit chunks and compress each chunk.
 If encryption is used, then each compressed chunk is encrypted with a length-preserving encryption  mode.
The paper shows that given the size of each compressed encrypted chunk, an attacker can  learn information about the plaintext, such as what language was spoken and where phrases of  interest were said.
 Another attack is known as CRIME (Compression Ratio Info-leak Made Easy)  [en.wikipedia.org/wiki/CRIME ], which is a vulnerability relevant to HTTP and cookies.
As a result  of this and other vulnerabilites due to compression, support for compression was removed in TLS  1.3.
Since very few implementations of TLS had implemented compression, removing support for  compression was not controversial.
 Compression is only problematic when it is performed on chunks whose compressed lengths  leak more information than knowing lengths of the chunks of plaintext would.
There is nothing
17.15 MINIM AL VS.
REDUND ANT DESIGNS 451  wrong with compressing large volumes of information before encrypting and sending fixed size  chunks of the resulting compressed data.
 17.15 MINIMAL VS.
REDUNDANT DESIGN S  Some protocols are designed so minimally that everything in them is essential.
If you cut corners anywhere and don’t follow the spec carefully, you will wind up with vulnerabilities.
In contrast, it  has become fashionable to over-engineer protocols, in the sense of accomplishing what is needed for security several different ways, when in fact n−1 of those ways are unnecessary as long as any  one of them are done.
An example is throwing in lots of nonces and other variables, each required  to be random, whereas for security only one of them would be required to be random.
We believe  this fashion of redundancy is a dangerous trend, since the protocols become much more difficult to  understand.
And as the person writing the code for choosing the nonce notices that there is no rea - son for the nonce to be random, and therefore doesn’t bother, and the person writing the code for a different portion of the protocol notices that their nonce doesn’t need to be random, you wind up  likely to have a protocol with flaws.
It’s okayto cut n−1 corners, but not n corners.
 17.16 OVERESTIM ATE T HE SIZE OF KEY  U.S. law enforcement was always trying to find the “right” keysize, sufficiently large to be “secure  enough” but sufficiently small that it would be breakable, if necessary, by law enforcement.
Of course, such a keysize couldn’t be really secure, because “secure” means nobody, including them,  would be able to break it.
Anyway, the assumption was that they were smarter or had more compu - tation power than the bad guys from whom you were trying to protect yourself.
Even if that were  true ( e.g., that law enforcement had ten times as much computation power as organized crime),  given that computers keep getting faster, and encrypted data often needs to be protected for many years, a system breakable by law enforcement today would be breakable by organized crime in a  few years.
But it’s also absurd to assume that the bad guys (whoever they are) would have signifi - cantly less computation power at their disposal than law enforcement.
So it’s dangerous to try to pick the smallest “sufficiently secure” keysize.
452 F OLKLORE 17.17  17.17 HARD WARE RAND OM NUMBER GENERA TOR S  Generating random numbers can be tricky.
Typically, computers are designed to be deterministic,  and it isn’t obvious how to cause software to choose a different number each time a random number  is required.
Various software-only strategies include recording timing of user keystrokes or mes - sage arrivals on a network, or statistics on attached devices.
There have been many examples of  implementations that were insecure because of faulty random number generators.
As a result, many  people wish that computers would include a source of true randomness, which can be done at the  hardware level by measuring noise of some sort.
Indeed, it would be convenient and enhance secu - rity to have a hardware source of random numbers.
However, instead of depending on it as the sole  source of randomness, it should be used to enhance other forms.
 Suppose an organization was designing a hardware random number generator that was likely  to become widely deployed.
It would be very tempting to purposely (and secretly) design it so that  it generated predictable numbers, but only predictable to the organization that designed the chip.
It  would be difficult or impossible for anyone to detect that this was done.
For instance, each chip could be initialized with a true random 128-bit number as the first 128 bits of output, and each sub - sequent 128 bits of output could be computed as the previous 128 bits of output hashed with a large  secret known only to the organization that designed and built the chips.
Assuming the hash was  good, the output would be indistinguishable from truly random.
It would be difficult to embed such  a trap in software, since it is more easily reverse engineered.
 Even if a hardware random number generator might have such a trap, it is still useful, and can  be used in such a way that would foil even the organization that embedded the trap.
The way it  should be used is to enhance any other scheme for generating random numbers.
So random num - bers should be generated as they would be without the chip (for instance, using mouse and key - board inputs and low-order bits of a fine-granularity clock).
But the output of the hardware random  number generator should also be hashed into the random number calculation.
 17.18 PUT CHEC KSU MS AT THE END OF DATA  Some protocols (for example, §12.5 AH (Authentication Header) use a format for integrity- protected data where the integrity check is before the data, or possibly even in the middle of the  data.
It is better instead to have the integrity check at the end [for example, §12.6 ESP (Encapsulat - ing Security Payload) ].
If the integrity check is at the end, then it is possible to be computing it  while transmitting the data and then just append the computed integrity check at the end.
The alter - native, where the integrity check comes before the data, requires the data to be buffered on output
17.19 FORW ARD COMPATI BILITY 453  until the integrity check is computed and tacked onto the beginning.
This adds delay and complex - ity.
This rule is mainly for transmitted data.
For received data, most likely it would be undesirable  to act on the data before the integrity check is verified, so the data would need to be buffered no  matter where the integrity check was stored.
 Having the integrity check in the middle of the data (as is done in AH) complicates both the  specification and the implementation.
Space for the integrity check must be provided, with the field set to zero, and then the integrity check calculated and stored into the field.
On receipt, the integrity  check must be copied to some other location, and then zeroed in its location in the message, so that the integrity check can be verified.
 Especially complex (again, as is done in AH) is when an integrity check is computed based  on selected fields, especially when some are optional and ordering is not strictly specified.
The  integrity check must be computed identically by the generator and verifier of the check.
 17.19 FORWARD COMPATIBILITY  History shows that protocols evolve.
It is important to design a protocol in such a way that new  capabilities can be added.
This is a desirable property of any sort of protocol, not just security pro - tocols.
There are some special security considerations in evolving protocols, such as preventing an  active attacker from tricking two parties into using an older, possibly less secure version of the pro - tocol (see §11.15.2 Downgrade Attack ).
 17.19.1 Options  It is useful to allow new fields to be added to messages in future versions of the protocol.
Some - times these fields can be ignored by implementations that don’t support them.
Sometimes a packet  with an unsupported option should be dropped.
In order to make it possible for an implementation  that does not recognize an option to skip over it and parse the rest of the message, it is essential that  there be some way to know where the option ends.
There are two techniques:  • Having a special marker at the end of the option.
This tends to be computation-intensive,  since the implementation must read all the option data as it searches for the end marker.
 • TLV encoding, which means each option starts with a TYPE field indicating the type of  option, a LENGTH field indicating the length of the data in this option, and a VALUE field,  which gives option-specific information.
454 F OLKLORE 17.19.2  TLV encoding is more common because it is more efficient.
However, the “L” must always be  present, and in the same units, in order for implementations to be able to skip unknown options.
 Sometimes protocol designers who don’t quite understand the concept of TLV encoding do clever  things like notice that the option they are defining is fixed length, so they don’t need the LENGTH  field.
Or that one option might be expressed in different units.
For instance, although AH (see §12.5  AH (Authentication Header) is designed to look like an IPv6 extension header, its length is  expressed in units of 32-bit words, when all the other IPv6 options are expressed in units of 64-bit  words.
 It is also useful to be able to add some options that should simply be skipped over by an  implementation that does not support it and to add other options that must be understood or else the packet must be dropped.
But if an implementation does not recognize the option, how would it  know whether it could be safely ignored or not?
There are several possible solutions.
One is to have  a flag in the option header known as the critical bit , which if not set on an unknown option, indi - cates the option can simply be skipped over and ignored.
Another possibility is to reserve some of the type numbers for critical options and some of them for noncritical options (those that can be  safely skipped if unrecognized).
 17.19.2 Version Numbers  A lot of protocols have a field for version number but don’t specify what to do with it other than to  specify what the implementation should write into that field.
The purpose of a version number field  is to allow the protocol to change in the future without confusing old implementations.
One way of  doing this without a version number is to declare the modified protocol to be a “new protocol”,  which would then need a different multiplexor value (a different TCP port or Ethertype, for  instance).
With a version number, you can keep the same multiplexor value, but there have to be  rules about handling version numbers so that an old implementation won’t be confused by a rede- signed packet format.
 17.19.2.1 Version Number Field Must Not Move  If versions are to be differentiated based on a version number field, then the version number field  must always be in the same place in the message .
Although this might seem obvious, when SSL  was redesigned to be version 3, the version number field was moved!
Luckily, there is a way to rec - ognize which version an SSL message is (in version 2’s client hello message, the first octet will be 128, and in version 3’s client hello message, the first octet will be something between 20 and 23).
17.19.2.2 FORW ARD COMPATI BILITY 455  17.19.2.2 Negotiating Highest Version Supported  Typically, when there is a new version of a protocol, the new implementations support both versions  for some time.
If you support both versions, how do you know what version to speak when talking  to another node?
Presumably the newer version is superior for some reason.
So you typically first  attempt to talk with the newer version, and if that fails, you attempt again with the older version.
With this strategy, it is important to make sure that two nodes that are both capable of speaking the  new version wind up speaking the newest version they both support and not getting fooled into  speaking the older version because of lost messages or active attackers sending, deleting, or modi - fying messages.
Why would an active attacker care enough to trick two nodes into speaking an ear - lier version of the protocol?
Perhaps the newer version is more secure or has features that the attacker would prefer the nodes not be able to use. (
We would hope there was some benefit to be  gained by having designed a new version of the protocol!)
 The right thing to do if you see a message with a higher version number than you support is to  drop the message and send an error report to the other side indicating you don’t support that ver - sion.
But there will be no way for that error message to be cryptographically integrity protected since the protocol has not been able to negotiate a key.
So unless care is taken, nodes could be  tricked into speaking the older version if an active attacker or network flakiness deleted the mes - sage of the initial attempt or an attacker sent an error: unsupported version number message.
 One method of ensuring that two nodes don’t get tricked into talking an older version is to  have two version numbers in the packet.
One would be the version number of the packet.
The other would be the highest version the sender supports.
But a single bit suffices, indicating that the sender  can support a higher version number than the message indicates.
If you establish a connection with  someone, using version n, and you support something higher than n, and you receive authenticated  messages with the  HIGHER VERSION NUM BER SUPPOR TED flag, then you can attempt to reconnect  with a higher version number.
 17.19.2.3 Minor Version Number Field  Another area in which protocol designers get confused is the proper use of a MINOR VER SION NUM - BER field.
Why should there be both MAJOR VERSION NUMB ER and MINOR VERSION NUMB ER fields?
 The proper use of a minor version number is to indicate new capabilities that are backward compat - ible.
The major version number should change if the protocol is incompatible.
The minor version  number is informational only.
If the node you are talking to indicates it is version 4.7 (where 4 is  the major version number and 7 is the minor version), and you are version 4.3, then you ignore the  minor version number.
But the version 4.7 node might use it to know that there are certain fields  you wouldn’t support, so it won’t send them.
 Most likely the confusion about the proper use of the minor version number is because soft - ware releases have major and minor version numbers, and the choice as to which to increment is a marketing decision.
Symmetric Encryption and Message ConfidentialityChapter 2
Model for network security
Model for network security•Using this model requires us to:•design a suitable algorithm for the security transformation •generate the secret information (keys) used by the algorithm •develop methods to distribute and share the secret information •specify a protocol enabling the principals to use the transformation and secret information for a security service
Symmetric Encryption Principles
Symmetric encryption•Sender and recipient share a common/same key•Was the only type of cryptography, prior to invention of public-key in 1970’s
Simplified model of symmetric encryption
Symmetric encryption•Has five ingredients•Plaintext:  the original message or data•Encryption algorithm: performs various substitutions and transformations on the plaintext•Secret key•Ciphertext: the coded message•Decryption algorithm: takes the ciphertext and the same secret key and produces the original plaintext
Other basic terminology•cipher-algorithm for transforming plaintext to ciphertext •encipher (encrypt)-converting plaintext to ciphertext•decipher (decrypt)-recovering plaintext from ciphertext•cryptography-study of encryption principles/methods•cryptanalysis (codebreaking)-the study of principles/ methods of deciphering ciphertext withoutknowing key
Requirements•Two requirements for secure use of symmetric encryption:•a strong encryption algorithm•a secret key known only to sender / receiverY = EK(X)X = DK(Y)•assume encryption algorithm is known•the security of symmetric encryption depends on the secrecy of the key•implies a secure channel to distribute key
A strong encryption algorithm attackerencryption algorithmplaintext / enquirycyphertext / response
TA & Grader•TA Name: Faiyaz, Amir (Project, Review & Quiz) •Email:  afaiyaz@ttu.edu•Reminder: Submit the namesandemailsofyour group members to FALL 2024 CS5342 PROJECT GROUP NAMES.xlsx•Grader Name: Han, Namgyu(Homework, Quiz, Exam grading)•Email:Namgyu.Han@ttu.edu
Symmetric Block Encryption
Block cipher•the most commonly used symmetric encryption algorithms•input: fixed-size blocks (Typically 64, 128 bit blocks), output: equal size blocks•provide secrecy and/or authentication services•Data Encryption Standard (DES), triple DES (3DES), and the Advanced Encryption Standard (AES)s•Usually employ Feistel structure
Feistel Cipher Structure
Feistel Cipher Structure•most symmetric block ciphers are based on a Feistel Cipher Structure•based on the two primitive cryptographic operations•substitution(S-box)•permutation (P-box)•provide confusionand diffusionof message
Feistel Cipher Structure•Horst Feistel devised the feistelcipher in the 1973•based on concept of invertible product cipher•partitions input block into two halves•process through multiple rounds which•perform a substitution on left data half•based on round function of right half & subkey•then have permutation swapping halves•implements Shannon’s substitution-permutation network concept
Feistel Encryption and Decryption
Computer Networking A Top-Down Approach Seventh Edition James F. Kurose University of Massachusetts, AmherstKeith W. RossNYU and NYU Shanghai Boston  Columbus  Indianapolis  New York San Francisco  Hoboken Amsterdam  Cape Town Dubai  London  Madrid Milan Munich Paris Montréal  Toronto Delhi  Mexico City São Paulo Sydney Hong Kong Seoul  Singapore  Taipei Tokyo Vice President, Editorial Director, ECS:  Marcia Horton Acquisitions Editor:  Matt Goldstein Editorial Assistant: Kristy Alaura Vice President of Marketing: Christy LeskoDirector of Field Marketing:  Tim Galligan Product Marketing Manager: Bram Van KempenField Marketing Manager: Demetrius HallMarketing Assistant: Jon BryantDirector of Product Management: Erin GreggTeam Lead, Program and Project Management: Scott DisannoProgram Manager: Joanne Manning and Carole SnyderProject Manager: Katrina Ostler, Ostler Editorial, Inc.Senior Specialist, Program Planning and Support:  Maura Zaldivar-Garcia
Cover Designer:  Joyce Wells Manager, Rights and Permissions: Ben Ferrini Project Manager, Rights and Permissions:  Jenny Hoffman, Aptara Corporation Inventory Manager: Ann LamCover Image: Marc Gutierrez/Getty ImagesMedia Project Manager: Steve WrightComposition:  Cenveo Publishing Services Printer/Binder: Edwards Brothers MalloyCover and Insert Printer: Phoenix Color/ HagerstownCredits and acknowledgments borrowed from other sources and reproduced, with ­permission, in this textbook appear on appropriate page within text.
Copyright © 2017, 2013, 2010 Pearson Education, Inc.  All rights reserved.
Manufactured in the United States of America.
This publication is protected by Copyright, and permission should be obtained from the publisher prior to any prohibited reproduction, storage in a retrieval system, or transmission in any form or by any means, electronic, mechanical, photocopying, recording, or likewise.
For information regarding permissions, request forms and the appropriate contacts within the Pearson Education Global Rights & Permissions Department, please visit www.pearsoned.com/ permissions/. Many of the designations by manufacturers and seller to distinguish their products are claimed as trademarks.
Where those designations appear in this book, and the publisher was aware of a trademark claim, the designations have been printed in initial caps or all caps.
Library of Congress Cataloging-in-Publication DataNames: Kurose, James F. | Ross, Keith W., 1956-Title: Computer networking: a top-down approach / James F. Kurose, University of Massachusetts, Amherst, Keith W. Ross, NYU and NYU Shanghai.
Description: Seventh edition.
| Hoboken, New Jersey: Pearson, [2017] | Includes bibliographical references and index.
Identifiers: LCCN 2016004976 | ISBN 9780133594140 | ISBN 0133594149Subjects: LCSH: Internet.
| Computer networks.
Classification: LCC TK5105.875.I57 K88 2017 | DDC 004.6-dc23
LC record available at http:/ /lccn.loc.gov/ 2016004976 ISBN-10:     0-13-359414-9 ISBN-13: 978-0-13-359414-0 About the Authors Jim Kurose Jim Kurose is a Distinguished University Professor of Computer Science at the University of Massachusetts, Amherst.
He is currently on leave from the University of Massachusetts, serving as an Assistant Director at the US National Science Foundation, where he leads the Directorate of Computer and Information Science and Engineering.
Dr. Kurose has received a number of recognitions for his educational activities including Outstanding Teacher Awards from the National Technological University (eight times), the University of Massachusetts, and the Northeast Association of Graduate Schools.
He received the IEEE Taylor Booth Education Medal and was recognized for his leadership of Massachusetts’ Commonwealth Information Technology Initiative.
He has won several conference best paper awards and received the IEEE Infocom Achievement Award and the ACM Sigcomm Test of Time Award.
Dr. Kurose is a former Editor-in-Chief of IEEE Transactions on Communications  and of IEEE/ACM Transactions on Networking .
He has served as Technical Program co-Chair for IEEE Infocom, ACM SIGCOMM, ACM Internet Measurement Conference , and ACM SIGMETRICS.
He is a Fellow of the IEEE and the ACM.
His research ­interests include network protocols and architecture, network measurement, multimedia communication, and modeling and performance ­evaluation.
He holds a PhD in Computer Science from Columbia University.
Keith Ross
Keith Ross is the Dean of Engineering and Computer Science at NYU Shanghai and the Leonard J. Shustek Chair Professor in the Computer Science and Engineering Department at NYU.
Previously he was at University of Pennsylvania (13 years), Eurecom Institute (5 years) and Polytechnic University (10 years).
He received a B.S.E.E from Tufts University, a M.S.E.E. from Columbia University, and a Ph.D. in Computer and Control Engineering from The University of Michigan.
Keith Ross is also the co-founder and original CEO of Wimba, which develops online multimedia applications for e-learning and was acquired by Blackboard in 2010.
Professor Ross’s research interests are in privacy, social networks, peer-to-peer networking, Internet measurement, content distribution networks, and stochastic modeling.
He is an ACM Fellow, an IEEE Fellow, recipient of the Infocom 2009 Best Paper Award, and recipient of 2011 and 2008 Best Paper Awards for Multimedia Communications (awarded by IEEE Communications Society).
He has served on numerous journal editorial boards and conference program committees, including IEEE/ACM Transactions on Networking, ACM SIGCOMM, ACM CoNext, and ACM Internet Measurement Conference .
He also has served as an advisor to the Federal Trade Commission on P2P file sharing.
To Julie and our three precious ones—Chris, Charlie, and Nina JFK A big THANKS to my professors, colleagues, and students all over the world.
KWR Preface Welcome to the seventh edition of Computer Networking: A Top-Down Approach.
 Since the publication of the first edition 16 years ago, our book has been adopted for use at many hundreds of colleges and universities, translated into 14 languages, and used by over one hundred thousand students and practitioners worldwide.
We’ve heard from many of these readers and have been overwhelmed by the ­positive ­response.
What’s New in the Seventh Edition?
We think one important reason for this success has been that our book continues to offer a fresh and timely approach to computer networking instruction.
We’ve made changes in this seventh edition, but we’ve also kept unchanged what we believe (and the instructors and students who have used our book have confirmed) to be the most important aspects of this book: its top-down approach, its focus on the Internet and a modern treatment of computer networking, its attention to both principles and practice, and its accessible style and approach toward learning about computer networking.
Nevertheless, the seventh edition has been revised and updated substantially.
Long-time readers of our book will notice that for the first time since this text was published, we’ve changed the organization of the chapters themselves.
The network layer, which had been previously covered in a single chapter, is now covered in Chapter 4 (which focuses on the so-called “data plane” component of the network layer) and Chapter 5 (which focuses on the network layer’s “control plane”).
This expanded coverage of the network layer reflects the swift rise in importance of software-defined networking (SDN), arguably the most important and exciting advance in networking in decades.
Although a relatively recent innovation, SDN has been rapidly adopted in practice—so much so that it’s already hard to imagine an introduction to modern computer networking that doesn’t cover SDN.
The topic of network management, previously covered in Chapter 9, has now been folded into the new Chapter 5.
As always, we’ve also updated many other sections of the text to reflect recent changes in the dynamic field of networking since the sixth edition.
As always, material that has been retired from the printed text can always be found on this book’s Companion Website.
The most important updates are the following: Chapter 1 has been updated to reflect the ever-growing reach and use of the ­Internet.
Chapter 2, which covers the application layer, has been significantly updated.
We’ve removed the material on the FTP protocol and distributed hash tables to make room for a new section on application-level video streaming and ­content distribution networks, together with Netflix and YouTube case studies.
The socket programming sections have been updated from Python 2 to Python 3.
Chapter 3, which covers the transport layer, has been modestly updated.
The ­material on asynchronous transport mode (ATM) networks has been replaced by more modern material on the Internet’s explicit congestion notification (ECN), which teaches the same principles.
Chapter 4 covers the “data plane” component of the network layer—the per-router  forwarding function that determine how a packet arriving on one of a router’s input links is forwarded to one of that router’s output links.
We updated the material on traditional Internet forwarding found in all previous editions, and added material on packet scheduling.
We’ve also added a new section on generalized forwarding, as practiced in SDN.
There are also numerous updates throughout the chapter.
Material on multicast and broadcast communication has been removed to make way for the new material.
In Chapter 5, we cover the control plane functions of the network layer—the ­network-wide  logic that controls how a datagram is routed along an end-to-end path of routers from the source host to the destination host.
As in previous ­editions, we cover routing algorithms, as well as routing protocols (with an updated treatment of BGP) used in today’s Internet.
We’ve added a significant new section on the SDN control plane, where routing and other functions are implemented in so-called SDN controllers.
Chapter 6, which now covers the link layer, has an updated treatment of Ethernet, and of data center networking.
Chapter 7, which covers wireless and mobile networking, contains updated ­material on 802.11 (so-called “WiFi) networks and cellular networks, including 4G and LTE.
Chapter 8, which covers network security and was extensively updated in the sixth edition, has only
modest updates in this seventh edition.
Chapter 9, on multimedia networking, is now slightly “thinner” than in the sixth edition, as material on video streaming and content distribution networks has been moved to Chapter 2, and material on packet scheduling has been incorporated into Chapter 4.
Significant new material involving end-of-chapter problems has been added.
As with all previous editions, homework problems have been revised, added, and removed.
As always, our aim in creating this new edition of our book is to continue to provide a focused and modern treatment of computer networking, emphasizing both principles and practice.
Audience This textbook is for a first course on computer networking.
It can be used in both computer science and electrical engineering departments.
In terms of programming languages, the book assumes only that the student has experience with C, C++, Java, or Python (and even then only in a few places).
Although this book is more precise and analytical than many other introductory computer networking texts, it rarely uses any mathematical concepts that are not taught in high school.
We have made a deliberate effort to avoid using any advanced calculus, probability, or stochastic process concepts (although we’ve included some homework problems for students with this advanced background).
The book is therefore appropriate for undergraduate courses and for first-year graduate courses.
It should also be useful to practitioners in the telecommunications industry.
What Is Unique About This Textbook?
The subject of computer networking is enormously complex, involving many concepts, protocols, and technologies that are woven together in an intricate manner.
To cope with this scope and complexity, many computer networking texts are often organized around the “layers” of a network architecture.
With a layered organization, students can see through the complexity of computer networking—they learn about the distinct concepts and protocols in one part of the architecture while seeing the big picture of how all parts fit together.
From a pedagogical perspective, our personal experience has been that such a layered approach indeed works well.
Nevertheless, we have found that the traditional approach of teaching—bottom up; that is, from the physical layer towards the application layer—is not the best approach for a modern course on computer networking.
A Top-Down Approach Our book broke new ground 16 years ago by treating networking in a top-down ­manner—that is, by beginning at the application layer and working its way down toward the physical layer.
The feedback we received from teachers and students alike have confirmed that this top-down approach has many advantages and does indeed work well pedagogically.
First, it places emphasis on the application layer (a “high growth area” in networking).
Indeed, many of the recent revolutions in ­computer networking—including the Web, peer-to-peer file sharing, and media streaming—have taken place at the application layer.
An early emphasis on application-layer issues differs from the approaches taken in most other texts, which have only a small amount of material on network applications, their requirements, application-layer paradigms (e.g., client-server and peer-to-peer), and application programming ­interfaces.
­Second, our experience as instructors (and that of many instructors who have used this text) has been that teaching networking applications near the beginning of the course is a powerful motivational tool.
Students are thrilled to learn about how networking
applications work—applications such as e-mail and the Web, which most students use on a daily basis.
Once a student understands the applications, the student can then understand the network services needed to support these applications.
The student can then, in turn, examine the various ways in which such services might be provided and implemented in the lower layers.
Covering applications early thus provides motivation for the remainder of the text.
Third, a top-down approach enables instructors to introduce network application development at an early stage.
Students not only see how popular applications and protocols work, but also learn how easy it is to create their own network ­applications and application-level protocols.
With the top-down approach, students get early ­exposure to the notions of socket programming, service models, and ­protocols—important concepts that resurface in all subsequent layers.
By providing socket programming examples in Python, we highlight the central ideas without confusing students with complex code.
Undergraduates in electrical engineering and computer science should not have difficulty following the Python code.
An Internet Focus Although we dropped the phrase “Featuring the Internet” from the title of this book with the fourth edition, this doesn’t mean that we dropped our focus on the Internet.
Indeed, nothing could be further from the case!
Instead, since the Internet has become so pervasive, we felt that any networking textbook must have a significant focus on the Internet, and thus this phrase was somewhat unnecessary.
We continue to use the Internet’s architecture and protocols as primary vehicles for studying fundamental computer networking concepts.
Of course, we also include concepts and protocols from other network architectures.
But the spotlight is clearly on the Internet, a fact reflected in our organizing the book around the Internet’s five-layer architecture: the application, transport, network, link, and physical layers.
Another benefit of spotlighting the Internet is that most computer science and electrical engineering students are eager to learn about the Internet and its protocols.
They know that the Internet has been a revolutionary and disruptive technology and can see that it is profoundly changing our world.
Given the enormous relevance of the Internet, students are naturally curious about what is “under the hood.”
Thus, it is easy for an instructor to get students excited about basic principles when using the Internet as the guiding focus.
Teaching Networking Principles Two of the unique features of the book—its top-down approach and its focus on the Internet—have appeared in the titles of our book.
If we could have squeezed a third phrase into the subtitle, it would have contained the word principles.
 The field of networking is now mature enough that a number of fundamentally important issues can be identified.
For example, in the transport layer, the fundamental issues include reliable communication over an unreliable network layer, connection establishment/ teardown and handshaking, congestion and flow control, and multiplexing.
Three fundamentally important network-layer issues are determining “good” paths between two routers, interconnecting a large number of heterogeneous networks, and managing the complexity of a modern network.
In the link layer, a fundamental problem is sharing a multiple access channel.
In network security, techniques for providing confidentiality, authentication, and message integrity are all based on cryptographic fundamentals.
This text identifies fundamental networking issues and studies approaches towards addressing these issues.
The student learning these principles will gain knowledge with a long “shelf life”—long after today’s network standards and protocols have become obsolete, the principles they embody will remain important and relevant.
We believe that the combination of using the Internet to get the student’s foot in the door and then emphasizing fundamental issues and solution approaches will allow the student to
quickly understand just about any networking technology.
The Website Each new copy of this textbook includes twelve months of access to a Companion ­Website for all book readers at http:/ /www.pearsonhighered.com/ cs-resources/ , which includes: Interactive learning material.
The book’s Companion Website contains ­VideoNotes—video presentations of important topics throughout the book done by the authors, as well as walkthroughs of solutions to problems similar to those at the end of the chapter.
We’ve seeded the Web site with VideoNotes and ­online problems for Chapters 1 through 5 and will continue to actively add and update this material over time.
As in earlier editions, the Web site contains the interactive Java applets that animate many key networking concepts.
The site also has interactive quizzes that permit students to check their basic understanding of the subject matter.
Professors can integrate these interactive features into their lectures or use them as mini labs.
Additional technical material.
 As we have added new material in each edition of our book, we’ve had to remove coverage of some existing topics to keep the book at manageable length.
For example, to make room for the new ­material in this ­edition, we’ve removed material on FTP, distributed hash tables, and multicasting, Material that appeared in earlier editions of the text is still of ­interest, and thus can be found on the book’s Web site.
Programming assignments.
The Web site also provides a number of detailed programming assignments, which include building a multithreaded Web ­server, building an e-mail client with a GUI interface, programming the sender and ­receiver sides of a reliable data transport protocol, programming a distributed routing algorithm, and more.
Wireshark labs.
One’s understanding of network protocols can be greatly ­deepened by seeing them in action.
The Web site provides numerous Wireshark assignments that enable students to actually observe the sequence of messages exchanged between two protocol entities.
The Web site includes separate Wireshark labs on HTTP, DNS, TCP, UDP, IP, ICMP, Ethernet, ARP, WiFi, SSL, and on tracing all protocols involved in satisfying a request to fetch a Web page.
We’ll continue to add new labs over time.
In addition to the Companion Website, the authors maintain a public Web site,http://gaia.cs.umass.edu/kurose_ross /interactive, containing interactive exercises that create (and present solutions for) problems similar to selected end-of-chapter problems.
Since students can generate (and view solutions for) an unlimited number of similar problem instances, they can work until the material is truly mastered.
Pedagogical Features We have each been teaching computer networking for more than 30 years.
Together, we bring more than 60 years of teaching experience to this text, during which time we have taught many thousands of students.
We have also been active researchers in computer networking during this time. (
In fact, Jim and Keith first met each other as master’s students in a computer networking course taught by Mischa Schwartz in 1979 at Columbia University.)
We think all this gives us a good perspective on where networking has been and where it is likely to go in the future.
Nevertheless, we have resisted temptations to bias the material in this book towards our own pet research projects.
We figure you can visit our personal Web sites if you are interested in our research.
Thus, this book is about modern computer networking—it is about contemporary protocols and technologies as well as the underlying principles behind these protocols and technologies.
We also believe
that learning (and teaching!)
about networking can be fun.
A sense of humor, use of analogies, and real-world examples in this book will hopefully make this material more fun.
Supplements for Instructors We provide a complete supplements package to aid instructors in teaching this course.
This material can be accessed from Pearson’s Instructor Resource Center ( http:/ /www.pearsonhighered.com/ irc).
Visit the Instructor Resource Center for ­information about accessing these instructor’s supplements.
PowerPoint  slides.
We provide PowerPoint slides for all nine chapters.
The slides have been completely updated with this seventh edition.
The slides cover each chapter in detail.
They use graphics and animations (rather than relying only on monotonous text bullets) to make the slides interesting and visually appealing.
We provide the original PowerPoint slides so you can customize them to best suit your own teaching needs.
Some of these slides have been contributed by other instructors who have taught from our book.
Homework solutions.
 We provide a solutions manual for the homework problems in the text, programming assignments, and Wireshark labs.
As noted ­earlier, we’ve introduced many new homework problems in the first six chapters of the book.
Chapter DependenciesThe first chapter of this text presents a self-contained overview of computer networking.
Introducing many key concepts and terminology, this chapter sets the stage for the rest of the book.
All of the other chapters directly depend on this first chapter.
After completing Chapter 1, we recommend instructors cover Chapters 2 through 6 in sequence, following our top-down philosophy.
Each of these five chapters leverages material from the preceding chapters.
After completing the first six chapters, the instructor has quite a bit of flexibility.
There are no interdependencies among the last three chapters, so they can be taught in any order.
However, each of the last three chapters depends on the material in the first six chapters.
Many instructors first teach the first six chapters and then teach one of the last three chapters for “dessert.”
One Final Note: We’d Love to Hear from YouWe encourage students and instructors to e-mail us with any comments they might have about our book.
It’s been wonderful for us to hear from so many instructors and students from around the world about our first five editions.
We’ve incorporated many of these suggestions into later editions of the book.
We also encourage instructors to send us new homework problems (and solutions) that would complement the current homework problems.
We’ll post these on the instructor-only portion of the Web site.
We also encourage instructors and students to create new Java applets that illustrate the concepts and protocols in this book.
If you have an applet that you think would be appropriate for this text, please submit it to us.
If the applet (including notation and terminology) is appropriate, we’ll be happy to include it on the text’s Web site, with an appropriate reference to the applet’s authors.
So, as the saying goes, “Keep those cards and letters coming!”
Seriously, please do continue to send us interesting URLs, point out typos, disagree with any of our claims, and tell us what works and what doesn’t work.
Tell us what you think should or shouldn’t be included in the next edition.
Send your e-mail to kurose@cs.umass.edu  and keithwross@nyu.edu.
®
Acknowledgments Since we began writing this book in 1996, many people have given us invaluable help and have been influential in shaping our thoughts on how to best organize and teach a networking course.
We want to say A BIG THANKS to everyone who has helped us from the earliest first drafts of this book, up to this seventh edition.
We are also very thankful to the many hundreds of readers from around the world—students, faculty, practitioners—who have sent us thoughts and comments on earlier editions of the book and suggestions for future editions of the book.
Special thanks go out to: Al Aho (Columbia University) Hisham Al-Mubaid (University of Houston-Clear Lake)Pratima Akkunoor (Arizona State University)Paul Amer (University of Delaware)Shamiul Azom (Arizona State University)Lichun Bao (University of California at Irvine)Paul Barford (University of Wisconsin)Bobby Bhattacharjee (University of Maryland)Steven Bellovin (Columbia University)Pravin Bhagwat (Wibhu)Supratik Bhattacharyya (previously at Sprint)Ernst Biersack (Eurécom Institute)Shahid Bokhari (University of Engineering & Technology, Lahore)Jean Bolot (Technicolor Research)Daniel Brushteyn (former University of Pennsylvania student)Ken Calvert (University of Kentucky)Evandro Cantu (Federal University of Santa Catarina)Jeff Case (SNMP Research International)Jeff Chaltas (Sprint)Vinton Cerf (Google)Byung Kyu Choi (Michigan Technological University)Bram Cohen (BitTorrent, Inc.)Constantine Coutras (Pace University)John Daigle (University of Mississippi) Edmundo A. de Souza e Silva (Federal University of Rio de Janeiro)
Philippe Decuetos (Eurécom Institute) Christophe Diot (Technicolor Research)Prithula Dhunghel (Akamai)Deborah Estrin (University of California, Los Angeles)Michalis Faloutsos (University of California at Riverside)Wu-chi Feng (Oregon Graduate Institute)Sally Floyd (ICIR, University of California at Berkeley)Paul Francis (Max Planck Institute)David Fullager (Netflix)Lixin Gao (University of Massachusetts)JJ Garcia-Luna-Aceves (University of California at Santa Cruz)Mario Gerla (University of California at Los Angeles)David Goodman (NYU-Poly)Yang Guo (Alcatel/Lucent Bell Labs)Tim Griffin (Cambridge University)Max Hailperin (Gustavus Adolphus College)Bruce Harvey (Florida A&M University, Florida State University)Carl Hauser (Washington State University)Rachelle Heller (George Washington University)Phillipp Hoschka (INRIA/W3C)Wen Hsin (Park University)Albert Huang (former University of Pennsylvania student)Cheng Huang (Microsoft Research)Esther A. Hughes (Virginia Commonwealth University)Van Jacobson (Xerox PARC)Pinak Jain (former NYU-Poly student)Jobin James (University of California at Riverside)Sugih Jamin (University of Michigan)Shivkumar Kalyanaraman (IBM Research, India)Jussi Kangasharju (University of Helsinki)Sneha Kasera (University of Utah)
Parviz Kermani (formerly of IBM Research) Hyojin Kim (former University of Pennsylvania student) Leonard Kleinrock (University of California at Los Angeles)David Kotz (Dartmouth College)Beshan Kulapala (Arizona State University)Rakesh Kumar (Bloomberg)Miguel A. Labrador (University of South Florida)Simon Lam (University of Texas)Steve Lai (Ohio State University)Tom LaPorta (Penn State University)Tim-Berners Lee (World Wide Web Consortium)Arnaud Legout (INRIA)Lee Leitner (Drexel University)Brian Levine (University of Massachusetts)Chunchun Li (former NYU-Poly student)Yong Liu (NYU-Poly)William Liang (former University of Pennsylvania student)Willis Marti (Texas A&M University)Nick McKeown (Stanford University)Josh McKinzie (Park University)Deep Medhi (University of Missouri, Kansas City)Bob Metcalfe (International Data Group)Sue Moon (KAIST)Jenni Moyer (Comcast)Erich Nahum (IBM Research)Christos Papadopoulos (Colorado Sate University)Craig Partridge (BBN Technologies)Radia Perlman (Intel)Jitendra Padhye (Microsoft Research)Vern Paxson (University of California at Berkeley)Kevin Phillips (Sprint)
George Polyzos (Athens University of Economics and Business) Sriram Rajagopalan (Arizona State University) Ramachandran Ramjee (Microsoft Research)Ken Reek (Rochester Institute of Technology)Martin Reisslein (Arizona State University)Jennifer Rexford (Princeton University)Leon Reznik (Rochester Institute of Technology)Pablo Rodrigez (Telefonica)Sumit Roy (University of Washington)Dan Rubenstein (Columbia University)Avi Rubin (Johns Hopkins University)Douglas Salane (John Jay College)Despina Saparilla (Cisco Systems)John Schanz (Comcast)Henning Schulzrinne (Columbia University)Mischa Schwartz (Columbia University)Ardash Sethi (University of Delaware)Harish Sethu (Drexel University)K. Sam Shanmugan (University of Kansas)Prashant Shenoy (University of Massachusetts)Clay Shields (Georgetown University)Subin Shrestra (University of Pennsylvania)Bojie Shu (former NYU-Poly student)Mihail L. Sichitiu (NC State University)Peter Steenkiste (Carnegie Mellon University)Tatsuya Suda (University of California at Irvine)Kin Sun Tam (State University of New York at Albany)Don Towsley (University of Massachusetts)David Turner (California State University, San Bernardino)Nitin Vaidya (University of Illinois)Michele Weigle (Clemson University)
David Wetherall (University of Washington) Ira Winston (University of Pennsylvania)Di Wu (Sun Yat-sen University)Shirley Wynn (NYU-Poly)Raj Yavatkar (Intel)Yechiam Yemini (Columbia University)Dian Yu (NYU Shanghai)Ming Yu (State University of New York at Binghamton)Ellen Zegura (Georgia Institute of Technology)Honggang Zhang (Suffolk University)Hui Zhang (Carnegie Mellon University)Lixia Zhang (University of California at Los Angeles)Meng Zhang (former NYU-Poly student)Shuchun Zhang (former University of Pennsylvania student)Xiaodong Zhang (Ohio State University)ZhiLi Zhang (University of Minnesota)Phil Zimmermann (independent consultant)Mike Zink (University of Massachusetts)Cliff C. Zou (University of Central Florida) We  also want to thank the entire Pearson team—in particular, Matt Goldstein and Joanne Manning—who have done an absolutely outstanding job  on this seventh ­edition (and who have put up with two very finicky authors who seem congenitally ­unable to meet deadlines!).
Thanks also to our artists, Janet Theurer and Patrice Rossi Calkin, for their work on the beautiful figures in this and earlier editions of our book, and to Katie Ostler and her team at Cenveo for their wonderful production work on this edition.
Finally, a most special thanks go to our previous two editors at ­Addison-Wesley—Michael Hirsch and Susan Hartman.
This book would not be what it is (and may well not have been at all) without  their graceful management, constant encouragement, nearly infinite patience, good humor, and perseverance.
Table of Contents Chapter 1 Computer Networks and the Internet 1 1.1 What Is the Internet?
 2 1.1.1 A Nuts-and-Bolts Description 2 1.1.2 A Services Description 51.1.3 What Is a Protocol?
 7 1.2 The Network Edge  9 1.2.1 Access Networks  12 1.2.2 Physical Media 18 1.3 The Network Core 21 1.3.1 Packet Switching  23 1.3.2 Circuit Switching  27 1.3.3 A Network of Networks 31 1.4 Delay, Loss, and Throughput in Packet-Switched Networks  35 1.4.1 Overview of Delay in Packet-Switched Networks 351.4.2 Queuing Delay and Packet Loss  39 1.4.3 End-to-End Delay  41 1.4.4 Throughput in Computer Networks  43 1.5 Protocol Layers and Their Service Models  47 1.5.1 Layered Architecture  47 1.5.2 Encapsulation  53 1.6 Networks Under Attack  55 1.7 History of Computer Networking and the Internet  59 1.7.1 The Development of Packet Switching: 1961–1972 591.7.2 Proprietary Networks and Internetworking: 1972–1980  60 1.7.3 A Proliferation of Networks: 1980–1990  62 1.7.4 The Internet Explosion: The 1990s  63 1.7.5 The New Millennium  64 1.8 Summary 65
Homework Problems and Questions  67 Wireshark Lab 77 Interview: Leonard Kleinrock 79 Chapter 2 Application Layer  83 2.1 Principles of Network Applications  84 2.1.1 Network Application Architectures 862.1.2 Processes Communicating 882.1.3 Transport Services Available to Applications  90 2.1.4 Transport Services Provided by the Internet  93 2.1.5 Application-Layer Protocols  96 2.1.6 Network Applications Covered in This Book  97 2.2 The Web and HTTP  98 2.2.1 Overview of HTTP  98 2.2.2 Non-Persistent and Persistent Connections 1002.2.3 HTTP Message Format 1032.2.4 User-Server Interaction: Cookies  108 2.2.5 Web Caching  110 2.3 Electronic Mail in the Internet  116 2.3.1 SMTP 1182.3.2 Comparison with HTTP  121 2.3.3 Mail Message Formats 1212.3.4 Mail Access Protocols 122 2.4 DNS—The Internet’s Directory Service  126 2.4.1 Services Provided by DNS 1272.4.2 Overview of How DNS Works 1292.4.3 DNS Records and Messages  135 2.5 Peer-to-Peer Applications 140 2.5.1 P2P File Distribution  140 2.6 Video Streaming and Content Distribution Networks  147 2.6.1 Internet Video  148 2.6.2 HTTP Streaming and DASH 148
2.6.3 Content Distribution Networks  149 2.6.4 Case Studies: Netflix, YouTube, and Kankan  153 2.7 Socket Programming: Creating Network Applications  157 2.7.1 Socket Programming with UDP  159 2.7.2 Socket Programming with TCP  164 2.8 Summary 170 Homework Problems and Questions  171 Socket Programming Assignments 180 Wireshark Labs: HTTP, DNS 182Interview: Marc Andreessen  184 Chapter 3 Transport Layer 187 3.1 Introduction and Transport-Layer Services  188 3.1.1 Relationship Between Transport and Network Layers  188 3.1.2 Overview of the Transport Layer in the Internet  191 3.2 Multiplexing and Demultiplexing  193 3.3 Connectionless Transport: UDP 200 3.3.1 UDP Segment Structure 2043.3.2 UDP Checksum  204 3.4 Principles of Reliable Data Transfer 206 3.4.1 Building a Reliable Data Transfer Protocol  208 3.4.2 Pipelined Reliable Data Transfer Protocols  217 3.4.3 Go-Back-N (GBN) 2213.4.4 Selective Repeat (SR)  226 3.5 Connection-Oriented Transport: TCP  233 3.5.1 The TCP Connection  233 3.5.2 TCP Segment Structure 2363.5.3 Round-Trip Time Estimation and Timeout  241 3.5.4 Reliable Data Transfer 2443.5.5 Flow Control  252 3.5.6 TCP Connection Management  255 3.6 Principles of Congestion Control  261
3.6.1 The Causes and the Costs of Congestion  261 3.6.2 Approaches to Congestion Control  268 3.7 TCP Congestion Control  269 3.7.1 Fairness 279 3.7.2 Explicit Congestion Notification (ECN): Network-assisted Congestion Control  282 3.8 Summary 284 Homework Problems and Questions  286 Programming Assignments 301 Wireshark Labs: Exploring TCP, UDP  302 Interview: Van Jacobson 303 Chapter 4 The Network Layer: Data Plane 305 4.1 Overview of Network Layer 306 4.1.1 Forwarding and Routing: The Network Data and Control Planes  306 4.1.2 Network Service Models 311 4.2 What’s Inside a Router?
 313 4.2.1 Input Port Processing and Destination-Based Forwarding  316 4.2.2 Switching  319 4.2.3 Output Port Processing  321 4.2.4 Where Does Queuing Occur?
 321 4.2.5 Packet Scheduling  325 4.3 The Internet Protocol (IP): IPv4, Addressing, IPv6, and More  329 4.3.1 IPv4 Datagram Format  330 4.3.2 IPv4 Datagram Fragmentation 3324.3.3 IPv4 Addressing 3344.3.4 Network Address Translation (NAT) 3454.3.5 IPv6 348 4.4 Generalized Forwarding and SDN  354 4.4.1 Match 3564.4.2 Action 3584.4.3 OpenFlow Examples of Match-plus-action in Action  358 4.5 Summary 361
Homework Problems and Questions  361 Wireshark Lab 370 Interview: Vinton G. Cerf  371 Chapter 5 The Network Layer: Control Plane  373 5.1 Introduction  374 5.2 Routing Algorithms  376 5.2.1 The Link-State (LS) Routing Algorithm  379 5.2.2 The Distance-Vector (DV) Routing Algorithm  384 5.3 Intra-AS Routing in the Internet: OSPF  391 5.4 Routing Among the ISPs: BGP  395 5.4.1 The Role of BGP  395 5.4.2 Advertising BGP Route Information  396 5.4.3 Determining the Best Routes 3985.4.4 IP-Anycast 4025.4.5 Routing Policy  403 5.4.6 Putting the Pieces Together: Obtaining Internet Presence  406 5.5 The SDN Control Plane  407 5.5.1 The SDN Control Plane: SDN Controller and SDN Control Applications  410 5.5.2 OpenFlow Protocol  412 5.5.3 Data and Control Plane Interaction: An Example  414 5.5.4 SDN: Past and Future 415 5.6 ICMP: The Internet Control Message Protocol  419 5.7 Network Management and SNMP 421 5.7.1 The Network Management Framework 4225.7.2 The Simple Network Management Protocol (SNMP)  424 5.8 Summary 426 Homework Problems and Questions  427 Socket Programming Assignment 433 Programming Assignment 434Wireshark Lab 435 Interview: Jennifer Rexford 436
Chapter 6 The Link Layer and LANs  439 6.1 Introduction to the Link Layer  440 6.1.1 The Services Provided by the Link Layer  442 6.1.2 Where Is the Link Layer Implemented?
 443 6.2 Error-Detection and -Correction Techniques  444 6.2.1 Parity Checks  446 6.2.2 Checksumming Methods 448 6.2.3 Cyclic Redundancy Check (CRC)  449 6.3 Multiple Access Links and Protocols  451 6.3.1 Channel Partitioning Protocols  453 6.3.2 Random Access Protocols 4556.3.3 Taking-Turns Protocols  464 6.3.4 DOCSIS: The Link-Layer Protocol for Cable Internet Access  465 6.4 Switched Local Area Networks 467 6.4.1 Link-Layer Addressing and ARP 4686.4.2 Ethernet 4746.4.3 Link-Layer Switches 4816.4.4 Virtual Local Area Networks (VLANs) 487 6.5 Link Virtualization: A Network as a Link Layer  491 6.5.1 Multiprotocol Label Switching (MPLS)  492 6.6 Data Center Networking 4956.7 Retrospective: A Day in the Life of a Web Page Request  500 6.7.1 Getting Started: DHCP, UDP, IP, and Ethernet  500 6.7.2 Still Getting Started: DNS and ARP  502 6.7.3 Still Getting Started: Intra-Domain Routing to the DNS Server  503 6.7.4 Web Client-Server Interaction: TCP and HTTP  504 6.8 Summary 506 Homework Problems and Questions  507 Wireshark Lab 515 Interview: Simon S. Lam  516
Chapter 7 Wireless and Mobile Networks  519 7.1 Introduction  520 7.2 Wireless Links and Network Characteristics 525 7.2.1 CDMA 528 7.3 WiFi: 802.11 Wireless LANs 532 7.3.1 The 802.11 Architecture  533 7.3.2 The 802.11 MAC Protocol 537 7.3.3 The IEEE 802.11 Frame 5427.3.4 Mobility in the Same IP Subnet  546 7.3.5 Advanced Features in 802.11 5477.3.6 Personal Area Networks: Bluetooth and Zigbee  548 7.4 Cellular Internet Access 551 7.4.1 An Overview of Cellular Network Architecture 5517.4.2 3G Cellular Data Networks: Extending the Internet to Cellular Subscribers  554 7.4.3 On to 4G: LTE  557 7.5 Mobility Management: Principles  560 7.5.1 Addressing 5627.5.2 Routing to a Mobile Node  564 7.6 Mobile IP  570 7.7 Managing Mobility in Cellular Networks  574 7.7.1 Routing Calls to a Mobile User  576 7.7.2 Handoffs in GSM  577 7.8 Wireless and Mobility: Impact on Higher-Layer Protocols  580 7.9 Summary 582 Homework Problems and Questions  583 Wireshark Lab 588 Interview: Deborah Estrin 589 Chapter 8 Security in Computer Networks 593 8.1 What Is Network Security?
5948.2 Principles of Cryptography  596 8.2.1 Symmetric Key Cryptography  598 8.2.2 Public Key Encryption  604
8.3 Message Integrity and Digital Signatures  610 8.3.1 Cryptographic Hash Functions  611 8.3.2 Message Authentication Code 613 8.3.3 Digital Signatures  614 8.4 End-Point Authentication  621 8.4.1 Authentication Protocol ap1.0 622 8.4.2 Authentication Protocol ap2.0 622 8.4.3 Authentication Protocol ap3.0 623 8.4.4 Authentication Protocol ap3.1 623 8.4.5 Authentication Protocol ap4.0 624 8.5 Securing E-Mail  626 8.5.1 Secure E-Mail 6278.5.2 PGP 630 8.6 Securing TCP Connections: SSL  631 8.6.1 The Big Picture  632 8.6.2 A More Complete Picture 635 8.7 Network-Layer Security: IPsec and Virtual Private Networks  637 8.7.1 IPsec and Virtual Private Networks (VPNs) 6388.7.2 The AH and ESP Protocols  640 8.7.3 Security Associations 6408.7.4 The IPsec Datagram 6418.7.5 IKE: Key Management in IPsec 645 8.8 Securing Wireless LANs 646 8.8.1 Wired Equivalent Privacy (WEP)  646 8.8.2 IEEE 802.11i 648 8.9 Operational Security: Firewalls and Intrusion Detection Systems  651 8.9.1 Firewalls 6518.9.2 Intrusion Detection Systems 659 8.10 Summary 662 Homework Problems and Questions  664 Wireshark Lab 672
IPsec Lab 672 Interview: Steven M. Bellovin  673 Chapter 9 Multimedia Networking  675 9.1 Multimedia Networking Applications  676 9.1.1 Properties of Video  676 9.1.2 Properties of Audio  677 9.1.3 Types of Multimedia Network Applications  679 9.2 Streaming Stored Video  681 9.2.1 UDP Streaming 6839.2.2 HTTP Streaming 684 9.3 Voice-over-IP 688 9.3.1 Limitations of the Best-Effort IP Service  688 9.3.2 Removing Jitter at the Receiver for Audio 6919.3.3 Recovering from Packet Loss 6949.3.4 Case Study: VoIP with Skype  697 9.4 Protocols for Real-Time Conversational Applications  700 9.4.1 RTP 7009.4.2 SIP 703 9.5 Network Support for Multimedia  709 9.5.1 Dimensioning Best-Effort Networks 7119.5.2 Providing Multiple Classes of Service  712 9.5.3 Diffserv 7199.5.4 Per-Connection Quality-of-Service (QoS) Guarantees: Resource Reservation and Call Admission 723 9.6 Summary 726 Homework Problems and Questions  727 Programming Assignment 735 Interview: Henning Schulzrinne  736 References  741 Index 783
Chapter 1 Computer Networks and the Internet Today’s Internet is arguably the largest engineered system ever created by ­mankind, with hundreds of millions of connected computers, communication links, and switches; with billions of users who connect via laptops, tablets, and smartphones; and with an array of new Internet-connected “things” includinggame consoles, surveillance systems, watches, eye glasses, thermostats, body scales, and cars.
Giventhat the Internet is so large and has so many diverse components and uses, is there any hope of understanding how it works?
Are there guiding principles and structure that can provide a foundation for understanding such an amazingly large and complex system?
And if so, is it possible that it actually could be both interesting and fun to learn about computer networks?
Fortunately, the answer to all of these questions is a resounding YES!
Indeed, it’s our aim in this book to provide you with a modern introduction to the dynamic field of computer networking, giving you the principles and practical insightsyou’ll need to understand not only today’s networks, but tomorrow’s as well.
This first chapter presents a broad overview of computer networking and the Internet.
Our goal here is to paint a broad picture and set the context for the rest of this book, to see the forest through the trees.
We’ll cover a lot of ground in this introductory chapter and discuss a lot of the pieces of a computernetwork, without losing sight of the big picture.
We’ll structure our overview of computer networks in this chapter as follows.
After introducing some basic terminology and concepts, we’ll first examine the basic hardware and software components thatmake up a network.
We’ll begin at the network’s edge and look at the end systems and networkapplications running in the network.
We’ll then explore the core of a computer network, examining thelinks and the switches that transport data, as well as the access networks and physical media that connect end systems to the network core.
We’ll learn that the Internet is a network of networks, and we’ll learn how these networks connect with each other.
After having completed this overview of the edge and core of a computer network, we’ll take the broader and more abstract view in the second half of this chapter.
We’ll examine delay, loss, and throughput ofdata in a computer network and provide simple quantitative models for end-to-end throughput and delay:models that take into account transmission, propagation, and queuing delays.
We’ll then introduce someof the key architectural principles in computer networking, namely, protocol layering and service models.
We’ll also learn that computer networks are vulnerable to many different types of attacks; we’ll survey
some of these attacks and consider how computer networks can be made more secure.
Finally, we’ll close this chapter with a brief history of computer networking.
1.1 What Is the Internet?
In this book, we’ll use the public Internet, a specific computer network, as our principal vehicle for discussing computer networks and their protocols.
But what is the Internet?
There are a couple of ways to answer this question.
First, we can describe the nuts and bolts of the Internet, that is, the basic hardware and software components that make up the Internet.
Second, we can describe the Internet interms of a networking infrastructure that provides services to distributed applications.
Let’s begin with the nuts-and-bolts description, using Figure 1.1 to illustrate our discussion.
1.1.1 A Nuts-and-Bolts Description The Internet is a computer network that interconnects billions of computing devices throughout the world.
Not too long ago, these computing devices were primarily traditional desktop PCs, Linuxworkstations, and so-called servers that store and transmit information such as Web pages and e-mailmessages.
Increasingly, however, nontraditional Internet “things” such as laptops, smartphones, tablets,TVs, gaming consoles, thermostats, home security systems, home appliances, watches, eye glasses, cars, traffic control systems and more are being connected to the Internet.
Indeed, the term computer network is beginning to sound a bit dated, given the many nontraditional devices that are being hooked up to the Internet.
In Internet jargon, all of these devices are called hosts or end systems .
By some estimates, in 2015 there were about 5 billion devices connected to the Internet, and the number will reach 25 billion by 2020 [Gartner 2014] .
It is estimated that in 2015 there were over 3.2 billion Internet users worldwide, approximately 40% of the world population [ITU 2015].
Figure 1.1 Some pieces of the Internet End systems are connected together by a network of communication links  and packet switches.
We’ll see in Section 1.2 that there are many types of communication links, which are made up of
different types of physical media, including coaxial cable, copper wire, optical fiber, and radio spectrum.
Different links can transmit data at different rates, with the transmission rate of a link measured in bits/second.
When one end system has data to send to another end system, the sending end systemsegments the data and adds header bytes to each segment.
The resulting packages of information,known as packets  in the jargon of computer networks, are then sent through the network to the destination end system, where they are reassembled into the original data.
A packet switch takes a packet arriving on one of its incoming communication links and forwards that packet on one of its outgoing communication links.
Packet switches come in many shapes and flavors,but the two most prominent types in today’s Internet are routers and link-layer switches.
Both types of switches forward packets toward their ultimate destinations.
Link-layer switches are typically used inaccess networks, while routers are typically used in the network core.
The sequence of communicationlinks and packet switches traversed by a packet from the sending end system to the receiving endsystem is known as a route or path through the network.
Cisco predicts annual global IP traffic will pass the zettabyte (10  bytes) threshold by the end of 2016, and will reach 2 zettabytes per year by 2019 [Cisco VNI 2015].
Packet-switched networks (which transport packets) are in many ways similar to transportation networks of highways, roads, and intersections (which transport vehicles).
Consider, for example, a factory thatneeds to move a large amount of cargo to some destination warehouse located thousands of kilometers away.
At the factory, the cargo is segmented and loaded into a fleet of trucks.
Each of the trucks then independently travels through the network of highways, roads, and intersections to the destinationwarehouse.
At the destination warehouse, the cargo is unloaded and grouped with the rest of the cargoarriving from the same shipment.
Thus, in many ways, packets are analogous to trucks, communicationlinks are analogous to highways and roads, packet switches are analogous to intersections, and endsystems are analogous to buildings.
Just as a truck takes a path through the transportation network, apacket takes a path through a computer network.
End systems access the Internet through Internet Service Providers (ISPs), including residential ISPs such as local cable or telephone companies; corporate ISPs; university ISPs; ISPs that provide WiFi access in airports, hotels, coffee shops, and other public places; and cellular data ISPs, providingmobile access to our smartphones and other devices.
Each ISP is in itself a network of packet switchesand communication links.
ISPs provide a variety of types of network access to the end systems,including residential broadband access such as cable modem or DSL, high-speed local area networkaccess, and mobile wireless access.
ISPs also provide ­Internet access to content providers, connecting Web sites and video servers directly to the Internet.
The Internet is all about connecting endsystems to each other, so the ISPs that provide access to end systems must also be interconnected.
These lower-tier ISPs are interconnected through national and international upper-tier ISPs such as Level 3 Communications, AT&T, Sprint, and NTT.
An upper-tier ISP consists of high-speed routersinterconnected with high-speed fiber-optic links.
Each ISP network, whether upper-tier or lower-tier, is21
managed independently, runs the IP protocol (see below), and conforms to certain naming and address conventions.
We’ll examine ISPs and their interconnection more closely in Section 1.3.
End systems, packet switches, and other pieces of the Internet run protocols that control the sending and receiving of information within the Internet.
The Transmission Control Protocol (TCP)  and the Internet Protocol (IP)  are two of the most important protocols in the Internet.
The IP protocol specifies the format of the packets that are sent and received among routers and end systems.
The Internet’s principal protocols are collectively known as TCP/IP.
We’ll begin looking into protocols in this introductory chapter.
But that’s just a start—much of this book is concerned with computer networkprotocols!
Given the importance of protocols to the Internet, it’s important that everyone agree on what each and every protocol does, so that people can create systems and products that interoperate.
This is wherestandards come into play.
Internet ­standards are developed by the Internet Engineering Task Force (IETF) [IETF 2016].
The IETF standards documents are called requests for comments (RFCs) .
RFCs started out as general requests for comments (hence the name) to resolve network and protocol design problems that faced the precursor to the Internet [Allman 2011].
RFCs tend to be quite technical and detailed.
They define protocols such as TCP, IP, HTTP (for the Web), and SMTP (for e-mail).
There are currently more than 7,000 RFCs.
Other bodies also specify standards for network components, most notably for network links.
The IEEE 802 LAN/MAN Standards Committee [IEEE 802 2016] , for example, specifies the Ethernet and wireless WiFi standards.
1.1.2 A Services Description Our discussion above has identified many of the pieces that make up the Internet.
But we can alsodescribe the Internet from an entirely different angle—namely, as an infrastructure that provides services to applications .
In addition to traditional applications such as e-mail and Web surfing, Internet applications include mobile smartphone and tablet applications, including Internet messaging, mapping with real-time road-traffic information, music streaming from the cloud, movie and television streaming, online social networks, video conferencing, multi-person games, and location-based recommendation systems.
The applications are said to be distributed applications , since they involve multiple end systems that exchange data with each other.
Importantly, Internet applications run on end systems—they do not run in the packet switches in the network core.
Although packet switches facilitate theexchange of data among end systems, they are not concerned with the application that is the source orsink of data.
Let’s explore a little more what we mean by an infrastructure that provides ­services to applications.
To this end, suppose you have an exciting new idea for a distributed Internet application, one that may greatly benefit humanity or one that may simply make you rich and famous.
How might you go about
transforming this idea into an actual Internet application?
Because applications run on end systems, you are going to need to write programs that run on the end systems.
You might, for example, write your programs in Java, C, or Python.
Now, because you are developing a distributed Internet application, theprograms running on the different end systems will need to send data to each other.
And here we get toa central issue—one that leads to the alternative way of describing the Internet as a platform forapplications.
How does one program running on one end system instruct the Internet to deliver data to another program running on another end system?
End systems attached to the Internet provide a socket interface  that specifies how a program running on one end system asks the Internet infrastructure to deliver data to a specific destination program running on another end system.
This Internet socket interface is a set of rules that the sending programmust follow so that the Internet can deliver the data to the destination program.
We’ll discuss the Internet socket interface in detail in Chapter 2.
For now, let’s draw upon a simple analogy, one that we will frequently use in this book.
Suppose Alice wants to send a letter to Bob using the postal service.
Alice, of course, can’t just write the letter (the data) and drop the letter out her window.
Instead, the postal service requires that Alice put the letter in an envelope; write Bob’s full name, address, and zipcode in the center of the envelope; seal the envelope; put a stamp in the upper-right-hand corner of theenvelope; and finally, drop the envelope into an official postal service mailbox.
Thus, the postal servicehas its own “postal service interface,” or set of rules, that Alice must follow to have the postal servicedeliver her letter to Bob.
In a similar manner, the Internet has a socket interface that the programsending data must follow to have the Internet deliver the data to the program that will receive the data.
The postal service, of course, provides more than one service to its customers.
It provides express delivery, reception confirmation, ordinary use, and many more services.
In a similar manner, the Internet provides multiple services to its applications.
When you develop an Internet application, you too mustchoose one of the Internet’s services for your application.
We’ll describe the Internet’s services in Chapter 2.
We have just given two descriptions of the Internet; one in terms of its hardware and software components, the other in terms of an infrastructure for providing services to distributed applications.
Butperhaps you are still confused as to what the Internet is.
What are packet switching and TCP/IP?
What are routers?
What kinds of communication links are present in the Internet?
What is a distributed application?
How can a thermostat or body scale be attached to the Internet?
If you feel a bitoverwhelmed by all of this now, don’t worry—the purpose of this book is to introduce you to both thenuts and bolts of the Internet and the principles that govern how and why it works.
We’ll explain theseimportant terms and questions in the following sections and chapters.
1.1.3 What Is a Protocol?
Now that we’ve got a bit of a feel for what the Internet is, let’s consider another important buzzword in computer networking: protocol .
What is a protocol?
What does a protocol do ?
A Human Analogy It is probably easiest to understand the notion of a computer network protocol by first considering some human analogies, since we humans execute protocols all of the time.
Consider what you do when you want to ask someone for the time of day.
A typical exchange is shown in Figure 1.2.
Human protocol (or good manners, at least) dictates that one first offer a greeting (the first “Hi” in Figure 1.2) to initiate communication with someone else.
The typical response to a “Hi” is a returned “Hi” message.
Implicitly, one then takes a cordial “Hi” response as an indication that one can proceed and ask for the time of day.
A different response to the initial “Hi” (such as “Don’t bother me!”
or “I don’t speak English,” or someunprintable reply) might Figure 1.2 A human protocol and a computer network protocol indicate an unwillingness or inability to communicate.
In this case, the human protocol would be not to ask for the time of day.
Sometimes one gets no response at all to a question, in which case one typically gives up asking that person for the time.
Note that in our human protocol, there are specific messages
we send, and specific actions we take in response to the received reply messages or other events  (such as no reply within some given amount of time).
Clearly, transmitted and received messages, and actions taken when these messages are sent or received or other events occur, play a central role in a humanprotocol.
If people run different protocols (for example, if one person has manners but the other doesnot, or if one understands the concept of time and the other does not) the protocols do not interoperateand no useful work can be accomplished.
The same is true in networking—it takes two (or more) communicating entities running the same protocol in order to accomplish a task.
Let’s consider a second human analogy.
Suppose you’re in a college class (a computer networking class, for example!).
The teacher is droning on about protocols and you’re confused.
The teacher stopsto ask, “Are there any questions?” (
a message that is transmitted to, and received by, all students whoare not sleeping).
You raise your hand (transmitting an implicit message to the teacher).
Your teacheracknowledges you with a smile, saying “Yes . . .” (
a transmitted message encouraging you to ask your question—teachers love to be asked questions), and you then ask your question (that is, transmit your message to your teacher).
Your teacher hears your question (receives your question message) and answers (transmits a reply to you).
Once again, we see that the transmission and receipt of messages,and a set of conventional actions taken when these messages are sent and received, are at the heart ofthis question-and-answer protocol.
Network Protocols A network protocol is similar to a human protocol, except that the entities exchanging messages and taking actions are hardware or software components of some device (for example, computer, smartphone, tablet, router, or other network-capable device).
All activity in the Internet that involves two or more communicating remote entities is governed by a protocol.
For example, hardware-implementedprotocols in two physically connected computers control the flow of bits on the “wire” between the twonetwork interface cards; congestion-control protocols in end systems control the rate at which packetsare transmitted between sender and receiver; protocols in routers determine a packet’s path fromsource to destination.
Protocols are running everywhere in the Internet, and consequently much of thisbook is about computer network protocols.
As an example of a computer network protocol with which you are probably familiar, consider what happens when you make a request to a Web server, that is, when you type the URL of a Web page into your Web browser.
The scenario is illustrated in the right half of Figure 1.2.
First, your computer will send a connection request message to the Web server and wait for a reply.
The Web server will eventually receive your connection request message and return a connection reply message.
Knowing that it is now OK to request the Web document, your computer then sends the name of the Web page itwants to fetch from that Web server in a GET message.
Finally, the Web server returns the Web page(file) to your computer.
Given the human and networking examples above, the exchange of messages and the actions taken when these messages are sent and received are the key defining elements of a protocol: A protocol  defines the format and the order of messages exchanged between two or more communicating entities, as well as the actions taken on the transmission and/or receipt of a message or other event.
The Internet, and computer networks in general, make extensive use of protocols.
Different protocolsare used to accomplish different communication tasks.
As you read through this book, you will learn thatsome protocols are simple and straightforward, while others are complex and intellectually deep.
Mastering the field of computer networking is equivalent to understanding the what, why, and how ofnetworking protocols.
1.2 The Network Edge In the previous section we presented a high-level overview of the Internet and networking protocols.
We are now going to delve a bit more deeply into the components of a computer network (and the Internet,in particular).
We begin in this section at the edge of a network and look at the components with whichwe are most ­familiar—namely, the computers, smartphones and other devices that we use on a dailybasis.
In the next section we’ll move from the network edge to the network core and examine switching and routing in computer networks.
Recall from the previous section that in computer networking jargon, the computers and other devices connected to the Internet are often referred to as end systems.
They are referred to as end systems because they sit at the edge of the Internet, as shown in Figure 1.3.
The Internet’s end systems include desktop computers (e.g., desktop PCs, Macs, and Linux boxes), servers (e.g., Web and e-mail servers), and mobile devices (e.g., laptops, smartphones, and tablets).
Furthermore, an increasing number ofnon-traditional “things” are being attached to the Internet as end ­systems (see the Case History feature).
End systems are also referred to as hosts because they host (that is, run) application programs such as a Web browser program, a Web server program, an e-mail client program, or an e-mail server program.
Throughout this book we will use the
Figure 1.3 End-system interaction CASE HISTORY THE INTERNET OF THINGS Can you imagine a world in which just about everything is wirelessly connected to the Internet?
A world in which most people, cars, bicycles, eye glasses, watches, toys, hospital equipment,home sensors, classrooms, video surveillance systems, atmospheric sensors, store-shelf
products, and pets are connected?
This world of the Internet of Things (IoT) may actually be just around the corner.
By some estimates, as of 2015 there are already 5 billion things connected to the Internet, and the number could reach 25 billion by 2020 [Gartner 2014] .
These things include our smartphones, which already follow us around in our homes, offices, and cars, reporting our geo- locations and usage data to our ISPs and Internet applications.
But in addition to our smartphones, a wide-variety of non-traditional “things” are already available as products.
Forexample, there are Internet-connected wearables, including watches (from Apple and manyothers) and eye glasses.
Internet-connected glasses can, for example, upload everything we seeto the cloud, allowing us to share our visual experiences with people around the world in real-time.
There are Internet-connected things already available for the smart home, includingInternet-connected thermostats that can be controlled remotely from our smartphones, andInternet-connected body scales, enabling us to graphically review the progress of our diets fromour smartphones.
There are Internet-connected toys, including dolls that recognize and interpret a child’s speech and respond appropriately.
The IoT offers potentially revolutionary benefits to users.
But at the same time there are also huge security and privacy risks.
For example, attackers, via the Internet, might be able to hackinto IoT devices or into the servers collecting data from IoT devices.
For example, an attackercould hijack an Internet-connected doll and talk directly with a child; or an attacker could hackinto a database that stores ­personal health and activity information collected from wearabledevices.
These security and privacy concerns could undermine the consumer confidencenecessary for the ­technologies to meet their full potential and may result in less widespread adoption [FTC 2015].
terms hosts and end systems interchangeably; that is, host = end system.
Hosts are sometimes further divided into two categories: clients and servers .
Informally, clients tend to be desktop and mobile PCs, smartphones, and so on, whereas servers tend to be more powerful machines that store and distribute Web pages, stream video, relay e-mail, and so on.
Today, most of the servers from which we receivesearch results, e-mail, Web pages, and videos reside in large data centers .
For example, Google has 50-100 data centers, including about 15 large centers, each with more than 100,000 servers.
1.2.1 Access Networks Having considered the applications and end systems at the “edge of the network,” let’s next consider theaccess network—the network that physically connects an end system to the first router (also known as the “edge router”) on a path from the end system to any other distant end system.
Figure 1.4 shows several types of access
Figure 1.4 Access networks networks with thick, shaded lines and the settings (home, enterprise, and wide-area mobile wireless) in which they are used.
Home Access: DSL, Cable, FTTH, Dial-Up, and Satellite
In developed countries as of 2014, more than 78 percent of the households have Internet access, with Korea, Netherlands, Finland, and Sweden leading the way with more than 80 percent of households having Internet access, almost all via a high-speed broadband connection [ITU 2015].
Given this widespread use of home access networks let’s begin our overview of access networks by considering how homes connect to the Internet.
Today, the two most prevalent types of broadband residential access are digital subscriber line (DSL) and cable.
A residence typically obtains DSL Internet access from the same local telephone company (telco) that provides its wired local phone access.
Thus, when DSL is used, a customer’s telco is also its ISP.
As shown in Figure 1.5, each customer’s DSL modem uses the existing telephone line (twisted- pair copper wire, which we’ll discuss in Section 1.2.2) to exchange data with a digital subscriber line access multiplexer (DSLAM) located in the telco’s local central office (CO).
The home’s DSL modemtakes digital data and translates it to high- ­frequency tones for transmission over telephone wires to the CO; the analog signals from many such houses are translated back into digital format at the DSLAM.
The residential telephone line carries both data and traditional telephone signals simultaneously, which are encoded at different frequencies: A high-speed downstream channel, in the 50 kHz to 1 MHz band A medium-speed upstream channel, in the 4 kHz to 50 kHz band An ordinary two-way telephone channel, in the 0 to 4 kHz band This approach makes the single DSL link appear as if there were three separate links, so that atelephone call and an Internet connection can share the DSL link at the same time.
Figure 1.5 DSL Internet access (We’ll describe this technique of frequency-division multiplexing in Section 1.3.1.)
On the customer side, a splitter separates the data and telephone signals arriving to the home and forwards the data signal to
the DSL modem.
On the telco side, in the CO, the DSLAM separates the data and phone signals and sends the data into the Internet.
Hundreds or even thousands of households connect to a single DSLAM [Dischinger 2007].
The DSL standards define multiple transmission rates, including 12 Mbps downstream and 1.8 Mbps upstream [ITU 1999], and 55 Mbps downstream and 15 Mbps upstream [ITU 2006].
Because the downstream and upstream rates are different, the access is said to be asymmetric.
The actual downstream and upstream transmission rates achieved may be less than the rates noted above, as theDSL provider may purposefully limit a residential rate when tiered service (different rates, available atdifferent prices) are offered.
The maximum rate is also limited by the distance between the home andthe CO, the gauge of the twisted-pair line and the degree of electrical interference.
Engineers haveexpressly designed DSL for short distances between the home and the CO; generally, if the residence isnot located within 5 to 10 miles of the CO, the residence must resort to an alternative form of Internetaccess.
While DSL makes use of the telco’s existing local telephone infrastructure, cable Internet access makes use of the cable television company’s existing cable television infrastructure.
A residence obtains cable Internet access from the same company that provides its cable television.
As illustrated in Figure 1.6, fiber optics connect the cable head end to neighborhood-level junctions, from which traditional coaxial cable is then used to reach individual houses and apartments.
Each neighborhood junction typically supports 500 to 5,000 homes.
Because both fiber and coaxial cable are employed in thissystem, it is often referred to as hybrid fiber coax (HFC).
Figure 1.6 A hybrid fiber-coaxial access network Cable internet access requires special modems, called cable modems.
As with a DSL modem, the cable
modem is typically an external device and connects to the home PC through an Ethernet port. (
We will discuss Ethernet in great detail in Chapter 6.)
At the cable head end, the cable modem termination system (CMTS) serves a similar function as the DSL network’s DSLAM—turning the analog signal sent from the cable modems in many downstream homes back into digital format.
Cable modems divide theHFC network into two channels, a downstream and an upstream channel.
As with DSL, access is typically asymmetric, with the downstream channel typically allocated a higher transmission rate than the upstream channel.
The ­DOCSIS 2.0 standard defines downstream rates up to 42.8 Mbps and upstream rates of up to 30.7 Mbps.
As in the case of DSL networks, the maximum achievable rate maynot be realized due to lower contracted data rates or media impairments.
One important characteristic of cable Internet access is that it is a shared broadcast medium.
In particular, every packet sent by the head end travels downstream on every link to every home and everypacket sent by a home travels on the upstream channel to the head end.
For this reason, if severalusers are simultaneously downloading a video file on the downstream channel, the actual rate at which each user receives its video file will be significantly lower than the aggregate cable downstream rate.
On the other hand, if there are only a few active users and they are all Web surfing, then each of the usersmay actually receive Web pages at the full cable downstream rate, because the users will rarely requesta Web page at exactly the same time.
Because the upstream channel is also shared, a distributedmultiple access protocol is needed to coordinate transmissions and avoid collisions. (
We’ll discuss this collision issue in some detail in Chapter 6.)
Although DSL and cable networks currently represent more than 85 percent of residential broadband access in the United States, an up-and-coming technology that provides even higher speeds is fiber to the home (FTTH)  [FTTH Council 2016] .
As the name suggests, the FTTH concept is simple—provide an optical fiber path from the CO directly to the home.
Many countries today—including the UAE, South Korea, Hong Kong, Japan, Singapore, Taiwan, Lithuania, and Sweden—now have household penetration rates exceeding 30% [FTTH Council 2016] .
There are several competing technologies for optical distribution from the CO to the homes.
The simplest optical distribution network is called direct fiber, with one fiber leaving the CO for each home.
More commonly, each fiber leaving the central office is actually shared by many homes; it is not until the fiber gets relatively close to the homes that it is split into individual customer-specific fibers.
There are two competing optical-distribution network architectures that perform this splitting: active opticalnetworks (AONs) and passive optical networks (PONs).
AON is essentially switched Ethernet, which is discussed in Chapter 6.
Here, we briefly discuss PON, which is used in Verizon’s FIOS service.
Fig­ure 1.7 shows FTTH using the PON distribution architecture.
Each home has an optical network terminator (ONT), which is connected by dedicated optical fiber to a neighborhood splitter.
The splitter combines a number of homes (typically less
Figure 1.7 FTTH Internet access than 100) onto a single, shared optical fiber, which connects to an optical line ­terminator (OLT) in the telco’s CO.
The OLT, providing conversion between optical and electrical signals, connects to the Internet via a telco router.
In the home, users connect a home router (typically a wireless router) to theONT and access the ­Internet via this home router.
In the PON architecture, all packets sent from OLTto the splitter are replicated at the splitter (similar to a cable head end).
FTTH can potentially provide Internet access rates in the gigabits per second range.
However, most FTTH ISPs provide different rate offerings, with the higher rates naturally costing more money.
Theaverage downstream speed of US FTTH customers was approximately 20 Mbps in 2011 (comparedwith 13 Mbps for cable access networks and less than 5 Mbps for DSL) [FTTH Council 2011b].
Two other access network technologies are also used to provide Internet access to the home.
In locations where DSL, cable, and FTTH are not available (e.g., in some rural settings), a satellite link canbe used to connect a residence to the Internet at speeds of more than 1 Mbps; StarBand andHughesNet are two such satellite access providers.
Dial-up access over traditional phone lines is basedon the same model as DSL—a home modem connects over a phone line to a modem in the ISP.Compared with DSL and other broadband access networks, dial-up access is excruciatingly slow at 56kbps.
Access in the Enterprise (and the Home): Ethernet and WiFi On corporate and university campuses, and increasingly in home settings, a local area network (LAN) is used to connect an end system to the edge router.
Although there are many types of LAN technologies,Ethernet is by far the most prevalent access technology in corporate, university, and home networks.
As shown in Figure 1.8, Ethernet users use twisted-pair copper wire to connect to an Ethernet switch, a technology discussed in detail in Chapter 6.
The Ethernet switch, or a network of such
Figure 1.8 Ethernet Internet access interconnected switches, is then in turn connected into the larger Internet.
With Ethernet access, users typically have 100 Mbps or 1 Gbps access to the Ethernet switch, whereas servers may have 1 Gbps or even 10 Gbps access.
Increasingly, however, people are accessing the Internet wirelessly from laptops, smartphones, tablets, and other “things” (see earlier sidebar on “ Internet of Things ”).
In a wireless LAN setting, wireless users transmit/receive packets to/from an access point that is connected into the enterprise’s network (most likely using wired Ethernet), which in turn is connected to the wired Internet.
A wireless LAN usermust typically be within a few tens of meters of the access point.
Wireless LAN access based on IEEE802.11 technology, more colloquially known as WiFi, is now just about everywhere—universities, business offices, cafes, airports, homes, and even in airplanes.
In many cities, one can stand on a street corner and be within range of ten or twenty base stations (for a browseable global map of 802.11 basestations that have been discovered and logged on a Web site by people who take great enjoyment in doing such things, see [wigle.net 2016]).
As discussed in detail in Chapter 7, 802.11 today provides a shared transmission rate of up to more than 100 Mbps.
Even though Ethernet and WiFi access networks were initially deployed in enterprise (corporate, university) settings, they have recently become relatively common components of home networks.
Many homes combine broadband residential access (that is, cable modems or DSL) with these inexpensive wireless LAN technologies to create powerful home networks [Edwards 2011].
Figure 1.9 shows a typical home network.
This home network consists of a roaming laptop as well as a wired PC; a base station (the wireless access point), which communicates with the wireless PC and other wirelessdevices in the home; a cable modem, providing broadband access to the Internet; and a router, whichinterconnects the base station and the stationary PC with the cable modem.
This network allowshousehold members to have broadband access to the Internet with one member roaming from the
kitchen to the backyard to the bedrooms.
Figure 1.9 A typical home network Wide-Area Wireless Access: 3G and LTE Increasingly, devices such as iPhones and Android devices are being used to message, share photos in social networks, watch movies, and stream music while on the run.
These devices employ the samewireless infrastructure used for cellular telephony to send/receive packets through a base station that isoperated by the cellular network provider.
Unlike WiFi, a user need only be within a few tens ofkilometers (as opposed to a few tens of meters) of the base station.
Telecommunications companies have made enormous investments in so-called third-generation (3G) wireless, which provides packet-switched wide-area wireless Internet access at speeds in excess of 1 Mbps.
But even higher-speed wide-area access technologies—a fourth-generation (4G) of wide-area wireless networks—are already being deployed.
LTE (for “Long-Term Evolution”—a candidate for BadAcronym of the Year Award) has its roots in 3G technology, and can achieve rates in excess of 10Mbps.
LTE downstream rates of many tens of Mbps have been reported in commercial deployments.
We’ll cover the basic principles of wireless networks and mobility, as well as WiFi, 3G, and LTE technologies (and more!)
in Chapter 7.
1.2.2 Physical Media In the previous subsection, we gave an overview of some of the most important network access technologies in the Internet.
As we described these technologies, we also indicated the physical mediaused.
For example, we said that HFC uses a combination of fiber cable and coaxial cable.
We said thatDSL and Ethernet use copper wire.
And we said that mobile access networks use the radio spectrum.
Inthis subsection we provide a brief overview of these and other transmission media that are commonly used in the Internet.
In order to define what is meant by a physical medium, let us reflect on the brief life of a bit.
Consider a bit traveling from one end system, through a series of links and routers, to another end system.
Thispoor bit gets kicked around and transmitted many, many times!
The source end system first transmits the bit, and shortly thereafter the first router in the series receives the bit; the first router then transmitsthe bit, and shortly thereafter the second router receives the bit; and so on.
Thus our bit, when traveling from source to destination, passes through a series of transmitter-receiver pairs.
For each transmitter- receiver pair, the bit is sent by propagating electromagnetic waves or optical pulses across a physical medium.
The physical medium can take many shapes and forms and does not have to be of the sametype for each transmitter-receiver pair along the path.
Examples of physical media include twisted-paircopper wire, coaxial cable, multimode fiber-optic cable, terrestrial radio spectrum, and satellite radiospectrum.
Physical media fall into two categories: guided media  and unguided media .
With guided media, the waves are guided along a solid medium, such as a fiber-optic cable, a twisted-pair copperwire, or a coaxial cable.
With unguided media, the waves propagate in the atmosphere and in outer space, such as in a wireless LAN or a digital satellite channel.
But before we get into the characteristics of the various media types, let us say a few words about their costs.
The actual cost of the physical link (copper wire, fiber-optic cable, and so on) is often relativelyminor compared with other networking costs.
In particular, the labor cost associated with the installationof the physical link can be orders of magnitude higher than the cost of the material.
For this reason,many builders install twisted pair, optical fiber, and coaxial cable in every room in a building.
Even if onlyone medium is initially used, there is a good chance that another medium could be used in the nearfuture, and so money is saved by not having to lay additional wires in the future.
Twisted-Pair Copper Wire The least expensive and most commonly used guided transmission medium is twisted-pair copper wire.
For over a hundred years it has been used by telephone networks.
In fact, more than 99 percent of thewired connections from the telephone handset to the local telephone switch use twisted-pair copperwire.
Most of us have seen twisted pair in our homes (or those of our parents or grandparents!)
andwork environments.
Twisted pair consists of two insulated copper wires, each about 1 mm thick,arranged in a regular spiral pattern.
The wires are twisted together to reduce the electrical interference from similar pairs close by.
Typically, a number of pairs are bundled together in a cable by wrapping the pairs in a protective shield.
A wire pair constitutes a single communication link.
Unshielded twisted pair (UTP) is commonly used for computer networks within a building, that is, for LANs.
Data rates forLANs using twisted pair today range from 10 Mbps to 10 Gbps.
The data rates that can be achieveddepend on the thickness of the wire and the distance between transmitter and receiver.
When fiber-optic technology emerged in the 1980s, many people disparaged twisted pair because of its relatively low bit rates.
Some people even felt that fiber-optic technology would completely replace twisted pair.
But twisted pair did not give up so easily.
Modern twisted-pair technology, such as category
6a cable, can achieve data rates of 10 Gbps for distances up to a hundred meters.
In the end, twisted pair has emerged as the dominant solution for high-speed LAN networking.
As discussed earlier, twisted pair is also commonly used for residential Internet access.
We saw that dial-up modem technology enables access at rates of up to 56 kbps over twisted pair.
We also saw that DSL (digital subscriber line) technology has enabled residential users to access the Internet at tens of Mbps over twisted pair (when users live close to the ISP’s central office).
Coaxial Cable Like twisted pair, coaxial cable consists of two copper conductors, but the two conductors are concentric rather than parallel.
With this construction and special insulation and shielding, coaxial cable canachieve high data transmission rates.
Coaxial cable is quite common in cable television systems.
As wesaw earlier, cable television systems have recently been coupled with cable modems to provide residential users with Internet access at rates of tens of Mbps.
In cable television and cable Internet access, the transmitter shifts the digital signal to a specific frequency band, and the resulting analogsignal is sent from the transmitter to one or more receivers.
Coaxial cable can be used as a guidedshared medium.
Specifically, a number of end systems can be connected directly to the cable, witheach of the end systems receiving whatever is sent by the other end systems.
Fiber Optics An optical fiber is a thin, flexible medium that conducts pulses of light, with each pulse representing a bit.
A single optical fiber can support tremendous bit rates, up to tens or even hundreds of gigabits per second.
They are immune to electromagnetic interference, have very low signal attenuation up to 100kilometers, and are very hard to tap.
These characteristics have made fiber optics the preferred long-haul guided transmission media, particularly for overseas links.
Many of the long-distance telephonenetworks in the United States and elsewhere now use fiber optics exclusively.
Fiber optics is alsoprevalent in the backbone of the Internet.
However, the high cost of optical devices—such astransmitters, receivers, and switches—has hindered their deployment for short-haul transport, such as ina LAN or into the home in a residential access network.
The Optical Carrier (OC) standard link speeds range from 51.8 Mbps to 39.8 Gbps; these specifications are often referred to as OC- n, where the link speed equals n ∞ 51.8 Mbps.
Standards in use today include OC-1, OC-3, OC-12, OC-24, OC-48, OC- 96, OC-192, OC-768. [
Mukherjee 2006 , Ramaswami 2010]  provide coverage of various aspects of optical networking.
Terrestrial Radio Channels Radio channels carry signals in the electromagnetic spectrum.
They are an attractive medium because they require no physical wire to be installed, can penetrate walls, provide connectivity to a mobile user,
and can potentially carry a signal for long distances.
The characteristics of a radio channel depend significantly on the propagation environment and the distance over which a signal is to be carried.
Environmental considerations determine path loss and shadow fading (which decrease the signalstrength as the signal travels over a distance and around/through obstructing objects), multipath fading(due to signal reflection off of interfering objects), and interference (due to other transmissions and electromagnetic signals).
Terrestrial radio channels can be broadly classified into three groups: those that operate over very short distance (e.g., with one or two meters); those that operate in local areas, typically spanning from ten to afew hundred meters; and those that operate in the wide area, spanning tens of kilometers.
Personaldevices such as wireless headsets, keyboards, and medical devices operate over short distances; the wireless LAN technologies described in Section 1.2.1 use local-area radio channels; the cellular access technologies use wide-area radio channels.
We’ll discuss radio channels in detail in Chapter 7.
Satellite Radio Channels A communication satellite links two or more Earth-based microwave transmitter/ receivers, known as ground stations.
The satellite receives transmissions on one frequency band, regenerates the signalusing a repeater (discussed below), and transmits the signal on another frequency.
Two types ofsatellites are used in communications: geostationary satellites and low-earth orbiting (LEO) satellites  [Wiki Satellite 2016].
Geostationary satellites permanently remain above the same spot on Earth.
This stationary presence isachieved by placing the satellite in orbit at 36,000 kilometers above Earth’s surface.
This huge distancefrom ground station through satellite back to ground station introduces a substantial signal propagationdelay of 280 milliseconds.
Nevertheless, satellite links, which can operate at speeds of hundreds ofMbps, are often used in areas without access to DSL or cable-based Internet access.
LEO satellites are placed much closer to Earth and do not remain permanently above one spot on Earth.
They rotate around Earth (just as the Moon does) and may communicate with each other, as well aswith ground stations.
To provide continuous coverage to an area, many satellites need to be placed in orbit.
There are currently many low-altitude communication systems in development.
LEO satellite technology may be used for Internet access sometime in the future.
1.3 The Network Core Having examined the Internet’s edge, let us now delve more deeply inside the network core—the mesh of packet switches and links that interconnects the Internet’s end systems.
Figure 1.10 highlights the network core with thick, shaded lines.
Figure 1.10 The network core 1.3.1 Packet Switching In a network application, end systems exchange messages  with each other.
Messages can contain anything the application designer wants.
Messages may perform a control function (for example, the “Hi” messages in our handshaking example in Figure 1.2) or can contain data, such as an e-mail message, a JPEG image, or an MP3 audio file.
To send a message from a source end system to a destination end system, the source breaks long messages into smaller chunks of data known as packets .
Between source and destination, each packet travels through communication links and packet switches (for which there are two predominant types, routers and link-layer switches).
Packets are transmitted over each communication link at a rate equal to the full transmission rate of the link.
So, if a source end system or a packet switch is sending a packet of L bits over a link with transmission rate R bits/sec, then the time to transmit the packet is L / R seconds.
Store-and-Forward TransmissionMost packet switches use store-and-forward transmission at the inputs to the links.
Store-and-forward transmission means that the packet switch must receive the entire packet before it can begin to transmit the first bit of the packet onto the outbound link.
To explore store-and-forward transmission in moredetail, consider a simple network consisting of two end systems connected by a single router, as shown in Figure 1.11.
A router will typically have many incident links, since its job is to switch an incoming packet onto an outgoing link; in this simple example, the router has the rather simple task of transferring a packet from one (input) link to the only other attached link.
In this example, the source has three packets, each consisting of L bits, to send to the destination.
At the snapshot of time shown in Figure 1.11, the source has transmitted some of packet 1, and the front of packet 1 has already arrived at the router.
Because the router employs store-and-forwarding, at this instant of time, the router cannot transmit the bits it has received; instead it must first buffer (i.e., “store”) the packet’s bits.
Only after the router has received all of the packet’s bits can it begin to transmit (i.e., “forward”) the packet onto the outbound link.
To gain some insight into store-and-forward transmission, let’s now calculate the amount of time that elapses from when the source begins to send the packet until the destination has receivedthe entire packet. (
Here we will ignore propagation delay—the time it takes for the bits to travel across the wire at near the speed of light—which will be discussed in Section 1.4.)
The source begins to transmit at time 0; at time L/R seconds, the source has transmitted the entire packet, and the entire packet has been received and stored at the router (since there is no propagation delay).
At time L/R seconds, since the router has just received the entire packet, it can begin to transmit the packet onto theoutbound link towards the destination; at time 2 L/R, the router has transmitted the entire packet, and the
entire packet has been received by the destination.
Thus, the total delay is 2 L/R. If the Figure 1.11 Store-and-forward packet switching switch instead forwarded bits as soon as they arrive (without first receiving the entire packet), then the total delay would be L/R since bits are not held up at the router.
But, as we will discuss in Section 1.4, routers need to receive, store, and process the entire packet before forwarding.
Now let’s calculate the amount of time that elapses from when the source begins to send the first packetuntil the destination has received all three packets.
As before, at time L/R, the router begins to forward the first packet.
But also at time L/R the source will begin to send the second packet, since it has just finished sending the entire first packet.
Thus, at time 2 L/R, the destination has received the first packet and the router has received the second packet.
Similarly, at time 3 L/R, the destination has received the first two packets and the router has received the third packet.
Finally, at time 4 L/R the destination has received all three packets!
Let’s now consider the general case of sending one packet from source to destination over a path consisting of N links each of rate R (thus, there are N-1 routers between source and destination).
Applying the same logic as above, we see that the end-to-end delay is: You may now want to try to determine what the delay would be for P packets sent over a series of N links.
Queuing Delays and Packet Loss Each packet switch has multiple links attached to it.
For each attached link, the packet switch has an output buffer  (also called an output queue ), which stores packets that the router is about to send into that link.
The output buffers play a key role in packet switching.
If an arriving packet needs to betransmitted onto a link but finds the link busy with the transmission of another packet, the arriving packet must wait in the output buffer.
Thus, in addition to the store-and-forward delays, packets suffer output buffer queuing delays .
These delays are variable and depend on the level of congestion in the network.dend-to-end =NLR (1.1)
Since the amount of buffer space is finite, an Figure 1.12 Packet switching arriving packet may find that the buffer is completely full with other packets waiting for transmission.
In this case, packet loss will occur—either the arriving packet or one of the already-queued packets will be dropped.
Figure 1.12 illustrates a simple packet-switched network.
As in Figure 1.11, packets are represented by three-dimensional slabs.
The width of a slab represents the number of bits in the packet.
In this figure, all packets have the same width and hence the same length.
Suppose Hosts A and B are sendingpackets to Host E. Hosts A and B first send their packets along 100 Mbps Ethernet links to the firstrouter.
The router then directs these packets to the 15 Mbps link.
If, during a short interval of time, thearrival rate of packets to the router (when converted to bits per second) exceeds 15 Mbps, congestion will occur at the router as packets queue in the link’s output buffer before being transmitted onto the link.
For example, if Host A and B each send a burst of five packets back-to-back at the same time, thenmost of these packets will spend some time waiting in the queue.
The situation is, in fact, entirelyanalogous to many common-day situations—for example, when we wait in line for a bank teller or wait in front of a tollbooth.
We’ll examine this queuing delay in more detail in Section 1.4.
Forwarding Tables and Routing Protocols Earlier, we said that a router takes a packet arriving on one of its attached communication links and forwards that packet onto another one of its attached communication links.
But how does the router determine which link it should forward the packet onto?
Packet forwarding is actually done in differentways in different types of computer networks.
Here, we briefly describe how it is done in the Internet.
In the Internet, every end system has an address called an IP address.
When a source end system wants to send a packet to a destination end system, the source includes the destination’s IP address inthe packet’s header.
As with postal addresses, this address has a hierarchical structure.
When a packetarrives at a router in the network, the router examines a portion of the packet’s destination address andforwards the packet to an adjacent router.
More specifically, each router has a forwarding table  that maps destination addresses (or portions of the destination addresses) to that router’s outbound links.
When a packet arrives at a router, the router examines the address and searches its forwarding table,using this destination address, to find the appropriate outbound link.
The router then directs the packetto this outbound link.
The end-to-end routing process is analogous to a car driver who does not use maps but instead prefers to ask for directions.
For example, suppose Joe is driving from Philadelphia to 156 Lakeside Drive inOrlando, Florida.
Joe first drives to his neighborhood gas station and asks how to get to 156 Lakeside Drive in Orlando, Florida.
The gas station attendant extracts the Florida portion of the address and tells Joe that he needs to get onto the interstate highway I-95 South, which has an entrance just next to thegas station.
He also tells Joe that once he enters Florida, he should ask someone else there.
Joe thentakes I-95 South until he gets to Jacksonville, Florida, at which point he asks another gas stationattendant for directions.
The attendant extracts the Orlando portion of the address and tells Joe that heshould continue on I-95 to Daytona Beach and then ask someone else.
In Daytona Beach, another gasstation attendant also extracts the Orlando portion of the address and tells Joe that he should take I-4directly to Orlando.
Joe takes I-4 and gets off at the Orlando exit.
Joe goes to another gas stationattendant, and this time the attendant extracts the Lakeside Drive portion of the address and tells Joe the road he must follow to get to Lakeside Drive.
Once Joe reaches Lakeside Drive, he asks a kid on a bicycle how to get to his destination.
The kid extracts the 156 portion of the address and points to thehouse.
Joe finally reaches his ultimate destination.
In the above analogy, the gas station attendants andkids on bicycles are analogous to routers.
We just learned that a router uses a packet’s destination address to index a forwarding table and determine the appropriate outbound link.
But this statement begs yet another question: How doforwarding tables get set?
Are they configured by hand in each and every router, or does the Internet use a more automated procedure?
This issue will be studied in depth in Chapter 5.
But to whet your appetite here, we’ll note now that the Internet has a number of special routing protocols  that are used to automatically set the forwarding tables.
A routing protocol may, for example, determine the shortest path from each router to each destination and use the shortest path results to configure the forwardingtables in the routers.
How would you actually like to see the end-to-end route that packets take in the Internet?
We now invite you to get your hands dirty by interacting with the Trace-route program.
Simply visit the site www.traceroute.org, choose a source in a particular country, and trace the route from that source to your computer. (
For a discussion of Traceroute, see Section 1.4.)
1.3.2 Circuit Switching There are two fundamental approaches to moving data through a network of links and switches: circuit switching  and packet switching.
Having covered packet-switched networks in the previous subsection, we now turn our attention to circuit-switched networks.
In circuit-switched networks, the resources needed along a path (buffers, link transmission rate) to provide for communication between the end systems are reserved  for the duration of the communication session between the end systems.
In packet-switched networks, these resources are not reserved; a session’s messages use the resources on demand and, as a consequence, may have to wait (that is, queue) for access to a communication link.
As a simple analogy, consider two restaurants, one thatrequires reservations and another that neither requires reservations nor accepts them.
For therestaurant that requires reservations, we have to go through the hassle of calling before we leave home.
But when we arrive at the restaurant we can, in principle, immediately be seated and order our meal.
For the restaurant that does not require reservations, we don’t need to bother to reserve a table.
But when we arrive at the restaurant, we may have to wait for a table before we can be seated.
Traditional telephone networks are examples of circuit-switched networks.
­Consider what happens when one person wants to send information (voice or facsimile) to another over a telephone network.
Before the sender can send the information, the network must establish a connection between the sender and the receiver.
This is a bona fide  connection for which the switches on the path between the sender and receiver maintain connection state for that connection.
In the jargon of telephony, this connection is called a circuit.
When the network establishes the circuit, it also reserves a constant transmission rate in the network’s links (representing a fraction of each link’s transmission capacity) for the duration of the connection.
Since a given transmission rate has been reserved for this sender-to- receiver connection, the sender can transfer the data to the receiver at the guaranteed  constant rate.
Figure 1.13 illustrates a circuit-switched network.
In this network, the four circuit switches are interconnected by four links.
Each of these links has four circuits, so that each link can support four simultaneous connections.
The hosts (for example, PCs and workstations) are each directly connectedto one of the switches.
When two hosts want to communicate, the network establishes a dedicated end- to-end connection  between the two hosts.
Thus, in order for Host A to communicate with Host B, the network must first reserve one circuit on each of two links.
In this example, the dedicated end-to-endconnection uses the second circuit in the first link and the fourth circuit in the second link.
Because eachlink has four circuits, for each link used by the end-to-end connection, the connection gets one fourth ofthe link’s total transmission capacity for the duration of the connection.
Thus, for example, if each linkbetween adjacent switches has a transmission rate of 1 Mbps, then each end-to-end circuit-switchconnection gets 250 kbps of dedicated transmission rate.
Figure 1.13 A simple circuit-switched network consisting of four switches and four links In contrast, consider what happens when one host wants to send a packet to another host over a packet-switched network, such as the Internet.
As with circuit switching, the packet is transmitted over a series of communication links.
But different from circuit switching, the packet is sent into the networkwithout reserving any link resources whatsoever.
If one of the links is congested because other packetsneed to be transmitted over the link at the same time, then the packet will have to wait in a buffer at thesending side of the transmission link and suffer a delay.
The Internet makes its best effort to deliverpackets in a timely manner, but it does not make any guarantees.
Multiplexing in Circuit-Switched Networks A circuit in a link is implemented with either frequency-division multiplexing (FDM)  or time-division multiplexing (TDM) .
With FDM, the frequency spectrum of a link is divided up among the connections established across the link.
Specifically, the link dedicates a frequency band to each connection for the duration of the connection.
In telephone networks, this frequency band typically has a width of 4 kHz(that is, 4,000 hertz or 4,000 cycles per second).
The width of the band is called, not surprisingly, the bandwidth .
FM radio stations also use FDM to share the frequency spectrum between 88 MHz and 108 MHz, with each station being allocated a specific frequency band.
For a TDM link, time is divided into frames of fixed duration, and each frame is divided into a fixed number of time slots.
When the network establishes a connection across a link, the network dedicates one time slot in every frame to this connection.
These slots are dedicated for the sole use of thatconnection, with one time slot available for use (in every frame) to transmit the connection’s data.
Figure 1.14 With FDM, each circuit continuously gets a fraction of the bandwidth.
With TDM, each circuit gets all of the bandwidth periodically during brief intervals of time (that is, during slots) Figure 1.14 illustrates FDM and TDM for a specific network link supporting up to four circuits.
For FDM, the frequency domain is segmented into four bands, each of bandwidth 4 kHz.
For TDM, the time domain is segmented into frames, with four time slots in each frame; each circuit is assigned the samededicated slot in the revolving TDM frames.
For TDM, the transmission rate of a circuit is equal to theframe rate multiplied by the number of bits in a slot.
For example, if the link transmits 8,000 frames persecond and each slot consists of 8 bits, then the transmission rate of each circuit is 64 kbps.
Proponents of packet switching have always argued that circuit switching is wasteful because the dedicated circuits are idle during silent periods .
For example, when one person in a telephone call stops talking, the idle network resources (frequency bands or time slots in the links along the connection’s route) cannot be used by other ongoing connections.
As another example of how theseresources can be underutilized, consider a radiologist who uses a circuit-switched network to remotelyaccess a series of x-rays.
The radiologist sets up a connection, requests an image, contemplates theimage, and then requests a new image.
Network resources are allocated to the connection but are notused (i.e., are wasted) during the radiologist’s contemplation periods.
Proponents of packet switchingalso enjoy pointing out that establishing end-to-end circuits and reserving end-to-end transmission capacity is complicated and requires complex signaling software to coordinate the operation of the switches along the end-to-end path.
Before we finish our discussion of circuit switching, let’s work through a numerical example that should shed further insight on the topic.
Let us consider how long it takes to send a file of 640,000 bits fromHost A to Host B over a circuit-switched network.
Suppose that all links in the network use TDM with 24slots and have a bit rate of 1.536 Mbps.
Also suppose that it takes 500 msec to establish an end-to-endcircuit before Host A can begin to transmit the file.
How long does it take to send the file?
Each circuithas a transmission rate of  so it takes  seconds to transmit the file.
To this 10 seconds we add the circuit establishment time, giving 10.5 seconds to sendthe file.
Note that the transmission time is independent of the number of links: The transmission timewould be 10 seconds if the end-to-end circuit passed through one link or a hundred links. (
The actual (1.536 Mbps)/24=64 kbps, (640,000 bits)/(64 kbps)=10
end-to-end delay also includes a propagation delay; see Section 1.4.)
Packet Switching Versus Circuit Switching Having described circuit switching and packet switching, let us compare the two.
Critics of packet switching have often argued that packet switching is not suitable for real-time services (for example, telephone calls and video conference calls) because of its variable and unpredictable end-to-end delays(due primarily to variable and unpredictable queuing delays).
Proponents of packet switching argue that(1) it offers better sharing of transmission capacity than circuit switching and (2) it is simpler, moreefficient, and less costly to implement than circuit switching.
An interesting discussion of packet switching versus circuit switching is [Molinero-Fernandez 2002].
Generally speaking, people who do not like to hassle with ­restaurant reservations prefer packet switching to circuit switching.
Why is packet switching more efficient?
Let’s look at a simple example.
Suppose users share a 1 Mbps link.
Also suppose that each user alternates between periods of activity, when a user generates data ata constant rate of 100 kbps, and periods of inactivity, when a user generates no data.
Suppose furtherthat a user is active only 10 percent of the time (and is idly drinking coffee during the remaining 90 percent of the time).
With circuit switching, 100 kbps must be reserved  for each user at all times.
For example, with circuit-switched TDM, if a one-second frame is divided into 10 time slots of 100 ms each, then each user would be allocated one time slot per frame.
Thus, the circuit-switched link can support only  simultaneous users.
With packet switching, the probability that a specific user is active is 0.1 (that is, 10 percent).
If there are 35 users, the probability that there are 11 or more simultaneously active users is approximately 0.0004. (
Homework Problem P8 outlines how this probability is obtained.)
When there are 10 or fewer simultaneously active users (which happens with probability 0.9996), the aggregate arrival rate of data is less than or equal to 1 Mbps, the output rate of the link.
Thus, when there are 10 or fewer active users,users’ packets flow through the link essentially without delay, as is the case with circuit switching.
When there are more than 10 simultaneously active users, then the aggregate arrival rate of packets exceedsthe output capacity of the link, and the output queue will begin to grow. (
It continues to grow until the aggregate input rate falls back below 1 Mbps, at which point the queue will begin to diminish in length.)
Because the probability of having more than 10 simultaneously active users is minuscule in this example, packet switching provides essentially the same performance as circuit switching, but does so while allowing for more than three times the number of users.
Let’s now consider a second simple example.
Suppose there are 10 users and that one user suddenly generates one thousand 1,000-bit packets, while other users remain quiescent and do not generatepackets.
Under TDM circuit switching with 10 slots per frame and each slot consisting of 1,000 bits, theactive user can only use its one time slot per frame to transmit data, while the remaining nine time slots in each frame remain idle.
It will be 10 seconds before all of the active user’s one million bits of data has10(=1 Mbps/100 kbps)
been transmitted.
In the case of packet switching, the active user can continuously send its packets at the full link rate of 1 Mbps, since there are no other users generating packets that need to be multiplexed with the active user’s packets.
In this case, all of the active user’s data will be transmittedwithin 1 second.
The above examples illustrate two ways in which the performance of packet switching can be superior to that of circuit switching.
They also highlight the crucial difference between the two forms of sharing a link’s transmission rate among multiple data streams.
Circuit switching pre-allocates use of thetransmission link regardless of demand, with allocated but unneeded link time going unused.
Packet switching on the other hand allocates link use on demand.
 Link transmission capacity will be shared on a packet-by-packet basis only among those users who have packets that need to be transmitted over the link.
Although packet switching and circuit switching are both prevalent in today’s telecommunication networks, the trend has certainly been in the direction of packet switching.
Even many of today’s circuit- switched telephone networks are slowly migrating toward packet switching.
In particular, telephonenetworks often use packet switching for the expensive overseas portion of a telephone call.
1.3.3 A Network of Networks We saw earlier that end systems (PCs, smartphones, Web servers, mail servers, and so on) connectinto the Internet via an access ISP.
The access ISP can provide either wired or wireless connectivity,using an array of access technologies including DSL, cable, FTTH, Wi-Fi, and cellular.
Note that theaccess ISP does not have to be a telco or a cable company; instead it can be, for example, a university(providing Internet access to students, staff, and faculty), or a company (providing access for its employees).
But connecting end users and content providers into an access ISP is only a small piece of solving the puzzle of connecting the billions of end systems that make up the Internet.
To complete this puzzle, the access ISPs themselves must be interconnected.
This is done by creating a network of networks—understanding this phrase is the key to understanding the Internet.
Over the years, the network of networks that forms the Internet has evolved into a very complex structure.
Much of this evolution is driven by economics and national policy, rather than by performanceconsiderations.
In order to understand today’s Internet network structure, let’s incrementally build a series of network structures, with each new structure being a better approximation of the complex Internet that we have today.
Recall that the overarching goal is to interconnect the access ISPs so thatall end systems can send packets to each other.
One naive approach would be to have each access ISP directly connect with every other access ISP.
Such a mesh design is, of course, much too costly for the access ISPs, as it would require each access ISP to have a separate communication link to each of the hundreds of thousands of other access ISPs all over the world.
Our first network structure, Network Structure 1, interconnects all of the access ISPs with a single  global transit ISP.
Our (imaginary) global transit ISP is a network of routers and communication links that not only spans the globe, but also has at least one router near each of the hundreds of thousands of access ISPs.
Of course, it would be very costly for the global ISP to build such an extensive network.
To be profitable, it would naturally charge each of the access ISPs for connectivity, with the pricing reflecting (but not necessarily directly proportional to) the amount of traffic an access ISP exchanges with theglobal ISP.
Since the access ISP pays the global transit ISP, the access ISP is said to be a customer and the global transit ISP is said to be a provider.
Now if some company builds and operates a global transit ISP that is profitable, then it is natural forother companies to build their own global transit ISPs and compete with the original global transit ISP.
This leads to Network Structure 2, which consists of the hundreds of thousands of access ISPs and multiple  global ­transit ISPs.
The access ISPs certainly prefer Network Structure 2 over Network Structure 1 since they can now choose among the competing global transit providers as a function oftheir pricing and services.
Note, however, that the global transit ISPs themselves must interconnect:Otherwise access ISPs connected to one of the global transit providers would not be able tocommunicate with access ISPs connected to the other global transit providers.
Network Structure 2, just described, is a two-tier hierarchy with global transit providers residing at the top tier and access ISPs at the bottom tier.
This assumes that global transit ISPs are not only capable ofgetting close to each and every access ISP, but also find it economically desirable to do so.
In reality, although some ISPs do have impressive global coverage and do directly connect with many access ISPs, no ISP has presence in each and every city in the world.
Instead, in any given region, there maybe a regional ISP  to which the access ISPs in the region connect.
Each regional ISP then connects to tier-1 ISPs.
Tier-1 ISPs are similar to our (imaginary) global transit ISP; but tier-1 ISPs, which actuallydo exist, do not have a presence in every city in the world.
There are approximately a dozen tier-1 ISPs,including Level 3 Communications, AT&T, Sprint, and NTT.
Interestingly, no group officially sanctions tier-1 status; as the saying goes—if you have to ask if you’re a member of a group, you’re probably not.
Returning to this network of networks, not only are there multiple competing tier-1 ISPs, there may be multiple competing regional ISPs in a region.
In such a hierarchy, each access ISP pays the regional ISP to which it connects, and each regional ISP pays the tier-1 ISP to which it connects. (
An access ISPcan also connect directly to a tier-1 ISP, in which case it pays the tier-1 ISP).
Thus, there is customer-provider relationship at each level of the hierarchy.
Note that the tier-1 ISPs do not pay anyone as theyare at the top of the hierarchy.
To further complicate matters, in some regions, there may be a largerregional ISP (possibly spanning an entire country) to which the smaller regional ISPs in that regionconnect; the larger regional ISP then connects to a tier-1 ISP.
For example, in China, there are accessISPs in each city, which connect to provincial ISPs, which in turn connect to national ISPs, which finally connect to tier-1 ISPs [Tian 2012].
We refer to this multi-tier hierarchy, which is still only a crude
approximation of today’s Internet, as Network Structure 3.
To build a network that more closely resembles today’s Internet, we must add points of presence (PoPs), multi-homing, peering, and Internet exchange points (IXPs) to the hierarchical NetworkStructure 3.
PoPs exist in all levels of the hierarchy, except for the bottom (access ISP) level.
A PoP is simply a group of one or more routers (at the same location) in the provider’s network where customerISPs can connect into the provider ISP.
For a customer network to connect to a provider’s PoP, it can lease a high-speed link from a third-party telecommunications provider to directly connect one of its routers to a router at the PoP. Any ISP (except for tier-1 ISPs) may choose to multi-home, that is, to connect to two or more provider ISPs.
So, for example, an access ISP may multi-home with two regionalISPs, or it may multi-home with two regional ISPs and also with a tier-1 ISP.
Similarly, a regional ISPmay multi-home with multiple tier-1 ISPs.
When an ISP multi-homes, it can continue to send and receivepackets into the Internet even if one of its providers has a failure.
As we just learned, customer ISPs pay their provider ISPs to obtain global Internet interconnectivity.
The amount that a customer ISP pays a provider ISP reflects the amount of traffic it exchanges with the provider.
To reduce these costs, a pair of nearby ISPs at the same level of the hierarchy can peer, that is, they can directly connect their networks together so that all the traffic between them passes over thedirect connection rather than through upstream intermediaries.
When two ISPs peer, it is typicallysettlement-free, that is, neither ISP pays the other.
As noted earlier, tier-1 ISPs also peer with oneanother, settlement-free.
For a readable discussion of peering and customer-provider relationships, see [Van der Berg 2008] .
Along these same lines, a third-party company can create an Internet Exchange Point (IXP) , which is a meeting point where multiple ISPs can peer together.
An IXP is typically in a stand-alone building with its own switches [Ager 2012] .
There are over 400 IXPs in the Internet today [IXP List 2016].
We refer to this ecosystem—consisting of access ISPs, regional ISPs, tier-1 ISPs, PoPs, multi-homing, peering, and IXPs—as Network Structure 4.
We now finally arrive at Network Structure 5, which describes today’s Internet.
Network Structure 5, illustrated in Figure 1.15, builds on top of Network Structure 4 by adding content-provider networks .
Google is currently one of the leading examples of such a content-provider network.
As of this writing, itis estimated that Google has 50–100 data centers distributed across North America, Europe, Asia,South America, and Australia.
Some of these data centers house over one hundred thousand servers, while other data centers are smaller, housing only hundreds of servers.
The Google data centers are all interconnected via Google’s private TCP/IP network, which spans the entire globe but is neverthelessseparate from the public Internet.
Importantly, the Google private network only carries traffic to/from Google servers.
As shown in Figure 1.15, the Google private network attempts to “bypass” the upper tiers of the Internet by peering (settlement free) with lower-tier ISPs, either by directly connecting withthem or by connecting with them at IXPs [Labovitz 2010].
However, because many access ISPs can still only be reached by transiting through tier-1 networks, the Google network also connects to tier-1 ISPs, and pays those ISPs for the traffic it exchanges with them.
By creating its own network, a content
provider not only reduces its payments to upper-tier ISPs, but also has greater control of how its services are ultimately delivered to end users.
Google’s network infrastructure is described in greater detail in Section 2.6.
In summary, today’s Internet—a network of networks—is complex, consisting of a dozen or so tier-1 ISPs and hundreds of thousands of lower-tier ISPs.
The ISPs are diverse in their coverage, with some spanning multiple continents and oceans, and others limited to narrow geographic regions.
The lower- tier ISPs connect to the higher-tier ISPs, and the higher-tier ISPs interconnect with one another.
Usersand content providers are customers of lower-tier ISPs, and lower-tier ISPs are customers of higher-tierISPs.
In recent years, major content providers have also created their own networks and connectdirectly into lower-tier ISPs where possible.
Figure 1.15 Interconnection of ISPs
1.4 Delay, Loss, and Throughput in Packet-Switched Networks Back in Section 1.1 we said that the Internet can be viewed as an infrastructure that provides services to distributed applications running on end systems.
Ideally, we would like Internet services to be able to move as much data as we want between any two end systems, instantaneously, without any loss ofdata.
Alas, this is a lofty goal, one that is unachievable in reality.
Instead, computer networks necessarily constrain throughput (the amount of data per second that can be transferred) between end systems, introduce delays between end systems, and can actually lose packets.
On one hand, it is unfortunatethat the physical laws of reality introduce delay and loss as well as constrain throughput.
On the otherhand, because computer networks have these problems, there are many fascinating issues surroundinghow to deal with the problems—more than enough issues to fill a course on computer networking and tomotivate thousands of PhD theses!
In this section, we’ll begin to examine and quantify delay, loss, andthroughput in computer networks.
1.4.1 Overview of Delay in Packet-Switched Networks Recall that a packet starts in a host (the source), passes through a series of routers, and ends itsjourney in another host (the destination).
As a packet travels from one node (host or router) to the subsequent node (host or router) along this path, the packet suffers from several types of delays at each node along the path.
The most important of these delays are the nodal processing delay , queuing delay, transmission delay, and propagation delay ; together, these delays accumulate to give a total nodal delay .
The performance of many Internet applications—such as search, Web browsing, e-mail, maps, instant messaging, and voice-over-IP—are greatly affected by network delays.
In order to acquire a deep understanding of packet switching and computer networks, we must understand the nature andimportance of these delays.
Types of DelayLet’s explore these delays in the context of Figure 1.16.
As part of its end-to-end route between source and destination, a packet is sent from the upstream node through router A to router B. Our goal is to characterize the nodal delay at router A. Note that router A has an outbound link leading to router B. This link is preceded by a queue (also known as a buffer).
When the packet arrives at router A from theupstream node, router A examines the packet’s header to determine the appropriate outbound link forthe packet and then directs the packet to this link.
In this example, the outbound link for the packet is theone that leads to router B. A packet can be transmitted on a link only if there is no other packet currently
being transmitted on the link and if there are no other packets preceding it in the queue; if the link is Figure 1.16 The nodal delay at router A currently busy or if there are other packets already queued for the link, the newly arriving packet will then join the queue.
Processing Delay The time required to examine the packet’s header and determine where to direct the packet is part of the processing delay.
The processing delay can also include other factors, such as the time needed tocheck for bit-level errors in the packet that occurred in transmitting the packet’s bits from the upstreamnode to router A. Processing delays in high-speed routers are typically on the order of microseconds or less.
After this nodal processing, the router directs the packet to the queue that precedes the link to router B. (In Chapter 4 we’ll study the details of how a router operates.)
Queuing Delay At the queue, the packet experiences a queuing delay  as it waits to be transmitted onto the link.
The length of the queuing delay of a specific packet will depend on the number of earlier-arriving packets that are queued and waiting for transmission onto the link.
If the queue is empty and no other packet is currently being transmitted, then our packet’s queuing delay will be zero.
On the other hand, if the trafficis heavy and many other packets are also waiting to be transmitted, the queuing delay will be long.
Wewill see shortly that the number of packets that an arriving packet might expect to find is a function of theintensity and nature of the traffic arriving at the queue.
­Queuing delays can be on the order of microseconds to milliseconds in practice.
Transmission Delay Assuming that packets are transmitted in a first-come-first-served manner, as is common in packet- switched networks, our packet can be transmitted only after all the packets that have arrived before it have been transmitted.
Denote the length of the packet by L bits, and denote the transmission rate of
the link from router A to router B by R bits/sec.
For example, for a 10 Mbps Ethernet link, the rate is  for a 100 Mbps Ethernet link, the rate is  The transmission delay  is L/R. This is the amount of time required to push (that is, transmit) all of the packet’s bits into the link.
Transmission delays are typically on the order of microseconds to milliseconds in practice.
Propagation Delay Once a bit is pushed into the link, it needs to propagate to router B. The time required to propagate from the beginning of the link to router B is the propagation delay .
The bit propagates at the propagation speed of the link.
The propagation speed depends on the physical medium of the link (that is, fiberoptics, twisted-pair copper wire, and so on) and is in the range of which is equal to, or a little less than, the speed of light.
The propagation delay is the distance between two routers divided by the propagation speed.
That is, the propagation delay is d/s, where d is the distance between router A and router B and s is the propagation speed of the link.
Once the last bit of the packet propagates to node B, it and all the preceding bits of the packet are stored in router B. The whole process then continues with router B now performing the forwarding.
In wide-area networks,propagation delays are on the order of milliseconds.
Comparing Transmission and Propagation Delay Exploring propagation delay and transmission delay Newcomers to the field of computer networking sometimes have difficulty understanding the difference between transmission delay and propagation delay.
The difference is subtle but important.
Thetransmission delay is the amount of time required for the router to push out the packet; it is a function ofthe packet’s length and the transmission rate of the link, but has nothing to do with the distance betweenthe two routers.
The propagation delay, on the other hand, is the time it takes a bit to propagate fromone router to the next; it is a function of the distance between the two routers, but has nothing to do withthe packet’s length or the transmission rate of the link.
An analogy might clarify the notions of transmission and propagation delay.
Consider a highway that has a tollbooth every 100 kilometers, as shown in Figure 1.17.
You can think of the highway segmentsR=10 Mbps; R=100 Mbps.
2⋅108 meters/sec to 3⋅108 meters/sec
between tollbooths as links and the tollbooths as routers.
Suppose that cars travel (that is, propagate) on the highway at a rate of 100 km/hour (that is, when a car leaves a tollbooth, it instantaneouslyaccelerates to 100 km/hour and maintains that speed between tollbooths).
Suppose next that 10 cars,traveling together as a caravan, follow each other in a fixed order.
You can think of each car as a bit andthe caravan as a packet.
Also suppose that each Figure 1.17 Caravan analogy tollbooth services (that is, transmits) a car at a rate of one car per 12 seconds, and that it is late at night so that the caravan’s cars are the only cars on the highway.
Finally, suppose that whenever the first car of the caravan arrives at a tollbooth, it waits at the entrance until the other nine cars have arrived andlined up behind it. (
Thus the entire caravan must be stored at the tollbooth before it can begin to beforwarded.)
The time required for the tollbooth to push the entire caravan onto the highway is .
This time is analogous to the transmission delay in a router.
The time required for a car to travel from the exit of one tollbooth to the next tollbooth is .
This time is analogous to propagation delay.
Therefore, the time from when the caravan is stored in front of a tollbooth until the caravan is stored in front of the next tollboothis the sum of transmission delay and propagation delay—in this example, 62 minutes.
Let’s explore this analogy a bit more.
What would happen if the tollbooth service time for a caravan were greater than the time for a car to travel between tollbooths?
For example, suppose now that the carstravel at the rate of 1,000 km/hour and the tollbooth services cars at the rate of one car per minute.
Thenthe traveling delay between two tollbooths is 6 minutes and the time to serve a caravan is 10 minutes.
Inthis case, the first few cars in the caravan will arrive at the second tollbooth before the last cars in thecaravan leave the first tollbooth.
This situation also arises in packet-switched networks—the first bits in apacket can arrive at a router while many of the remaining bits in the packet are still waiting to be transmitted by the preceding router.
If a picture speaks a thousand words, then an animation must speak a million words.
The Web site for this textbook provides an interactive Java applet that nicely illustrates and contrasts transmission delay and propagation delay.
The reader is highly encouraged to visit that applet. [
Smith 2009] also provides a very readable discussion of propagation, queueing, and transmission delays.
If we let d , d , d , and d  denote the processing, queuing, transmission, and propagation(10 cars)/(5 cars/minute)=2 minutes 100 km/(100 km/hour)=1 hour proc queue trans prop
delays, then the total nodal delay is given by The contribution of these delay components can vary significantly.
For example, d can be negligible (for example, a couple of microseconds) for a link connecting two routers on the same university campus; however, d is hundreds of milliseconds for two routers interconnected by a geostationary satellite link, and can be the dominant term in d. Similarly, d  can range from negligible to significant.
Its contribution is typically negligible for transmission rates of 10 Mbps and higher (for example, for LANs); however, it can be hundreds of milliseconds for large Internet packets sent over low-speed dial-up modem links.
The processing delay, d, is often negligible; however, it strongly influences a router’s maximum throughput, which is the maximum rate at which a router can forward packets.
1.4.2 Queuing Delay and Packet Loss The most complicated and interesting component of nodal delay is the queuing delay, d .
In fact, queuing delay is so important and interesting in computer networking that thousands of papers and numerous books have been written about it [Bertsekas 1991 ; Daigle 1991; Kleinrock 1975 , Kleinrock 1976 ; Ross 1995].
We give only a high-level, intuitive discussion of queuing delay here; the more curious reader may want to browse through some of the books (or even eventually write a PhD thesis on the subject!).
Unlike the other three delays (namely, d, d , and d ), the queuing delay can vary from packet to packet.
For example, if 10 packets arrive at an empty queue at the same time, the first packet transmitted will suffer no queuing delay, while the last packet transmitted will suffer a relatively large queuing delay (while it waits for the other nine packets to be transmitted).
Therefore, when characterizing queuing delay, one typically uses statistical measures, such as average queuing delay,variance of queuing delay, and the probability that the queuing delay exceeds some specified value.
When is the queuing delay large and when is it insignificant?
The answer to this question depends on the rate at which traffic arrives at the queue, the transmission rate of the link, and the nature of thearriving traffic, that is, whether the traffic arrives periodically or arrives in bursts.
To gain some insight here, let a denote the average rate at which packets arrive at the queue ( a is in units of packets/sec).
Recall that R is the transmission rate; that is, it is the rate (in bits/sec) at which bits are pushed out of the queue.
Also suppose, for simplicity, that all packets consist of L bits.
Then the average rate at which bits arrive at the queue is La bits/sec.
Finally, assume that the queue is very big, so that it can hold essentially an infinite number of bits.
The ratio La/R, called the traffic intensity , often plays an important role in estimating the extent of the queuing delay.
If La/R > 1, then the average rate at which bits arrive at the queue exceeds the rate at which the bits can be transmitted from the queue.
In thisdnodal =dproc+dqueue +dtrans+dprop prop prop nodal trans proc queue proc trans prop
unfortunate situation, the queue will tend to increase without bound and the queuing delay will approach infinity!
Therefore, one of the golden rules in traffic engineering is: Design your system so that the traffic intensity is no greater than 1.
Now consider the case La/R ≤ 1.
Here, the nature of the arriving traffic impacts the queuing delay.
For example, if packets arrive periodically—that is, one packet arrives every L/R seconds—then every packet will arrive at an empty queue and there will be no queuing delay.
On the other hand, if packets arrive in bursts but periodically, there can be a significant average queuing delay.
For example, suppose N packets arrive simultaneously every (L/R)N seconds.
Then the first packet transmitted has no queuing delay; the second packet transmitted has a queuing delay of L/R seconds; and more generally, the nth packet transmitted has a queuing delay of L/R seconds.
We leave it as an exercise for you to calculate the average queuing delay in this example.
The two examples of periodic arrivals described above are a bit academic.
­Typically, the arrival process to a queue is random; that is, the arrivals do not follow any pattern and the packets are spaced apart by random amounts of time.
In this more realistic case, the quantity La/R is not usually sufficient to fully characterize the queuing delay statistics.
Nonetheless, it is useful in gaining an intuitive understanding of the extent of the queuing delay.
In particular, if the traffic intensity is close to zero, thenpacket arrivals are few and far between and it is unlikely that an arriving packet will find another packetin the queue.
Hence, the average queuing delay will be close to zero.
On the other hand, when thetraffic intensity is close to 1, there will be intervals of time when the arrival rate exceeds the transmissioncapacity (due to variations in packet arrival rate), and a queue will form during these periods of time; when the arrival rate is less than the transmission capacity, the length of the queue will shrink.
Nonetheless, as the traffic intensity approaches 1, the average queue length gets larger and larger.
The qualitative dependence of average queuing delay on the traffic intensity is shown in Figure 1.18.
One important aspect of Figure 1.18 is the fact that as the traffic intensity approaches 1, the average queuing delay increases rapidly.
A small percentage increase in the intensity will result in a much larger percentage-wise increase in delay.
Perhaps you have experienced this phenomenon on the highway.
Ifyou regularly drive on a road that is typically congested, the fact that the road is typically(n−1)
Figure 1.18 Dependence of average queuing delay on traffic intensity congested means that its traffic intensity is close to 1.
If some event causes an even slightly larger-than- usual amount of traffic, the delays you experience can be huge.
To really get a good feel for what queuing delays are about, you are encouraged once again to visit the textbook Web site, which provides an interactive Java applet for a queue.
If you set the packet arrivalrate high enough so that the traffic intensity exceeds 1, you will see the queue slowly build up over time.
Packet Loss In our discussions above, we have assumed that the queue is capable of holding an infinite number of packets.
In reality a queue preceding a link has finite capacity, although the queuing capacity greatly depends on the router design and cost.
Because the queue capacity is finite, packet delays do not reallyapproach infinity as the traffic intensity approaches 1.
Instead, a packet can arrive to find a full queue.
With no place to store such a packet, a router will drop that packet; that is, the packet will be lost.
This overflow at a queue can again be seen in the Java applet for a queue when the traffic intensity is greaterthan 1.
From an end-system viewpoint, a packet loss will look like a packet having been transmitted into the network core but never emerging from the network at the destination.
The fraction of lost packets increases as the traffic intensity increases.
Therefore, performance at a node is often measured not only in terms of delay, but also in terms of the probability of packet loss.
As we’ll discuss in the subsequentchapters, a lost packet may be retransmitted on an end-to-end basis in order to ensure that all data areeventually transferred from source to destination.
1.4.3 End-to-End Delay
Our discussion up to this point has focused on the nodal delay, that is, the delay at a single router.
Let’s now consider the total delay from source to destination.
To get a handle on this concept, suppose thereare  routers between the source host and the destination host.
Let’s also suppose for the moment that the network is uncongested (so that queuing delays are negligible), the processing delay at each router and at the source host is d , the transmission rate out of each router and out of the source host is R bits/sec, and the propagation on each link is d. The nodal delays accumulate and give an end-to- end delay, where, once again,  where L is the packet size.
Note that Equation  1.2 is a generalization of Equation  1.1, which did not take into account processing and propagation delays.
We leave it to you to generalize Equation  1.2 to the case of ­heterogeneous delays at the nodes and to the presence of an average queuing delay at each node.
Traceroute Using Traceroute to discover network paths and measure network delay To get a hands-on feel for end-to-end delay in a computer network, we can make use of the Traceroute program.
Traceroute is a simple program that can run in any Internet host.
When the user specifies adestination hostname, the program in the source host sends multiple, special packets toward thatdestination.
As these packets work their way toward the destination, they pass through a series ofrouters.
When a router receives one of these special packets, it sends back to the source a shortmessage that contains the name and address of the router.
More specifically, suppose there are  routers between the source and the destination.
Then the source will send N special packets into the network, with each packet addressed to the ultimate destination.
These N special packets are marked 1 through N, with the first packet marked 1 and the last packet marked N. When the nth router receives the nth packet marked n, the router does not forward the packet toward its destination, but instead sends a message back to the source.
When the destination host receives the Nth packet, it too returns a message back to the source.
The source records the time that elapses between when it sends a packet and when it receives the correspondingN−1 proc prop dend−end=N(dproc+dtrans+dprop) (1.2) dtrans=L/R, N−1
return message; it also records the name and address of the router (or the destination host) that returns the message.
In this manner, the source can reconstruct the route taken by packets flowing from source to destination, and the source can determine the round-trip delays to all the intervening routers.
Traceroute actually repeats the experiment just described three times, so the source actually sends 3 • N packets to the destination.
RFC 1393 describes Traceroute in detail.
Here is an example of the output of the Traceroute program, where the route was being traced from the source host gaia.cs.umass.edu (at the University of ­Massachusetts) to the host cis.poly.edu  (at Polytechnic University in Brooklyn).
The output has six columns: the first column is the n value described above, that is, the number of the router along the route; the second column is the name of the router; the third column is the address of the router (of the form xxx.xxx.xxx.xxx); the last three columnsare the round-trip delays for three experiments.
If the source receives fewer than three messages fromany given router (due to packet loss in the network), Traceroute places an asterisk just after the routernumber and reports fewer than three round-trip times for that router.
1  cs-gw (128.119.240.254) 1.009 ms 0.899 ms 0.993 ms2  128.119.3.154 (128.119.3.154) 0.931 ms 0.441 ms 0.651 ms3  -border4-rt-gi-1-3.gw.umass.edu (128.119.2.194) 1.032 ms 0.484 ms   0.451 ms4  -acr1-ge-2-1-0.Boston.cw.net (208.172.51.129) 10.006 ms 8.150 ms 8.460   ms 5  -agr4-loopback.
NewYork.cw.net (206.24.194.104) 12.272 ms 14.344 ms   13.267 ms 6  -acr2-loopback.
NewYork.cw.net (206.24.194.62) 13.225 ms 12.292 ms   12.148 ms7  -pos10-2.core2.NewYork1.Level3.net (209.244.160.133) 12.218 ms 11.823   ms 11.793 ms8   -gige9-1-52.hsipaccess1.NewYork1.Level3.net (64.159.17.39) 13.081 ms   11.556 ms 13.297 ms 9  -p0-0.polyu.bbnplanet.net (4.25.109.122) 12.716 ms 13.052 ms 12.786 ms 10 cis.poly.edu (128.238.32.126) 14.080 ms 13.035 ms 12.802 ms In the trace above there are nine routers between the source and the destination.
Most of these routers have a name, and all of them have addresses.
For example, the name of Router 3 is border4-rt-gi- 1-3.gw.umass.edu  and its address is 128.119.2.194 .
Looking at the data provided for this same router, we see that in the first of the three trials the round-trip delay between the source and the router was 1.03 msec.
The round-trip delays for the subsequent two trials were 0.48 and 0.45 msec.
These
round-trip delays include all of the delays just discussed, including transmission delays, propagation delays, router processing delays, and queuing delays.
Because the queuing delay is varying with time, the round-trip delay of packet n sent to a router n can sometimes be longer than the round-trip delay of packet n+1 sent to router n+1.
Indeed, we observe this phenomenon in the above example: the delays to Router 6 are larger than the delays to Router 7!
Want to try out Traceroute for yourself?
We highly  recommended that you visit http:/ / www.traceroute.org, which provides a Web interface to an extensive list of sources for route tracing.
You choose a source and supply the hostname for any destination.
The Traceroute program then does all the work.
There are a number of free software programs that provide a graphical interface to Traceroute; one of our favorites is PingPlotter [PingPlotter 2016].
End System, Application, and Other Delays In addition to processing, transmission, and propagation delays, there can be additional significant delays in the end systems.
For example, an end system wanting to transmit a packet into a shared medium (e.g., as in a WiFi or cable modem scenario) may purposefully  delay its transmission as part of its protocol for sharing the medium with other end systems; we’ll consider such protocols in detail inChapter 6.
Another important delay is media packetization delay, which is present in Voice-over-IP (VoIP) applications.
In VoIP, the sending side must first fill a packet with encoded digitized speech before passing the packet to the Internet.
This time to fill a packet—called the packetization delay—canbe significant and can impact the user-perceived quality of a VoIP call.
This issue will be furtherexplored in a homework problem at the end of this chapter.
1.4.4 Throughput in Computer Networks In addition to delay and packet loss, another critical performance measure in computer networks is end-to-end throughput.
To define throughput, consider transferring a large file from Host A to Host B acrossa computer network.
This transfer might be, for example, a large video clip from one peer to another in aP2P file sharing system.
The instantaneous throughput  at any instant of time is the rate (in bits/sec) at which Host B is receiving the file. (
Many applications, including many P2P file sharing ­systems, display the instantaneous throughput during downloads in the user interface—perhaps you have observed this before!)
If the file consists of F bits and the transfer takes T seconds for Host B to receive all F bits, then the average throughput  of the file transfer is F/T  bits/sec.
For some applications, such as Internet telephony, it is desirable to have a low delay and an instantaneous throughput consistently above somethreshold (for example, over 24 kbps for some Internet telephony applications and over 256 kbps forsome real-time video applications).
For other applications, including those involving file transfers, delayis not critical, but it is desirable to have the highest possible throughput.
To gain further insight into the important concept of throughput, let’s consider a few examples.
Figure 1.19(a) shows two end systems, a server and a client, connected by two communication links and a router.
Consider the throughput for a file transfer from the server to the client.
Let R denote the rate of the link between the server and the router; and R denote the rate of the link between the router and the client.
Suppose that the only bits being sent in the entire network are those from the server to the client.
We now ask, in this ideal scenario, what is the server-to-client throughput?
To answer this question, we may think of bits as fluid and communication links as pipes .
Clearly, the server cannot pump bits through its link at a rate faster than R  bps; and the router cannot forward bits at a rate faster than R  bps.
If  then the bits pumped by the server will “flow” right through the router and arrive at the client at a rate of R  bps, giving a throughput of R bps.
If, on the other hand,  then the router will not be able to forward bits as quickly as it receives them.
In this case, bits will only leave the router at rate R, giving an end-to-end throughput of R. (Note also that if bits continue to arrive at the router at rate R , and continue to leave the router at R, the backlog of bits at the router waiting Figure 1.19 Throughput for a file transfer from server to client for transmission to the client will grow and grow—a most undesirable situation!)
Thus, for this simpletwo-link network, the throughput is min{ R, R}, that is, it is the transmission rate of the bottleneck link .
Having determined the throughput, we can now approximate the time it takes to transfer a large file of F bits from server to client as F/min{R , R}.
For a specific example, suppose you are downloading an MP3 file of  million bits, the server has a transmission rate of  Mbps, and you have an access link of  Mbps.
The time needed to transfer the file is then 32 seconds.
Of course, these expressions for throughput and transfer time are only approximations, as they do not account for store-and-forward and processing delays as well as protocol issues.
Figure 1.19(b) now shows a network with N links between the server and the client, with the transmission rates of the N links being  Applying the same analysis as for the two-link network, we find that the throughput for a file transfer from server to client is min , whichs c s c Rs<Rc, s s Rc<Rs, c c s c c s s c F=32 Rs=2 Rc=1 R1,R2,…, RN. {
R1,R2,…, RN}
is once again the transmission rate of the bottleneck link along the path between server and client.
Now consider another example motivated by today’s Internet.
Figure 1.20(a) shows two end systems, a server and a client, connected to a computer network.
Consider the throughput for a file transfer from the server to the client.
The server is connected to the network with an access link of rate R and the client is connected to the network with an access link of rate R. Now suppose that all the links in the core of the communication network have very high transmission rates, much higher than R and R .
Indeed, today, the core of the Internet is over-provisioned with high speed links that experience little congestion.
Also suppose that the only bits being sent in the entire network are those from the server tothe client.
Because the core of the computer network is like a wide pipe in this example, the rate at which bits can flow from source to destination is again the minimum of R and R , that is, throughput  min{R , R}.
Therefore, the constraining factor for throughput in today’s Internet is typically the access network.
For a final example, consider Figure 1.20(b) in which there are 10 servers and 10 clients connected to the core of the computer network.
In this example, there are 10 simultaneous downloads taking place, involving 10 client-server pairs.
Suppose that these 10 downloads are the only traffic in the network atthe current time.
As shown in the figure, there is a link in the core that is traversed by all 10 downloads.
Denote R for the transmission rate of this link R. Let’s suppose that all server access links have the same rate R , all client access links have the same rate R, and the transmission rates of all the links in the core—except the one common link of rate R—are much larger than R, R, and R. Now we ask, what are the throughputs of the downloads?
Clearly, if the rate of the common link, R, is large—say a hundred times larger than both R and R —then the throughput for each download will once again be min{R , R}.
But what if the rate of the common link is of the same order as R  and R ?
What will the throughput be in this case?
Let’s take a look at a specific example.
Suppose  Mbps,  Mbps,  Mbps, and thes c s c s c = s c s c s c s c s c s c Rs=2 Rc=1 R=5
Figure 1.20 End-to-end throughput: (a) Client downloads a file from ­server; (b) 10 clients downloading with 10 servers common link divides its transmission rate equally among the 10 downloads.
Then the bottleneck for each download is no longer in the access network, but is now instead the shared link in the core, whichonly provides each download with 500 kbps of throughput.
Thus the end-to-end throughput for eachdownload is now reduced to 500 kbps.
The examples in Figure 1.19 and Figure 1.20(a) show that throughput depends on the transmission rates of the links over which the data flows.
We saw that when there is no other intervening traffic, the throughput can simply be approximated as the minimum transmission rate along the path between source and destination.
The example in Figure 1.20(b) shows that more generally the throughput depends not only on the transmission rates of the links along the path, but also on the intervening traffic.
In particular, a link with a high transmission rate may nonetheless be the bottleneck link for a file transferif many other data flows are also passing through that link.
We will examine throughput in computernetworks more closely in the homework problems and in the subsequent chapters.
1.5 Protocol Layers and Their Service Models From our discussion thus far, it is apparent that the Internet is an extremely complicated system.
We have seen that there are many pieces to the Internet: numerous applications and protocols, various types of end systems, packet switches, and various types of link-level media.
Given this enormouscomplexity, is there any hope of organizing a network architecture, or at least our discussion of network architecture?
Fortunately, the answer to both questions is yes.
1.5.1 Layered Architecture Before attempting to organize our thoughts on Internet architecture, let’s look for a human analogy.
Actually, we deal with complex systems all the time in our everyday life.
Imagine if someone asked youto describe, for example, the airline system.
How would you find the structure to describe this complexsystem that has ticketing agents, baggage checkers, gate personnel, pilots, airplanes, air traffic control,and a worldwide system for routing airplanes?
One way to describe this system might be to describe the series of actions you take (or others take for you) when you fly on an airline.
You purchase your ticket, check your bags, go to the gate, and eventually get loaded onto the plane.
The plane takes off and isrouted to its destination.
After your plane lands, you deplane at the gate and claim your bags.
If the tripwas bad, you complain about the flight to the ticket agent (getting nothing for your effort).
This scenario is shown in Figure 1.21.
Figure 1.21 Taking an airplane trip: actions
Figure 1.22 Horizontal layering of airline functionality Already, we can see some analogies here with computer networking: You are being shipped from source to destination by the airline; a packet is shipped from source host to destination host in the Internet.
But this is not quite the analogy we are after.
We are looking for some structure in Figure 1.21.
Looking at Figure 1.21, we note that there is a ticketing function at each end; there is also a baggage function for already-ticketed passengers, and a gate function for already-ticketed and already-baggage- checked passengers.
For passengers who have made it through the gate (that is, passengers who arealready ticketed, baggage-checked, and through the gate), there is a takeoff and landing function, andwhile in flight, there is an airplane-routing function.
This suggests that we can look at the functionality in Figure 1.21 in a horizontal  manner, as shown in Figure 1.22.
Figure 1.22 has divided the airline functionality into layers, providing a framework in which we can discuss airline travel.
Note that each layer, combined with the layers below it, implements some functionality, some service.
At the ticketing layer and below, airline-counter-to-airline-counter transfer of a person is accomplished.
At the baggage layer and below, baggage-check-to-baggage-claim transfer of a person and bags is accomplished.
Note that the baggage layer provides this service only to analready-ticketed person.
At the gate layer, departure-gate-to-arrival-gate transfer of a person and bagsis accomplished.
At the takeoff/landing layer, runway-to-runway transfer of people and their bags isaccomplished.
Each layer provides its service by (1) performing certain actions within that layer (for example, at the gate layer, loading and unloading people from an airplane) and by (2) using the services of the layer directly below it (for example, in the gate layer, using the runway-to-runway passengertransfer service of the takeoff/landing layer).
A layered architecture allows us to discuss a well-defined, specific part of a large and complex system.
This simplification itself is of considerable value by providing modularity, making it much easier tochange the implementation of the service provided by the layer.
As long as the layer provides the sameservice to the layer above it, and uses the same services from the layer below it, the remainder of the system remains unchanged when a layer’s implementation is changed. (
Note that changing the
implementation of a service is very different from changing the service itself!)
For example, if the gate functions were changed (for instance, to have people board and disembark by height), the remainder of the airline system would remain unchanged since the gate layer still provides the same function (loadingand unloading people); it simply implements that function in a different manner after the change.
Forlarge and complex systems that are constantly being updated, the ability to change the implementationof a service without affecting other components of the system is another important advantage of layering.
Protocol Layering But enough about airlines.
Let’s now turn our attention to network protocols.
To provide structure to the design of network protocols, network designers organize protocols—and the network hardware andsoftware that implement the protocols—in layers .
Each protocol belongs to one of the layers, just as each function in the airline architecture in Figure 1.22 belonged to a layer.
We are again interested in the services  that a layer offers to the layer above—the so-called service model of a layer.
Just as in the case of our airline example, each layer provides its service by (1) performing certain actions withinthat layer and by (2) using the services of the layer directly below it.
For example, the services provided by layer n may include reliable delivery of messages from one edge of the network to the other.
This might be implemented by using an unreliable edge-to-edge message delivery service of layer , and adding layer n functionality to detect and retransmit lost messages.
A protocol layer can be implemented in software, in hardware, or in a combination of the two.
Application-layer protocols—such as HTTP and SMTP—are almost always implemented in software in the end systems; so are transport-layer protocols.
Because the physical layer and data link layers are responsible for handling communication over a specific link, they are typically implemented in a networkinterface card (for example, Ethernet or WiFi interface cards) associated with a given link.
The networklayer is often a mixed implementation of hardware and software.
Also note that just as the functions inthe layered airline architecture were distributed among the various airports and flight control centers that make up the system, so too is a layer n protocol distributed  among the end systems, packet switches, and other components that make up the network.
That is, there’s often a piece of a layer n protocol in each of these network components.
Protocol layering has conceptual and structural advantages [RFC 3439] .
As we have seen, layering provides a structured way to discuss system components.
Modularity makes it easier to update system components.
We mention, however, that some researchers and networking engineers are vehemently opposed to layering [Wakeman 1992] .
One potential drawback of layering is that one layer may duplicate lower-layer functionality.
For example, many protocol stacks provide error recoveryn−1
Figure 1.23 The Internet protocol stack (a) and OSI reference model (b) on both a per-link basis and an end-to-end basis.
A second potential drawback is that functionality at one layer may need information (for example, a timestamp value) that is present only in another layer; this violates the goal of separation of layers.
When taken together, the protocols of the various layers are called the protocol stack.
The Internet protocol stack consists of five layers: the physical, link, network, transport, and application layers, as shown in Figure 1.23(a).
If you examine the Table of Contents, you will see that we have roughly organized this book using the layers of the Internet protocol stack.
We take a top-down approach , first covering the application layer and then proceeding downward.
Application Layer The application layer is where network applications and their application-layer protocols reside.
The Internet’s application layer includes many protocols, such as the HTTP protocol (which provides for Webdocument request and transfer), SMTP (which provides for the transfer of e-mail messages), and FTP(which provides for the transfer of files between two end systems).
We’ll see that certain network functions, such as the translation of human-friendly names for Internet end systems like www.ietf.org  to a 32-bit network address, are also done with the help of a specific application-layer protocol, namely, thedomain name system (DNS).
We’ll see in Chapter 2 that it is very easy to create and deploy our own new application-layer protocols.
An application-layer protocol is distributed over multiple end systems, with the application in one end system using the protocol to exchange packets of information with the application in another endsystem.
We’ll refer to this packet of information at the application layer as a message .
Transport Layer
The Internet’s transport layer transports application-layer messages between application endpoints.
In the Internet there are two transport protocols, TCP and UDP, either of which can transport application-layer messages.
TCP provides a ­connection-oriented service to its applications.
This service includes guaranteed delivery of application-layer messages to the destination and flow control (that is,sender/receiver speed matching).
TCP also breaks long messages into shorter ­segments and provides a congestion-control mechanism, so that a source throttles its transmission rate when the network iscongested.
The UDP protocol provides a connectionless service to its applications.
This is a no-frillsservice that provides no reliability, no flow control, and no congestion control.
In this book, we’ll refer toa transport-layer packet as a segment.
Network Layer The Internet’s network layer is responsible for moving network-layer packets known as datagrams  from one host to another.
The Internet transport-layer protocol (TCP or UDP) in a source host passes a transport-layer segment and a destination address to the network layer, just as you would give the postal service a letter with a destination address.
The network layer then provides the service ofdelivering the segment to the transport layer in the destination host.
The Internet’s network layer includes the celebrated IP protocol, which defines the fields in the datagram as well as how the end systems and routers act on these fields.
There is only one IP protocol, and allInternet components that have a network layer must run the IP protocol.
The Internet’s network layeralso contains routing protocols that determine the routes that datagrams take between sources and destinations.
The Internet has many routing protocols.
As we saw in Section 1.3, the Internet is a network of networks, and within a network, the network administrator can run any routing protocol desired.
Although the network layer contains both the IP protocol and numerous routing protocols, it isoften simply referred to as the IP layer, reflecting the fact that IP is the glue that binds the Internettogether.
Link Layer The Internet’s network layer routes a datagram through a series of routers between the source and destination.
To move a packet from one node (host or router) to the next node in the route, the network layer relies on the services of the link layer.
In particular, at each node, the network layer passes thedatagram down to the link layer, which delivers the datagram to the next node along the route.
At thisnext node, the link layer passes the datagram up to the network layer.
The services provided by the link layer depend on the specific link-layer protocol that is employed over the link.
For example, some link-layer protocols provide reliable delivery, from transmitting node, over one link, to receiving node.
Note that this reliable delivery service is different from the reliable deliveryservice of TCP, which provides reliable delivery from one end system to another.
Examples of link-layer
protocols include Ethernet, WiFi, and the cable access network’s DOCSIS protocol.
As datagrams typically need to traverse several links to travel from source to destination, a datagram may be handled by different link-layer protocols at different links along its route.
For example, a datagram may behandled by Ethernet on one link and by PPP on the next link.
The network layer will receive a differentservice from each of the different link-layer protocols.
In this book, we’ll refer to the link-layer packets asframes .
Physical Layer While the job of the link layer is to move entire frames from one network element to an adjacent network element, the job of the physical layer is to move the individual bits  within the frame from one node to the next.
The protocols in this layer are again link dependent and further depend on the actual transmission medium of the link (for example, twisted-pair copper wire, single-mode fiber optics).
For example,Ethernet has many physical-layer protocols: one for twisted-pair copper wire, another for coaxial cable,another for fiber, and so on.
In each case, a bit is moved across the link in a different way.
The OSI Model Having discussed the Internet protocol stack in detail, we should mention that it is not the only protocol stack around.
In particular, back in the late 1970s, the International Organization for Standardization(ISO) proposed that computer networks be organized around seven layers, called the Open Systems Interconnection (OSI) model [ISO 2016].
The OSI model took shape when the protocols that were to become the Internet protocols were in their infancy, and were but one of many different protocol suites under development; in fact, the inventors of the original OSI model probably did not have the Internet in mind when creating it.
Nevertheless, beginning in the late 1970s, many training and university courses picked up on the ISO mandate and organized courses around the seven-layer model.
Because of itsearly impact on networking education, the seven-layer model continues to linger on in some networkingtextbooks and training courses.
The seven layers of the OSI reference model, shown in Figure 1.23(b), are: application layer, presentation layer, session layer, transport layer, network layer, data link layer, and physical layer.
The functionality of five of these layers is roughly the same as their similarly named Internet counterparts.
Thus, let’s consider the two additional layers present in the OSI reference model—the presentation layer and the session layer.
The role of the presentation layer is to provide services that allow communicatingapplications to interpret the meaning of data exchanged.
These services include data compression and data encryption (which are self-explanatory) as well as data description (which frees the applicationsfrom having to worry about the internal format in which data are represented/stored—formats that maydiffer from one computer to another).
The session layer provides for delimiting and synchronization ofdata exchange, including the means to build a checkpointing and recovery scheme.
The fact that the Internet lacks two layers found in the OSI reference model poses a couple of interesting questions: Are the services provided by these layers unimportant?
What if an application needs  one of these services?
The Internet’s answer to both of these questions is the same—it’s up to the application developer.
It’s up to the application developer to decide if a service is important, and ifthe service is important, it’s up to the application developer to build that functionality into the application.
1.5.2 Encapsulation Figure 1.24 shows the physical path that data takes down a sending end system’s protocol stack, up and down the protocol stacks of an intervening link-layer switch Figure 1.24 Hosts, routers, and link-layer switches; each contains a ­different set of layers, reflecting their differences in ­functionality and router, and then up the protocol stack at the receiving end system.
As we discuss later in this book, routers and link-layer switches are both packet switches.
Similar to end systems, routers and link-layerswitches organize their networking hardware and software into layers.
But routers and link-layer switches do not implement all of the layers in the protocol stack; they typically implement only the bottom layers.
As shown in Figure 1.24, link-layer switches implement layers 1 and 2; routers implement layers 1 through 3.
This means, for example, that Internet routers are capable of implementing the IP protocol (a layer 3 protocol), while link-layer switches are not.
We’ll see later that
while link-layer switches do not recognize IP addresses, they are capable of recognizing layer 2 addresses, such as Ethernet addresses.
Note that hosts implement all five layers; this is consistent with the view that the Internet architecture puts much of its complexity at the edges of the network.
Figure 1.24 also illustrates the important concept of encapsulation .
At the sending host, an application-layer message  (M in Figure 1.24) is passed to the transport layer.
In the simplest case, the transport layer takes the message and appends additional information (so-called transport-layer header information, H  in Figure 1.24) that will be used by the receiver-side transport layer.
The application-layer message and the transport-layer header information together constitute the transport- layer segment .
The transport-layer segment thus encapsulates the application-layer message.
The added information might include information allowing the receiver-side transport layer to deliver the message up to the appropriate application, and error-detection bits that allow the receiver to determine whether bits in the message have been changed in route.
The transport layer then passes the segment to the network layer, which adds network-layer header information (H  in Figure 1.24) such as source and destination end system addresses, creating a network-layer datagram .
The datagram is then passed to the link layer, which (of course!)
will add its own link-layer header information and create a link-layer frame .
Thus, we see that at each layer, a packet has two types of fields: header fields and a payload field .
The payload is typically a packet from the layer above.
A useful analogy here is the sending of an interoffice memo from one corporate branch office to anothervia the public postal service.
Suppose Alice, who is in one branch office, wants to send a memo to Bob, who is in another branch office.
The memo is analogous to the application-layer message .
Alice puts the memo in an interoffice envelope with Bob’s name and department written on the front of the envelope.
The interoffice envelope  is analogous to a transport-layer segment —it contains header information (Bob’s name and department number) and it encapsulates the application-layer message (the memo).
When the sending branch-office mailroom receives the interoffice envelope, it puts the interoffice envelope inside yet another envelope, which is suitable for sending through the public postal service.
The sending mailroom also writes the postal address of the sending and receiving branch offices on the postal envelope.
Here, the postal envelope  is analogous to the datagram —it encapsulates the transport- layer segment (the interoffice envelope), which encapsulates the original message (the memo).
The postal service delivers the postal envelope to the receiving branch-office mailroom.
There, the process of de-encapsulation is begun.
The mailroom extracts the interoffice memo and forwards it to Bob.
Finally, Bob opens the envelope and removes the memo.
The process of encapsulation can be more complex than that described above.
For example, a large message may be divided into multiple transport-layer segments (which might themselves each be divided into multiple network-layer datagrams).
At the receiving end, such a segment must then bereconstructed from its constituent datagrams.t n
1.6 Networks Under Attack The Internet has become mission critical for many institutions today, including large and small companies, universities, and government agencies.
Many individuals also rely on the Internet for manyof their professional, social, and personal activities.
Billions of “things,” including wearables and homedevices, are currently being connected to the Internet.
But behind all this utility and excitement, there isa dark side, a side where “bad guys” attempt to wreak havoc in our daily lives by damaging our Internet- connected computers, violating our privacy, and rendering inoperable the Internet services on which we depend.
The field of network security is about how the bad guys can attack computer networks and about how we, soon-to-be experts in computer networking, can defend networks against those attacks, or betteryet, design new architectures that are immune to such attacks in the first place.
Given the frequency andvariety of existing attacks as well as the threat of new and more destructive future attacks, networksecurity has become a central topic in the field of computer networking.
One of the features of this textbook is that it brings network security issues to the forefront.
Since we don’t yet have expertise in computer networking and Internet protocols, we’ll begin here by surveying some of today’s more prevalent security-related problems.
This will whet our appetite for moresubstantial discussions in the upcoming chapters.
So we begin here by simply asking, what can gowrong?
How are computer networks vulnerable?
What are some of the more prevalent types of attackstoday?
The Bad Guys Can Put Malware into Your Host Via the Internet We attach devices to the Internet because we want to receive/send data from/to the Internet.
This includes all kinds of good stuff, including Instagram posts, Internet search results, streaming music,video conference calls, streaming movies, and so on.
But, unfortunately, along with all that good stuffcomes malicious stuff— ­collectively known as malware—that can also enter and infect our devices.
Once malware infects our device it can do all kinds of devious things, including deleting our files andinstalling spyware that collects our private information, such as social security numbers, passwords, and keystrokes, and then sends this (over the Internet, of course!)
back to the bad guys.
Our compromisedhost may also be enrolled in a network of thousands of similarly compromised devices, collectively known as a botnet, which the bad guys control and leverage for spam e-mail distribution or distributed denial-of-service attacks (soon to be discussed) against targeted hosts.
Much of the malware out there today is self-replicating: once it infects one host, from that host it seeks entry into other hosts over the Internet, and from ­the newly infected hosts, it seeks entry into yet more hosts.
In this manner, self-­replicating malware can spread exponentially fast.
Malware can spread inthe form of a virus or a worm.
Viruses are malware that require some form of user interaction to infectthe user’s device.
The classic example is an e-mail attachment containing malicious executable code.
Ifa user receives and opens such an attachment, the user inadvertently runs the malware on the device.
Typically, such e-mail viruses are self-replicating: once executed, the virus may send an identical message with an identical malicious attachment to, for example, every recipient in the user’s addressbook.
Worms are malware that can enter a device without any explicit user interaction.
For example, auser may be running a vulnerable network application to which an attacker can send malware.
In somecases, without any user intervention, the application may accept the malware from the Internet and runit, creating a worm.
The worm in the newly infected device then scans the Internet, searching for otherhosts running the same vulnerable network application.
When it finds other vulnerable hosts, it sends acopy of itself to those hosts.
Today, malware, is pervasive and costly to defend against.
As you work through this textbook, we encourage you to think about the following question: What can computer network designers do to defend Internet-attached devices from malware attacks?
The Bad Guys Can Attack Servers and Network Infrastructure Another broad class of security threats are known as denial-of-service (DoS) attacks .
As the name suggests, a DoS attack renders a network, host, or other piece of infrastructure unusable by legitimate users.
Web servers, e-mail servers, DNS servers (discussed in Chapter 2), and institutional networks can all be subject to DoS attacks.
Internet DoS attacks are extremely common, with thousands of DoSattacks occurring every year [Moore 2001].
The site Digital Attack Map allows use to visualize the top daily DoS attacks worldwide [DAM 2016] .
Most Internet DoS attacks fall into one of three categories: Vulnerability attack.
This involves sending a few well-crafted messages to a vulnerable application or operating system running on a targeted host.
If the right sequence of packets is sent to a vulnerable application or operating system, the service can stop or, worse, the host can crash.
Bandwidth flooding.
 The attacker sends a deluge of packets to the targeted host—so many packets that the target’s access link becomes clogged, preventing legitimate packets from reachingthe server.
Connection flooding.
 The attacker establishes a large number of half-open or fully open TCP connections (TCP connections are discussed in Chapter 3) at the target host.
The host can become so bogged down with these bogus connections that it stops accepting legitimate connections.
Let’s now explore the bandwidth-flooding attack in more detail.
Recalling our delay and loss analysis discussion in Section 1.4.2, it’s evident that if the server has an access rate of R bps, then the attacker will need to send traffic at a rate of approximately R bps to cause damage.
If R is very large, a single attack source may not be able to generate enough traffic to harm the server.
Furthermore, if all the
traffic emanates from a single source, an upstream router may be able to detect the attack and block all traffic from that source before the traffic gets near the server.
In a distributed DoS (DDoS)  attack, illustrated in Figure 1.25, the attacker controls multiple sources and has each source blast traffic at the target.
With this approach, the aggregate traffic rate across all the controlled sources needs to be approximately R to cripple the ­service.
DDoS attacks leveraging botnets with thousands of comprised hosts are a common occurrence today [DAM 2016] .
DDos attacks are much harder to detect and defend against than a DoS attack from a single host.
We encourage you to consider the following question as you work your way through this book: What can computer network designers do to defend against DoS attacks?
We will see that different defenses areneeded for the three types of DoS attacks.
Figure 1.25 A distributed denial-of-service attack The Bad Guys Can Sniff Packets Many users today access the Internet via wireless devices, such as WiFi-connected laptops or handheld devices with cellular Internet connections (covered in Chapter 7).
While ubiquitous Internet access is extremely convenient and enables marvelous new applications for mobile users, it also creates a major security vulnerability—by placing a passive receiver in the vicinity of the wireless transmitter, thatreceiver can obtain a copy of every packet that is transmitted!
These packets can contain all kinds ofsensitive information, including passwords, social security numbers, trade secrets, and private personalmessages.
A passive receiver that records a copy of every packet that flies by is called a packet sniffer.
Sniffers can be deployed in wired environments as well.
In wired broadcast environments, as in many Ethernet LANs, a packet sniffer can obtain copies of broadcast packets sent over the LAN.
As described in Section 1.2, cable access technologies also broadcast packets and are thus vulnerable to sniffing.
Furthermore, a bad guy who gains access to an institution’s access router or access link to the Internet may be able to plant a sniffer that makes a copy of every packet going to/from the organization.
Sniffedpackets can then be analyzed offline for sensitive information.
Packet-sniffing software is freely available at various Web sites and as commercial products.
Professors teaching a networking course have been known to assign lab exercises that involve writing a packet- sniffing and application-layer data reconstruction program.
Indeed, the Wireshark [Wireshark 2016] labs associated with this text (see the introductory Wireshark lab at the end of this chapter) use exactly such a packet sniffer!
Because packet sniffers are passive—that is, they do not inject packets into the channel—they are difficult to detect.
So, when we send packets into a wireless channel, we must accept the possibility that some bad guy may be recording copies of our packets.
As you may have guessed, some of the bestdefenses against packet sniffing involve cryptography.
We will examine cryptography as it applies to network security in Chapter 8.
The Bad Guys Can Masquerade as Someone You Trust It is surprisingly easy ( you will have the knowledge to do so shortly as you proceed through this text!)
to create a packet with an arbitrary source address, packet content, and destination address and then transmit this hand-crafted packet into the Internet, which will dutifully forward the packet to its destination.
Imagine the unsuspecting receiver (say an Internet router) who receives such a packet,takes the (false) source address as being truthful, and then performs some command embedded in thepacket’s contents (say modifies its forwarding table).
The ability to inject packets into the Internet with afalse source address is known as IP spoofing , and is but one of many ways in which one user can masquerade as another user.
To solve this problem, we will need end-point authentication,  that is, a mechanism that will allow us to determine with certainty if a message originates from where we think it does.
Once again, we encourage you to think about how this can be done for network applications and protocols as you progress through the chapters of this book.
We will explore mechanisms for end-point authentication in Chapter 8.
In closing this section, it’s worth considering how the Internet got to be such an insecure place in the first place.
The answer, in essence, is that the Internet was originally designed to be that way, based on the model of “a group of mutually trusting users attached to a transparent network” [Blumenthal 2001]—a model in which (by definition) there is no need for security.
Many aspects of the original Internet architecture deeply reflect this notion of mutual trust.
For example, the ability for one user to send a
packet to any other user is the default rather than a requested/granted capability, and user identity is taken at declared face value, rather than being authenticated by default.
But today’s Internet certainly does not involve “mutually trusting users.”
Nonetheless, today’s users still need to communicate when they don’t necessarily trust each other, may wish to communicateanonymously, may communicate indirectly through third parties (e.g., Web caches, which we’ll study in Chapter 2, or mobility-assisting agents, which we’ll study in Chapter 7), and may distrust the hardware, software, and even the air through which they communicate.
We now have many security-related challenges before us as we progress through this book: We should seek defenses against sniffing, end-point masquerading, man-in-the-middle attacks, DDoS attacks, malware, and more.
We should keep inmind that communication among mutually trusted users is the exception rather than the rule.
Welcometo the world of modern computer networking!
1.7 History of Computer Networking and the Internet Sections 1.1 through 1.6 presented an overview of the technology of computer networking and the Internet.
You should know enough now to impress your family and friends!
However, if you really want to be a big hit at the next cocktail party, you should sprinkle your discourse with tidbits about the fascinating history of the Internet [Segaller 1998].
1.7.1 The Development of Packet Switching: 1961–1972 The field of computer networking and today’s Internet trace their beginnings back to the early 1960s,when the telephone network was the world’s dominant communication network.
Recall from Section 1.3 that the telephone network uses circuit switching to transmit information from a sender to a receiver—an appropriate choice given that voice is transmitted at a constant rate between sender and receiver.
Giventhe increasing importance of computers in the early 1960s and the advent of timeshared computers, it was perhaps natural to consider how to hook computers together so that they could be shared among geographically distributed users.
The traffic generated by such users was likely to be bursty—intervals of activity, such as the sending of a command to a remote computer, followed by periods of inactivity while waiting for a reply or while contemplating the received response.
Three research groups around the world, each unaware of the others’ work [Leiner 1998], began inventing packet switching as an efficient and robust alternative to circuit switching.
The first published work on packet-switching techniques was that of Leonard Kleinrock [Kleinrock 1961 ; Kleinrock 1964], then a graduate student at MIT.
Using queuing theory, Kleinrock’s work elegantly demonstrated theeffectiveness of the packet-switching approach for bursty traffic sources.
In 1964, Paul Baran [Baran 1964]  at the Rand Institute had begun investigating the use of packet switching for secure voice over military networks, and at the National Physical Laboratory in England, Donald Davies and Roger Scantlebury were also developing their ideas on packet switching.
The work at MIT, Rand, and the NPL laid the foundations for today’s Internet.
But the Internet also has a long history of a let’s-build-it-and-demonstrate-it attitude that also dates back to the 1960s.
J. C. R. Licklider [DEC 1990]  and Lawrence Roberts, both colleagues of Kleinrock’s at MIT, went on to lead the computer science program at the Advanced Research Projects Agency (ARPA) in the United States.
Roberts published an overall plan for the ARPAnet [Roberts 1967] , the first packet-switched computer network and a direct ancestor of today’s public Internet.
On Labor Day in 1969, the first packet switch was installed at UCLA under Kleinrock’s supervision, and three additional packet switches were installed
shortly thereafter at the Stanford Research Institute (SRI), UC Santa Barbara, and the University of Utah (Figure 1.26).
The fledgling precursor to the Internet was four nodes large by the end of 1969.
Kleinrock recalls the very first use of the network to perform a remote login from UCLA to SRI, crashing the system [Kleinrock 2004] .
By 1972, ARPAnet had grown to approximately 15 nodes and was given its first public demonstration by Robert Kahn.
The first host-to-host protocol between ARPAnet end systems, known as the network- control protocol (NCP), was completed [RFC 001].
With an end-to-end protocol available, applications could now be written.
Ray Tomlinson wrote the first e-mail program in 1972.
1.7.2 Proprietary Networks and Internetworking: 1972–1980 The initial ARPAnet was a single, closed network.
In order to communicate with an ARPAnet host, one had to be actually attached to another ARPAnet IMP.
In the early to mid-1970s, additional stand-alonepacket-switching networks besides ARPAnet came into being: ALOHANet, a microwave network linking universities on the Hawaiian islands [Abramson 1970] , as well as DARPA’s packet-satellite [RFC 829]
Figure 1.26 An early packet switch and packet-radio networks [Kahn 1978]; Telenet, a BBN commercial packet- ­switching network based on ARPAnet technology; Cyclades, a French packet-switching network pioneered by Louis Pouzin [Think 2012]; Time-sharing networks such as Tymnet and the GE Information Services network, among others, in the late 1960s and early 1970s [Schwartz 1977] ; IBM’s SNA (1969–1974), which paralleled the ARPAnet work [Schwartz 1977] .
The number of networks was growing.
With perfect hindsight we can see that the time was ripe for developing an encompassing architecture for connecting networks together.
Pioneering work on interconnecting networks (under the sponsorship of the Defense Advanced Research Projects Agency (DARPA)), in essence creating a network of networks, was done by Vinton Cerf and Robert Kahn [Cerf 1974] ; the term internetting  was coined to describe this work.
These architectural principles were embodied in TCP.
The early versions of TCP, however, were quite different from today’s TCP.
The early versions of TCP combined a reliable in-sequence delivery of datavia end-system retransmission (still part of today’s TCP) with forwarding functions (which today areperformed by IP).
Early experimentation with TCP, combined with the recognition of the importance ofan unreliable, non-flow-controlled, end-to-end transport service for applications such as packetizedvoice, led to the separation of IP out of TCP and the development of the UDP protocol.
The three keyInternet protocols that we see today—TCP, UDP, and IP—were conceptually in place by the end of the1970s.
In addition to the DARPA Internet-related research, many other important networking activities were underway.
In Hawaii, Norman Abramson was developing ALOHAnet, a packet-based radio network thatallowed multiple remote sites on the Hawaiian Islands to communicate with each other.
The ALOHA protocol [Abramson 1970]  was the first multiple-access protocol, allowing geographically distributed users to share a single broadcast communication medium (a radio ­frequency).
Metcalfe and Boggs built on Abramson’s multiple-access protocol work when they developed the Ethernet protocol [Metcalfe 1976]  for wire-based shared broadcast networks.
Interestingly, Metcalfe and Boggs’ Ethernet protocol was motivated by the need to connect multiple PCs, printers, and shared disks [Perkins 1994] .
Twenty- five years ago, well before the PC revolution and the explosion of networks, Metcalfe and Boggs were laying the foundation for today’s PC LANs.
1.7.3 A Proliferation of Networks: 1980–1990 By the end of the 1970s, approximately two hundred hosts were connected to the ARPAnet.
By the endof the 1980s the number of hosts connected to the public ­Internet, a confederation of networks looking much like today’s Internet, would reach a hundred thousand.
The 1980s would be a time of tremendousgrowth.
Much of that growth resulted from several distinct efforts to create computer networks linking universities together.
BITNET provided e-mail and file transfers among several universities in the Northeast.
CSNET(computer science network) was formed to link university researchers who did not have access toARPAnet.
In 1986, NSFNET was created to provide access to NSF-sponsored supercomputing centers.
Starting with an initial backbone speed of 56 kbps, NSFNET’s backbone would be running at 1.5 Mbpsby the end of the decade and would serve as a primary backbone linking regional networks.
In the ARPAnet community, many of the final pieces of today’s Internet architecture were falling into place.
January 1, 1983 saw the official deployment of TCP/IP as the new standard host protocol for ARPAnet (replacing the NCP protocol).
The transition [RFC 801] from NCP to TCP/IP was a flag day event—all hosts were required to transfer over to TCP/IP as of that day.
In the late 1980s, importantextensions were made to TCP to implement host-based congestion control [Jacobson 1988].
The DNS, used to map between a human-readable Internet name (for example, gaia.cs.umass.edu) and its 32-bitIP address, was also developed [RFC 1034] .
Paralleling this development of the ARPAnet (which was for the most part a US effort), in the early 1980s the French launched the Minitel project, an ambitious plan to bring data networking intoeveryone’s home.
Sponsored by the French government, the Minitel system consisted of a publicpacket-switched network (based on the X.25 protocol suite), Minitel servers, and inexpensive terminalswith built-in low-speed modems.
The Minitel became a huge success in 1984 when the French government gave away a free Minitel terminal to each French household that wanted one.
Minitel sites included free sites—such as a telephone directory site—as well as private sites, which collected ausage-based fee from each user.
At its peak in the mid 1990s, it offered more than 20,000 services,ranging from home banking to specialized research databases.
The Minitel was in a large proportion ofFrench homes 10 years before most Americans had ever heard of the Internet.
1.7.4 The Internet Explosion: The 1990s The 1990s were ushered in with a number of events that symbolized the continued evolution and thesoon-to-arrive commercialization of the Internet.
ARPAnet, the progenitor of the Internet, ceased toexist.
In 1991, NSFNET lifted its restrictions on the use of NSFNET for commercial purposes.
NSFNETitself would be decommissioned in 1995, with Internet backbone traffic being carried by commercialInternet Service Providers.
The main event of the 1990s was to be the emergence of the World Wide Web application, which brought the Internet into the homes and businesses of millions of people worldwide.
The Web served asa platform for enabling and deploying hundreds of new applications that we take for granted today,including search (e.g., Google and Bing) Internet commerce (e.g., Amazon and eBay) and socialnetworks (e.g., Facebook).
The Web was invented at CERN by Tim Berners-Lee between 1989 and 1991 [Berners-Lee 1989] , based on ideas originating in earlier work on hypertext from the 1940s by Vannevar Bush [Bush 1945] and since the 1960s by Ted Nelson [Xanadu 2012].
Berners-Lee and his associates developed initial versions of HTML, HTTP, a Web server, and a browser—the four key components of the Web.
Around the end of 1993 there were about two hundred Web servers in operation, this collection of servers being
just a harbinger of what was about to come.
At about this time several researchers were developing Web browsers with GUI interfaces, including Marc Andreessen, who along with Jim Clark, formed Mosaic Communications, which later became Netscape Communications Corporation [Cusumano 1998 ; Quittner 1998].
By 1995, university students were using Netscape browsers to surf the Web on a daily basis.
At about this time companies—big and small—began to operate Web servers and transact commerce over the Web.
In 1996, Microsoft started to make browsers, which started the browser war between Netscape and Microsoft, which Microsoft won a few years later [Cusumano 1998].
The second half of the 1990s was a period of tremendous growth and innovation for the Internet, with major corporations and thousands of startups creating Internet products and services.
By the end of themillennium the Internet was supporting hundreds of popular applications, including four killerapplications: E-mail, including attachments and Web-accessible e-mail The Web, including Web browsing and Internet commerce Instant messaging, with contact lists Peer-to-peer file sharing of MP3s, pioneered by Napster Interestingly, the first two killer applications came from the research community, whereas the last twowere created by a few young entrepreneurs.
The period from 1995 to 2001 was a roller-coaster ride for the Internet in the financial markets.
Before they were even profitable, hundreds of Internet startups made initial public offerings and started to betraded in a stock market.
Many companies were valued in the billions of dollars without having anysignificant revenue streams.
The Internet stocks collapsed in 2000–2001, and many startups shut down.
Nevertheless, a number of companies emerged as big winners in the Internet space, includingMicrosoft, Cisco, Yahoo, e-Bay, Google, and Amazon.
1.7.5 The New Millennium Innovation in computer networking continues at a rapid pace.
Advances are being made on all fronts,including deployments of faster routers and higher transmission speeds in both access networks and innetwork backbones.
But the following developments merit special attention: Since the beginning of the millennium, we have been seeing aggressive deployment of broadband Internet access to homes—not only cable modems and DSL but also fiber to the home, as discussed in Section 1.2.
This high-speed Internet access has set the stage for a wealth of video applications, including the distribution of user-generated video (for example, YouTube), on-demand streaming of movies and television shows (e.g., Netflix), and multi-person video conference (e.g., Skype,
Facetime, and Google Hangouts).
The increasing ubiquity of high-speed (54 Mbps and higher) public WiFi networks and medium- speed (tens of Mbps) Internet access via 4G cellular telephony networks is not only making it possible to remain constantly connected while on the move, but also enabling new location-specificapplications such as Yelp, Tinder, Yik Yak, and Waz.
The number of wireless devices connecting to the Internet surpassed the number of wired devices in 2011.
This high-speed wireless access has set the stage for the rapid emergence of hand-held computers (iPhones, Androids, iPads, and so on), which enjoy constant and untethered access to the Internet.
Online social networks—such as Facebook, Instagram, Twitter, and WeChat (hugely popular in China)—have created massive people networks on top of the Internet.
Many of these social networks are extensively used for messaging as well as photo sharing.
Many Internet users today“live” primarily within one or more social networks.
Through their APIs, the online social networks create platforms for new networked applications and distributed games.
As discussed in Section 1.3.3, online service providers, such as Google and Microsoft, have deployed their own extensive private networks, which not only connect together their globally distributed data centers, but are used to bypass the Internet as much as possible by peering directlywith lower-tier ISPs.
As a result, Google provides search results and e-mail access almost instantaneously, as if their data centers were running within one’s own computer.
Many Internet commerce companies are now running their applications in the “cloud”—such as in Amazon’s EC2, in Google’s Application Engine, or in Microsoft’s Azure.
Many companies and universities have also migrated their Internet applications (e.g., e-mail and Web hosting) to thecloud.
Cloud companies not only provide applications scalable computing and storage environments, but also provide the applications implicit access to their high-performance private networks.
1.8 Summary In this chapter we’ve covered a tremendous amount of material!
We’ve looked at the various pieces of hardware and software that make up the Internet in particular and computer networks in general.
Westarted at the edge of the network, looking at end systems and applications, and at the transport serviceprovided to the applications running on the end systems.
We also looked at the link-layer technologiesand physical media typically found in the access network.
We then dove deeper inside the network, into the network core, identifying packet switching and circuit switching as the two basic approaches for transporting data through a telecommunication network, and we examined the strengths andweaknesses of each approach.
We also examined the structure of the global Internet, learning that theInternet is a network of networks.
We saw that the Internet’s hierarchical structure, consisting of higher-and lower-tier ISPs, has allowed it to scale to include thousands of networks.
In the second part of this introductory chapter, we examined several topics central to the field of computer networking.
We first examined the causes of delay, throughput and packet loss in a packet- switched network.
We developed simple quantitative models for transmission, propagation, and queuing delays as well as for throughput; we’ll make extensive use of these delay models in the homework problems throughout this book.
Next we examined protocol layering and service models, keyarchitectural principles in networking that we will also refer back to throughout this book.
We alsosurveyed some of the more prevalent security attacks in the Internet day.
We finished our introduction tonetworking with a brief history of computer networking.
The first chapter in itself constitutes a mini-course in computer networking.
So, we have indeed covered a tremendous amount of ground in this first chapter!
If you’re a bit overwhelmed, don’t worry.
In the following chapters we’ll revisit all of these ideas, covering them in much more detail (that’s a promise, not a threat!).
At this point, we hope you leave this chapter with astill-developing intuition for the pieces that make up a network, a still-developing command of thevocabulary of networking (don’t be shy about referring back to this chapter), and an ever-growing desireto learn more about networking.
That’s the task ahead of us for the rest of this book.
Road-Mapping This Book Before starting any trip, you should always glance at a road map in order to become familiar with themajor roads and junctures that lie ahead.
For the trip we are about to embark on, the ultimatedestination is a deep understanding of the how, what, and why of computer networks.
Our road map is
the sequence of chapters of this book: 1.
Computer Networks and the Internet 2.
Application Layer 3.
Transport Layer 4.
Network Layer: Data Plane 5.
Network Layer: Control Plane 6.
The Link Layer and LANs 7.
Wireless and Mobile Networks 8.
Security in Computer Networks 9.
Multimedia Networking Chapters 2 through 6 are the five core chapters of this book.
You should notice that these chapters are organized around the top four layers of the five-layer Internet protocol.
Further note that our journey will begin at the top of the Internet protocol stack, namely, the application layer, and will work its way downward.
The rationale behind this top-down journey is that once we understand the applications, wecan understand the network services needed to support these applications.
We can then, in turn,examine the various ways in which such services might be implemented by a network architecture.
Covering applications early thus provides motivation for the remainder of the text.
The second half of the book— Chapters 7 through 9—zooms in on three enormously important (and somewhat independent) topics in modern computer networking.
In Chapter 7, we examine wireless and mobile networks, including wireless LANs (including WiFi and Bluetooth), Cellular telephony networks (including GSM, 3G, and 4G), and mobility (in both IP and GSM networks).
Chapter 8, which addresses security in computer networks, first looks at the underpinnings of encryption and network security, and then we examine how the basic theory is being applied in a broad range of Internet contexts.
The lastchapter, which addresses multimedia networking, examines audio and video applications such asInternet phone, video conferencing, and streaming of stored media.
We also look at how a packet-switched network can be designed to provide consistent quality of service to audio and videoapplications.
Homework Problems and Questions Chapter 1 Review Questions SECTION 1.1 SECTION 1.2 SECTION 1.3R1.
What is the difference between a host and an end system?
List several different types of end systems.
Is a Web server an end system?
R2.
The word protocol  is often used to describe diplomatic relations.
How does Wikipedia describe diplomatic protocol?R3.
Why are standards important for protocols?
R4.
List six access technologies.
Classify each one as home access, enterprise access, or wide- area wireless access.
R5.
Is HFC transmission rate dedicated or shared among users?
Are collisions possible in a downstream HFC channel?
Why or why not?
R6.
List the available residential access technologies in your city.
For each type of access, provide the advertised downstream rate, upstream rate, and monthly price.
R7.
What is the transmission rate of Ethernet LANs?
R8.
What are some of the physical media that Ethernet can run over?R9.
Dial-up modems, HFC, DSL and FTTH are all used for residential access.
For each of these access technologies, provide a range of ­transmission rates and comment on whether the transmission rate is shared or dedicated.
R10.
Describe the most popular wireless Internet access technologies today.
­Compare and contrast them.
R11.
Suppose there is exactly one packet switch between a sending host and a receiving host.
The transmission rates between the sending host and the switch and between the switch and the receiving host are R and R , respectively.
Assuming that the switch uses store-and-forward packet switching, what is the total end-to-end delay to send a packet of length L? (
Ignore queuing, propagation delay, and processing delay.)
1 2
SECTION 1.4R12.
What advantage does a circuit-switched network have over a packet-switched network?
What advantages does TDM have over FDM in a circuit-switched network?
R13.
Suppose users share a 2 Mbps link.
Also suppose each user transmits continuously at 1 Mbps when transmitting, but each user transmits only 20 percent of the time. (
See the discussion of statistical multiplexing in Section 1.3 .)
a. When circuit switching is used, how many users can be supported?
b. For the remainder of this problem, suppose packet switching is used.
Why will there be essentially no queuing delay before the link if two or fewer users transmit at the same time?
Why will there be a queuing delay if three users transmit at the same time?
c. Find the probability that a given user is transmitting.
d. Suppose now there are three users.
Find the probability that at any given time, all three users are transmitting simultaneously.
Find the fraction of time during which the queue grows.
R14.
Why will two ISPs at the same level of the hierarchy often peer with each other?
How does an IXP earn money?
R15.
Some content providers have created their own networks.
Describe Google’s network.
What motivates content providers to create these networks?
R16.
Consider sending a packet from a source host to a destination host over a fixed route.
List the delay components in the end-to-end delay.
Which of these delays are constant and which are variable?
R17.
Visit the Transmission Versus Propagation Delay applet at the companion Web site.
Among the rates, propagation delay, and packet sizes available, find a combination for which the sender finishes transmitting before the first bit of the packet reaches the receiver.
Find anothercombination for which the first bit of the packet reaches the receiver before the sender finishestransmitting.
R18.
How long does it take a packet of length 1,000 bytes to propagate over a link of distance 2,500 km, propagation speed  m/s, and transmission rate 2 Mbps?
More generally, how long does it take a packet of length L to propagate over a link of distance d, propagation speed s, and transmission rate R bps?
Does this delay depend on packet length?
Does this delay depend on transmission rate?
R19.
Suppose Host A wants to send a large file to Host B. The path from Host A to Host B has three links, of rates  a. Assuming no other traffic in the network, what is the throughput for the file transfer?
b. Suppose the file is 4 million bytes.
Dividing the file size by the throughput, roughly howlong will it take to transfer the file to Host B?
c. Repeat (a) and (b), but now with R reduced to 100 kbps.2.5⋅108 R1=500 kbps, R2=2 Mbps, and R3=1 Mbps.
2
SECTION 1.5 SECTION 1.6 ProblemsR20.
Suppose end system A wants to send a large file to end system B. At a very high level, describe how end system A creates packets from the file.
When one of these packets arrives to a router, what information in the packet does the router use to determine the link onto which thepacket is forwarded?
Why is packet switching in the Internet analogous to driving from one city to another and asking directions along the way?
R21.
Visit the Queuing and Loss applet at the companion Web site.
What is the maximum emission rate and the minimum transmission rate?
With those rates, what is the traffic intensity?
Run the applet with these rates and determine how long it takes for packet loss to occur.
Thenrepeat the experiment a second time and determine again how long it takes for packet loss tooccur.
Are the values different?
Why or why not?
R22.
List five tasks that a layer can perform.
Is it possible that one (or more) of these tasks could be performed by two (or more) layers?
R23.
What are the five layers in the Internet protocol stack?
What are the principal responsibilities of each of these layers?
R24.
What is an application-layer message?
A transport-layer segment?
A network-layer datagram?
A link-layer frame?
R25.
Which layers in the Internet protocol stack does a router process?
Which layers does a link-layer switch process?
Which layers does a host process?
R26.
What is the difference between a virus and a worm?
R27.
Describe how a botnet can be created and how it can be used for a DDoS attack.
R28.
Suppose Alice and Bob are sending packets to each other over a computer network.
Suppose Trudy positions herself in the network so that she can capture all the packets sent by Alice and send whatever she wants to Bob; she can also capture all the packets sent by Bob andsend whatever she wants to Alice.
List some of the malicious things Trudy can do from thisposition.
P1.
Design and describe an application-level protocol to be used between an automatic teller machine and a bank’s centralized computer.
Your protocol should allow a user’s card and password to be verified, the account balance (which is maintained at the centralized computer)to be queried, and an account withdrawal to be made (that is, money disbursed to the user).
Your protocol entities should be able to handle the all-too-common case in which there is not enough money in the account to cover the withdrawal.
Specify your protocol by listing the messages exchanged and the action taken by the automatic teller machine or the bank’scentralized computer on transmission and receipt of messages.
Sketch the operation of yourprotocol for the case of a simple withdrawal with no errors, using a diagram similar to that in Figure 1.2 .
Explicitly state the assumptions made by your protocol about the underlying end-to- end transport service.
P2.
Equation  1.1 gives a formula for the end-to-end delay of sending one packet of length L over N links of transmission rate R. Generalize this formula for sending P such packets back-to- back over the N links.
P3.
Consider an application that transmits data at a steady rate (for example, the sendergenerates an N-bit unit of data every k time units, where k is small and fixed).
Also, when such an application starts, it will continue running for a relatively long period of time.
Answer thefollowing questions, briefly justifying your answer: a. Would a packet-switched network or a circuit-switched network be more appropriate for this application?
Why?
b. Suppose that a packet-switched network is used and the only traffic in this networkcomes from such applications as described above.
Furthermore, assume that the sum of the application data rates is less than the capacities of each and every link.
Is some formof congestion control needed?
Why?
P4.
Consider the circuit-switched network in Figure 1.13 .
Recall that there are 4 circuits on each link.
Label the four switches A, B, C, and D, going in the clockwise direction.
a. What is the maximum number of simultaneous connections that can be in progress at any one time in this network?
b. Suppose that all connections are between switches A and C. What is the maximumnumber of simultaneous connections that can be in progress?
c. Suppose we want to make four connections between switches A and C, and another fourconnections between switches B and D. Can we route these calls through the four links to accommodate all eight ­connections?
P5.
Review the car-caravan analogy in Section 1.4 .
Assume a propagation speed of 100 km/hour.
a. Suppose the caravan travels 150 km, beginning in front of one tollbooth, passing through a second tollbooth, and finishing just after a third tollbooth.
What is the end-to-end delay?
b. Repeat (a), now assuming that there are eight cars in the caravan instead of ten.
P6.
This elementary problem begins to explore propagation delay and transmission delay, twocentral concepts in data networking.
Consider two hosts, A and B, connected by a single link of rate R bps.
Suppose that the two hosts are separated by m meters, and suppose the
propagation speed along the link is s meters/sec.
Host A is to send a packet of size L bits to Host B. Exploring propagation delay and transmission delay a. Express the propagation delay, d, in terms of m and s. b. Determine the transmission time of the packet, d, in terms of L and R. c. Ignoring processing and queuing delays, obtain an expression for the end-to-end delay.
d. Suppose Host A begins to transmit the packet at time .
At time  d , where is the last bit of the packet?
e. Suppose d is greater than d .
At time , where is the first bit of the packet?
f. Suppose d is less than d .
At time , where is the first bit of the packet?
g. Suppose , , and  Find the distance m so that d  equals d. P7.
In this problem, we consider sending real-time voice from Host A to Host B over a packet- switched network (VoIP).
Host A converts analog voice to a digital 64 kbps bit stream on the fly.
Host A then groups the bits into 56-byte packets.
There is one link between Hosts A and B; itstransmission rate is 2 Mbps and its propagation delay is 10 msec.
As soon as Host A gathers apacket, it sends it to Host B. As soon as Host B receives an entire packet, it converts the packet’s bits to an analog signal.
How much time elapses from the time a bit is created (from the original analog signal at Host A) until the bit is decoded (as part of the analog signal at Host B)?
P8.
Suppose users share a 3 Mbps link.
Also suppose each user requires 150 kbps when transmitting, but each user transmits only 10 percent of the time. (
See the discussion of packet switching versus circuit switching in Section 1.3 .)
a. When circuit switching is used, how many users can be supported?
b. For the remainder of this problem, suppose packet switching is used.
Find the probability that a given user is transmitting.
c. Suppose there are 120 users.
Find the probability that at any given time, exactly n users are transmitting simultaneously. (
Hint: Use the binomial distribution.)
d. Find the probability that there are 21 or more users transmitting ­simultaneously.
P9.
Consider the discussion in Section 1.3 of packet switching versus circuit switching in which an example is provided with a 1 Mbps link.
Users are generating data at a rate of 100 kbps when busy, but are busy generating data only with probability .
Suppose that the 1 Mbps link isprop trans t=0 t=trans prop trans t=dtrans prop trans t=dtrans s=2.5⋅108 L=120 bits R=56 kbps.
prop trans p=0.1
replaced by a 1 Gbps link.
a. What is N, the maximum number of users that can be supported simultaneously under circuit switching?
b. Now consider packet switching and a user population of M users.
Give a formula (in terms of p, M, N) for the probability that more than N users are sending data.
P10.
Consider a packet of length L that begins at end system A and travels over three links to a destination end system.
These three links are connected by two packet switches.
Let d, s, and R denote the length, propagation speed, and the transmission rate of link i, for .
The packet switch delays each packet by d. Assuming no queuing delays, in terms of d, s, R, , and L, what is the total end-to-end delay for the packet?
Suppose now the packet is 1,500 bytes, the propagation speed on all three links is  the transmission rates of all three links are 2 Mbps, the packet switch processing delay is 3 msec, the length of the first link is 5,000 km, the length of the second link is 4,000 km, and the length of the last link is 1,000 km.
For these values, what is the end-to-end delay?
P11.
In the above problem, suppose  and .
Further suppose the packet switch does not store-and-forward packets but instead immediately transmits each bit it receives before waiting for the entire packet to arrive.
What is the end-to-end delay?
P12.
A packet switch receives a packet and determines the outbound link to which the packet should be forwarded.
When the packet arrives, one other packet is halfway done being transmitted on this outbound link and four other packets are waiting to be transmitted.
Packetsare transmitted in order of arrival.
Suppose all packets are 1,500 bytes and the link rate is 2Mbps.
What is the queuing delay for the packet?
More generally, what is the queuing delay when all packets have length L, the transmission rate is R, x bits of the currently-being-transmitted packet have been transmitted, and n packets are already in the queue?
P13.
a. Suppose N packets arrive simultaneously to a link at which no packets are currently being transmitted or queued.
Each packet is of length L and the link has transmission rate R. What is the average queuing delay for the N packets?
b. Now suppose that N such packets arrive to the link every LN/R seconds.
What is the average queuing delay of a packet?
P14.
Consider the queuing delay in a router buffer.
Let I denote traffic intensity; that is, .
Suppose that the queuing delay takes the form  for .
a. Provide a formula for the total delay, that is, the queuing delay plus the transmission delay.
b. Plot the total delay as a function of L /R. P15.
Let a denote the rate of packets arriving at a link in packets/sec, and let µ denote the link’s transmission rate in packets/sec.
Based on the formula for the total delay (i.e., the queuing delayii i i=1,2,3 proc ii i (i=1,2,3) 2.5⋅108m/s, R1=R2=R3=R dproc=0 I=La/R IL/R(1−I) I<1
plus the transmission delay) derived in the previous problem, derive a formula for the total delay in terms of a and µ. P16.
Consider a router buffer preceding an outbound link.
In this problem, you will use Little’sformula, a famous formula from queuing theory.
Let N denote the average number of packets in the buffer plus the packet being transmitted.
Let a denote the rate of packets arriving at the link.
Let d denote the average total delay (i.e., the queuing delay plus the transmission delay) experienced by a packet.
Little’s formula is .
Suppose that on average, the buffer contains 10 packets, and the average packet queuing delay is 10 msec.
The link’s transmission rate is 100 packets/sec.
Using Little’s formula, what is the average packet arrival rate, assuming thereis no packet loss?
P17.
a. Generalize Equation  1.2 in Section 1.4.3 for heterogeneous processing rates, transmission rates, and propagation delays.
b. Repeat (a), but now also suppose that there is an average queuing delay of d  at each node.
P18.
Perform a Traceroute between source and destination on the same continent at three different hours of the day.
Using Traceroute to discover network paths and measure network delay a. Find the average and standard deviation of the round-trip delays at each of the threehours.
b. Find the number of routers in the path at each of the three hours.
Did the paths changeduring any of the hours?
c. Try to identify the number of ISP networks that the Traceroute packets pass through fromsource to destination.
Routers with similar names and/or similar IP addresses should be considered as part of the same ISP.
In your experiments, do the largest delays occur atthe peering interfaces between adjacent ISPs?
d. Repeat the above for a source and destination on different continents.
Compare the intra-continent and inter-continent results.
P19.
a. Visit the site www.traceroute.org and perform traceroutes from two different cities inFrance to the same destination host in the United States.
How many links are the sameN=a⋅d queue
in the two traceroutes?
Is the transatlantic link the same?
b. Repeat (a) but this time choose one city in France and another city in Germany.
c. Pick a city in the United States, and perform traceroutes to two hosts, each in a different city in China.
How many links are common in the two traceroutes?
Do the two traceroutes diverge before reaching China?
P20.
Consider the throughput example corresponding to Figure 1.20(b) .
Now suppose that there are M client-server pairs rather than 10.
Denote R, R, and R for the rates of the server links, client links, and network link.
Assume all other links have abundant capacity and that there is no other traffic in the network besides the traffic generated by the M client-server pairs.
Derive a general expression for throughput in terms of R, R, R, and M. P21.
Consider Figure 1.19(b) .
Now suppose that there are M paths between the server and the client.
No two paths share any link.
Path  consists of N links with transmission rates .
If the server can only use one path to send data to the client, what is the maximum throughput that the server can achieve?
If the server can use all M paths to send data, what is the maximum throughput that the server can achieve?P22.
Consider Figure 1.19(b) .
Suppose that each link between the server and the client has a packet loss probability p, and the packet loss probabilities for these links are independent.
What is the probability that a packet (sent by the server) is successfully received by the receiver?
If a packet is lost in the path from the server to the client, then the server will re-transmit the packet.
On average, how many times will the server re-transmit the packet in order for the client tosuccessfully receive the packet?
P23.
Consider Figure 1.19(a) .
Assume that we know the bottleneck link along the path from the server to the client is the first link with rate R  bits/sec.
Suppose we send a pair of packets back to back from the server to the client, and there is no other traffic on this path.
Assume eachpacket of size L bits, and both links have the same propagation delay d. a. What is the packet inter-arrival time at the destination?
That is, how much time elapses from when the last bit of the first packet arrives until the last bit of the second packet arrives?
b. Now assume that the second link is the bottleneck link (i.e., ).
Is it possible that the second packet queues at the input queue of the second link?
Explain.
Now suppose that the server sends the second packet T seconds after sending the first packet.
How large must T be to ensure no queuing before the second link?
Explain.
P24.
Suppose you would like to urgently deliver 40 terabytes data from Boston to Los Angeles.
You have available a 100 Mbps dedicated link for data transfer.
Would you prefer to transmit the data via this link or instead use FedEx over-night delivery?
Explain.
P25.
Suppose two hosts, A and B, are separated by 20,000 kilometers and are connected by a direct link of  Mbps.
Suppose the propagation speed over the link is  meters/sec.
a. Calculate the bandwidth-delay product, .s c s c k(k=1,…,M) R1k,R2k,…,RNk s prop Rc<Rs R=2 2.5⋅108 R⋅dprop
b. Consider sending a file of 800,000 bits from Host A to Host B. Suppose the file is sent continuously as one large message.
What is the maximum number of bits that will be in the link at any given time?
c. Provide an interpretation of the bandwidth-delay product.
d. What is the width (in meters) of a bit in the link?
Is it longer than a ­football field?
e. Derive a general expression for the width of a bit in terms of the propagation speed s, the transmission rate R, and the length of the link m. P26.
Referring to problem P25, suppose we can modify R. For what value of R is the width of a bit as long as the length of the link?
P27.
Consider problem P25 but now with a link of  Gbps.
a. Calculate the bandwidth-delay product, .
b. Consider sending a file of 800,000 bits from Host A to Host B. Suppose the file is sent continuously as one big message.
What is the maximum number of bits that will be in the link at any given time?
c. What is the width (in meters) of a bit in the link?
P28.
Refer again to problem P25.
a. How long does it take to send the file, assuming it is sent continuously?
b. Suppose now the file is broken up into 20 packets with each packet containing 40,000 bits.
Suppose that each packet is acknowledged by the receiver and the transmission time of an acknowledgment packet is negligible.
Finally, assume that the sender cannotsend a packet until the preceding one is acknowledged.
How long does it take to send the file?
c. Compare the results from (a) and (b).
P29.
Suppose there is a 10 Mbps microwave link between a geostationary satellite and its base station on Earth.
Every minute the satellite takes a digital photo and sends it to the base station.
Assume a propagation speed of  meters/sec.
a. What is the propagation delay of the link?
b. What is the bandwidth-delay product, ?
c. Let x denote the size of the photo.
What is the minimum value of x for the microwave link to be continuously transmitting?
P30.
Consider the airline travel analogy in our discussion of layering in Section 1.5 , and the addition of headers to protocol data units as they flow down the protocol stack.
Is there an equivalent notion of header information that is added to passengers and baggage as they movedown the airline protocol stack?
P31.
In modern packet-switched networks, including the Internet, the source host segments long, application-layer messages (for example, an image or a music file) into smaller packetsR=1 R⋅dprop 2.4⋅108 R⋅dprop
and sends the packets into the network.
The receiver then reassembles the packets back into the original message.
We refer to this process as message  segmentation .
Figure 1.27 illustrates the end-to-end transport of a message with and without message segmentation.
Consider a message that is  bits long that is to be sent from source to destination in Figure 1.27 .
Suppose each link in the figure is 2 Mbps.
Ignore propagation, queuing, and processing delays.
a. Consider sending the message from source to destination without message segmentation.
How long does it take to move the message from the source host to the first packet switch?
Keeping in mind that each switch uses store-and-forward packetswitching, what is the total time to move the message from source host to destination host?
b. Now suppose that the message is segmented into 800 packets, with each packet being 10,000 bits long.
How long does it take to move the first packet from source host to the first switch?
When the first packet is being sent from the first switch to the second switch,the second packet is being sent from the source host to the first switch.
At what time will the second packet be fully received at the first switch?
c. How long does it take to move the file from source host to destination host when message segmentation is used?
Compare this result with your answer in part (a) and comment.
  Figure 1.27 End-to-end message transport: (a) without message ­segmentation; (b) with message segmentation d. In addition to reducing delay, what are reasons to use message ­segmentation?
e. Discuss the drawbacks of message segmentation.
P32.
Experiment with the Message Segmentation applet at the book’s Web site.
Do the delays in the applet correspond to the delays in the previous problem?
How do link propagation delays affect the overall end-to-end delay for packet switching (with message segmentation) and for message switching?
P33.
Consider sending a large file of F bits from Host A to Host B. There are three links (and two switches) between A and B, and the links are uncongested (that is, no queuing delays).
Host A8⋅106
Wireshark Lab “Tell me and I forget.
Show me and I remember.
Involve me and I understand.”
Chinese proverb One’s understanding of network protocols can often be greatly deepened by seeing them in action and by playing around with them—observing the sequence of messagesexchanged between two protocol entities, delving into the details of protocol operation,causing protocols to perform certain actions, and observing these actions and theirconsequences.
This can be done in simulated scenarios or in a real network environment such as the Internet.
The Java applets at the textbook Web site take the first approach.
In the Wireshark labs, we’ll take the latter approach.
You’ll run networkapplications in various scenarios using a computer on your desk, at home, or in a lab.
You’ll observe the network protocols in your computer, interacting and exchangingmessages with protocol entities executing elsewhere in the Internet.
Thus, you and yourcomputer will be an integral part of these live labs.
You’ll observe—and you’ll learn—bydoing.
The basic tool for observing the messages exchanged between executing protocol entities is called a packet sniffer.
As the name suggests, a packet sniffer passively copies (sniffs) messages being sent from and received by your computer; it also displaysthe contents of the various protocol fields of these captured messages.
A screenshot of the Wireshark packet sniffer is shown in Figure 1.28.
Wireshark is a free packet sniffer that runs on Windows, Linux/Unix, and Mac computers.segments the file into segments of S bits each and adds 80 bits of header to each segment, forming packets of   S bits.
Each link has a transmission rate of R bps.
Find the value of S that minimizes the delay of moving the file from Host A to Host B. Disregard propagation delay.
P34.
Skype offers a service that allows you to make a phone call from a PC to an ordinary phone.
This means that the voice call must pass through both the Internet and through a telephone network.
Discuss how this might be done.
L=80 +
Figure 1.28 A Wireshark screenshot (Wireshark screenshot reprinted by permission of the Wireshark Foundation.)
Throughout the textbook, you will find Wireshark labs that allow you to explore a number of the protocols studied in the chapter.
In this first Wireshark lab, you’ll obtain and installa copy of Wireshark, access a Web site, and capture and examine the protocolmessages being exchanged between your Web browser and the Web server.
You can find full details about this first Wireshark lab (including instructions about how to obtain and install Wireshark) at the Web site http:/ /www.pearsonhighered.com/ cs- resources/ .
AN INTERVIEW WITH… Leonard Kleinrock Leonard Kleinrock is a professor of computer science at the University of California, Los Angeles.
In 1969, his computer at UCLA became the first node of the Internet.
His creation ofpacket-switching principles in 1961 became the technology behind the Internet.
He received hisB.E.E. from the City College of New York (CCNY) and his masters and PhD in electricalengineering from MIT.
What made you decide to specialize in networking/Internet technology?
As a PhD student at MIT in 1959, I looked around and found that most of my classmates were doing research in the area of information theory and coding theory.
At MIT, there was the greatresearcher, Claude Shannon, who had launched these fields and had solved most of theimportant problems already.
The research problems that were left were hard and of lesser consequence.
So I decided to launch out in a new area that no one else had yet conceived of.
Remember that at MIT I was surrounded by lots of computers, and it was clear to me that soonthese machines would need to communicate with each other.
At the time, there was no effectiveway for them to do so, so I decided to develop the technology that would permit efficient andreliable data networks to be created.
What was your first job in the computer industry?
What did it entail?
I went to the evening session at CCNY from 1951 to 1957 for my bachelor’s degree in electrical engineering.
During the day, I worked first as a technician and then as an engineer at a small,industrial electronics firm called Photobell.
While there, I introduced digital technology to theirproduct line.
Essentially, we were using photoelectric devices to detect the presence of certain items (boxes, people, etc.)
and the use of a circuit known then as a bistable  multivibrator  was just the kind of technology we needed to bring digital processing into this field of detection.
These circuits happen to be the building blocks for computers, and have come to be known as flip-flops  or switches in today’s vernacular.
What was going through your mind when you sent the first host-to-host message (from UCLA to the Stanford Research Institute)?
Frankly, we had no idea of the importance of that event.
We had not prepared a special message of historic significance, as did so many inventors of the past (Samuel Morse with “Whathath God wrought.”
or Alexander Graham Bell with “Watson, come here!
I want you.”
or NealAmstrong with “That’s one small step for a man, one giant leap for mankind.”)
Those guys were
smart!
They understood media and public relations.
All we wanted to do was to login to the SRI computer.
So we typed the “L”, which was correctly received, we typed the “o” which was received, and then we typed the “g” which caused the SRI host computer to crash!
So, it turned out that our message was the shortest and perhaps the most prophetic message ever, namely“Lo!”
as in “Lo and behold!”
Earlier that year, I was quoted in a UCLA press release saying that once the network was up and running, it would be possible to gain access to computer utilities from our homes and officesas easily as we gain access to electricity and telephone connectivity.
So my vision at that timewas that the Internet would be ubiquitous, always on, always available, anyone with any devicecould connect from any location, and it would be invisible.
However, I never anticipated that my99-year-old mother would use the Internet—and indeed she did!
What is your vision for the future of networking?
The easy part of the vision is to predict the infrastructure itself.
I anticipate that we see considerable deployment of nomadic computing, mobile devices, and smart spaces.
Indeed, theavailability of lightweight, inexpensive, high-performance, portable computing, and communication devices (plus the ubiquity of the Internet) has enabled us to become nomads.
Nomadic computing refers to the technology that enables end users who travel from place toplace to gain access to Internet services in a transparent fashion, no matter where they traveland no matter what device they carry or gain access to.
The harder part of the vision is to predictthe applications and services, which have consistently surprised us in dramatic ways (e-mail,search technologies, the World Wide Web, blogs, social networks, user generation, and sharingof music, photos, and videos, etc.).
We are on the verge of a new class of surprising andinnovative mobile applications delivered to our hand-held devices.
The next step will enable us to move out from the netherworld of cyberspace to the physical world of smart spaces.
Our environments (desks, walls, vehicles, watches, belts, and so on) willcome alive with technology, through actuators, sensors, logic, processing, storage, cameras,microphones, speakers, displays, and communication.
This embedded technology will allow ourenvironment to provide the IP services we want.
When I walk into a room, the room will know Ientered.
I will be able to communicate with my environment naturally, as in spoken English; myrequests will generate replies that present Web pages to me from wall displays, through myeyeglasses, as speech, holograms, and so forth.
Looking a bit further out, I see a networking future that includes the following additional key components.
I see intelligent software agents deployed across the network whose function it isto mine data, act on that data, observe trends, and carry out tasks dynamically and adaptively.
Isee considerably more network traffic generated not so much by humans, but by theseembedded devices and these intelligent software agents.
I see large collections of self-organizing systems controlling this vast, fast network.
I see huge amounts of information flashing
across this network instantaneously with this information undergoing enormous processing and filtering.
The Internet will essentially be a pervasive global nervous system.
I see all these things and more as we move headlong through the twenty-first century.
What people have inspired you professionally?
By far, it was Claude Shannon from MIT, a brilliant researcher who had the ability to relate his mathematical ideas to the physical world in highly intuitive ways.
He was on my PhD thesiscommittee.
Do you have any advice for students entering the networking/Internet field?
The Internet and all that it enables is a vast new frontier, full of amazing challenges.
There is room for great innovation.
Don’t be constrained by today’s technology.
Reach out and imagine what could be and then make it happen.
Chapter 2 Application Layer Network applications are the raisons d’être  of a computer network—if we couldn’t conceive of any useful applications, there wouldn’t be any need for networking infrastructure and protocols to support them.
Since the Internet’s inception, numerous useful and entertaining applications have indeed been created.
These applications have been the driving force behind the Internet’s success, motivating people in homes, schools, governments, and businesses to make the Internet an integral part of their daily activities.
Internet applications include the classic text-based applications that became popular in the 1970s and 1980s: text e-mail, remote access to computers, file transfers, and newsgroups.
They include the killer application of the mid-1990s, the World Wide Web, encompassing Web surfing, search, and electronic commerce.
They include instant messaging and P2P file sharing, the two killer applications introducedat the end of the millennium.
In the new millennium, new and highly compelling applications continue toemerge, including voice over IP and video conferencing such as Skype, Facetime, and Google Hangouts; user generated video such as YouTube and movies on demand such as Netflix; multiplayer online games such as Second Life and World of Warcraft.
During this same period, we have seen theemergence of a new generation of social networking applications—such as Facebook, Instagram,Twitter, and WeChat—which have created engaging human networks on top of the Internet’s network orrouters and communication links.
And most recently, along with the arrival of the smartphone, there hasbeen a profusion of location based mobile apps, including popular check-in, dating, and road-trafficforecasting apps (such as Yelp, Tinder, Waz, and Yik Yak).
Clearly, there has been no slowing down ofnew and exciting Internet applications.
Perhaps some of the readers of this text will create the next generation of killer Internet applications!
In this chapter we study the conceptual and implementation aspects of network applications.
We begin by defining key application-layer concepts, including network services required by applications, clientsand servers, processes, and transport-layer interfaces.
We examine several network applications indetail, including the Web, e-mail, DNS, peer-to-peer (P2P) file distribution, and video streaming. (
Chapter 9 will further examine multimedia applications, including streaming video and VoIP.)
We then cover network application development, over both TCP and UDP.
In particular, we study the socket interface and walk through some simple client-server applications in Python.
We also provide several fun and interesting socket programming assignments at the end of the chapter.
The application layer is a particularly good place to start our study of protocols.
It’s familiar ground.
We’re acquainted with many of the applications that rely on the protocols we’ll study.
It will give us agood feel for what protocols are all about and will introduce us to many of the same issues that we’ll seeagain when we study transport, network, and link layer protocols.
2.1 Principles of Network Applications Suppose you have an idea for a new network application.
Perhaps this application will be a great service to humanity, or will please your professor, or will bring you great wealth, or will simply be fun to develop.
Whatever the motivation may be, let’s now examine how you transform the idea into a real-worldnetwork application.
At the core of network application development is writing programs that run on different end systems and communicate with each other over the network.
For example, in the Web application there are twodistinct programs that communicate with each other: the browser program running in the user’s host(desktop, laptop, tablet, smartphone, and so on); and the Web server program running in the Webserver host.
As another example, in a P2P file-sharing system there is a program in each host thatparticipates in the file-sharing community.
In this case, the programs in the various hosts may be similaror identical.
Thus, when developing your new application, you need to write software that will run on multiple end systems.
This software could be written, for example, in C, Java, or Python.
Importantly, you do notneed to write software that runs on network-core devices, such as routers or link-layer switches.
Even ifyou wanted to write application software for these network-core devices, you wouldn’t be able to do so.
As we learned in Chapter 1, and as shown earlier in Figure 1.24, network-core devices do not function at the application layer but instead function at lower layers—specifically at the network layer and below.
This basic design—namely, confining application software to the end systems—as shown in Figure 2.1, has facilitated the rapid development and deployment of a vast array of network applications.
Figure 2.1 Communication for a network application takes place between end systems at the application layer 2.1.1 Network Application Architectures
Before diving into software coding, you should have a broad architectural plan for your application.
Keep in mind that an application’s architecture is distinctly different from the network architecture (e.g., the five-layer Internet architecture discussed in Chapter 1).
From the application developer’s perspective, the network architecture is fixed and provides a specific set of services to applications.
The application architecture , on the other hand, is designed by the application developer and dictates how the application is structured over the various end systems.
In choosing the application architecture, an application developer will likely draw on one of the two predominant architectural paradigms used in modern network applications: the client-server architecture or the peer-to-peer (P2P) architecture.
In a client-server architecture , there is an always-on host, called the server, which services requests from many other hosts, called clients.
A classic example is the Web application for which an always-on Web server services requests from browsers running on client hosts.
When a Web server receives a request for an object from a client host, it responds by sending the requested object to the client host.
Note that with the client-server architecture, clients do not directly communicate with each other; for example, in the Web application, two browsers do not directly communicate.
Another characteristic of the client-server architecture is that the server has a fixed, well-known address, called an IP address(which we’ll discuss soon).
Because the server has a fixed, well-known address, and because the serveris always on, a client can always contact the server by sending a packet to the server’s IP address.
Some of the better-known applications with a client-server architecture include the Web, FTP, Telnet, and e-mail.
The client-server architecture is shown in Figure 2.2(a).
Often in a client-server application, a single-server host is incapable of keeping up with all the requests from clients.
For example, a popular social-networking site can quickly become overwhelmed if it has only one server handling all of its requests.
For this reason, a data center, housing a large number of hosts, is often used to create a powerful virtual server.
The most popular Internet services—such as search engines (e.g., Google, Bing, Baidu), Internet commerce (e.g., Amazon, eBay, Alibaba), Web-based e-mail (e.g., Gmail and Yahoo Mail), social networking (e.g., Facebook, Instagram, Twitter, and WeChat)—employ one or more data centers.
As discussed in Section 1.3.3, Google has 30 to 50 data centers distributed around the world, which collectively handle search, YouTube, Gmail, and other services.
A data center can have hundreds of thousands of servers, which must be powered andmaintained.
Additionally, the service providers must pay recurring interconnection and bandwidth costs for sending data from their data centers.
In a P2P architecture , there is minimal (or no) reliance on dedicated servers in data centers.
Instead the application exploits direct communication between pairs of intermittently connected hosts, called peers.
The peers are not owned by the service provider, but are instead desktops and laptops controlled by users, with most of the
Figure 2.2 (a) Client-server architecture; (b) P2P architecture
peers residing in homes, universities, and offices.
Because the peers communicate without passing through a dedicated server, the architecture is called peer-to-peer.
Many of today’s most popular andtraffic-intensive applications are based on P2P architectures.
These applications include file sharing(e.g.,
BitTorrent), peer-assisted download acceleration (e.g., Xunlei), and Internet telephony and video conference (e.g., Skype).
The P2P architecture is illustrated in Figure 2.2(b).
We mention that some applications have hybrid architectures, combining both client-server and P2P elements.
For example, for many instant messaging applications, servers are used to track the IP addresses of users, but user-to-user messages are sent directly between user hosts (without passing through intermediate servers).
One of the most compelling features of P2P architectures is their self-scalability.
For example, in a P2P file-sharing application, although each peer generates workload by requesting files, each peer also adds service capacity to the system by distributing files to other peers.
P2P architectures are also costeffective, since they normally don’t require significant server infrastructure and server bandwidth (in contrast with clients-server designs with datacenters).
However, P2P applications face challenges ofsecurity, performance, and reliability due to their highly decentralized structure.
2.1.2 Processes Communicating Before building your network application, you also need a basic understanding of how the programs,running in multiple end systems, communicate with each other.
In the jargon of operating systems, it isnot actually programs but processes  that communicate.
A process can be thought of as a program that is running within an end system.
When processes are running on the same end system, they cancommunicate with each other with interprocess communication, using rules that are governed by the end system’s operating system.
But in this book we are not particularly interested in how processes in the same host communicate, but instead in how processes running on different hosts (with potentially different operating systems) communicate.
Processes on two different end systems communicate with each other by exchanging messages  across the computer network.
A sending process creates and sends messages into the network; a receiving process receives these messages and possibly responds by sending messages back.
Figure 2.1 illustrates that processes communicating with each other reside in the application layer of the five-layer protocol stack.
Client and Server Processes A network application consists of pairs of processes that send messages to each other over a network.
For example, in the Web application a client browser process exchanges messages with a Web server
process.
In a P2P file-sharing system, a file is transferred from a process in one peer to a process in another peer.
For each pair of communicating processes, we typically label one of the two processes as the client and the other process as the server .
With the Web, a browser is a client process and a Web server is a server process.
With P2P file sharing, the peer that is downloading the file is labeled as theclient, and the peer that is uploading the file is labeled as the server.
You may have observed that in some applications, such as in P2P file sharing, a process can be both a client and a server.
Indeed, a process in a P2P file-sharing system can both upload and download files.
Nevertheless, in the context of any given communication session between a pair of processes, we canstill label one process as the client and the other process as the server.
We define the client and serverprocesses as follows: In the context of a communication session between a pair of processes, the process that initiates thecommunication (that is, initially contacts the other process at the beginning of the session) is labeledas the client.
The process that waits to be contacted to begin the session is the  server.
In the Web, a browser process initializes contact with a Web server process; hence the browser processis the client and the Web server process is the server.
In P2P file sharing, when Peer A asks Peer B tosend a specific file, Peer A is the client and Peer B is the server in the context of this specificcommunication session.
When there’s no confusion, we’ll sometimes also use the terminology “clientside and server side of an application.”
At the end of this chapter, we’ll step through simple code for boththe client and server sides of network applications.
The Interface Between the Process and the Computer Network As noted above, most applications consist of pairs of communicating processes, with the two processes in each pair sending messages to each other.
Any message sent from one process to another must gothrough the underlying network.
A process sends messages into, and receives messages from, thenetwork through a software interface called a socket.
Let’s consider an analogy to help us understand processes and sockets.
A process is analogous to a house and its socket is analogous to its door.
Whena process wants to send a message to another process on another host, it shoves the message out itsdoor (socket).
This sending process assumes that there is a transportation infrastructure on the other side of its door that will transport the message to the door of the destination process.
Once the message arrives at the destination host, the message passes through the receiving process’s door (socket), andthe receiving process then acts on the message.
Figure 2.3 illustrates socket communication between two processes that communicate over the Internet. (
Figure 2.3 assumes that the underlying transport protocol used by the processes is the Internet’s TCP protocol.)
As shown in this figure, a socket is the interface between the application layer and the transport layer within a host.
It is also referred to as the Application Programming Interface (API)
between the application and the network, since the socket is the programming interface with which network applications are built.
The application developer has control of everything on the application- layer side of the socket but has little control of the transport-layer side of the socket.
The only controlthat the application developer has on the transport-layer side is (1) the choice of transport protocol and(2) perhaps the ability to fix a few transport-layer parameters such as maximum buffer and maximum segment sizes (to be covered in Chapter 3).
Once the application developer chooses a transport protocol (if a choice is available), the application is built using the transport-layer services provided bythat protocol.
We’ll explore sockets in some detail in Section 2.7.
Addressing Processes In order to send postal mail to a particular destination, the destination needs to have an address.
Similarly, in order for a process running on one host to send packets to a process running on anotherhost, the receiving process needs to have an address.
Figure 2.3 Application processes, sockets, and underlying transport protocol To identify the receiving process, two pieces of information need to be specified: (1) the address of the host and (2) an identifier that specifies the receiving process in the destination host.
In the Internet, the host is identified by its IP address.
We’ll discuss IP addresses in great detail in Chapter 4.
For now, all we need to know is that an IP address is a 32-bit quantity that we can think of as uniquely identifying the host.
In addition to knowing the address of the host to which a message is destined, the sending process must also identify the receiving process (more specifically, the receivingsocket) running in the host.
This information is needed because in general a host could be running manynetwork applications.
A destination port number serves this purpose.
Popular applications have been
assigned specific port numbers.
For example, a Web server is identified by port number 80.
A mail server process (using the SMTP protocol) is identified by port number 25.
A list of well-known port numbers for all Internet standard protocols can be found at www.iana.org .
We’ll examine port numbers in detail in Chapter 3.
2.1.3 Transport Services Available to Applications Recall that a socket is the interface between the application process and the transport-layer protocol.
The application at the sending side pushes messages through the socket.
At the other side of thesocket, the transport-layer protocol has the responsibility of getting the messages to the socket of thereceiving process.
Many networks, including the Internet, provide more than one transport-layer protocol.
When you develop an application, you must choose one of the available transport-layer protocols.
How do you make this choice?
Most likely, you would study the services provided by the available transport-layerprotocols, and then pick the protocol with the services that best match your application’s needs.
Thesituation is similar to choosing either train or airplane transport for travel between two cities.
You have tochoose one or the other, and each transportation mode offers different services. (
For example, the trainoffers downtown pickup and drop-off, whereas the plane offers shorter travel time.)
What are the services that a transport-layer protocol can offer to applications invoking it?
We can broadly classify the possible services along four dimensions: reliable data transfer, throughput, timing,and security.
Reliable Data TransferAs discussed in Chapter 1, packets can get lost within a computer network.
For example, a packet can overflow a buffer in a router, or can be discarded by a host or router after having some of its bits corrupted.
For many applications—such as electronic mail, file transfer, remote host access, Web document transfers, and financial applications—data loss can have devastating consequences (in the latter case, for either the bank or the customer!).
Thus, to support these applications, something has tobe done to guarantee that the data sent by one end of the application is delivered correctly andcompletely to the other end of the application.
If a protocol provides such a guaranteed data deliveryservice, it is said to provide reliable data transfer.
One important service that a transport-layer protocol can potentially provide to an application is process-to-process reliable data transfer.
When a transportprotocol provides this service, the sending process can just pass its data into the socket and know withcomplete confidence that the data will arrive without errors at the receiving process.
When a transport-layer protocol doesn’t provide reliable data transfer, some of the data sent by the
sending process may never arrive at the receiving process.
This may be acceptable for loss-tolerant applications , most notably multimedia applications such as conversational audio/video that can tolerate some amount of data loss.
In these multimedia applications, lost data might result in a small glitch in the audio/video—not a crucial impairment.
Throughput In Chapter 1 we introduced the concept of available throughput, which, in the context of a communication session between two processes along a network path, is the rate at which the sending process can deliver bits to the receiving process.
Because other sessions will be sharing the bandwidthalong the network path, and because these other sessions will be coming and going, the availablethroughput can fluctuate with time.
These observations lead to another natural service that a transport-layer protocol could provide, namely, guaranteed available throughput at some specified rate.
With such a service, the application could request a guaranteed throughput of r bits/sec, and the transport protocol would then ensure that the available throughput is always at least r bits/sec.
Such a guaranteed throughput service would appeal to many applications.
For example, if an Internet telephony applicationencodes voice at 32 kbps, it needs to send data into the network and have data delivered to thereceiving application at this rate.
If the transport protocol cannot provide this throughput, the applicationwould need to encode at a lower rate (and receive enough throughput to sustain this lower coding rate)or may have to give up, since receiving, say, half of the needed throughput is of little or no use to thisInternet telephony application.
Applications that have throughput requirements are said to bebandwidth-sensitive applications .
Many current multimedia applications are bandwidth sensitive, although some multimedia applications may use adaptive coding techniques to encode digitized voice or video at a rate that matches the currently available throughput.
While bandwidth-sensitive applications have specific throughput requirements, elastic applications can make use of as much, or as little, throughput as happens to be available.
Electronic mail, file transfer, and Web transfers are all elastic applications.
Of course, the more throughput, the better.
There’sanadage that says that one cannot be too rich, too thin, or have too much throughput!
Timing A transport-layer protocol can also provide timing guarantees.
As with throughput guarantees, timing guarantees can come in many shapes and forms.
An example guarantee might be that every bit that thesender pumps into the socket arrives at the receiver’s socket no more than 100 msec later.
Such aservice would be appealing to interactive real-time applications, such as Internet telephony, virtualenvironments, teleconferencing, and multiplayer games, all of which require tight timing constraints on data delivery in order to be effective. (
See Chapter 9, [Gauthier 1999; Ramjee 1994] .)
Long delays in Internet telephony, for example, tend to result in unnatural pauses in the conversation; in a multiplayer game or virtual interactive environment, a long delay between taking an action and seeing the response
from the environment (for example, from another player at the end of an end-to-end connection) makes the application feel less realistic.
For non-real-time applications, lower delay is always preferable to higher delay, but no tight constraint is placed on the end-to-end delays.
Security Finally, a transport protocol can provide an application with one or more security services.
For example, in the sending host, a transport protocol can encrypt all data transmitted by the sending process, and inthe receiving host, the transport-layer protocol can decrypt the data before delivering the data to thereceiving process.
Such a service would provide confidentiality between the two processes, even if thedata is somehow observed between sending and receiving processes.
A transport protocol can alsoprovide other security services in addition to confidentiality, including data integrity and end-point authentication, topics that we’ll cover in detail in Chapter 8.
2.1.4 Transport Services Provided by the Internet Up until this point, we have been considering transport services that a computer network could  provide in general.
Let’s now get more specific and examine the type of transport services provided by the Internet.
The Internet (and, more generally, TCP/IP networks) makes two transport protocols available toapplications, UDP and TCP.
When you (as an application developer) create a new network application for the Internet, one of the first decisions you have to make is whether to use UDP or TCP.
Each of these protocols offers a different set of services to the invoking applications.
Figure 2.4 shows the service requirements for some selected applications.
TCP Services The TCP service model includes a connection-oriented service and a reliable data transfer service.
When an application invokes TCP as its transport protocol, the application receives both of theseservices from TCP.
Connection-oriented service.
TCP has the client and server exchange transport-layer controlinformation with each other before the application-level messages begin to flow.
This so-called handshaking procedure alerts the client and server, allowing them to prepare for an onslaught ofpackets.
After the handshaking phase, a TCP connection  is said to exist between the sockets
Figure 2.4 Requirements of selected network applications of the two processes.
The connection is a full-duplex connection in that the two processes can send messages to each other over the connection at the same time.
When the application finishes sending messages, it must tear down the connection.
In Chapter 3 we’ll discuss connection-oriented service in detail and examine how it is implemented.
Reliable data transfer service.
 The communicating processes can rely on TCP to deliver all data sent without error and in the proper order.
When one side of the application passes a stream of bytes into a socket, it can count on TCP to deliver the same stream of bytes to the receiving socket,with no missing or duplicate bytes.
TCP also includes a congestion-control mechanism, a service for the general welfare of the Internetrather than for the direct benefit of the communicating processes.
The TCP congestion-controlmechanism throttles a sending process (client or server) when the network is congested betweensender and receiver.
As we will see FOCUS ON SECURITY SECURING TCP Neither TCP nor UDP provides any encryption—the data that the sending process passes into its socket is the same data that travels over the network to the destination process.
So, for example, if the sending process sends a password in cleartext (i.e., unencrypted) into its socket,the cleartext password will travel over all the links between sender and receiver, potentiallygetting sniffed and discovered at any of the intervening links.
Because privacy and other securityissues have become critical for many applications, the Internet community has developed anenhancement for TCP, called Secure Sockets Layer (SSL) .
TCP-enhanced-with-SSL not only
does everything that traditional TCP does but also provides critical process-to-process security services, including encryption, data integrity, and end-point authentication.
We emphasize that SSL is not a third Internet transport protocol, on the same level as TCP and UDP, but instead isan enhancement of TCP, with the enhancements being implemented in the application layer.
Inparticular, if an application wants to use the services of SSL, it needs to include SSL code(existing, highly optimized libraries and classes) in both the client and server sides of the application.
SSL has its own socket API that is similar to the traditional TCP socket API.
When an application uses SSL, the sending process passes cleartext data to the SSL socket; SSL inthe sending host then encrypts the data and passes the encrypted data to the TCP socket.
Theencrypted data travels over the Internet to the TCP socket in the receiving process.
Thereceiving socket passes the encrypted data to SSL, which decrypts the data.
Finally, SSLpasses the cleartext data through its SSL socket to the receiving process.
We’ll cover SSL in some detail in Chapter 8.
in Chapter 3, TCP congestion control also attempts to limit each TCP connection to its fair share of network bandwidth.
UDP Services UDP is a no-frills, lightweight transport protocol, providing minimal services.
UDP is connectionless, so there is no handshaking before the two processes start to communicate.
UDP provides an unreliable data transfer service—that is, when a process sends a message into a UDP socket, UDP provides no guarantee that the message will ever reach the receiving process.
Furthermore, messages that do arrive at the receiving process may arrive out of order.
UDP does not include a congestion-control mechanism, so the sending side of UDP can pump data into the layer below (the network layer) at any rate it pleases. (
Note, however, that the actual end-to-endthroughput may be less than this rate due to the limited transmission capacity of intervening links or dueto congestion).
Services Not Provided by Internet Transport Protocols We have organized transport protocol services along four dimensions: reliable data transfer, throughput, timing, and security.
Which of these services are provided by TCP and UDP?
We have already notedthat TCP provides reliable end-to-end data transfer.
And we also know that TCP can be easily enhancedat the application layer with SSL to provide security services.
But in our brief description of TCP and UDP, conspicuously missing was any mention of throughput or timing guarantees— services not provided by today’s Internet transport protocols.
Does this mean that time-sensitive applications such as Internet telephony cannot run in today’s Internet?
The answer is clearly no—the Internet has been hosting time-sensitive applications for many years.
These applications often work fairly well because
they have been designed to cope, to the greatest extent possible, with this lack of guarantee.
We’ll investigate several of these design tricks in Chapter 9.
Nevertheless, clever design has its limitations when delay is excessive, or the end-to-end throughput is limited.
In summary, today’s Internet can often provide satisfactory service to time-sensitive applications, but it cannot provide any timing or throughputguarantees.
Figure 2.5 indicates the transport protocols used by some popular Internet applications.
We see that e- mail, remote terminal access, the Web, and file transfer all use TCP.
These applications have chosen TCP primarily because TCP provides reliable data transfer, guaranteeing that all data will eventually getto its destination.
Because Internet telephony applications (such as Skype) can often tolerate some lossbut require a minimal rate to be effective, developers of Internet telephony applications usually prefer torun their applications over UDP, thereby circumventing TCP’s congestion control mechanism and packetoverheads.
But because many firewalls are configured to block (most types of) UDP traffic, Internet telephony applications often are designed to use TCP as a backup if UDP communication fails.
Figure 2.5 Popular Internet applications, their application-layer protocols, and their underlying transport protocols 2.1.5 Application-Layer Protocols We have just learned that network processes communicate with each other by sending messages into sockets.
But how are these messages structured?
What are the meanings of the various fields in themessages?
When do the processes send the messages?
These questions bring us into the realm ofapplication-layer protocols.
An application-layer protocol  defines how an application’s processes, running on different end systems, pass messages to each other.
In particular, an application-layer protocol defines:
The types of messages exchanged, for example, request messages and response messages The syntax of the various message types, such as the fields in the message and how the fields are delineated The semantics of the fields, that is, the meaning of the information in the fields Rules for determining when and how a process sends messages and responds to messages Some application-layer protocols are specified in RFCs and are therefore in the public domain.
For example, the Web’s application-layer protocol, HTTP (the HyperText Transfer Protocol [RFC 2616] ), is available as an RFC.
If a browser developer follows the rules of the HTTP RFC, the browser will be able to retrieve Web pages from any Web server that has also followed the rules of the HTTP RFC.
Manyother application-layer protocols are proprietary and intentionally not available in the public domain.
Forexample, Skype uses proprietary application-layer protocols.
It is important to distinguish between network applications and application-layer protocols.
An application-layer protocol is only one piece of a network application (albeit, a very important piece of theapplication from our point of view!).
Let’s look at a couple of examples.
The Web is a client-serverapplication that allows users to obtain documents from Web servers on demand.
The Web application consists of many components, including a standard for document formats (that is, HTML), Web browsers (for example, Firefox and Microsoft Internet Explorer), Web servers (for example, Apache andMicrosoft servers), and an application-layer protocol.
The Web’s application-layer protocol, HTTP,defines the format and sequence of messages exchanged between browser and Web server.
Thus,HTTP is only one piece (albeit, an important piece) of the Web application.
As another example, anInternet e-mail application also has many components, including mail servers that house usermailboxes; mail clients (such as Microsoft Outlook) that allow users to read and create messages; astandard for defining the structure of an e-mail message; and application-layer protocols that define how messages are passed between servers, how messages are passed between servers and mail clients, and how the contents of message headers are to be interpreted.
The principal application-layer protocol for electronic mail is SMTP (Simple Mail Transfer Protocol) [RFC 5321] .
Thus, e-mail’s principal application-layer protocol, SMTP, is only one piece (albeit an important piece) of the e-mail application.
2.1.6 Network Applications Covered in This Book New public domain and proprietary Internet applications are being developed every day.
Rather than covering a large number of Internet applications in an encyclopedic manner, we have chosen to focuson a small number of applications that are both pervasive and important.
In this chapter we discuss fiveimportant applications: the Web, electronic mail, directory service video streaming, and P2Papplications.
We first discuss the Web, not only because it is an enormously popular application, but also because its application-layer protocol, HTTP, is straightforward and easy to understand.
We then discuss electronic mail, the Internet’s first killer application.
E-mail is more complex than the Web in the
sense that it makes use of not one but several application-layer protocols.
After e-mail, we cover DNS, which provides a directory service for the Internet.
Most users do not interact with DNS directly; instead, users invoke DNS indirectly through other applications (including the Web, file transfer, and electronicmail).
DNS illustrates nicely how a piece of core network functionality (network-name to network-address translation) can be implemented at the application layer in the Internet.
We then discuss P2Pfile sharing applications, and complete our application study by discussing video streaming on demand, including distributing stored video over content distribution networks.
In Chapter 9, we’ll cover multimedia applications in more depth, including voice over IP and video conferencing.
2.2 The Web and HTTP Until the early 1990s the Internet was used primarily by researchers, academics, and university students to log in to remote hosts, to transfer files from local hosts to remote hosts and vice versa, to receive andsend news, and to receive and send electronic mail.
Although these applications were (and continue tobe) extremely useful, the Internet was essentially unknown outside of the academic and researchcommunities.
Then, in the early 1990s, a major new application arrived on the scene—the World Wide Web [Berners-Lee 1994] .
The Web was the first Internet application that caught the general public’s eye.
It dramatically changed, and continues to change, how people interact inside and outside their work environments.
It elevated the Internet from just one of many data networks to essentially the one andonly data network.
Perhaps what appeals the most to users is that the Web operates on demand .
Users receive what they want, when they want it.
This is unlike traditional broadcast radio and television, which force users to tune in when the content provider makes the content available.
In addition to being available on demand, the Web has many other wonderful features that people love and cherish.
It is enormously easy for any individual to make information available over the Web—everyone can become a publisherat extremely low cost.
Hyperlinks and search engines help us navigate through an ocean of information.
Photos and videos stimulate our senses.
Forms, JavaScript, Java applets, and many other devicesenable us to interact with pages and sites.
And the Web and its protocols serve as a platform forYouTube, Web-based e-mail (such as Gmail), and most mobile Internet applications, includingInstagram and Google Maps.
2.2.1 Overview of HTTP The HyperText Transfer Protocol (HTTP), the Web’s application-layer protocol, is at the heart of the Web.
It is defined in [RFC 1945]  and [RFC 2616] .
HTTP is implemented in two programs: a client program and a server program.
The client program and server program, executing on different end systems, talk to each other by exchanging HTTP messages.
HTTP defines the structure of thesemessages and how the client and server exchange the messages.
Before explaining HTTP in detail, we should review some Web terminology.
A Web page (also called a document) consists of objects.
An object is simply a file—such as an HTML file, a JPEG image, a Java applet, or a video clip—that is addressable by a single URL.
Most Web pages consist of a base HTML file and several referenced objects.
For example, if a Web page
contains HTML text and five JPEG images, then the Web page has six objects: the base HTML file plus the five images.
The base HTML file references the other objects in the page with the objects’ URLs.
Each URL has two components: the hostname of the server that houses the object and the object’s path name.
For example, the URL http://www.someSchool.edu/someDepartment/picture.gif has www.someSchool.edu  for a hostname and /someDepartment/picture.gif  for a path name.
Because Web browsers (such as Internet Explorer and Firefox) implement the client side of HTTP, in the context of the Web, we will use the words browser  and client interchangeably.
Web servers , which implement the server side of HTTP, house Web objects, each addressable by a URL.
Popular Web servers include Apache and Microsoft Internet Information Server.
HTTP defines how Web clients request Web pages from Web servers and how servers transfer Web pages to clients.
We discuss the interaction between client and server in detail later, but the general idea is illustrated in Figure 2.6.
When a user requests a Web page (for example, clicks on a hyperlink), the browser sends HTTP request messages for the objects in the page to the server.
The server receives the requests and responds with HTTP response messages that contain the objects.
HTTP uses TCP as its underlying transport protocol (rather than running on top of UDP).
The HTTP client first initiates a TCP connection with the server.
Once the connection is established, the browser and the server processes access TCP through their socket interfaces.
As described in Section 2.1, on the client side the socket interface is the door between the client process and the TCP connection; on the server side it is the door between the server process and the TCP connection.
The client sendsHTTP request messages into its socket interface and receives HTTP response messages from itssocket interface.
Similarly, the HTTP server receives request messages
Figure 2.6 HTTP request-response behavior from its socket interface and sends response messages into its socket interface.
Once the client sends a message into its socket interface, the message is out of the client’s hands and is “in the hands” of TCP.
Recall from Section 2.1 that TCP provides a reliable data transfer service to HTTP.
This implies that each HTTP request message sent by a client process eventually arrives intact at the server; similarly, each HTTP response message sent by the server process eventually arrives intact at the client.
Herewe see one of the great advantages of a layered architecture—HTTP need not worry about lost data orthe details of how TCP recovers from loss or reordering of data within the network.
That is the job ofTCP and the protocols in the lower layers of the protocol stack.
It is important to note that the server sends requested files to clients without storing any state information about the client.
If a particular client asks for the same object twice in a period of a few seconds, the server does not respond by saying that it just served the object to the client; instead, theserver resends the object, as it has completely forgotten what it did earlier.
Because an HTTP servermaintains no information about the clients, HTTP is said to be a stateless protocol.
We also remark that the Web uses the client-server application architecture, as described in Section 2.1.
A Web server is always on, with a fixed IP address, and it services requests from potentially millions of differentbrowsers.
2.2.2 Non-Persistent and Persistent Connections In many Internet applications, the client and server communicate for an extended period of time, with theclient making a series of requests and the server responding to each of the requests.
Depending on theapplication and on how the application is being used, the series of requests may be made back-to-back,periodically at regular intervals, or intermittently.
When this client-server interaction is taking place overTCP, the application developer needs to make an important decision—should each request/response pair be sent over a separate  TCP connection, or should all of the requests and their corresponding responses be sent over the same TCP connection?
In the former approach, the application is said to use non-persistent connections ; and in the latter approach, persistent connections.
To gain a deep understanding of this design issue, let’s examine the advantages and disadvantages of persistent connections in the context of a specific application, namely, HTTP, which can use both non-persistentconnections and persistent connections.
Although HTTP uses persistent connections in its defaultmode, HTTP clients and servers can be configured to use non-persistent connections instead.
HTTP with Non-Persistent Connections
Let’s walk through the steps of transferring a Web page from server to client for the case of non- persistent connections.
Let’s suppose the page consists of a base HTML file and 10 JPEG images, and that all 11 of these objects reside on the same server.
Further suppose the URL for the base HTML file is http://www.someSchool.edu/someDepartment/home.index Here is what happens: 1.
The HTTP client process initiates a TCP connection to the server www.someSchool.edu  on port number 80, which is the default port number for HTTP.
Associated with the TCP connection, there will be a socket at the client and a socket at the server.
2.
The HTTP client sends an HTTP request message to the server via its socket.
The request message includes the path name /someDepartment/home .index . (
We will discuss HTTP messages in some detail below.)
3.
The HTTP server process receives the request message via its socket, retrieves the object /someDepartment/home.index  from its storage (RAM or disk), encapsulates the object in an HTTP response message, and sends the response message to the client via its socket.
4.
The HTTP server process tells TCP to close the TCP connection. (
But TCP doesn’t actually terminate the connection until it knows for sure that the client has received the response message intact.)
5.
The HTTP client receives the response message.
The TCP connection terminates.
The message indicates that the encapsulated object is an HTML file.
The client extracts the file from the response message, examines the HTML file, and finds references to the 10 JPEG objects.
6.
The first four steps are then repeated for each of the referenced JPEG objects.
As the browser receives the Web page, it displays the page to the user.
Two different browsers mayinterpret (that is, display to the user) a Web page in somewhat different ways.
HTTP has nothing to do with how a Web page is interpreted by a client.
The HTTP specifications ( [RFC 1945]  and [RFC 2616] ) define only the communication protocol between the client HTTP program and the server HTTP program.
The steps above illustrate the use of non-persistent connections, where each TCP connection is closed after the server sends the object—the connection does not persist for other objects.
Note that each TCPconnection transports exactly one request message and one response message.
Thus, in this example,when a user requests the Web page, 11 TCP connections are generated.
In the steps described above, we were intentionally vague about whether the client obtains the 10
JPEGs over 10 serial TCP connections, or whether some of the JPEGs are obtained over parallel TCP connections.
Indeed, users can configure modern browsers to control the degree of parallelism.
In their default modes, most browsers open 5 to 10 parallel TCP connections, and each of these connectionshandles one request-response transaction.
If the user prefers, the maximum number of parallelconnections can be set to one, in which case the 10 connections are established serially.
As we’ll see in the next chapter, the use of parallel connections shortens the response time.
Before continuing, let’s do a back-of-the-envelope calculation to estimate the amount of time that elapses from when a client requests the base HTML file until the entire file is received by the client.
Tothis end, we define the round-trip time (RTT) , which is the time it takes for a small packet to travel from client to server and then back to the client.
The RTT includes packet-propagation delays, packet-queuing delays in intermediate routers and switches, and packet-processing delays. (
These delays were discussed in Section 1.4.)
Now consider what happens when a user clicks on a hyperlink.
As shown in Figure 2.7, this causes the browser to initiate a TCP connection between the browser and the Web server; this involves a “three-way handshake”—the client sends a small TCP segment to the server, the server acknowledges and responds with a small TCP segment, and, finally, the client acknowledgesback to the server.
The first two parts of the three-way handshake take one RTT.
After completing thefirst two parts of the handshake, the client sends the HTTP request message combined with the thirdpart of the three-way handshake (the acknowledgment) into the TCP connection.
Once the requestmessage arrives at
Figure 2.7 Back-of-the-envelope calculation for the time needed to request and receive an HTML file the server, the server sends the HTML file into the TCP connection.
This HTTP request/response eats up another RTT.
Thus, roughly, the total response time is two RTTs plus the transmission time at the server of the HTML file.
HTTP with Persistent Connections Non-persistent connections have some shortcomings.
First, a brand-new connection must be established and maintained for each requested object .
For each of these connections, TCP buffers must be allocated and TCP variables must be kept in both the client and server.
This can place a significant burden on the Web server, which may be serving requests from hundreds of different clientssimultaneously.
Second, as we just described, each object suffers a delivery delay of two RTTs—one RTT to establish the TCP connection and one RTT to request and receive an object.
With HTTP 1.1 persistent connections, the server leaves the TCP connection open after sending a response.
Subsequent requests and responses between the same client and server can be sent overthe same connection.
In particular, an entire Web page (in the example above, the base HTML file andthe 10 images) can be sent over a single persistent TCP connection.
Moreover, multiple Web pagesresiding on the same server can be sent from the server to the same client over a single persistent TCPconnection.
These requests for objects can be made back-to-back, without waiting for replies to pending requests (pipelining).
Typically, the HTTP server closes a connection when it isn’t used for a certain time (a configurable timeout interval).
When the server receives the back-to-back requests, it sends theobjects back-to-back.
The default mode of HTTP uses persistent connections with pipelining.
Most recently, HTTP/2 [RFC 7540]  builds on HTTP 1.1 by allowing multiple requests and replies to be interleaved in the same connection, and a mechanism for prioritizing HTTP message requests and replies within this connection.
We’ll quantitatively compare the performance of non-persistent andpersistent connections in the homework problems of Chapters 2 and 3.
You are also encouraged to see [Heidemann 1997 ; Nielsen 1997; RFC 7540] .
2.2.3 HTTP Message Format The HTTP specifications [RFC 1945 ; RFC 2616 ; RFC 7540]  include the definitions of the HTTP message formats.
There are two types of HTTP messages, request messages and response messages, both of which are discussed below.
HTTP Request Message
Below we provide a typical HTTP request message: GET /somedir/page.html HTTP/1.1 Host: www.someschool.edu Connection: close User-agent: Mozilla/5.0Accept-language: fr We can learn a lot by taking a close look at this simple request message.
First of all, we see that themessage is written in ordinary ASCII text, so that your ordinary computer-literate human being can readit.
Second, we see that the message consists of five lines, each followed by a carriage return and a linefeed.
The last line is followed by an additional carriage return and line feed.
Although this particularrequest message has five lines, a request message can have many more lines or as few as one line.
The first line of an HTTP request message is called the request line; the subsequent lines are called the header lines.
The request line has three fields: the method field, the URL field, and the HTTP version field.
The method field can take on several different values, including GET, POST, HEAD, PUT,  and DELETE .
The great majority of HTTP request messages use the GET method.
The GET method is used when the browser requests an object, with the requested object identified in the URLfield.
In this example, the browser is requesting the object /somedir/page.html .
The version is self- explanatory; in this example, the browser implements version HTTP/1.1.
Now let’s look at the header lines in the example.
The header line Host: www.someschool.edu specifies the host on which the object resides.
You might think that this header line is unnecessary, as there is already a TCP connection in place to the host.
But, as we’ll see in Section 2.2.5, the information provided by the host header line is required by Web proxy caches.
By including the Connection: close  header line, the browser is telling the server that it doesn’t want to bother with persistent connections; it wants the server to close the connection after sending the requested object.
The User- agent:  header line specifies the user agent, that is, the browser type that is making the request to the server.
Here the user agent is Mozilla/5.0, a Firefox browser.
This header line is useful because the server can actually send different versions of the same object to different types of user agents. (
Each of the versions is addressed by the same URL.)
Finally, the Accept-language:  header indicates that the user prefers to receive a French version of the object, if such an object exists on the server;otherwise, the server should send its default version.
The Accept-language:  header is just one of many content negotiation headers available in HTTP.
Having looked at an example, let’s now look at the general format of a request message, as shown in Figure 2.8.
We see that the general format closely follows our earlier example.
You may have noticed,
however, that after the header lines (and the additional carriage return and line feed) there is an “entity body.”
The entity body is empty with the GET method, but is used with the POST  method.
An HTTP client often uses the POST  method when the user fills out a form—for example, when a user provides search words to a search engine.
With a POST  message, the user is still requesting a Web page from the server, but the specific contents of the Web page Figure 2.8 General format of an HTTP request message depend on what the user entered into the form fields.
If the value of the method field is POST , then the entity body contains what the user entered into the form fields.
We would be remiss if we didn’t mention that a request generated with a form does not necessarily use the POST  method.
Instead, HTML forms often use the GET method and include the inputted data (in the form fields) in the requested URL.
For example, if a form uses the GET method, has two fields, and the inputs to the two fields are monkeys  and bananas , then the URL will have the structure www.somesite.com/ animalsearch?monkeys&bananas .
In your day-to-day Web surfing, you have probably noticed extended URLs of this sort.
The HEAD  method is similar to the GET method.
When a server receives a request with the HEAD method, it responds with an HTTP message but it leaves out the requested object.
Application developers often use the HEAD  method for debugging.
The PUT method is often used in conjunction with Web publishing tools.
It allows a user to upload an object to a specific path (directory) on a specific Web server.
The PUT method is also used by applications that need to upload objects to Web servers.
The DELETE  method allows a user, or an application, to delete an object on a Web server.
HTTP Response Message
Below we provide a typical HTTP response message.
This response message could be the response to the example request message just discussed.
HTTP/1.1 200 OK Connection: closeDate: Tue, 18 Aug 2015 15:44:04 GMTServer: Apache/2.2.3 (CentOS)Last-Modified: Tue, 18 Aug 2015 15:11:03 GMTContent-Length: 6821Content-Type: text/html (data data data data data ...) Let’s take a careful look at this response message.
It has three sections: an initial status line, six header lines, and then the entity body .
The entity body is the meat of the message—it contains the requested object itself (represented by data data data data data ... ).
The status line has three fields: the protocol version field, a status code, and a corresponding status message.
In this example, the status line indicates that the server is using HTTP/1.1 and that everything is OK (that is,the server has found, and is sending, the requested object).
Now let’s look at the header lines.
The server uses the Connection: close  header line to tell the client that it is going to close the TCP connection after sending the message.
The Date:  header line indicates the time and date when the HTTP response was created and sent by the server.
Note that this is not the time when the object was created or last modified; it is the time when the server retrieves theobject from its file system, inserts the object into the response message, and sends the response message.
The Server:  header line indicates that the message was generated by an Apache Web server; it is analogous to the User-agent:  header line in the HTTP request message.
The Last- Modified:  header line indicates the time and date when the object was created or last modified.
The Last-Modified:  header, which we will soon cover in more detail, is critical for object caching, both in the local client and in network cache servers (also known as proxy servers).
The Content-Length: header line indicates the number of bytes in the object being sent.
The Content-Type:  header line indicates that the object in the entity body is HTML text. (
The object type is officially indicated by the Content-Type:  header and not by the file extension.)
Having looked at an example, let’s now examine the general format of a response message, which is shown in Figure 2.9.
This general format of the response message matches the previous example of a response message.
Let’s say a few additional words about status codes and their phrases.
The status
code and associated phrase indicate the result of the request.
Some common status codes and associated phrases include: 200 OK:  Request succeeded and the information is returned in the response.
301 Moved Permanently:  Requested object has been permanently moved; the new URL is specified in Location : header of the response message.
The client software will automatically retrieve the new URL.
400 Bad Request:  This is a generic error code indicating that the request could not be understood by the server.
Figure 2.9 General format of an HTTP response message 404 Not Found:  The requested document does not exist on this server.
505 HTTP Version Not Supported:  The requested HTTP protocol version is not supported by the server.
How would you like to see a real HTTP response message?
This is highly recommended and very easy to do!
First Telnet into your favorite Web server.
Then type in a one-line request message for someobject that is housed on the server.
For example, if you have access to a command prompt, type: Using Wireshark to investigate the HTTP protocol
telnet gaia.cs.umass.edu 80 GET /kurose_ross/interactive/index.php HTTP/1.1 Host: gaia.cs.umass.edu (Press the carriage return twice after typing the last line.)
This opens a TCP connection to port 80 of the host gaia.cs.umass.edu  and then sends the HTTP request message.
You should see a response message that includes the base HTML file for the interactive homework problems for this textbook.
Ifyou’d rather just see the HTTP message lines and not receive the object itself, replace GET with HEAD .
In this section we discussed a number of header lines that can be used within HTTP request and response messages.
The HTTP specification defines many, many more header lines that can be inserted by browsers, Web servers, and network cache servers.
We have covered only a small numberof the totality of header lines.
We’ll cover a few more below and another small number when we discuss network Web caching in Section 2.2.5.
A highly readable and comprehensive discussion of the HTTP protocol, including its headers and status codes, is given in [Krishnamurthy 2001] .
How does a browser decide which header lines to include in a request message?
How does a Web server decide which header lines to include in a response message?
A browser will generate headerlines as a function of the browser type and version (for example, an HTTP/1.0 browser will not generate any 1.1 header lines), the user configuration of the browser (for example, preferred language), and whether the browser currently has a cached, but possibly out-of-date, version of the object.
Web serversbehave similarly: There are different products, versions, and configurations, all of which influence whichheader lines are included in response messages.
2.2.4 User-Server Interaction: Cookies We mentioned above that an HTTP server is stateless.
This simplifies server design and has permittedengineers to develop high-performance Web servers that can handle thousands of simultaneous TCPconnections.
However, it is often desirable for a Web site to identify users, either because the serverwishes to restrict user access or because it wants to serve content as a function of the user identity.
For these purposes, HTTP uses cookies.
Cookies, defined in [RFC 6265] , allow sites to keep track of users.
Most major commercial Web sites use cookies today.
As shown in Figure 2.10, cookie technology has four components: (1) a cookie header line in the HTTP response message; (2) a cookie header line in the HTTP request message; (3) a cookie file kept on the
user’s end system and managed by the user’s browser; and (4) a back-end database at the Web site.
Using Figure 2.10, let’s walk through an example of how cookies work.
Suppose Susan, who always accesses the Web using Internet Explorer from her home PC, contacts Amazon.com for the first time.
Let us suppose that in the past she has already visited the eBay site.
When the request comes into theAmazon Web server, the server creates a unique identification number and creates an entry in its back- end database that is indexed by the identification number.
The Amazon Web server then responds to Susan’s browser, including in the HTTP response a Set-cookie:  header, which contains the identification number.
For example, the header line might be: Set-cookie: 1678 When Susan’s browser receives the HTTP response message, it sees the Set-cookie:  header.
The browser then appends a line to the special cookie file that it manages.
This line includes the hostnameof the server and the identification number in the Set-cookie:  header.
Note that the cookie file already has an entry for eBay, since Susan has visited that site in the past.
As Susan continues to browse the Amazon site, each time she requests a Web page, her browser consults her cookie file,extracts her identification number for this site, and puts a cookie header line that
Figure 2.10 Keeping user state with cookies includes the identification number in the HTTP request.
Specifically, each of her HTTP requests to the Amazon server includes the header line: Cookie: 1678 In this manner, the Amazon server is able to track Susan’s activity at the Amazon site.
Although the Amazon Web site does not necessarily know Susan’s name, it knows exactly which pages user 1678visited, in which order, and at what times!
Amazon uses cookies to provide its shopping cart service— Amazon can maintain a list of all of Susan’s intended purchases, so that she can pay for them
collectively at the end of the session.
If Susan returns to Amazon’s site, say, one week later, her browser will continue to put the header line Cookie: 1678  in the request messages.
Amazon also recommends products to Susan based on Web pages she has visited at Amazon in the past.
If Susan also registers herself with Amazon— providing full name, e-mail address, postal address, and credit card information—Amazon can theninclude this information in its database, thereby associating Susan’s name with her identification number(and all of the pages she has visited at the site in the past!).
This is how Amazon and other e-commercesites provide “one-click shopping”—when Susan chooses to purchase an item during a subsequent visit,she doesn’t need to re-enter her name, credit card number, or address.
From this discussion we see that cookies can be used to identify a user.
The first time a user visits a site, the user can provide a user identification (possibly his or her name).
During the subsequent sessions, the browser passes a cookie header to the server, thereby identifying the user to the server.
Cookies can thus be used to create a user session layer on top of stateless HTTP.
For example, when auser logs in to a Web-based e-mail application (such as Hotmail), the browser sends cookie informationto the server, permitting the server to identify the user throughout the user’s session with the application.
Although cookies often simplify the Internet shopping experience for the user, they are controversial because they can also be considered as an invasion of privacy.
As we just saw, using a combination ofcookies and user-supplied account information, a Web site can learn a lot about a user and potentially sell this information to a third party.
Cookie Central [Cookie Central 2016]  includes extensive information on the cookie controversy.
2.2.5 Web Caching A Web cache—also called a proxy server —is a network entity that satisfies HTTP requests on the behalf of an origin Web server.
The Web cache has its own disk storage and keeps copies of recentlyrequested objects in this storage.
As shown in Figure 2.11, a user’s browser can be configured so that all of the user’s HTTP requests are first directed to the Web cache.
Once a browser is configured, each browser request for an object is first directed to the Web cache.
As an example, suppose a browser is requesting the object http://www.someschool.edu/campus.gif .
Here is what happens: 1.
The browser establishes a TCP connection to the Web cache and sends an HTTP request for the object to the Web cache.
2.
The Web cache checks to see if it has a copy of the object stored locally.
If it does, the Webcache returns the object within an HTTP response message to the client browser.
Figure 2.11 Clients requesting objects through a Web cache 3.
If the Web cache does not have the object, the Web cache opens a TCP connection to the origin server, that is, to www.someschool.edu .
The Web cache then sends an HTTP request for the object into the cache-to-server TCP connection.
After receiving this request, the origin serversends the object within an HTTP response to the Web cache.
4.
When the Web cache receives the object, it stores a copy in its local storage and sends a copy, within an HTTP response message, to the client browser (over the existing TCP connection between the client browser and the Web cache).
Note that a cache is both a server and a client at the same time.
When it receives requests from andsends responses to a browser, it is a server.
When it sends requests to and receives responses from anorigin server, it is a client.
Typically a Web cache is purchased and installed by an ISP.
For example, a university might install a cache on its campus network and configure all of the campus browsers to point to the cache.
Or a majorresidential ISP (such as Comcast) might install one or more caches in its network and preconfigure itsshipped browsers to point to the installed caches.
Web caching has seen deployment in the Internet for two reasons.
First, a Web cache can substantially reduce the response time for a client request, particularly if the bottleneck bandwidth between the clientand the origin server is much less than the bottleneck bandwidth between the client and the cache.
Ifthere is a high-speed connection between the client and the cache, as there often is, and if the cachehas the requested object, then the cache will be able to deliver the object rapidly to the client.
Second,as we will soon illustrate with an example, Web caches can substantially reduce traffic on an institution’saccess link to the Internet.
By reducing traffic, the institution (for example, a company or a university)does not have to upgrade bandwidth as quickly, thereby reducing costs.
Furthermore, Web caches can
substantially reduce Web traffic in the Internet as a whole, thereby improving performance for all applications.
To gain a deeper understanding of the benefits of caches, let’s consider an example in the context of Figure 2.12.
This figure shows two networks—the institutional network and the rest of the public Internet.
The institutional network is a high-speed LAN.
A router in the institutional network and a router in the Internet are connected by a 15 Mbps link.
The origin servers are attached to the Internet but are located all over the globe.
Suppose that the average object size is 1 Mbits and that the average requestrate from the institution’s browsers to the origin servers is 15 requests per second.
Suppose that theHTTP request messages are negligibly small and thus create no traffic in the networks or in the accesslink (from institutional router to Internet router).
Also suppose that the amount of time it takes from when the router on the Internet side of the access link in Figure 2.12 forwards an HTTP request (within an IP datagram) until it receives the response (typically within many IP datagrams) is two seconds on average.
Informally, we refer to this last delay as the “Internet delay.”
Figure 2.12 Bottleneck between an institutional network and the Internet The total response time—that is, the time from the browser’s request of an object until its receipt of the object—is the sum of the LAN delay, the access delay (that is, the delay between the two routers), and
the Internet delay.
Let’s now do a very crude calculation to estimate this delay.
The traffic intensity on the LAN (see Section 1.4.2) is whereas the traffic intensity on the access link (from the Internet router to institution router) is A traffic intensity of 0.15 on a LAN typically results in, at most, tens of milliseconds of delay; hence, we can neglect the LAN delay.
However, as discussed in Section 1.4.2, as the traffic intensity approaches 1 (as is the case of the access link in Figure 2.12), the delay on a link becomes very large and grows without bound.
Thus, the average response time to satisfy requests is going to be on the order of minutes, if not more, which is unacceptable for the institution’s users.
Clearly something must be done.
One possible solution is to increase the access rate from 15 Mbps to, say, 100 Mbps.
This will lower the traffic intensity on the access link to 0.15, which translates to negligible delays between the two routers.
In this case, the total response time will roughly be two seconds, that is, the Internet delay.
But thissolution also means that the institution must upgrade its access link from 15 Mbps to 100 Mbps, a costlyproposition.
Now consider the alternative solution of not upgrading the access link but instead installing a Web cache in the institutional network.
This solution is illustrated in Figure 2.13.
Hit rates—the fraction of requests that are satisfied by a cache— typically range from 0.2 to 0.7 in practice.
For illustrative purposes, let’s suppose that the cache provides a hit rate of 0.4 for this institution.
Because the clients and the cacheare connected to the same high-speed LAN, 40 percent of the requests will be satisfied almostimmediately, say, within 10 milliseconds, by the cache.
Nevertheless, the remaining 60 percent of therequests still need to be satisfied by the origin servers.
But with only 60 percent of the requested objectspassing through the access link, the traffic intensity on the access link is reduced from 1.0 to 0.6.Typically, a traffic intensity less than 0.8 corresponds to a small delay, say, tens of milliseconds, on a 15Mbps link.
This delay is negligible compared with the two-second Internet delay.
Given these considerations, average delay therefore is which is just slightly greater than 1.2 seconds.
Thus, this second solution provides an even lower response time than the first solution, and it doesn’t require the institution(15 requests/sec)⋅(1 Mbits/request)/(100 Mbps)=0.15 (15 requests/sec)⋅(1 Mbits/request)/(15 Mbps)=1 0.4⋅(0.01 seconds)+0.6⋅(2.01 seconds)
Figure 2.13 Adding a cache to the institutional network to upgrade its link to the Internet.
The institution does, of course, have to purchase and install a Web cache.
But this cost is low—many caches use public-domain software that runs on inexpensive PCs.
Through the use of Content Distribution Networks (CDNs), Web caches are increasingly playing an important role in the Internet.
A CDN company installs many geographically distributed cachesthroughout the Internet, thereby localizing much of the traffic.
There are shared CDNs (such as Akamaiand Limelight) and dedicated CDNs (such as Google and Netflix).
We will discuss CDNs in more detail in Section 2.6.
The Conditional GET Although caching can reduce user-perceived response times, it introduces a new problem—the copy of an object residing in the cache may be stale.
In other words, the object housed in the Web server mayhave been modified since the copy was cached at the client.
Fortunately, HTTP has a mechanism thatallows a cache to verify that its objects are up to date.
This mechanism is called the conditional GET .
An HTTP request message is a so-called conditional GET message if (1) the request message uses the GET method and (2) the request message includes an If-Modified-Since:  header line.
To illustrate how the conditional GET operates, let’s walk through an example.
First, on the behalf of a requesting browser, a proxy cache sends a request message to a Web server: GET /fruit/kiwi.gif HTTP/1.1 Host: www.exotiquecuisine.com Second, the Web server sends a response message with the requested object to the cache: HTTP/1.1 200 OK Date: Sat, 3 Oct 2015 15:39:29Server: Apache/1.3.0 (Unix)Last-Modified: Wed, 9 Sep 2015 09:23:24Content-Type: image/gif (data data data data data ...) The cache forwards the object to the requesting browser but also caches the object locally.
Importantly, the cache also stores the last-modified date along with the object.
Third, one week later, anotherbrowser requests the same object via the cache, and the object is still in the cache.
Since this objectmay have been modified at the Web server in the past week, the cache performs an up-to-date check byissuing a conditional GET.
Specifically, the cache sends: GET /fruit/kiwi.gif HTTP/1.1 Host: www.exotiquecuisine.comIf-modified-since: Wed, 9 Sep 2015 09:23:24 Note that the value of the If-modified-since:  header line is exactly equal to the value of the Last-Modified:  header line that was sent by the server one week ago.
This conditional GET is telling the server to send the object only if the object has been modified since the specified date.
Suppose the object has not been modified since 9 Sep 2015 09:23:24.
Then, fourth, the Web serversends a response message to the cache:
HTTP/1.1 304 Not Modified Date: Sat, 10 Oct 2015 15:39:29 Server: Apache/1.3.0 (Unix) (empty entity body) We see that in response to the conditional GET, the Web server still sends a response message but does not include the requested object in the response message.
Including the requested object would only waste bandwidth and increase user-perceived response time, particularly if the object is large.
Note that this last response message has 304 Not Modified  in the status line, which tells the cache that it can go ahead and forward its (the proxy cache’s) cached copy of the object to the requesting browser.
This ends our discussion of HTTP, the first Internet protocol (an application-layer protocol) that we’ve studied in detail.
We’ve seen the format of HTTP messages and the actions taken by the Web client and server as these messages are sent and received.
We’ve also studied a bit of the Web’s application infrastructure, including caches, cookies, and back-end databases, all of which are tied in some way tothe HTTP protocol.
2.3 Electronic Mail in the Internet Electronic mail has been around since the beginning of the Internet.
It was the most popular application when the Internet was in its infancy [Segaller 1998] , and has become more elaborate and powerful over the years.
It remains one of the Internet’s most important and utilized applications.
As with ordinary postal mail, e-mail is an asynchronous communication medium—people send and read messages when it is convenient for them, without having to coordinate with other people’s schedules.
In contrast with postal mail, electronic mail is fast, easy to distribute, and inexpensive.
Modern e-mail hasmany powerful features, including messages with attachments, hyperlinks, HTML-formatted text, andembedded photos.
In this section, we examine the application-layer protocols that are at the heart of Internet e-mail.
But before we jump into an in-depth discussion of these protocols, let’s take a high-level view of the Internetmail system and its key components.
Figure 2.14 presents a high-level view of the Internet mail system.
We see from this diagram that it has three major components: user agents, mail servers , and the Simple Mail Transfer Protocol (SMTP) .
We now describe each of these components in the context of a sender, Alice, sending an e-mail message to a recipient, Bob.
User agents allow users to read, reply to, forward, save, and composemessages.
Microsoft Outlook and Apple Mail are examples of user agents for e-mail.
When Alice isfinished composing her message, her user agent sends the message to her mail server, where themessage is placed in the mail server’s outgoing message queue.
When Bob wants to read a message,his user agent retrieves the message from his mailbox in his mail server.
Mail servers form the core of the e-mail infrastructure.
Each recipient, such as Bob, has a mailbox located in one of the mail servers.
Bob’s mailbox manages and
Figure 2.14 A high-level view of the Internet e-mail system maintains the messages that have been sent to him.
A typical message starts its journey in the sender’s user agent, travels to the sender’s mail server, and travels to the recipient’s mail server, where it is deposited in the recipient’s mailbox.
When Bob wants to access the messages in his mailbox, the mailserver containing his mailbox authenticates Bob (with usernames and passwords).
Alice’s mail servermust also deal with failures in Bob’s mail server.
If Alice’s server cannot deliver mail to Bob’s server,Alice’s server holds the message in a message queue and attempts to transfer the message later.
Reattempts are often done every 30 minutes or so; if there is no success after several days, the serverremoves the message and notifies the sender (Alice) with an e-mail message.
SMTP is the principal application-layer protocol for Internet electronic mail.
It uses the reliable data transfer service of TCP to transfer mail from the sender’s mail server to the recipient’s mail server.
As with most application-layer protocols, SMTP has two sides: a client side, which executes on the sender’s mail server, and a server side, which executes on the recipient’s mail server.
Both the client and serversides of SMTP run on every mail server.
When a mail server sends mail to other mail servers, it acts as an SMTP client.
When a mail server receives mail from other mail servers, it acts as an SMTP server.
2.3.1 SMTP SMTP, defined in RFC 5321, is at the heart of Internet electronic mail.
As mentioned above, SMTP transfers messages from senders’ mail servers to the recipients’ mail servers.
SMTP is much older thanHTTP. (
The original SMTP RFC dates back to 1982, and SMTP was around long before that.)
AlthoughSMTP has numerous wonderful qualities, as evidenced by its ubiquity in the Internet, it is nevertheless alegacy technology that possesses certain archaic characteristics.
For example, it restricts the body (not just the headers) of all mail messages to simple 7-bit ASCII.
This restriction made sense in the early 1980s when transmission capacity was scarce and no one was e-mailing large attachments or largeimage, audio, or video files.
But today, in the multimedia era, the 7-bit ASCII restriction is a bit of a pain—it requires binary multimedia data to be encoded to ASCII before being sent over SMTP; and itrequires the corresponding ASCII message to be decoded back to binary after SMTP transport.
Recall from Section 2.2 that HTTP does not require multimedia data to be ASCII encoded before transfer.
To illustrate the basic operation of SMTP, let’s walk through a common scenario.
Suppose Alice wants to send Bob a simple ASCII message.
1.
Alice invokes her user agent for e-mail, provides Bob’s e-mail address (for example, bob@someschool.edu ), composes a message, and instructs the user agent to send the message.
2.
Alice’s user agent sends the message to her mail server, where it is placed in a message queue.
3.
The client side of SMTP, running on Alice’s mail server, sees the message in the message queue.
It opens a TCP connection to an SMTP server, running on Bob’s mail server.
4.
After some initial SMTP handshaking, the SMTP client sends Alice’s message into the TCPconnection.
5.
At Bob’s mail server, the server side of SMTP receives the message.
Bob’s mail server thenplaces the message in Bob’s mailbox.
6.
Bob invokes his user agent to read the message at his convenience.
The scenario is summarized in Figure 2.15.
It is important to observe that SMTP does not normally use intermediate mail servers for sending mail, even when the two mail servers are located at opposite ends of the world.
If Alice’s server is in HongKong and Bob’s server is in St. Louis, the TCP
Figure 2.15 Alice sends a message to Bob connection is a direct connection between the Hong Kong and St. Louis servers.
In particular, if Bob’s mail server is down, the message remains in Alice’s mail server and waits for a new attempt—the message does not get placed in some intermediate mail server.
Let’s now take a closer look at how SMTP transfers a message from a sending mail server to a receiving mail server.
We will see that the SMTP protocol has many similarities with protocols that areused for face-to-face human interaction.
First, the client SMTP (running on the sending mail server host)has TCP establish a connection to port 25 at the server SMTP (running on the receiving mail serverhost).
If the server is down, the client tries again later.
Once this connection is established, the serverand client perform some application-layer handshaking—just as humans often introduce themselves before transferring information from one to another, SMTP clients and servers introduce themselves before transferring information.
During this SMTP handshaking phase, the SMTP client indicates the e-mail address of the sender (the person who generated the message) and the e-mail address of therecipient.
Once the SMTP client and server have introduced themselves to each other, the client sendsthe message.
SMTP can count on the reliable data transfer service of TCP to get the message to theserver without errors.
The client then repeats this process over the same TCP connection if it has othermessages to send to the server; otherwise, it instructs TCP to close the connection.
Let’s next take a look at an example transcript of messages exchanged between an SMTP client (C) and an SMTP server (S).
The hostname of the client is crepes.fr  and the hostname of the server is hamburger.edu .
The ASCII text lines prefaced with C: are exactly the lines the client sends into its TCP socket, and the ASCII text lines prefaced with S: are exactly the lines the server sends into its TCP socket.
The following transcript begins as soon as the TCP connection is established.
S:  220 hamburger.edu C:  HELO crepes.fr S:  250 Hello crepes.fr, pleased to meet you
C:  MAIL FROM: <alice@crepes.fr> S:  250 alice@crepes.fr ... Sender okC:  RCPT TO: <bob@hamburger.edu>S:  250 bob@hamburger.edu ... Recipient okC:  DATA S:  354 Enter mail, end with ”.”
on a line by itself C:  Do you like ketchup?C:  How about pickles?C:  .S:  250 Message accepted for deliveryC:  QUITS:  221 hamburger.edu closing connection In the example above, the client sends a message (“ Do you like ketchup?
How about pickles? ”)
from mail server crepes.fr  to mail server hamburger.edu .
As part of the dialogue, the client issued five commands: HELO  (an abbreviation for HELLO), MAIL FROM , RCPT TO , DATA , and QUIT .
These commands are self-explanatory.
The client also sends a line consisting of a single period, which indicates the end of the message to the server. (
In ASCII jargon, each message ends with CRLF.CRLF , where CR and LF stand for carriage return and line feed, respectively.)
The server issues replies to each command, with each reply having a reply code and some (optional) English-language explanation.
We mention here that SMTP uses persistent connections: If the sending mail server has several messages to send to the same receiving mail server, it can send all of the messages over the same TCP connection.
For each message, the client begins the process with a new MAIL FROM: crepes.fr , designates the end of message with an isolated period, and issues QUIT  only after all messages have been sent.
It is highly recommended that you use Telnet to carry out a direct dialogue with an SMTP server.
To do this, issue telnet serverName 25 where serverName  is the name of a local mail server.
When you do this, you are simply establishing a TCP connection between your local host and the mail server.
After typing this line, you should immediately receive the 220 reply from the server.
Then issue the SMTP commands HELO , MAIL FROM , RCPT TO , DATA , CRLF.CRLF , and QUIT  at the appropriate times.
It is also highly recommended that you do Programming Assignment 3 at the end of this chapter.
In that assignment, you’ll build a simple user agent that implements the client side of SMTP.
It will allow you to send an e-
mail message to an arbitrary recipient via a local mail server.
2.3.2 Comparison with HTTP Let’s now briefly compare SMTP with HTTP.
Both protocols are used to transfer files from one host to another: HTTP transfers files (also called objects) from a Web server to a Web client (typically abrowser); SMTP transfers files (that is, e-mail messages) from one mail server to another mail server.
When transferring the files, both persistent HTTP and SMTP use persistent connections.
Thus, the twoprotocols have common characteristics.
However, there are important differences.
First, HTTP is mainly a pull protocol —someone loads information on a Web server and users use HTTP to pull the information from the server at their convenience.
In particular, the TCP connection is initiated by the machine that wants to receive the file.
On the other hand, SMTP is primarily a push protocol —the sending mail server pushes the file to the receiving mail server.
In particular, the TCP connection isinitiated by the machine that wants to send the file.
A second difference, which we alluded to earlier, is that SMTP requires each message, including the body of each message, to be in 7-bit ASCII format.
If the message contains characters that are not 7-bit ASCII (for example, French characters with accents) or contains binary data (such as an image file), then the message has to be encoded into 7-bit ASCII.
HTTP data does not impose this restriction.
A third important difference concerns how a document consisting of text and images (along with possibly other media types) is handled.
As we learned in Section 2.2, HTTP encapsulates each object in its own HTTP response message.
SMTP places all of the message’s objects into one message.
2.3.3 Mail Message Formats When Alice writes an ordinary snail-mail letter to Bob, she may include all kinds of peripheral header information at the top of the letter, such as Bob’s address, her own return address, and the date.
Similarly, when an e-mail message is sent from one person to another, a header containing peripheralinformation precedes the body of the message itself.
This peripheral information is contained in a seriesof header lines, which are defined in RFC 5322.
The header lines and the body of the message are separated by a blank line (that is, by CRLF ).
RFC 5322 specifies the exact format for mail header lines as well as their semantic interpretations.
As with HTTP, each header line contains readable text, consisting of a keyword followed by a colon followed by a value.
Some of the keywords are required and others are optional.
Every header must have a From:  header line and a To: header line; a header may include a Subject:  header line as well as other optional header lines.
It is important to note that these header lines are different from the SMTP commands we studied in Section 2.4.1 (even though
they contain some common words such as “ from” and “to”).
The commands in that section were part of the SMTP handshaking protocol; the header lines examined in this section are part of the mail message itself.
A typical message header looks like this: From: alice@crepes.fr To: bob@hamburger.edu Subject: Searching for the meaning of life.
After the message header, a blank line follows; then the message body (in ASCII) follows.
You shoulduse Telnet to send a message to a mail server that contains some header lines, including the Subject:  header line.
To do this, issue telnet serverName 25,  as discussed in Section 2.4.1.
2.3.4 Mail Access Protocols Once SMTP delivers the message from Alice’s mail server to Bob’s mail server, the message is placedin Bob’s mailbox.
Throughout this discussion we have tacitly assumed that Bob reads his mail bylogging onto the server host and then executing a mail reader that runs on that host.
Up until the early1990s this was the standard way of doing things.
But today, mail access uses a client-serverarchitecture—the typical user reads e-mail with a client that executes on the user’s end system, for example, on an office PC, a laptop, or a smartphone.
By executing a mail client on a local PC, users enjoy a rich set of features, including the ability to view multimedia messages and attachments.
Given that Bob (the recipient) executes his user agent on his local PC, it is natural to consider placing a mail server on his local PC as well.
With this approach, Alice’s mail server would dialogue directly withBob’s PC.
There is a problem with this approach, however.
Recall that a mail server managesmailboxes and runs the client and server sides of SMTP.
If Bob’s mail server were to reside on his localPC, then Bob’s PC would have to remain always on, and connected to the Internet, in order to receive new mail, which can arrive at any time.
This is impractical for many Internet users.
Instead, a typical user runs a user agent on the local PC but accesses its mailbox stored on an always-on shared mailserver.
This mail server is shared with other users and is typically maintained by the user’s ISP (forexample, university or company).
Now let’s consider the path an e-mail message takes when it is sent from Alice to Bob.
We just learned that at some point along the path the e-mail message needs to be deposited in Bob’s mail server.
Thiscould be done simply by having Alice’s user agent send the message directly to Bob’s mail server.
And
this could be done with SMTP—indeed, SMTP has been designed for pushing e-mail from one host to another.
However, typically the sender’s user agent does not dialogue directly with the recipient’s mail server.
Instead, as shown in Figure 2.16, Alice’s user agent uses SMTP to push the e-mail message into her mail server, then Alice’s mail server uses SMTP (as an SMTP client) to relay the e-mail message to Bob’s mail server.
Why the two-step procedure?
Primarily because without relaying throughAlice’s mail server, Alice’s user agent doesn’t have any recourse to an unreachable destination Figure 2.16 E-mail protocols and their communicating entities mail server.
By having Alice first deposit the e-mail in her own mail server, Alice’s mail server can repeatedly try to send the message to Bob’s mail server, say every 30 minutes, until Bob’s mail server becomes operational. (
And if Alice’s mail server is down, then she has the recourse of complaining toher system administrator!)
The SMTP RFC defines how the SMTP commands can be used to relay amessage across multiple SMTP servers.
But there is still one missing piece to the puzzle!
How does a recipient like Bob, running a user agent on his local PC, obtain his messages, which are sitting in a mail server within Bob’s ISP?
Note that Bob’suser agent can’t use SMTP to obtain the messages because obtaining the messages is a pull operation,whereas SMTP is a push protocol.
The puzzle is completed by introducing a special mail access protocol that transfers messages from Bob’s mail server to his local PC.
There are currently a number of popular mail access protocols, including Post Office Protocol—Version 3 (POP3) , Internet Mail Access Protocol (IMAP), and HTTP.
Figure 2.16 provides a summary of the protocols that are used for Internet mail: SMTP is used to transfer mail from the sender’s mail server to the recipient’s mail server; SMTP is also used to transfer mail from the sender’s user agent to the sender’s mail server.
A mail access protocol, such as POP3, isused to transfer mail from the recipient’s mail server to the recipient’s user agent.
POP3 POP3 is an extremely simple mail access protocol.
It is defined in [RFC 1939] , which is short and quite readable.
Because the protocol is so simple, its functionality is rather limited.
POP3 begins when the user agent (the client) opens a TCP connection to the mail server (the server) on port 110.
With the TCP
connection established, POP3 progresses through three phases: authorization, transaction, and update.
During the first phase, authorization, the user agent sends a username and a password (in the clear) to authenticate the user.
During the second phase, transaction, the user agent retrieves messages; alsoduring this phase, the user agent can mark messages for deletion, remove deletion marks, and obtain mail statistics.
The third phase, update, occurs after the client has issued the quit  command, ending the POP3 session; at this time, the mail server deletes the messages that were marked for deletion.
In a POP3 transaction, the user agent issues commands, and the server responds to each command with a reply.
There are two possible responses: +OK (sometimes followed by server-to-client data), used by the server to indicate that the previous command was fine; and -ERR , used by the server to indicate that something was wrong with the previous command.
The authorization phase has two principal commands: user  <username>  and pass  <password> .
To illustrate these two commands, we suggest that you Telnet directly into a POP3 server, using port 110, and issue these commands.
Suppose that mailServer  is the name of your mail server.
You will see something like: telnet mailServer 110 +OK POP3 server ready user bob+OKpass hungry+OK user successfully logged on If you misspell a command, the POP3 server will reply with an -ERR  message.
Now let’s take a look at the transaction phase.
A user agent using POP3 can often be configured (by the user) to “download and delete” or to “download and keep.”
The sequence of commands issued by a POP3 user agent depends on which of these two modes the user agent is operating in.
In the download- and-delete mode, the user agent will issue the list , retr , and dele  commands.
As an example, suppose the user has two messages in his or her mailbox.
In the dialogue below, C: (standing for client) is the user agent and S: (standing for server) is the mail server.
The transaction will look something like: C: list S: 1 498 S: 2 912
S: .
C: retr 1 S: (blah blah ...S: .................S: ..........blah)S: .
C: dele 1 C: retr 2S: (blah blah ...S: .................S: ..........blah)S: .C: dele 2C: quit S: +OK POP3 server signing off The user agent first asks the mail server to list the size of each of the stored messages.
The user agentthen retrieves and deletes each message from the server.
Note that after the authorization phase, the user agent employed only four commands: list , retr , dele , and quit .
The syntax for these commands is defined in RFC 1939.
After processing the quit  command, the POP3 server enters the update phase and removes messages 1 and 2 from the mailbox.
A problem with this download-and-delete mode is that the recipient, Bob, may be nomadic and may want to access his mail messages from multiple machines, for example, his office PC, his home PC, andhis portable computer.
The download-and-delete mode partitions Bob’s mail messages over these threemachines; in particular, if Bob first reads a message on his office PC, he will not be able to reread themessage from his portable at home later in the evening.
In the download-and-keep mode, the useragent leaves the messages on the mail server after downloading them.
In this case, Bob can rereadmessages from different machines; he can access a message from work and access it again later in the week from home.
During a POP3 session between a user agent and the mail server, the POP3 server maintains some state information; in particular, it keeps track of which user messages have been marked deleted.
However, the POP3 server does not carry state information across POP3 sessions.
This lack of stateinformation across sessions greatly simplifies the implementation of a POP3 server.
IMAP With POP3 access, once Bob has downloaded his messages to the local machine, he can create mail
folders and move the downloaded messages into the folders.
Bob can then delete messages, move messages across folders, and search for messages (by sender name or subject).
But this paradigm— namely, folders and messages in the local machine—poses a problem for the nomadic user, who wouldprefer to maintain a folder hierarchy on a remote server that can be accessed from any computer.
Thisis not possible with POP3—the POP3 protocol does not provide any means for a user to create remotefolders and assign messages to folders.
To solve this and other problems, the IMAP protocol, defined in [RFC 3501] , was invented.
Like POP3, IMAP is a mail access protocol.
It has many more features than POP3, but it is also significantly more complex. (
And thus the client and server side implementations are significantly more complex.)
An IMAP server will associate each message with a folder; when a message first arrives at the server, it is associated with the recipient’s INBOX folder.
The recipient can then move the message into a new,user-created folder, read the message, delete the message, and so on.
The IMAP protocol provides commands to allow users to create folders and move messages from one folder to another.
IMAP also provides commands that allow users to search remote folders for messages matching specific criteria.
Note that, unlike POP3, an IMAP server maintains user state information across IMAP sessions—forexample, the names of the folders and which messages are associated with which folders.
Another important feature of IMAP is that it has commands that permit a user agent to obtain components of messages.
For example, a user agent can obtain just the message header of a messageor just one part of a multipart MIME message.
This feature is useful when there is a low-bandwidthconnection (for example, a slow-speed modem link) between the user agent and its mail server.
With a low-bandwidth connection, the user may not want to download all of the messages in its mailbox, particularly avoiding long messages that might contain, for example, an audio or video clip.
Web-Based E-Mail More and more users today are sending and accessing their e-mail through their Web browsers.
Hotmail introduced Web-based access in the mid 1990s.
Now Web-based e-mail is also provided by Google,Yahoo!,
as well as just about every major university and corporation.
With this service, the user agent isan ordinary Web browser, and the user communicates with its remote mailbox via HTTP.
When a recipient, such as Bob, wants to access a message in his mailbox, the e-mail message is sent from Bob’s mail server to Bob’s browser using the HTTP protocol rather than the POP3 or IMAP protocol.
When a sender, such as Alice, wants to send an e-mail message, the e-mail message is sent from herbrowser to her mail server over HTTP rather than over SMTP.
Alice’s mail server, however, still sendsmessages to, and receives messages from, other mail servers using SMTP.
2.4 DNS—The Internet’s Directory Service We human beings can be identified in many ways.
For example, we can be identified by the names that appear on our birth certificates.
We can be identified by our social security numbers.
We can beidentified by our driver’s license numbers.
Although each of these identifiers can be used to identifypeople, within a given context one identifier may be more appropriate than another.
For example, thecomputers at the IRS (the infamous tax-collecting agency in the United States) prefer to use fixed-length social security numbers rather than birth certificate names.
On the other hand, ordinary people prefer the more mnemonic birth certificate names rather than social security numbers. (
Indeed, can youimagine saying, “Hi.
My name is 132-67-9875.
Please meet my husband, 178-87-1146.”)
Just as humans can be identified in many ways, so too can Internet hosts.
One identifier for a host is its hostname .
Hostnames—such as www.facebook.com , www.google.com , gaia.cs.umass.edu —are mnemonic and are therefore appreciated by humans.
However, hostnames provide little, if any, information about the location within the Internet of the host. (
A hostname such as www.eurecom.fr , which ends with the country code .fr, tells us that the host is probably in France, but doesn’t say much more.)
Furthermore, because hostnames can consist of variable-length alphanumeric characters, they would be difficult to process by routers.
For thesereasons, hosts are also identified by so-called IP addresses.
We discuss IP addresses in some detail in Chapter 4, but it is useful to say a few brief words about them now.
An IP address consists of four bytes and has a rigid hierarchical structure.
An IP address looks like 121.7.106.83 , where each period separates one of the bytes expressed in decimal notation from 0 to 255.
An IP address is hierarchical because as we scan the address from left to right, we obtain more and more specific information about where the host is located in the Internet (that is,within which network, in the network of networks).
Similarly, when we scan a postal address from bottomto top, we obtain more and more specific information about where the addressee is located.
2.4.1 Services Provided by DNS We have just seen that there are two ways to identify a host—by a hostname and by an IP address.
People prefer the more mnemonic hostname identifier, while routers prefer fixed-length, hierarchicallystructured IP addresses.
In order to reconcile these preferences, we need a directory service that translates hostnames to IP addresses.
This is the main task of the Internet’s domain name system (DNS).
The DNS is (1) a distributed database implemented in a hierarchy of DNS servers , and (2) an
application-layer protocol that allows hosts to query the distributed database.
The DNS servers are often UNIX machines running the Berkeley Internet Name Domain (BIND) software [BIND 2016] .
The DNS protocol runs over UDP and uses port 53.
DNS is commonly employed by other application-layer protocols—including HTTP and SMTP to translate user-supplied hostnames to IP addresses.
As an example, consider what happens when a browser (that is, an HTTP client), running on some user’s host, requests the URL www.someschool.edu/index.html .
In order for the user’s host to be able to send an HTTP request message to the Web server www.someschool.edu , the user’s host must first obtain the IP address of www.someschool.edu .
This is done as follows.
1.
The same user machine runs the client side of the DNS application.
2.
The browser extracts the hostname, www.someschool.edu , from the URL and passes the hostname to the client side of the DNS application.
3.
The DNS client sends a query containing the hostname to a DNS server.
4.
The DNS client eventually receives a reply, which includes the IP address for the hostname.
5.
Once the browser receives the IP address from DNS, it can initiate a TCP connection to the HTTP server process located at port 80 at that IP address.
We see from this example that DNS adds an additional delay—sometimes substantial—to the Internet applications that use it.
Fortunately, as we discuss below, the desired IP address is often cached in a “nearby” DNS server, which helps to reduce DNS network traffic as well as the average DNS delay.
DNS provides a few other important services in addition to translating hostnames to IP addresses: Host aliasing.
 A host with a complicated hostname can have one or more alias names.
For example, a hostname such as relay1.west-coast.enterprise.com  could have, say, two aliases such as enterprise.com  and www.enterprise.com .
In this case, the hostname relay1.west-coast.enterprise.com  is said to be a canonical hostname .
Alias hostnames, when present, are typically more mnemonic than canonical hostnames.
DNS can be invoked by an application to obtain the canonical hostname for a supplied alias hostname as well as the IP address of the host.
Mail server aliasing.
 For obvious reasons, it is highly desirable that e-mail addresses be mnemonic.
For example, if Bob has an account with Yahoo Mail, Bob’s e-mail address might be as simple as bob@yahoo.mail .
However, the hostname of the Yahoo mail server is more complicated and much less mnemonic than simply yahoo.com  (for example, the canonical hostname might be something like relay1.west-coast.yahoo.com ).
DNS can be invoked by a mail application to obtain the canonical hostname for a supplied alias hostname as well as the IP address of the host.
In fact, the MX record (see below) permits a company’s mail server and Web server to have identical (aliased) hostnames; for example, a company’s Web server and mail server can both be called
enterprise.com .
Load distribution.
 DNS is also used to perform load distribution among replicated servers, such as replicated Web servers.
Busy sites, such as cnn.com , are replicated over multiple servers, with each server running on a different end system and each having a different IP address.
For replicated Web servers, a set of IP addresses is thus associated with one canonical hostname.
The DNS database contains this set of IP addresses.
When clients make a DNS query for a name mapped to a set of addresses, the server responds with the entire set of IP addresses, but rotates the orderingof the addresses within each reply.
Because a client typically sends its HTTP request message tothe IP address that is listed first in the set, DNS rotation distributes the traffic among the replicatedservers.
DNS rotation is also used for e-mail so that multiple mail servers can have the same aliasname.
Also, content distribution companies such as Akamai have used DNS in more sophisticated ways [Dilley 2002] to provide Web content distribution (see Section 2.6.3).
The DNS is specified in RFC 1034 and RFC 1035, and updated in several additional RFCs.
It is a complex system, and we only touch upon key aspects of its PRINCIPLES IN PRACTICE DNS: CRITICAL NETWORK FUNCTIONS VIA THE CLIENT-SERVER PARADIGM Like HTTP, FTP, and SMTP, the DNS protocol is an application-layer protocol since it (1) runs between communicating end systems using the client-server paradigm and (2) relies on anunderlying end-to-end transport protocol to transfer DNS messages between communicating end systems.
In another sense, however, the role of the DNS is quite different from Web, file transfer, and e-mail applications.
Unlike these applications, the DNS is not an application withwhich a user directly interacts.
Instead, the DNS provides a core Internet function—namely,translating hostnames to their underlying IP addresses, for user applications and other software in the Internet.
We noted in Section 1.2 that much of the complexity in the Internet architecture is located at the “edges” of the network.
The DNS, which implements the critical name-to- address translation process using clients and servers located at the edge of the network, is yetanother example of that design philosophy.
operation here.
The interested reader is referred to these RFCs and the book by Albitz and Liu [Albitz 1993] ; see also the retrospective paper [Mockapetris 1988] , which provides a nice description of the what and why of DNS, and [Mockapetris 2005] .
2.4.2 Overview of How DNS Works We now present a high-level overview of how DNS works.
Our discussion will focus on the hostname-to-
IP-address translation service.
Suppose that some application (such as a Web browser or a mail reader) running in a user’s host needs to translate a hostname to an IP address.
The application will invoke the client side of DNS, specifying the hostname that needs to be translated. (
On many UNIX-based machines, gethostbyname()  is the function call that an application calls in order to perform the translation.)
DNS in the user’s host then takes over, sending a query message into the network.
All DNS query and reply messages are sentwithin UDP datagrams to port 53.
After a delay, ranging from milliseconds to seconds, DNS in the user’shost receives a DNS reply message that provides the desired mapping.
This mapping is then passed tothe invoking application.
Thus, from the perspective of the invoking application in the user’s host, DNS isa black box providing a simple, straightforward translation service.
But in fact, the black box that implements the service is complex, consisting of a large number of DNS servers distributed around the globe, as well as an application-layer protocol that specifies how the DNS servers and querying hostscommunicate.
A simple design for DNS would have one DNS server that contains all the mappings.
In this centralized design, clients simply direct all queries to the single DNS server, and the DNS server responds directlyto the querying clients.
Although the simplicity of this design is attractive, it is inappropriate for today’s Internet, with its vast (and growing) number of hosts.
The problems with a centralized design include: A single point of failure .
If the DNS server crashes, so does the entire Internet!
Traffic volume.
A single DNS server would have to handle all DNS queries (for all the HTTP requests and e-mail messages generated from hundreds of millions of hosts).
Distant centralized database.
A single DNS server cannot be “close to” all the querying clients.
Ifwe put the single DNS server in New York City, then all queries from Australia must travel to the other side of the globe, perhaps over slow and congested links.
This can lead to significant delays.
Maintenance.
The single DNS server would have to keep records for all Internet hosts.
Not only would this centralized database be huge, but it would have to be updated frequently to account for every new host.
In summary, a centralized database in a single DNS server simply doesn’t scale.
 Consequently, the DNS is distributed by design.
In fact, the DNS is a wonderful example of how a distributed database canbe implemented in the Internet.
A Distributed, Hierarchical Database In order to deal with the issue of scale, the DNS uses a large number of servers, organized in a hierarchical fashion and distributed around the world.
No single DNS server has all of the mappings forall of the hosts in the Internet.
Instead, the mappings are distributed across the DNS servers.
To a first approximation, there are three classes of DNS servers—root DNS servers, top-level domain (TLD) DNS
servers, and authoritative DNS servers—organized in a hierarchy as shown in Figure 2.17.
To understand how these three classes of servers interact, suppose a DNS client wants to determine the IP address for the hostname www.amazon.com .
To a first Figure 2.17 Portion of the hierarchy of DNS servers approximation, the following events will take place.
The client first contacts one of the root servers,which returns IP addresses for TLD servers for the top-level domain com.
The client then contacts one of these TLD servers, which returns the IP address of an authoritative server for amazon.com .
Finally, the client contacts one of the authoritative servers for amazon.com , which returns the IP address for the hostname www.amazon.com .
We’ll soon examine this DNS lookup process in more detail.
But let’s first take a closer look at these three classes of DNS servers: Root DNS servers.
 There are over 400 root name servers scattered all over the world.
Figure 2.18 shows the countries that have root names servers, with countries having more than ten darkly shaded.
These root name servers are managed by 13 different organizations.
The full list of rootname servers, along with the organizations that manage them and their IP addresses can be found at [Root Servers 2016] .
Root name servers provide the IP addresses of the TLD servers.
Top-level domain (TLD) servers.
For each of the top-level domains — top-level domains such as com, org, net, edu, and gov, and all of the country top-level domains such as uk, fr, ca, and jp — there is TLD server (or server cluster).
The company Verisign Global Registry Services maintains the TLD servers for the com top-level domain, and the company Educause maintains the TLD servers for the edu top-level domain.
The network infrastructure supporting a TLD can be large and complex; see [Osterweil 2012] for a nice overview of the Verisign network.
See [TLD list 2016] for a list of all top-level domains.
TLD servers provide the IP addresses for authoritative DNS servers.
Figure 2.18 DNS root servers in 2016 Authoritative DNS servers .
Every organization with publicly accessible hosts (such as Web servers and mail servers) on the Internet must provide publicly accessible DNS records that map the names of those hosts to IP addresses.
An organization’s authoritative DNS server houses these DNSrecords.
An organization can choose to implement its own authoritative DNS server to hold these records; alternatively, the organization can pay to have these records stored in an authoritative DNS server of some service provider.
Most universities and large companies implement and maintaintheir own primary and secondary (backup) authoritative DNS server.
The root, TLD, and authoritative DNS servers all belong to the hierarchy of DNS servers, as shown in Figure 2.17.
There is another important type of DNS server called the local DNS server .
A local DNS server does not strictly belong to the hierarchy of servers but is nevertheless central to the DNS architecture.
Each ISP—such as a residential ISP or an institutional ISP—has a local DNS server (alsocalled a default name server).
When a host connects to an ISP, the ISP provides the host with the IP addresses of one or more of its local DNS servers (typically through DHCP, which is discussed in Chapter 4).
You can easily determine the IP address of your local DNS server by accessing network status windows in Windows or UNIX.
A host’s local DNS server is typically “close to” the host.
For an institutional ISP, the local DNS server may be on the same LAN as the host; for a residential ISP, it istypically separated from the host by no more than a few routers.
When a host makes a DNS query, thequery is sent to the local DNS server, which acts a proxy, forwarding the query into the DNS serverhierarchy, as we’ll discuss in more detail below.
Let’s take a look at a simple example.
Suppose the host cse.nyu.edu  desires the IP address of gaia.cs.umass.edu .
Also suppose that NYU’s ocal DNS server for cse.nyu.edu  is called
dns.nyu.edu  and that an authoritative DNS server for gaia.cs.umass.edu  is called dns.umass.edu .
As shown in Figure 2.19, the host cse.nyu.edu  first sends a DNS query message to its local DNS server, dns.nyu.edu .
The query message contains the hostname to be translated, namely, gaia.cs.umass.edu .
The local DNS server forwards the query message to a root DNS server.
The root DNS server takes note of the edu suffix and returns to the local DNS server a list of IP addresses for TLD servers responsible for edu.
The local DNS server then resends the query message to one of these TLD servers.
The TLD server takes note of the umass.edu  suffix and responds with the IP address of the authoritative DNS server for the University of Massachusetts,namely, dns.umass.edu .
Finally, the local DNS server resends the query message directly to dns.umass.edu , which responds with the IP address of gaia.cs.umass.edu .
Note that in this example, in order to obtain the mapping for one hostname, eight DNS messages were sent: four query messages and four reply messages!
We’ll soon see how DNS caching reduces this query traffic.
Our previous example assumed that the TLD server knows the authoritative DNS server for the hostname.
In general this not always true.
Instead, the TLD server Figure 2.19 Interaction of the various DNS servers
may know only of an intermediate DNS server, which in turn knows the authoritative DNS server for the hostname.
For example, suppose again that the University of Massachusetts has a DNS server for the university, called dns.umass.edu .
Also suppose that each of the departments at the University of Massachusetts has its own DNS server, and that each departmental DNS server is authoritative for allhosts in the department.
In this case, when the intermediate DNS server, dns.umass.edu , receives a query for a host with a hostname ending with cs.umass.edu , it returns to dns.nyu.edu  the IP address of dns.cs.umass.edu , which is authoritative for all hostnames ending with cs.umass.edu .
The local DNS server dns.nyu.edu  then sends the query to the authoritative DNS server, which returns the desired mapping to the local DNS server, which in turn returns the mapping to the requesting host.
In this case, a total of 10 DNS messages are sent!
The example shown in Figure 2.19 makes use of both recursive queries  and iterative queries .
The query sent from cse.nyu.edu  to dns.nyu.edu  is a recursive query, since the query asks dns.nyu.edu  to obtain the mapping on its behalf.
But the subsequent three queries are iterative since all of the replies are directly returned to dns.nyu.edu .
In theory, any DNS query can be iterative or recursive.
For example, Figure 2.20 shows a DNS query chain for which all of the queries are recursive.
In practice, the queries typically follow the pattern in Figure 2.19: The query from the requesting host to the local DNS server is recursive, and the remaining queries are iterative.
DNS Caching Our discussion thus far has ignored DNS caching, a critically important feature of the DNS system.
In truth, DNS extensively exploits DNS caching in order to improve the delay performance and to reduce the number of DNS messages
Figure 2.20 Recursive queries in DNS ricocheting around the Internet.
The idea behind DNS caching is very simple.
In a query chain, when a DNS server receives a DNS reply (containing, for example, a mapping from a hostname to an IP address), it can cache the mapping in its local memory.
For example, in Figure 2.19, each time the local DNS server dns.nyu.edu  receives a reply from some DNS server, it can cache any of the information contained in the reply.
If a hostname/IP address pair is cached in a DNS server and another query arrives to the DNS server for the same hostname, the DNS server can provide the desired IP address,even if it is not authoritative for the hostname.
Because hosts and mappings between hostnames and IP addresses are by no means permanent, DNS servers discard cached information after a period of time (often set to two days).
As an example, suppose that a host apricot.nyu.edu  queries dns.nyu.edu  for the IP address for the hostname cnn.com .
Furthermore, ­suppose that a few hours later, another NYU host, say, kiwi.nyu.edu , also queries dns.nyu.edu  with the same hostname.
Because of caching, the local DNS server will be able to immediately return the IP address of cnn.com  to this second requesting
host without having to query any other DNS servers.
A local DNS server can also cache the IP addresses of TLD servers, thereby allowing the local DNS server to bypass the root DNS servers in a query chain.
In fact, because of caching, root servers are bypassed for all but a very small fraction ofDNS queries.
2.4.3 DNS Records and Messages The DNS servers that together implement the DNS distributed database store resource records (RRs) , including RRs that provide hostname-to-IP address mappings.
Each DNS reply message carries one ormore resource records.
In this and the following subsection, we provide a brief overview of DNS resource records and messages; more details can be found in [Albitz 1993] or in the DNS RFCs [RFC 1034 ; RFC 1035] .
A resource record is a four-tuple that contains the following fields: (Name, Value, Type, TTL) TTL is the time to live of the resource record; it determines when a resource should be removed from a cache.
In the example records given below, we ignore the TTL field.
The meaning of Name  and Value depend on Type : If Type=A , then Name  is a hostname and Value  is the IP address for the hostname.
Thus, a Type A record provides the standard hostname-to-IP address mapping.
As an example, (relay1.bar.foo.com , 145.37.93.126, A)  is a Type A record.
If Type=NS , then Name  is a domain (such as foo.com ) and Value  is the hostname of an authoritative DNS server that knows how to obtain the IP addresses for hosts in the domain.
This record is used to route DNS queries further along in the query chain.
As an example, (foo.com, dns.foo.com , NS)  is a Type NS record.
If Type=CNAME , then Value  is a canonical hostname for the alias hostname Name .
This record can provide querying hosts the canonical name for a hostname.
As an example, (foo.com, relay1.bar.foo.com , CNAME)  is a CNAME record.
If Type=MX , then Value  is the canonical name of a mail server that has an alias hostname Name .
As an example, (foo.com, mail.bar.foo.com , MX)  is an MX record.
MX records allow the hostnames of mail servers to have simple aliases.
Note that by using the MX record, a company can have the same aliased name for its mail server and for one of its other servers (such as its Webserver).
To obtain the canonical name for the mail server, a DNS client would query for an MX
record; to obtain the canonical name for the other server, the DNS client would query for the CNAME record.
If a DNS server is authoritative for a particular hostname, then the DNS server will contain a Type A record for the hostname. (
Even if the DNS server is not authoritative, it may contain a Type A record inits cache.)
If a server is not authoritative for a hostname, then the server will contain a Type NS record for the domain that includes the hostname; it will also contain a Type A record that provides the IP address of the DNS server in the Value  field of the NS record.
As an example, suppose an edu TLD server is not authoritative for the host gaia.cs.umass.edu .
Then this server will contain a record for a domain that includes the host gaia.cs.umass.edu , for example, (umass.edu , dns.umass.edu , NS) .
The edu TLD server would also contain a Type A record, which maps the DNS server dns.umass.edu  to an IP address, for example, (dns.umass.edu , 128.119.40.111, A) .
DNS Messages Earlier in this section, we referred to DNS query and reply messages.
These are the only two kinds of DNS messages.
Furthermore, both query and reply messages have the same format, as shown in Figure 2.21.The semantics of the various fields in a DNS message are as follows: The first 12 bytes is the header section , which has a number of fields.
The first field is a 16-bit number that identifies the query.
This identifier is copied into the reply message to a query, allowing the client to match received replies with sent queries.
There are a number of flags in the flag field.
A1-bit query/reply flag indicates whether the message is a query (0) or a reply (1).
A 1-bit authoritative flag is
Figure 2.21 DNS message format set in a reply message when a DNS server is an authoritative server for a queried name.
A 1-bit recursion-desired flag is set when a client (host or DNS server) desires that the DNS server performrecursion when it doesn’t have the record.
A 1-bit recursion-available field is set in a reply if the DNSserver supports recursion.
In the header, there are also four number-of fields.
These fields indicatethe number of occurrences of the four types of data sections that follow the header.
The question section  contains information about the query that is being made.
This section includes (1) a name field that contains the name that is being queried, and (2) a type field that indicates thetype of question being asked about the name—for example, a host address associated with a name(Type A) or the mail server for a name (Type MX).
In a reply from a DNS server, the answer section  contains the resource records for the name that was originally queried.
Recall that in each resource record there is the Type  (for example, A, NS, CNAME, and MX), the Value , and the TTL.
A reply can return multiple RRs in the answer, since a hostname can have multiple IP addresses (for example, for replicated Web servers, as discussedearlier in this section).
The authority section  contains records of other authoritative servers.
The additional section  contains other helpful records.
For example, the answer field in a reply to an MX query contains a resource record providing the canonical hostname of a mail server.
Theadditional section contains a Type A record providing the IP address for the canonical hostname ofthe mail server.
How would you like to send a DNS query message directly from the host you’re working on to someDNS server?
This can easily be done with the nslookup program , which is available from most Windows and UNIX platforms.
For example, from a Windows host, open the Command Prompt andinvoke the nslookup program by simply typing “nslookup.”
After invoking nslookup, you can send a DNSquery to any DNS server (root, TLD, or authoritative).
After receiving the reply message from the DNSserver, nslookup will display the records included in the reply (in a human-readable format).
As analternative to running nslookup from your own host, you can visit one of many Web sites that allow youto remotely employ nslookup. (
Just type “nslookup” into a search engine and you’ll be brought to one of these sites.)
The DNS Wireshark lab at the end of this chapter will allow you to explore the DNS in much more detail.
Inserting Records into the DNS Database The discussion above focused on how records are retrieved from the DNS database.
You might be wondering how records get into the database in the first place.
Let’s look at how this is done in thecontext of a specific example.
Suppose you have just created an exciting new startup company calledNetwork Utopia.
The first thing you’ll surely want to do is register the domain name
networkutopia.com  at a registrar.
A registrar  is a commercial entity that verifies the uniqueness of the domain name, enters the domain name into the DNS database (as discussed below), and collects a small fee from you for its services.
Prior to 1999, a single registrar, Network Solutions, had a monopoly on domain name registration for com, net, and org domains.
But now there are many registrars competing for customers, and the Internet Corporation for Assigned Names and Numbers (ICANN)accredits the various registrars.
A complete list of accredited registrars is available at http:/ / www.internic.net .
When you register the domain name networkutopia.com  with some registrar, you also need to provide the registrar with the names and IP addresses of your primary and secondary authoritative DNSservers.
Suppose the names and IP addresses are dns1.networkutopia.com , dns2.networkutopia.com , 212.2.212.1,  and 212.212.212.2.
 For each of these two authoritative DNS servers, the registrar would then make sure that a Type NS and a Type A record are entered into the TLD com servers.
Specifically, for the primary authoritative server for networkutopia.com , the registrar would insert the following two resource records into the DNS system: (networkutopia.com, dns1.networkutopia.com, NS) (dns1.networkutopia.com, 212.212.212.1, A) You’ll also have to make sure that the Type A resource record for your Web server www.networkutopia.com  and the Type MX resource record for your mail server mail.networkutopia.com  are entered into your authoritative DNS FOCUS ON SECURITY DNS VULNERABILITIES We have seen that DNS is a critical component of the Internet infrastructure, with many important services—including the Web and e-mail—simply incapable of functioning without it.
We therefore naturally ask, how can DNS be attacked?
Is DNS a sitting duck, waiting to be knocked out of service, while taking most Internet applications down with it?
The first type of attack that comes to mind is a DDoS bandwidth-flooding attack (see Section 1.6) against DNS servers.
For example, an attacker could attempt to send to each DNS root server a deluge of packets, so many that the majority of legitimate DNS queries never get answered.
Such a large-scale DDoS attack against DNS root servers actually took place onOctober 21, 2002.
In this attack, the attackers leveraged a botnet to send truck loads of ICMPping messages to each of the 13 DNS root IP addresses. (
ICMP messages are discussed in
Section 5.6.
For now, it suffices to know that ICMP packets are special types of IP datagrams.)
Fortunately, this large-scale attack caused minimal damage, having little or no impact on users’ Internet experience.
The attackers did succeed at directing a deluge of packets at the rootservers.
But many of the DNS root servers were protected by packet filters, configured to alwaysblock all ICMP ping messages directed at the root servers.
These protected servers were thusspared and functioned as normal.
Furthermore, most local DNS servers cache the IP addresses of top-level-domain servers, allowing the query process to often bypass the DNS root servers.
A potentially more effective DDoS attack against DNS would be send a deluge of DNS queries to top-level-domain servers, for example, to all the top-level-domain servers that handle the .comdomain.
It would be harder to filter DNS queries directed to DNS servers; and top-level-domainservers are not as easily bypassed as are root servers.
But the severity of such an attack wouldbe partially mitigated by caching in local DNS servers.
DNS could potentially be attacked in other ways.
In a man-in-the-middle attack, the attacker intercepts queries from hosts and returns bogus replies.
In the DNS poisoning attack, the attacker sends bogus replies to a DNS server, tricking the server into accepting bogus recordsinto its cache.
Either of these attacks could be used, for example, to redirect an unsuspectingWeb user to the attacker’s Web site.
These attacks, however, are difficult to implement, as they require intercepting packets or throttling servers [Skoudis 2006].
In summary, DNS has demonstrated itself to be surprisingly robust against attacks.
To date, there hasn’t been an attack that has successfully impeded the DNS service.
servers. (
Until recently, the contents of each DNS server were configured statically, for example, from aconfiguration file created by a system manager.
More recently, an UPDATE option has been added tothe DNS protocol to allow data to be dynamically added or deleted from the database via DNS messages. [
RFC 2136]  and [RFC 3007]  specify DNS dynamic updates.)
Once all of these steps are completed, people will be able to visit your Web site and send e-mail to the employees at your company.
Let’s conclude our discussion of DNS by verifying that this statement istrue.
This verification also helps to solidify what we have learned about DNS.
Suppose Alice in Australia wants to view the Web page www.networkutopia.com .
As discussed earlier, her host will first send a DNS query to her local DNS server.
The local DNS server will then contact a TLD com server. (
The local DNS server will also have to contact a root DNS server if the address of a TLD com server is not cached.)
This TLD server contains the Type NS and Type A resource records listed above, because the registrar had these resource records inserted into all of the TLD com servers.
The TLD com serversends a reply to Alice’s local DNS server, with the reply containing the two resource records.
The local DNS server then sends a DNS query to 212.212.212.1 , asking for the Type A record corresponding to www.networkutopia.com .
This record provides the IP address of the desired Web server, say, 212.212.71.4 , which the local DNS server passes back to Alice’s host.
Alice’s browser can now
initiate a TCP connection to the host 212.212.71.4  and send an HTTP request over the connection.
Whew!
There’s a lot more going on than what meets the eye when one surfs the Web!
2.5 Peer-to-Peer File Distribution The applications described in this chapter thus far—including the Web, e-mail, and DNS—all employ client-server architectures with significant reliance on always-on infrastructure servers.
Recall from Section 2.1.1 that with a P2P architecture, there is minimal (or no) reliance on always-on infrastructure servers.
Instead, pairs of intermittently connected hosts, called peers, communicate directly with each other.
The peers are not owned by a service provider, but are instead desktops and laptops controlled by users.
In this section we consider a very natural P2P application, namely, distributing a large file from a single server to a large number of hosts (called peers).
The file might be a new version of the Linux operatingsystem, a software patch for an existing operating system or application, an MP3 music file, or anMPEG video file.
In client-server file distribution, the server must send a copy of the file to each of thepeers—placing an enormous burden on the server and consuming a large amount of server bandwidth.
In P2P file distribution, each peer can redistribute any portion of the file it has received to any other peers, thereby assisting the server in the distribution process.
As of 2016, the most popular P2P filedistribution protocol is BitTorrent.
Originally developed by Bram Cohen, there are now many differentindependent BitTorrent clients conforming to the BitTorrent protocol, just as there are a number of Webbrowser clients that conform to the HTTP protocol.
In this subsection, we first examine the self-scalability of P2P architectures in the context of file distribution.
We then describe BitTorrent in somedetail, highlighting its most important characteristics and features.
Scalability of P2P Architectures To compare client-server architectures with peer-to-peer architectures, and illustrate the inherent self- scalability of P2P, we now consider a simple quantitative model for distributing a file to a fixed set of peers for both architecture types.
As shown in Figure 2.22, the server and the peers are connected to the Internet with access links.
Denote the upload rate of the server’s access link by u, the upload rate of the ith peer’s access link by u, and the download rate of the ith peer’s access link by d. Also denote the size of the file to be distributed (in bits) by F and the number of peers that want to obtain a copy of the file by N. The distribution time  is the time it takes to get s i i
Figure 2.22 An illustrative file distribution problem a copy of the file to all N peers.
In our analysis of the distribution time below, for both client-server and P2P architectures, we make the simplifying (and generally accurate [Akella 2003] ) assumption that the Internet core has abundant bandwidth, implying that all of the bottlenecks are in access networks.
Wealso suppose that the server and clients are not participating in any other network applications, so thatall of their upload and download access bandwidth can be fully devoted to distributing this file.
Let’s first determine the distribution time for the client-server architecture, which we denote by D. In the client-server architecture, none of the peers aids in distributing the file.
We make the following observations: The server must transmit one copy of the file to each of the N peers.
Thus the server must transmitNF bits.
Since the server’s upload rate is u, the time to distribute the file must be at least NF/u .
Let d  denote the download rate of the peer with the lowest download rate, that is,  The peer with the lowest download rate cannot obtain all F bits of the file in less than F/d  seconds.
Thus the minimum distribution time is at least F/d .
Putting these two observations together, we obtaincs s s min dmin=min{d1,dp,. .
.,dN}.
min min Dcs≥max{NFus,Fdmin}.
This provides a lower bound on the minimum distribution time for the client-server architecture.
In the homework problems you will be asked to show that the server can schedule its transmissions so that thelower bound is actually achieved.
So let’s take this lower bound provided above as the actual distributiontime, that is, We see from Equation  2.1 that for N large enough, the client-server distribution time is given by NF/u .
Thus, the distribution time increases linearly with the number of peers N. So, for example, if the number of peers from one week to the next increases a thousand-fold from a thousand to a million, the time required to distribute the file to all peers increases by 1,000.
Let’s now go through a similar analysis for the P2P architecture, where each peer can assist the server in distributing the file.
In particular, when a peer receives some file data, it can use its own uploadcapacity to redistribute the data to other peers.
Calculating the distribution time for the P2P architectureis somewhat more complicated than for the client-server architecture, since the distribution timedepends on how each peer distributes portions of the file to the other peers.
Nevertheless, a simple expression for the minimal distribution time can be obtained [Kumar 2006] .
To this end, we first make the following observations: At the beginning of the distribution, only the server has the file.
To get this file into the community of peers, the server must send each bit of the file at least once into its access link.
Thus, the minimum distribution time is at least F/u. (Unlike the client-server scheme, a bit sent once by the server may not have to be sent by the server again, as the peers may redistribute the bit among themselves.)
As with the client-server architecture, the peer with the lowest download rate cannot obtain all F bits of the file in less than F/d  seconds.
Thus the minimum distribution time is at least F/d .
Finally, observe that the total upload capacity of the system as a whole is equal to the upload rate of the server plus the upload rates of each of the individual peers, that is,  The system must deliver (upload) F bits to each of the N peers, thus delivering a total of NF bits.
This cannot be done at a rate faster than u. Thus, the minimum distribution time is also at least Putting these three observations together, we obtain the minimum distribution time for P2P, denoted by D. Equation  2.2 provides a lower bound for the minimum distribution time for the P2P architecture.
It turns out that if we imagine that each peer can redistribute a bit as soon as it receives the bit, then there is aDcs=max{NFus,Fdmin} (2.1) s s min min utotal=us+u1+⋯+uN. total NF/(us+u1+⋯+uN).
P2P DP2P≥max{Fus,Fdmin,NFus+∑i=1Nui} (2.2)
redistribution scheme that actually achieves this lower bound [Kumar 2006] . (
We will prove a special case of this result in the homework.)
In reality, where chunks of the file are redistributed rather than individual bits, Equation  2.2 serves as a good approximation of the actual minimum distribution time.
Thus, let’s take the lower bound provided by Equation  2.2 as the actual minimum distribution time, that is, Figure 2.23 compares the minimum distribution time for the client-server and P2P architectures assuming that all peers have the same upload rate u. In Figure 2.23, we have set  and  Thus, a peer can transmit the entire file in one hour, the server transmission rate is 10 times the peer upload rate, Figure 2.23 Distribution time for P2P and client-server architectures and (for simplicity) the peer download rates are set large enough so as not to have an effect.
We see from Figure 2.23 that for the client-server architecture, the distribution time increases linearly and without bound as the number of peers increases.
However, for the P2P architecture, the minimal distribution time is not only always less than the distribution time of the client-server architecture; it is also less than one hour for any number of peers N. Thus, applications with the P2P architecture can be self-scaling.
This scalability is a direct consequence of peers being redistributors as well as consumers of bits.
BitTorrent BitTorrent is a popular P2P protocol for file distribution [Chao 2011].
In BitTorrent lingo, the collection ofDP2P=max{Fus,Fdmin,NFus+∑i=1Nui} (2.3) F/u=1 hour, us=10u, dmin≥us.
all peers participating in the distribution of a particular file is called a torrent.
Peers in a torrent download equal-size chunks of the file from one another, with a typical chunk size of 256 KBytes.
When a peer first joins a torrent, it has no chunks.
Over time it accumulates more and more chunks.
While it downloads chunks it also uploads chunks to other peers.
Once a peer has acquired the entire file, it may (selfishly) leave the torrent, or (altruistically) remain in the torrent and continue to upload chunks to other peers.
Also, any peer may leave the torrent at any time with only a subset of chunks, and later rejoin thetorrent.
Let’s now take a closer look at how BitTorrent operates.
Since BitTorrent is a rather complicated protocol and system, we’ll only describe its most important mechanisms, sweeping some of the detailsunder the rug; this will allow us to see the forest through the trees.
Each torrent has an infrastructure node called a tracker.
Figure 2.24 File distribution with BitTorrent When a peer joins a torrent, it registers itself with the tracker and periodically informs the tracker that it is still in the torrent.
In this manner, the tracker keeps track of the peers that are participating in the torrent.
A given torrent may have fewer than ten or more than a thousand peers participating at any instant oftime.
As shown in Figure 2.24, when a new peer, Alice, joins the torrent, the tracker randomly selects a subset of peers (for concreteness, say 50) from the set of participating peers, and sends the IP addresses of these 50 peers to Alice.
Possessing this list of peers, Alice attempts to establishconcurrent TCP connections with all the peers on this list.
Let’s call all the peers with which Alice succeeds in establishing a TCP connection “neighboring peers.” (
In Figure 2.24, Alice is shown to have only three neighboring peers.
Normally, she would have many more.)
As time evolves, some of these peers may leave and other peers (outside the initial 50) may attempt to establish TCP connections with Alice.
So a peer’s neighboring peers will fluctuate over time.
At any given time, each peer will have a subset of chunks from the file, with different peers having different subsets.
Periodically, Alice will ask each of her neighboring peers (over the TCP connections) for the list of the chunks they have.
If Alice has L different neighbors, she will obtain L lists of chunks.
With this knowledge, Alice will issue requests (again over the TCP connections) for chunks she currently does not have.
So at any given instant of time, Alice will have a subset of chunks and will know which chunks her neighbors have.
With this information, Alice will have two important decisions to make.
First, whichchunks should she request first from her neighbors?
And second, to which of her neighbors should shesend requested chunks?
In deciding which chunks to request, Alice uses a technique called rarest first .
The idea is to determine, from among the chunks she does not have, the chunks that are the rarestamong her neighbors (that is, the chunks that have the fewest repeated copies among her neighbors)and then request those rarest chunks first.
In this manner, the rarest chunks get more quicklyredistributed, aiming to (roughly) equalize the numbers of copies of each chunk in the torrent.
To determine which requests she responds to, BitTorrent uses a clever trading algorithm.
The basic idea is that Alice gives priority to the neighbors that are currently supplying her data at the highest rate.
Specifically, for each of her neighbors, Alice continually measures the rate at which she receives bits and determines the four peers that are feeding her bits at the highest rate.
She then reciprocates bysending chunks to these same four peers.
Every 10 seconds, she recalculates the rates and possiblymodifies the set of four peers.
In BitTorrent lingo, these four peers are said to be unchoked.
Importantly, every 30 seconds, she also picks one additional neighbor at random and sends it chunks.
Let’s call the randomly chosen peer Bob.
In BitTorrent lingo, Bob is said to be optimistically unchoked .
Because Alice is sending data to Bob, she may become one of Bob’s top four uploaders, in which case Bob would start to send data to Alice.
If the rate at which Bob sends data to Alice is high enough, Bobcould then, in turn, become one of Alice’s top four uploaders.
In other words, every 30 seconds, Alicewill randomly choose a new trading partner and initiate trading with that partner.
If the two peers aresatisfied with the trading, they will put each other in their top four lists and continue trading with eachother until one of the peers finds a better partner.
The effect is that peers capable of uploading atcompatible rates tend to find each other.
The random neighbor selection also allows new peers to getchunks, so that they can have something to trade.
All other neighboring peers besides these five peers
(four “top” peers and one probing peer) are “choked,” that is, they do not receive any chunks from Alice.
BitTorrent has a number of interesting mechanisms that are not discussed here, including pieces (mini- chunks), pipelining, random first selection, endgame mode, and anti-snubbing [Cohen 2003].
The incentive mechanism for trading just described is often referred to as tit-for-tat [Cohen 2003].
It has been shown that this incentive scheme can be circumvented [Liogkas 2006; Locher 2006; Piatek 2007] .
Nevertheless, the BitTorrent ecosystem is wildly successful, with millions of simultaneous peers actively sharing files in hundreds of thousands of torrents.
If BitTorrent had been designed without tit-for- tat (or a variant), but otherwise exactly the same, BitTorrent would likely not even exist now, as the majority of the users would have been freeriders [Saroiu 2002].
We close our discussion on P2P by briefly mentioning another application of P2P, namely, Distributed Hast Table (DHT).
A distributed hash table is a simple database, with the database records beingdistributed over the peers in a P2P system.
DHTs have been widely implemented (e.g., in BitTorrent) and have been the subject of extensive research.
An overview is provided in a Video Note in the companion website.
Walking though distributed hash tables
2.6 Video Streaming and Content Distribution Networks Streaming prerecorded video now accounts for the majority of the traffic in residential ISPs in North America.
In particular, the Netflix and YouTube services alone consumed a whopping 37% and 16%, respectively, of residential ISP traffic in 2015 [Sandvine 2015].
In this section we will provide an overview of how popular video streaming services are implemented in today’s Internet.
We will see they are implemented using application-level protocols and servers that function in some ways like a cache.
In Chapter 9, devoted to multimedia networking, we will further examine Internet video as well as other Internet multimedia services.
2.6.1 Internet Video In streaming stored video applications, the underlying medium is prerecorded video, such as a movie, a television show, a prerecorded sporting event, or a prerecorded user-generated video (such as thosecommonly seen on YouTube).
These prerecorded videos are placed on servers, and users send requests to the servers to view the videos on demand .
Many Internet companies today provide streaming video, including, Netflix, YouTube (Google), Amazon, and Youku.
But before launching into a discussion of video streaming, we should first get a quick feel for the video medium itself.
A video is a sequence of images, typically being displayed at a constant rate, forexample, at 24 or 30 images per second.
An uncompressed, digitally encoded image consists of anarray of pixels, with each pixel encoded into a number of bits to represent luminance and color.
Animportant characteristic of video is that it can be compressed, thereby trading off video quality with bitrate.
Today’s off-the-shelf compression algorithms can compress a video to essentially any bit ratedesired.
Of course, the higher the bit rate, the better the image quality and the better the overall user viewing experience.
From a networking perspective, perhaps the most salient characteristic of video is its high bit rate.
Compressed Internet video typically ranges from 100 kbps for low-quality video to over 3 Mbps forstreaming high-definition movies; 4K streaming envisions a bitrate of more than 10 Mbps.
This cantranslate to huge amount of traffic and storage, particularly for high-end video.
For example, a single 2Mbps video with a duration of 67 minutes will consume 1 gigabyte of storage and traffic.
By far, the mostimportant performance measure for streaming video is average end-to-end throughput.
In order toprovide continuous playout, the network must provide an average throughput to the streaming application that is at least as large as the bit rate of the compressed video.
We can also use compression to create multiple versions of the same video, each at a different quality level.
For example, we can use compression to create, say, three versions of the same video, at rates of300 kbps, 1 Mbps, and 3 Mbps.
Users can then decide which version they want to watch as a function oftheir current available bandwidth.
Users with high-speed Internet connections might choose the 3 Mbpsversion; users watching the video over 3G with a smartphone might choose the 300 kbps version.
2.6.2 HTTP Streaming and DASH In HTTP streaming, the video is simply stored at an HTTP server as an ordinary file with a specific URL.When a user wants to see the video, the client establishes a TCP connection with the server and issues an HTTP GET request for that URL.
The server then sends the video file, within an HTTP response message, as quickly as the underlying network protocols and traffic conditions will allow.
On the client side, the bytes are collected in a client application buffer.
Once the number of bytes in this bufferexceeds a predetermined threshold, the client application begins playback—specifically, the streamingvideo application periodically grabs video frames from the client application buffer, decompresses theframes, and displays them on the user’s screen.
Thus, the video streaming application is displayingvideo as it is receiving and buffering frames corresponding to latter parts of the video.
Although HTTP streaming, as described in the previous paragraph, has been extensively deployed in practice (for example, by YouTube since its inception), it has a major shortcoming: All clients receive the same encoding of the video, despite the large variations in the amount of bandwidth available to a client,both across different clients and also over time for the same client.
This has led to the development of anew type of HTTP-based streaming, often referred to as Dynamic Adaptive Streaming over HTTP (DASH).
In DASH, the video is encoded into several different versions, with each version having adifferent bit rate and, correspondingly, a different quality level.
The client dynamically requests chunks ofvideo segments of a few seconds in length.
When the amount of available bandwidth is high, the clientnaturally selects chunks from a high-rate version; and when the available bandwidth is low, it naturally selects from a low-rate version.
The client selects different chunks one at a time with HTTP GET request messages [Akhshabi 2011].
DASH allows clients with different Internet access rates to stream in video at different encoding rates.
Clients with low-speed 3G connections can receive a low bit-rate (and low-quality) version, and clientswith fiber connections can receive a high-quality version.
DASH also allows a client to adapt to theavailable bandwidth if the available end-to-end bandwidth changes during the session.
This feature isparticularly important for mobile users, who typically see their bandwidth availability fluctuate as theymove with respect to the base stations.
With DASH, each video version is stored in the HTTP server, each with a different URL.
The HTTP
server also has a manifest file, which provides a URL for each version along with its bit rate.
The client first requests the manifest file and learns about the various versions.
The client then selects one chunk at a time by specifying a URL and a byte range in an HTTP GET request message for each chunk.
While downloading chunks, the client also measures the received bandwidth and runs a ratedetermination algorithm to select the chunk to request next.
Naturally, if the client has a lot of videobuffered and if the measured receive bandwidth is high, it will choose a chunk from a high-bitrate version.
And naturally if the client has little video buffered and the measured received bandwidth is low, it will choose a chunk from a low-bitrate version.
DASH therefore allows the client to freely switch amongdifferent quality levels.
2.6.3 Content Distribution Networks Today, many Internet video companies are distributing on-demand multi-Mbps streams to millions ofusers on a daily basis.
YouTube, for example, with a library of hundreds of millions of videos, distributeshundreds of millions of video streams to users around the world every day.
Streaming all this traffic tolocations all over the world while providing continuous playout and high interactivity is clearly achallenging task.
For an Internet video company, perhaps the most straightforward approach to providing streaming video service is to build a single massive data center, store all of its videos in the data center, and stream thevideos directly from the data center to clients worldwide.
But there are three major problems with thisapproach.
First, if the client is far from the data center, server-to-client packets will cross manycommunication links and likely pass through many ISPs, with some of the ISPs possibly located ondifferent continents.
If one of these links provides a throughput that is less than the video consumptionrate, the end-to-end throughput will also be below the consumption rate, resulting in annoying freezing delays for the user. (
Recall from Chapter 1 that the end-to-end throughput of a stream is governed by the throughput at the bottleneck link.)
The likelihood of this happening increases as the number of links in the end-to-end path increases.
A second drawback is that a popular video will likely be sent manytimes over the same communication links.
Not only does this waste network bandwidth, but the Internet video company itself will be paying its provider ISP (connected to the data center) for sending the same bytes into the Internet over and over again.
A third problem with this solution is that a single data center represents a single point of failure—if the data center or its links to the Internet goes down, it would not be able to distribute any video streams.
In order to meet the challenge of distributing massive amounts of video data to users distributed around the world, almost all major video-streaming companies make use of Content Distribution Networks (CDNs) .
A CDN manages servers in multiple geographically distributed locations, stores copies of the videos (and other types of Web content, including documents, images, and audio) in its servers, andattempts to direct each user request to a CDN location that will provide the best user experience.
The
CDN may be a private CDN , that is, owned by the content provider itself; for example, Google’s CDN distributes YouTube videos and other types of content.
The CDN may alternatively be a third-party CDN that distributes content on behalf of multiple content providers; Akamai, Limelight and Level-3 all operate third-party CDNs.
A very readable overview of modern CDNs is [Leighton 2009; Nygren 2010] .
CDNs typically adopt one of two different server placement philosophies [Huang 2008]: Enter Deep.
One philosophy, pioneered by Akamai, is to enter deep  into the access networks of Internet Service Providers, by deploying server clusters in access ISPs all over the world. (
Accessnetworks are described in Section 1.3.)
Akamai takes this approach with clusters in approximately 1,700 locations.
The goal is to get close to end users, thereby improving user-perceived delay and throughput by decreasing the number of links and routers between the end user and the CDN server from which it receives content.
Because of this highly distributed design, the task of maintaining andmanaging the clusters becomes challenging.
Bring Home.
A second design philosophy, taken by Limelight and many other CDN companies, is tobring the ISPs home by building large clusters at a smaller number (for example, tens) of sites.
Instead of getting inside the access ISPs, these CDNs typically place their clusters in Internet Exchange Points (IXPs) (see Section 1.3).
Compared with the enter-deep design philosophy, the bring-home design typically results in lower maintenance and management overhead, possibly at the expense of higher delay and lower throughput to end users.
Once its clusters are in place, the CDN replicates content across its clusters.
The CDN may not want toplace a copy of every video in each cluster, since some videos are rarely viewed or are only popular insome countries.
In fact, many CDNs do not push videos to their clusters but instead use a simple pullstrategy: If a client requests a video from a cluster that is not storing the video, then the cluster retrievesthe video (from a central repository or from another cluster) and stores a copy locally while streaming the video to the client at the same time.
Similar Web caching (see Section 2.2.5), when a cluster’s storage becomes full, it removes videos that are not frequently requested.
CDN Operation Having identified the two major approaches toward deploying a CDN, let’s now dive down into the nuts and bolts of how a CDN operates.
When a browser in a user’s CASE STUDY GOOGLE’S NETWORK INFRASTRUCTURE To support its vast array of cloud services—including search, Gmail, calendar, YouTube video, maps, documents, and social networks—Google has deployed an extensive private network andCDN infrastructure.
Google’s CDN infrastructure has three tiers of server clusters:
Fourteen “mega data centers,” with eight in North America, four in Europe, and two in Asia [Google Locations 2016] , with each data center having on the order of 100,000 servers.
These mega data centers are responsible for serving dynamic (and often personalized)content, including search results and Gmail messages.
An estimated 50 clusters in IXPs scattered throughout the world, with each cluster consistingon the order of 100–500 servers [Adhikari 2011a] .
These clusters are responsible for serving static content, including YouTube videos [Adhikari 2011a] .
Many hundreds of “enter-deep” clusters located within an access ISP.
Here a cluster typically consists of tens of servers within a single rack.
These enter-deep ­servers perform TCP splitting (see Section 3.7) and serve static content [Chen 2011], including the staticportions of Web pages that embody search results.
All of these data centers and cluster locations are networked together with Google’s own private network.
When a user makes a search query, often the query is first sent over the local ISP to anearby enter-deep cache, from where the static content is retrieved; while providing the staticcontent to the client, the nearby cache also forwards the query over Google’s private network toone of the mega data centers, from where the personalized search results are retrieved.
For aYouTube video, the video itself may come from one of the bring-home caches, whereas portionsof the Web page surrounding the video may come from the nearby enter-deep cache, and theadvertisements surrounding the video come from the data centers.
In summary, except for the local ISPs, the Google cloud services are largely provided by a network infrastructure that is independent of the public Internet.
host is instructed to retrieve a specific video (identified by a URL), the CDN must intercept the requestso that it can (1) determine a suitable CDN server cluster for that client at that time, and (2) redirect theclient’s request to a server in that cluster.
We’ll shortly discuss how a CDN can determine a suitablecluster.
But first let’s examine the mechanics behind intercepting and redirecting a request.
Most CDNs take advantage of DNS to intercept and redirect requests; an interesting discussion of such a use of the DNS is [Vixie 2009].
Let’s consider a simple example to illustrate how the DNS is typically involved.
Suppose a content provider, NetCinema, employs the third-party CDN company, KingCDN, to distribute its videos to its customers.
On the NetCinema Web pages, each of its videos is assigned aURL that includes the string “video” and a unique identifier for the video itself; for example, Transformers 7 might be assigned http:/ /video.netcinema.com/6Y7B23V .
Six steps then occur, as shown in Figure 2.25: 1.
The user visits the Web page at NetCinema.
2.
When the user clicks on the link http:/ /video.netcinema.com/6Y7B23V , the user’s host sends a DNS query for video.netcinema.com.
3.
The user’s Local DNS Server (LDNS) relays the DNS query to an authoritative DNS server for NetCinema, which observes the string “video” in the hostname video.netcinema.com.
To “hand over” the DNS query to KingCDN, instead of returning an IP address, the NetCinema authoritative DNS server returns to the LDNS a hostname in the KingCDN’s domain, for example, a1105.kingcdn.com .
4.
From this point on, the DNS query enters into KingCDN’s private DNS infrastructure.
The user’s LDNS then sends a second query, now for a1105.kingcdn.com , and KingCDN’s DNS system eventually returns the IP addresses of a KingCDN content server to the LDNS.
It is thus here,within the KingCDN’s DNS system, that the CDN server from which the client will receive its content is specified.
Figure 2.25 DNS redirects a user’s request to a CDN server 5.
The LDNS forwards the IP address of the content-serving CDN node to the user’s host.
6.
Once the client receives the IP address for a KingCDN content server, it establishes a direct TCP connection with the server at that IP address and issues an HTTP GET request for the video.
If DASH is used, the server will first send to the client a manifest file with a list of URLs,one for each version of the video, and the client will dynamically select chunks from the different versions.
Cluster Selection Strategies At the core of any CDN deployment is a cluster selection strategy, that is, a mechanism for dynamically directing clients to a server cluster or a data center within the CDN.
As we just saw, the
CDN learns the IP address of the client’s LDNS server via the client’s DNS lookup.
After learning this IP address, the CDN needs to select an appropriate cluster based on this IP address.
CDNs generally employ proprietary cluster selection strategies.
We now briefly survey a few approaches, each of whichhas its own advantages and disadvantages.
One simple strategy is to assign the client to the cluster that is geographically closest.
Using commercial geo-location databases (such as Quova [Quova 2016] and Max-Mind [MaxMind 2016]), each LDNS IP address is mapped to a geographic location.
When a DNS request is received from a particular LDNS, the CDN chooses the geographically closest cluster, that is, the cluster that is thefewest kilometers from the LDNS “as the bird flies.”
Such a solution can work reasonably well for a large fraction of the clients [Agarwal 2009] .
However, for some clients, the solution may perform poorly, since the geographically closest cluster may not be the closest cluster in terms of the length or number of hops of the network path.
Furthermore, a problem inherent with all DNS-based approaches is that some end-users are configured to use remotely located LDNSs [Shaikh 2001; Mao 2002], in which case the LDNS location may be far from the client’s location.
Moreover, this simple strategy ignores the variation in delay and available bandwidth over time of Internet paths, always assigning the same cluster to aparticular client.
In order to determine the best cluster for a client based on the current traffic conditions, CDNs can instead perform periodic real-time measurements  of delay and loss performance between their clusters and clients.
For instance, a CDN can have each of its clusters periodically send probes (for example, ping messages or DNS queries) to all of the LDNSs around the world.
One drawback of thisapproach is that many LDNSs are configured to not respond to such probes.
2.6.4 Case Studies: Netflix, YouTube, and Kankan We conclude our discussion of streaming stored video by taking a look at three highly successful large-scale deployments: Netflix, YouTube, and Kankan.
We’ll see that each of these systems take a verydifferent approach, yet employ many of the underlying principles discussed in this section.
Netflix Generating 37% of the downstream traffic in residential ISPs in North America in 2015, Netflix has become the leading service provider for online movies and TV series in the United States [Sandvine 2015] .
As we discuss below, Netflix video distribution has two major components: the Amazon cloud and its own private CDN infrastructure.
Netflix has a Web site that handles numerous functions, including user registration and login, billing, movie catalogue for browsing and searching, and a movie recommendation system.
As shown in  Figure
2.26, this Web site (and its associated backend databases) run entirely on Amazon servers in the Amazon cloud.
Additionally, the Amazon cloud handles the following critical functions: Content ingestion.
 Before Netflix can distribute a movie to its customers, it must first ingest and process the movie.
Netflix receives studio master versions of movies and uploads them to hosts in the Amazon cloud.
Content processing.
 The machines in the Amazon cloud create many different formats for each movie, suitable for a diverse array of client video players running on desktop computers,smartphones, and game consoles connected to televisions.
A different version is created for each ofthese formats and at multiple bit rates, allowing for adaptive streaming over HTTP using DASH.
Uploading versions to its CDN.
 Once all of the versions of a movie have been created, the hosts in the Amazon cloud upload the versions to its CDN.
Figure 2.26 Netflix video streaming platform When Netflix first rolled out its video streaming service in 2007, it employed three third-party CDN companies to distribute its video content.
Netflix has since created its own private CDN, from which it now streams all of its videos. (
Netflix still uses Akamai to distribute its Web pages, however.)
To createits own CDN, Netflix has installed server racks both in IXPs and within residential ISPs themselves.
Netflix currently has server racks in over 50 IXP locations; see [Netflix Open Connect 2016] for a current list of IXPs housing Netflix racks.
There are also hundreds of ISP locations housing Netflix racks;also see [Netflix Open Connect 2016], where Netflix provides to potential ISP partners instructions about installing a (free) Netflix rack for their networks.
Each server in the rack has several 10 Gbps
Ethernet ports and over 100 terabytes of storage.
The number of servers in a rack varies: IXP installations often have tens of servers and contain the entire Netflix streaming video library, including multiple versions of the videos to support DASH; local IXPs may only have one server and contain only the most popular videos.
Netflix does not use pull-caching ( Section 2.2.5) to populate its CDN servers in the IXPs and ISPs.
Instead, Netflix distributes by pushing the videos to its CDN servers during off- peak hours.
For those locations that cannot hold the entire library, Netflix pushes only the most popular videos, which are determined on a day-to-day basis.
The Netflix CDN design is described in some detail in the YouTube videos [Netflix Video 1] and [Netflix Video 2].
Having described the components of the Netflix architecture, let’s take a closer look at the interaction between the client and the various servers that are involved in movie delivery.
As indicated earlier, theWeb pages for browsing the Netflix video library are served from servers in the Amazon cloud.
When auser selects a movie to play, the Netflix software, running in the Amazon cloud, first determines which ofits CDN servers have copies of the movie.
Among the servers that have the movie, the software then determines the “best” server for that client request.
If the client is using a residential ISP that has a Netflix CDN server rack installed in that ISP, and this rack has a copy of the requested movie, then aserver in this rack is typically selected.
If not, a server at a nearby IXP is typically selected.
Once Netflix determines the CDN server that is to deliver the content, it sends the client the IP address of the specific server as well as a manifest file, which has the URLs for the different versions of therequested movie.
The client and that CDN server then directly interact using a proprietary version of DASH.
Specifically, as described in Section 2.6.2, the client uses the byte-range header in HTTP GET request messages, to request chunks from the different versions of the movie.
Netflix uses chunks that are approximately four-seconds long [Adhikari 2012].
While the chunks are being downloaded, the client measures the received throughput and runs a rate-determination algorithm to determine the quality of the next chunk to request.
Netflix embodies many of the key principles discussed earlier in this section, including adaptive streaming and CDN distribution.
However, because Netflix uses its own private CDN, which distributesonly video (and not Web pages), Netflix has been able to simplify and tailor its CDN design.
In particular, Netflix does not need to employ DNS redirect, as discussed in Section 2.6.3, to connect a particular client to a CDN server; instead, the Netflix software (running in the Amazon cloud) directly tells the client to use a particular CDN server.
Furthermore, the Netflix CDN uses push caching rather than pull caching ( Section 2.2.5): content is pushed into the servers at scheduled times at off-peak hours, rather than dynamically during cache misses.
YouTube With 300 hours of video uploaded to YouTube every minute and several billion video views per day [YouTube 2016], YouTube is indisputably the world’s largest video-sharing site.
YouTube began its
service in April 2005 and was acquired by Google in November 2006.
Although the Google/YouTube design and protocols are proprietary, through several independent measurement efforts we can gain a basic understanding about how YouTube operates [Zink 2009; Torres 2011 ; Adhikari 2011a].
As with Netflix, YouTube makes extensive use of CDN technology to distribute its videos [Torres 2011] .
Similar to Netflix, Google uses its own private CDN to distribute YouTube videos, and has installed server clusters in many hundreds of different IXP and ISP locations.
From these locations and directly from its huge data centers, Google distributes YouTube videos [Adhikari 2011a] .
Unlike Netflix, however, Google uses pull caching, as described in Section 2.2.5, and DNS redirect, as described in Section 2.6.3.
Most of the time, Google’s cluster-selection strategy directs the client to the cluster for which the RTT between client and cluster is the lowest; however, in order to balance the load across clusters, sometimes the client is directed (via DNS) to a more distant cluster [Torres 2011] .
YouTube employs HTTP streaming, often making a small number of different versions available for a video, each with a different bit rate and corresponding quality level.
YouTube does not employ adaptive streaming (such as DASH), but instead requires the user to manually select a version.
In order to save bandwidth and server resources that would be wasted by repositioning or early termination, YouTubeuses the HTTP byte range request to limit the flow of transmitted data after a target amount of video isprefetched.
Several million videos are uploaded to YouTube every day.
Not only are YouTube videos streamed from server to client over HTTP, but YouTube uploaders also upload their videos from client to server overHTTP.
YouTube processes each video it receives, converting it to a YouTube video format and creatingmultiple versions at different bit rates.
This processing takes place entirely within Google data centers. (
See the case study on Google’s network infrastructure in Section 2.6.3.)
Kankan We just saw that dedicated servers, operated by private CDNs, stream Netflix and YouTube videos to clients.
Netflix and YouTube have to pay not only for the server hardware but also for the bandwidth the servers use to distribute the videos.
Given the scale of these services and the amount of bandwidth theyare consuming, such a CDN deployment can be costly.
We conclude this section by describing an entirely different approach for providing video on demand over the Internet at a large scale—one that allows the service provider to significantly reduce itsinfrastructure and bandwidth costs.
As you might suspect, this approach uses P2P delivery instead of(or along with) client-server delivery.
Since 2011, Kankan (owned and operated by Xunlei) has been deploying P2P video delivery with great success, with tens of millions of users every month [Zhang 2015] .
At a high level, P2P video streaming is very similar to BitTorrent file downloading.
When a peer wants to
see a video, it contacts a tracker to discover other peers in the system that have a copy of that video.
This requesting peer then requests chunks of the video in parallel from the other peers that have the video.
Different from downloading with BitTorrent, however, requests are preferentially made for chunks that are to be played back in the near future in order to ensure continuous playback [Dhungel 2012].
Recently, Kankan has migrated to a hybrid CDN-P2P streaming system [Zhang 2015].
Specifically, Kankan now deploys a few hundred servers within China and pushes video content to these servers.
This Kankan CDN plays a major role in the start-up stage of video streaming.
In most cases, the clientrequests the beginning of the content from CDN servers, and in parallel requests content from peers.
When the total P2P traffic is sufficient for video playback, the client will cease streaming from the CDNand only stream from peers.
But if the P2P streaming traffic becomes insufficient, the client will restartCDN connections and return to the mode of hybrid CDN-P2P streaming.
In this manner, Kankan canensure short initial start-up delays while minimally relying on costly infrastructure servers and bandwidth.
2.7 Socket Programming: Creating Network Applications Now that we’ve looked at a number of important network applications, let’s explore how network application programs are actually created.
Recall from Section 2.1 that a typical network application consists of a pair of programs—a client program and a server program—residing in two different end systems.
When these two programs are executed, a client process and a server process are created,and these processes communicate with each other by reading from, and writing to, sockets.
When creating a network application, the developer’s main task is therefore to write the code for both the client and server programs.
There are two types of network applications.
One type is an implementation whose operation is specified in a protocol standard, such as an RFC or some other standards document; such anapplication is sometimes referred to as “open,” since the rules specifying its operation are known to all.
For such an implementation, the client and server programs must conform to the rules dictated by theRFC.
For example, the client program could be an implementation of the client side of the HTTP protocol, described in Section 2.2 and precisely defined in RFC 2616; similarly, the server program could be an implementation of the HTTP server protocol, also precisely defined in RFC 2616.
If one developer writes code for the client program and another developer writes code for the server program,and both developers carefully follow the rules of the RFC, then the two programs will be able tointeroperate.
Indeed, many of today’s network applications involve communication between client andserver programs that have been created by independent developers—for example, a Google Chromebrowser communicating with an Apache Web server, or a BitTorrent client communicating withBitTorrent tracker.
The other type of network application is a proprietary network application.
In this case the client and server programs employ an application-layer protocol that has not been openly published in an RFC or elsewhere.
A single developer (or development team) creates both the client and server programs, and the developer has complete control over what goes in the code.
But because the code does notimplement an open protocol, other independent developers will not be able to develop code thatinteroperates with the application.
In this section, we’ll examine the key issues in developing a client-server application, and we’ll “get our hands dirty” by looking at code that implements a very simple client-server application.
During the development phase, one of the first decisions the developer must make is whether the application is torun over TCP or over UDP.
Recall that TCP is connection oriented and provides a reliable byte-streamchannel through which data flows between two end systems.
UDP is connectionless and sends independent packets of data from one end system to the other, without any guarantees about delivery.
Recall also that when a client or server program implements a protocol defined by an RFC, it should use the well-known port number associated with the protocol; conversely, when developing a proprietaryapplication, the developer must be careful to avoid using such well-known port numbers. (
Port numbers were briefly discussed in Section 2.1.
They are covered in more detail in Chapter 3.)
We introduce UDP and TCP socket programming by way of a simple UDP application and a simple TCP application.
We present the simple UDP and TCP applications in Python 3.
We could have written thecode in Java, C, or C++, but we chose Python mostly because Python clearly exposes the key socketconcepts.
With Python there are fewer lines of code, and each line can be explained to the noviceprogrammer without difficulty.
But there’s no need to be frightened if you are not familiar with Python.
You should be able to easily follow the code if you have experience programming in Java, C, or C++.
If you are interested in client-server programming with Java, you are encouraged to see the Companion Website for this textbook; in fact, you can find there all the examples in this section (and associated labs) in Java.
For readers who are interested in client-server programming in C, there are several good references available [Donahoo 2001; Stevens 1997 ; Frost 1994; Kurose 1996]; our Python examples below have a similar look and feel to C. 2.7.1 Socket Programming with UDP In this subsection, we’ll write simple client-server programs that use UDP; in the following section, we’ll write similar programs that use TCP.
Recall from Section 2.1 that processes running on different machines communicate with each other by sending messages into sockets.
We said that each process is analogous to a house and the process’s socket is analogous to a door.
The application resides on one side of the door in the house; the transport-layer protocol resides on the other side of the door in the outside world.
The applicationdeveloper has control of everything on the application-layer side of the socket; however, it has littlecontrol of the transport-layer side.
Now let’s take a closer look at the interaction between two communicating processes that use UDP sockets.
Before the sending process can push a packet of data out the socket door, when using UDP, itmust first attach a destination address to the packet.
After the packet passes through the sender’s socket, the Internet will use this destination address to route the packet through the Internet to the socket in the receiving process.
When the packet arrives at the receiving socket, the receiving processwill retrieve the packet through the socket, and then inspect the packet’s contents and take appropriateaction.
So you may be now wondering, what goes into the destination address that is attached to the packet?
As you might expect, the destination host’s IP address is part of the destination address.
By including the destination IP address in the packet, the routers in the Internet will be able to route the packetthrough the Internet to the destination host.
But because a host may be running many networkapplication processes, each with one or more sockets, it is also necessary to identify the particular socket in the destination host.
When a socket is created, an identifier, called a port number, is assigned to it.
So, as you might expect, the packet’s destination address also includes the socket’s port number.
In summary, the sending process attaches to the packet a destination address, which consists of thedestination host’s IP address and the destination socket’s port number.
Moreover, as we shall soon see,the sender’s source address—consisting of the IP address of the source host and the port number of thesource socket—are also attached to the packet.
However, attaching the source address to the packet is typically not done by the UDP application code; instead it is automatically done by the underlying operating system.
We’ll use the following simple client-server application to demonstrate socket programming for both UDP and TCP: 1.
The client reads a line of characters (data) from its keyboard and sends the data to the server.
2.
The server receives the data and converts the characters to uppercase.
3.
The server sends the modified data to the client.
4.
The client receives the modified data and displays the line on its screen.
Figure 2.27 highlights the main socket-related activity of the client and server that communicate over the UDP transport service.
Now let’s get our hands dirty and take a look at the client-server program pair for a UDP implementation of this simple application.
We also provide a detailed, line-by-line analysis after each program.
We’llbegin with the UDP client, which will send a simple application-level message to the server.
In order for
Figure 2.27 The client-server application using UDP the server to be able to receive and reply to the client’s message, it must be ready and running—that is, it must be running as a process before the client sends its message.
The client program is called UDPClient.py, and the server program is called UDPServer.py.
In order to emphasize the key issues, we intentionally provide code that is minimal. “
Good code” would certainlyhave a few more auxiliary lines, in particular for handling error cases.
For this application, we havearbitrarily chosen 12000 for the server port number.
UDPClient.py Here is the code for the client side of the application: from socket import * serverName = ’hostname’ serverPort = 12000
clientSocket = socket(AF_INET, SOCK_DGRAM) message = raw_input(’Input lowercase sentence:’) clientSocket.sendto(message.encode(),(serverName, serverPort))modifiedMessage, serverAddress = clientSocket.recvfrom(2048)print(modifiedMessage.decode())clientSocket.close() Now let’s take a look at the various lines of code in UDPClient.py.
from socket import * The socket  module forms the basis of all network communications in Python.
By including this line, we will be able to create sockets within our program.
serverName = ’hostname’ serverPort = 12000 The first line sets the variable serverName  to the string ‘hostname’.
Here, we provide a string containing either the IP address of the server (e.g., “128.138.32.126”) or the hostname of the server(e.g., “
cis.poly.edu”).
If we use the hostname, then a DNS lookup will automatically be performed to get the IP address.)
The second line sets the integer variable serverPort  to 12000.
clientSocket = socket(AF_INET, SOCK_DGRAM) This line creates the client’s socket, called clientSocket .
The first parameter indicates the address family; in particular, AF_INET  indicates that the underlying network is using IPv4. (
Do not worry about this now—we will discuss IPv4 in Chapter 4.)
The second parameter indicates that the socket is of type SOCK_DGRAM , which means it is a UDP socket (rather than a TCP socket).
Note that we are not specifying the port number of the client socket when we create it; we are instead letting the operating system do this for us.
Now that the client process’s door has been created, we will want to create amessage to send through the door.
message = raw_input( ’Input lowercase sentence: ’)
raw_input()  is a built-in function in Python.
When this command is executed, the user at the client is prompted with the words “Input lowercase sentence:” The user then uses her keyboard to input a line, which is put into the variable message .
Now that we have a socket and a message, we will want to send the message through the socket to the destination host.
clientSocket.sendto(message.encode(),(serverName, serverPort)) In the above line, we first convert the message from string type to byte type, as we need to send bytes into a socket; this is done with the encode()  method.
The method sendto()  attaches the destination address (serverName, serverPort ) to the message and sends the resulting packet into the process’s socket, clientSocket . (
As mentioned earlier, the source address is also attached to the packet, although this is done automatically rather than explicitly by the code.)
Sending a client-to-server message via a UDP socket is that simple!
After sending the packet, the client waits to receive data fromthe server.
modifiedMessage, serverAddress = clientSocket.recvfrom(2048) With the above line, when a packet arrives from the Internet at the client’s socket, the packet’s data is put into the variable modifiedMessage  and the packet’s source address is put into the variable serverAddress .
The variable serverAddress  contains both the server’s IP address and the server’s port number.
The program UDPClient doesn’t actually need this server address information, since it already knows the server address from the outset; but this line of Python provides the server address nevertheless.
The method recvfrom  also takes the buffer size 2048 as input. (
This buffer size works for most purposes.)
print(modifiedMessage.decode()) This line prints out modifiedMessage on the user’s display, after converting the message from bytes to string.
It should be the original line that the user typed, but now capitalized.
clientSocket.close()
This line closes the socket.
The process then terminates.
UDPServer.py Let’s now take a look at the server side of the application: from socket import * serverPort = 12000 serverSocket = socket(AF_INET, SOCK_DGRAM)serverSocket.bind((’’, serverPort))print(”The server is ready to receive”)while True:    message, clientAddress = serverSocket.recvfrom(2048)    modifiedMessage = message.decode().upper()     serverSocket.sendto(modifiedMessage.encode(), clientAddress) Note that the beginning of UDPServer is similar to UDPClient.
It also imports the socket module, also sets the integer variable serverPort  to 12000, and also creates a socket of type SOCK_DGRAM  (a UDP socket).
The first line of code that is significantly different from UDPClient is: serverSocket.bind((’’, serverPort)) The above line binds (that is, assigns) the port number 12000 to the server’s socket.
Thus in UDPServer, the code (written by the application developer) is explicitly assigning a port number to thesocket.
In this manner, when anyone sends a packet to port 12000 at the IP address of the server, that packet will be directed to this socket.
UDPServer then enters a while loop; the while loop will allow UDPServer to receive and process packets from clients indefinitely.
In the while loop, UDPServer waitsfor a packet to arrive.
message, clientAddress = serverSocket.recvfrom(2048) This line of code is similar to what we saw in UDPClient.
When a packet arrives at the server’s socket, the packet’s data is put into the variable message  and the packet’s source address is put into the variable clientAddress .
The variable ­clientAddress contains both the client’s IP address and the client’s port number.
Here, UDPServer will make use of this address information, as it provides a return
address, similar to the return address with ordinary postal mail.
With this source address information, the server now knows to where it should direct its reply.
modifiedMessage = message.decode().upper() This line is the heart of our simple application.
It takes the line sent by the client and, after converting the message to a string, uses the method upper()  to capitalize it.
serverSocket.sendto(modifiedMessage.encode(), clientAddress) This last line attaches the client’s address (IP address and port number) to the capitalized message (after converting the string to bytes), and sends the resulting packet into the server’s socket. (
Asmentioned earlier, the server address is also attached to the packet, although this is done automaticallyrather than explicitly by the code.)
The Internet will then deliver the packet to this client address.
Afterthe server sends the packet, it remains in the while loop, waiting for another UDP packet to arrive (fromany client running on any host).
To test the pair of programs, you run UDPClient.py on one host and UDPServer.py on another host.
Be sure to include the proper hostname or IP address of the server in UDPClient.py.
Next, you executeUDPServer.py, the compiled server program, in the server host.
This creates a process in the serverthat idles until it is contacted by some client.
Then you execute UDPClient.py, the compiled clientprogram, in the client.
This creates a process in the client.
Finally, to use the application at the client,you type a sentence followed by a carriage return.
To develop your own UDP client-server application, you can begin by slightly modifying the client or server programs.
For example, instead of converting all the letters to uppercase, the server could count the number of times the letter s appears and return this number.
Or you can modify the client so that after receiving a capitalized sentence, the user can continue to send more sentences to the server.
2.7.2 Socket Programming with TCP Unlike UDP, TCP is a connection-oriented protocol.
This means that before the client and server can start to send data to each other, they first need to handshake and establish a TCP connection.
One endof the TCP connection is attached to the client socket and the other end is attached to a server socket.
When creating the TCP connection, we associate with it the client socket address (IP address and port
number) and the server socket address (IP address and port number).
With the TCP connection established, when one side wants to send data to the other side, it just drops the data into the TCP connection via its socket.
This is different from UDP, for which the server must attach a destination address to the packet before dropping it into the socket.
Now let’s take a closer look at the interaction of client and server programs in TCP.
The client has the job of initiating contact with the server.
In order for the server to be able to react to the client’s initial contact, the server has to be ready.
This implies two things.
First, as in the case of UDP, the TCP servermust be running as a process before the client attempts to initiate contact.
Second, the server programmust have a special door—more precisely, a special socket—that welcomes some initial contact from aclient process running on an arbitrary host.
Using our house/door analogy for a process/socket, we willsometimes refer to the client’s initial contact as “knocking on the welcoming door.”
With the server process running, the client process can initiate a TCP connection to the server.
This is done in the client program by creating a TCP socket.
When the client creates its TCP socket, it specifies the address of the welcoming socket in the server, namely, the IP address of the server host and theport number of the socket.
After creating its socket, the client initiates a three-way handshake andestablishes a TCP connection with the server.
The three-way handshake, which takes place within thetransport layer, is completely invisible to the client and server programs.
During the three-way handshake, the client process knocks on the welcoming door of the server process.
When the server “hears” the knocking, it creates a new door—more precisely, a new socket that is dedicated to that particular ­client.
In our example below, the welcoming door is a TCP socket object that we call ­serverSocket ; the newly created socket dedicated to the client making the connection is called connectionSocket .
Students who are encountering TCP sockets for the first time sometimes confuse the welcoming socket (which is the initial point of contact for all clients wanting to communicate with the server), and each newly created server-side connection socket that is subsequently created for communicating with each client.
From the application’s perspective, the client’s socket and the server’s connection socket are directly connected by a pipe.
As shown in Figure 2.28, the client process can send arbitrary bytes into its socket, and TCP guarantees that the server process will receive (through the connection socket) each byte in the order sent.
TCP thus provides a reliable service between the client and server processes.
Furthermore, just as people can go in and out the same door, the client process not only sends bytesinto but also receives bytes from its socket; similarly, the server process not only receives bytes from but also sends bytes into its connection socket.
We use the same simple client-server application to demonstrate socket programming with TCP: The client sends one line of data to the server, the server capitalizes the line and sends it back to the client.
Figure 2.29 highlights the main socket-related activity of the client and server that communicate over
the TCP transport service.
Figure 2.28 The TCPServer  process has two sockets TCPClient.py Here is the code for the client side of the application: from socket import * serverName = ’servername’ serverPort = 12000clientSocket = socket(AF_INET, SOCK_STREAM)clientSocket.connect((serverName, serverPort)) sentence = raw_input(’Input lowercase sentence:’) clientSocket.send(sentence.encode())modifiedSentence = clientSocket.recv(1024)print(’From Server: ’, modifiedSentence.decode())clientSocket.close() Let’s now take a look at the various lines in the code that differ significantly from the UDPimplementation.
The first such line is the creation of the client socket.
clientSocket = socket(AF_INET, SOCK_STREAM) This line creates the client’s socket, called clientSocket .
The first parameter again indicates that the underlying network is using IPv4.
The second parameter Figure 2.29 The client-server application using TCP indicates that the socket is of type SOCK_STREAM , which means it is a TCP socket (rather than a UDP socket).
Note that we are again not specifying the port number of the client socket when we create it; we are instead letting the operating system do this for us.
Now the next line of code is very different fromwhat we saw in UDPClient:
clientSocket.connect((serverName, serverPort)) Recall that before the client can send data to the server (or vice versa) using a TCP socket, a TCP connection must first be established between the client and server.
The above line initiates the TCP connection between the client and server.
The parameter of the connect()  method is the address of the server side of the connection.
After this line of code is executed, the three-way handshake isperformed and a TCP connection is established between the client and server.
sentence = raw_input(’Input lowercase sentence:’) As with UDPClient, the above obtains a sentence from the user.
The string sentence  continues to gather characters until the user ends the line by typing a carriage return.
The next line of code is alsovery different from UDPClient: clientSocket.send(sentence.encode()) The above line sends the sentence  through the client’s socket and into the TCP connection.
Note that the program does not explicitly create a packet and attach the destination address to the packet, as was the case with UDP sockets.
Instead the client program simply drops the bytes in the string sentence into the TCP connection.
The client then waits to receive bytes from the server.
modifiedSentence = clientSocket.recv(2048) When characters arrive from the server, they get placed into the string modifiedSentence .
Characters continue to accumulate in modifiedSentence  until the line ends with a carriage return character.
After printing the capitalized sentence, we close the client’s socket: clientSocket.close() This last line closes the socket and, hence, closes the TCP connection between the client and the server.
It causes TCP in the client to send a TCP message to TCP in the server (see Section 3.5).
TCPServer.py Now let’s take a look at the server program.
from socket import * serverPort = 12000 serverSocket = socket(AF_INET, SOCK_STREAM)serverSocket.bind((’’, serverPort))serverSocket.listen(1)print(’The server is ready to receive’)while True:    connectionSocket, addr = serverSocket.accept()     sentence = connectionSocket.recv(1024).decode()     capitalizedSentence = sentence.upper()    connectionSocket.send(capitalizedSentence.encode())    connectionSocket.close() Let’s now take a look at the lines that differ significantly from UDPServer and TCPClient.
As withTCPClient, the server creates a TCP socket with: serverSocket=socket(AF_INET, SOCK_STREAM) Similar to UDPServer, we associate the server port number, serverPort , with this socket: serverSocket.bind((’’, serverPort)) But with TCP, serverSocket  will be our welcoming socket.
After establishing this welcoming door, we will wait and listen for some client to knock on the door: serverSocket.listen(1) This line has the server listen for TCP connection requests from the client.
The parameter specifies themaximum number of queued connections (at least 1).
connectionSocket, addr = serverSocket.accept() When a client knocks on this door, the program invokes the accept()  method for serverSocket, which creates a new socket in the server, called ­connectionSocket , dedicated to this particular client.
The client and server then complete the handshaking, creating a TCP connection between the client’s clientSocket  and the server’s connectionSocket .
With the TCP connection established, the client and server can now send bytes to each other over the connection.
With TCP, all bytes sent from one side not are not only guaranteed to arrive at the other side but also guaranteed arrive in order.
connectionSocket.close() In this program, after sending the modified sentence to the client, we close the connection socket.
But since serverSocket  remains open, another client can now knock on the door and send the server a sentence to modify.
This completes our discussion of socket programming in TCP.
You are encouraged to run the two programs in two separate hosts, and also to modify them to achieve slightly different goals.
You should compare the UDP program pair with the TCP program pair and see how they differ.
You should also do many of the socket programming assignments described at the ends of Chapter 2, 4, and 9.
Finally, we hope someday, after mastering these and more advanced socket programs, you will write your own popular network application, become very rich and famous, and remember the authors of this textbook!
2.8 Summary In this chapter, we’ve studied the conceptual and the implementation aspects of network applications.
We’ve learned about the ubiquitous client-server architecture adopted by many Internet applications andseen its use in the HTTP, SMTP, POP3, and DNS protocols.
We’ve studied these important application-level protocols, and their corresponding associated applications (the Web, file transfer, e-mail, and DNS)in some detail.
We’ve learned about the P2P architecture and how it is used in many applications.
We’ve also learned about streaming video, and how modern video distribution systems leverage CDNs.
We’ve examined how the socket API can be used to build network applications.
We’ve walked throughthe use of sockets for connection-oriented (TCP) and connectionless (UDP) end-to-end transportservices.
The first step in our journey down the layered network architecture is now complete!
At the very beginning of this book, in Section 1.1, we gave a rather vague, bare-bones definition of a protocol: “the format and the order of messages exchanged between two or more communicating entities, as well as the actions taken on the transmission and/or receipt of a message or other event.”
The material in this chapter, and in particular our detailed study of the HTTP, SMTP, POP3, and DNS protocols, has now added considerable substance to this definition.
Protocols are a key concept innetworking; our study of application protocols has now given us the opportunity to develop a moreintuitive feel for what protocols are all about.
In Section 2.1, we described the service models that TCP and UDP offer to applications that invoke them.
We took an even closer look at these service models when we developed simple applications that run over TCP and UDP in Section 2.7.
However, we have said little about how TCP and UDP provide these service models.
For example, we know that TCP provides a reliable data service, but we haven’t said yet how it does so.
In the next chapter we’ll take a careful look at not only the what, but also the how and why of transport protocols.
Equipped with knowledge about Internet application structure and application-level protocols, we’re now ready to head further down the protocol stack and examine the transport layer in Chapter 3.
Homework Problems and Questions Chapter 2 Review Questions SECTION 2.1 SECTION 2.2–2.5R1.
List five nonproprietary Internet applications and the application-layer protocols that they use.
R2.
What is the difference between network architecture and application architecture?
R3.
For a communication session between a pair of processes, which process is the client and which is the server?
R4.
For a P2P file-sharing application, do you agree with the statement, “There is no notion of client and server sides of a communication session”?
Why or why not?
R5.
What information is used by a process running on one host to identify a process running on another host?
R6.
Suppose you wanted to do a transaction from a remote client to a server as fast as possible.
Would you use UDP or TCP?
Why?
R7.
Referring to Figure 2.4 , we see that none of the applications listed in Figure 2.4 requires both no data loss and timing.
Can you conceive of an application that requires no data loss and that is also highly time-sensitive?
R8.
List the four broad classes of services that a transport protocol can provide.
For each of the service classes, indicate if either UDP or TCP (or both) provides such a service.
R9.
Recall that TCP can be enhanced with SSL to provide process-to-process security services, including encryption.
Does SSL operate at the transport layer or the application layer?
If the application developer wants TCP to be enhanced with SSL, what does the developer have to do?
R10.
What is meant by a handshaking protocol?
R11.
Why do HTTP, SMTP, and POP3 run on top of TCP rather than on UDP?R12.
Consider an e-commerce site that wants to keep a purchase record for each of its customers.
Describe how this can be done with cookies.
R13.
Describe how Web caching can reduce the delay in receiving a requested object.
Will Web caching reduce the delay for all objects requested by a user or for only some of the objects?
SECTION 2.5 SECTION 2.6Why?
R14.
Telnet into a Web server and send a multiline request message.
Include in the request message the If-modified-since:  header line to force a response message with the 304 Not Modified  status code.
R15.
List several popular messaging apps.
Do they use the same protocols as SMS?R16.
Suppose Alice, with a Web-based e-mail account (such as Hotmail or Gmail), sends a message to Bob, who accesses his mail from his mail server using POP3.
Discuss how the message gets from Alice’s host to Bob’s host.
Be sure to list the series of application-layerprotocols that are used to move the message between the two hosts.
R17.
Print out the header of an e-mail message you have recently received.
How many Received:  header lines are there?
Analyze each of the header lines in the message.
R18.
From a user’s perspective, what is the difference between the download-and-delete mode and the download-and-keep mode in POP3?
R19.
Is it possible for an organization’s Web server and mail server to have exactly the same alias for a hostname (for example, foo.com )?
What would be the type for the RR that contains the hostname of the mail server?
R20.
Look over your received e-mails, and examine the header of a message sent from a user with a .edu e-mail address.
Is it possible to determine from the header the IP address of the host from which the message was sent?
Do the same for a message sent from a Gmail account.
R21.
In BitTorrent, suppose Alice provides chunks to Bob throughout a 30-second interval.
Will Bob necessarily return the favor and provide chunks to Alice in this same interval?
Why or why not?
R22.
Consider a new peer Alice that joins BitTorrent without possessing any chunks.
Without any chunks, she cannot become a top-four uploader for any of the other peers, since she has nothing to upload.
How then will Alice get her first chunk?
R23.
What is an overlay network?
Does it include routers?
What are the edges in the overlay network?
R24.
CDNs typically adopt one of two different server placement philosophies.
Name and briefly describe them.
R25.
Besides network-related considerations such as delay, loss, and bandwidth performance, there are other important factors that go into designing a CDN server selection strategy.
What are they?
SECTION 2.7 ProblemsR26.
In Section 2.7, the UDP server described needed only one socket, whereas the TCP server needed two sockets.
Why?
If the TCP server were to support n simultaneous connections, each from a different client host, how many sockets would the TCP server need?
R27.
For the client-server application over TCP described in Section 2.7 , why must the server program be executed before the client program?
For the client-server application over UDP, why may the client program be executed before the server program?
P1.
True or false?
a. A user requests a Web page that consists of some text and three images.
For this page, the client will send one request message and receive four response messages.
b. Two distinct Web pages (for example, www.mit.edu/ research.html  and www.mit.edu/students.html ) can be sent over the same persistent connection.
c. With nonpersistent connections between browser and origin server, it is possible for asingle TCP segment to carry two distinct HTTP request messages.
d. The Date:  header in the HTTP response message indicates when the object in the response was last modified.
e. HTTP response messages never have an empty message body.
P2.
SMS, iMessage, and WhatsApp are all smartphone real-time messaging systems.
Afterdoing some research on the Internet, for each of these systems write one paragraph about the protocols they use.
Then write a paragraph explaining how they differ.
P3.
Consider an HTTP client that wants to retrieve a Web document at a given URL.
The IP address of the HTTP server is initially unknown.
What transport and application-layer protocols besides HTTP are needed in this scenario?
P4.
Consider the following string of ASCII characters that were captured by Wireshark when the browser sent an HTTP GET message (i.e., this is the actual content of an HTTP GET message).
The characters <cr><lf> are carriage return and line-feed characters (that is, the italized character string <cr> in the text below represents the single carriage-return character that was contained at that point in the HTTP header).
Answer the following questions, indicating where in the HTTP GET message below you find the answer.
GET /cs453/index.html HTTP/1.1 <cr><lf> Host: gai a.cs.umass.edu <cr><lf> User-Agent: Mozilla/5.0 ( Windows;U; Windows NT 5.1; en-US; rv:1.7.2) Gec ko/20040804 Netscape/7.2 (ax) <cr><lf> Accept:ex
t/xml, application/xml, application/xhtml+xml, text /html;q=0.9, text/plain;q=0.8, image/png,*/*;q=0.5 <cr><lf> Accept-Language: en-us, en;q=0.5 <cr><lf> Accept- Encoding: zip, deflate <cr><lf> Accept-Charset: ISO -8859-1, utf-8;q=0.7,*;q=0.7 <cr><lf> Keep-Alive: 300 <cr> <lf>Connection:keep-alive <cr><lf><cr><lf> a. What is the URL of the document requested by the browser?
b. What version of HTTP is the browser running?
c. Does the browser request a non-persistent or a persistent connection?
d. What is the IP address of the host on which the browser is running?
e. What type of browser initiates this message?
Why is the browser type needed in an HTTP request message?
P5.
The text below shows the reply sent from the server in response to the HTTP GET message in the question above.
Answer the following questions, indicating where in the message below you find the answer.
HTTP/1.1 200 OK <cr><lf> Date: Tue, 07 Mar 2008 12:39:45GMT <cr><lf> Server: Apache/2.0.52 (Fedora) <cr><lf>Last-Modified: Sat, 10 Dec2005 18:27:46GMT<cr><lf> ETag: ”526c3-f22-a88a4c80” <cr><lf> Accept- Ranges: bytes <cr><lf> Content-Length: 3874 <cr><lf> Keep-Alive: timeout=max=100 <cr><lf> Connection: Keep-Alive <cr><lf> Content-Type: text/html; charset= ISO-8859-1 <cr><lf><cr><lf> <!
doctype html public ”- //w3c//dtd html 4.0 transitional//en”> <lf><html> <lf> <head><lf> <meta http-equiv=”Content-Type” content=”text/html; charset=iso-8859-1”> <lf> <meta name=”GENERATOR” content=”Mozilla/4.79 [en] (Windows NT5.0; U) Netscape]”> <lf> <title>CMPSCI 453 / 591 / NTU-ST550ASpring 2005 homepage</title> <lf></head> <lf> <much more document text following here (not shown) > a. Was the server able to successfully find the document or not?
What time was the document reply provided?
b. When was the document last modified?
c. How many bytes are there in the document being returned?
d. What are the first 5 bytes of the document being returned?
Did the server agree to a
persistent connection?
P6.
Obtain the HTTP/1.1 specification (RFC 2616).
Answer the following questions: a. Explain the mechanism used for signaling between the client and server to indicate that a persistent connection is being closed.
Can the client, the server, or both signal the close of a connection?
b. What encryption services are provided by HTTP?
c. Can a client open three or more simultaneous connections with a given server?
d. Either a server or a client may close a transport connection between them if either one detects the connection has been idle for some time.
Is it possible that one side starts closing a connection while the other side is transmitting data via this connection?Explain.
P7.
Suppose within your Web browser you click on a link to obtain a Web page.
The IP address for the associated URL is not cached in your local host, so a DNS lookup is necessary to obtain the IP address.
Suppose that n DNS servers are visited before your host receives the IP address from DNS; the successive visits incur an RTT of  Further suppose that the Web page associated with the link contains exactly one object, consisting of a small amount of HTML text.
Let RTT  denote the RTT between the local host and the server containing the object.
Assuming zero transmission time of the object, how much time elapses from when the client clicks on the link until the client receives the object?
P8.
Referring to Problem P7, suppose the HTML file references eight very small objects on the same server.
Neglecting transmission times, how much time elapses with a. Non-persistent HTTP with no parallel TCP connections?
b. Non-persistent HTTP with the browser configured for 5 parallel connections?
c. Persistent HTTP?
P9.
Consider Figure 2.12 , for which there is an institutional network connected to the Internet.
Suppose that the average object size is 850,000 bits and that the average request rate from the institution’s browsers to the origin servers is 16 requests per second.
Also suppose that the amount of time it takes from when the router on the Internet side of the access link forwards an HTTP request until it receives the response is three seconds on average (see Section 2.2.5).
Model the total average response time as the sum of the average access delay (that is, the delay from Internet router to institution router) and the average Internet delay.
For the average accessdelay, use  where Δ is the average time required to send an object over the access link and b is the arrival rate of objects to the access link.
a. Find the total average response time.
b. Now suppose a cache is installed in the institutional LAN.
Suppose the miss rate is 0.4.
Find the total response time.
RTT1,. .
.,RTTn.
0 Δ/(1−Δβ),
P10.
Consider a short, 10-meter link, over which a sender can transmit at a rate of 150 bits/sec in both directions.
Suppose that packets containing data are 100,000 bits long, and packets containing only control (e.g., ACK or handshaking) are 200 bits long.
Assume that N parallel connections each get 1/ N of the link bandwidth.
Now consider the HTTP protocol, and suppose that each downloaded object is 100 Kbits long, and that the initial downloaded object contains 10 referenced objects from the same sender.
Would parallel downloads via parallel instances of non-persistent HTTP make sense in this case?
Now consider persistent HTTP.
Do you expect significant gains over the non-persistent case?
Justify and explain your answer.
P11.
Consider the scenario introduced in the previous problem.
Now suppose that the link is shared by Bob with four other users.
Bob uses parallel instances of non-persistent HTTP, and the other four users use non-persistent HTTP without parallel downloads.
a. Do Bob’s parallel connections help him get Web pages more quickly?
Why or why not?
b. If all five users open five parallel instances of non-persistent HTTP, then would Bob’s parallel connections still be beneficial?
Why or why not?
P12.
Write a simple TCP program for a server that accepts lines of input from a client and prints the lines onto the server’s standard output. (
You can do this by modifying the TCPServer.py program in the text.)
Compile and execute your program.
On any other machine that contains aWeb browser, set the proxy server in the browser to the host that is running your serverprogram; also configure the port number appropriately.
Your browser should now send its GETrequest messages to your server, and your server should display the messages on its standardoutput.
Use this platform to determine whether your browser generates conditional GETmessages for objects that are locally cached.
P13.
What is the difference between MAIL FROM : in SMTP and From : in the mail message itself?P14.
How does SMTP mark the end of a message body?
How about HTTP?
Can HTTP use the same method as SMTP to mark the end of a message body?
Explain.
P15.
Read RFC 5321 for SMTP.
What does MTA stand for?
Consider the following received spam e-mail (modified from a real spam e-mail).
Assuming only the originator of this spam e-mail is malicious and all other hosts are honest, identify the malacious host that has generated thisspam e-mail.
From - Fri Nov 07 13:41:30 2008Return-Path: <tennis5@pp33head.com>Received: from barmail.cs.umass.edu (barmail.cs.umass.edu[128.119.240.3]) by cs.umass.edu (8.13.1/8.12.6) for<hg@cs.umass.edu>; Fri, 7 Nov 2008 13:27:10 -0500Received: from asusus-4b96 (localhost [127.0.0.1]) by barmail.cs.umass.edu (Spam Firewall) for <hg@cs.umass.edu>; Fri, 7
Nov 2008 13:27:07 -0500 (EST) Received: from asusus-4b96 ([58.88.21.177]) by barmail.cs.umass.edu for <hg@cs.umass.edu>; Fri, 07 Nov 2008 13:27:07 -0500 (EST)Received: from [58.88.21.177] by inbnd55.exchangeddd.com; Sat, 8Nov 2008 01:27:07 +0700From: ”Jonny” <tennis5@pp33head.com> To: <hg@cs.umass.edu> Subject: How to secure your savings P16.
Read the POP3 RFC, RFC 1939.
What is the purpose of the UIDL POP3 command?
P17.
Consider accessing your e-mail with POP3.
a. Suppose you have configured your POP mail client to operate in the download-and- delete mode.
Complete the following transaction: C: list S: 1 498 S: 2 912 S: .
C: retr 1 S: blah blah ... S: ..........blah S: . ? ?
b. Suppose you have configured your POP mail client to operate in the download-and-keep mode.
Complete the following transaction: C: list S: 1 498 S: 2 912 S: .
C: retr 1 S: blah blah ... S: ..........blah S: . ?
?
c. Suppose you have configured your POP mail client to operate in the download-and-keep mode.
Using your transcript in part (b), suppose you retrieve messages 1 and 2, exit POP, and then five minutes later you again access POP to retrieve new e-mail.
Supposethat in the five-minute interval no new messages have been sent to you.
Provide a transcript of this second POP session.
P18.
a. What is a whois  database?
b. Use various whois databases on the Internet to obtain the names of two DNS servers.
Indicate which whois databases you used.
c. Use nslookup on your local host to send DNS queries to three DNS servers: your localDNS server and the two DNS servers you found in part (b).
Try querying for Type A, NS, and MX reports.
Summarize your findings.
d. Use nslookup to find a Web server that has multiple IP addresses.
Does the Web server of your institution (school or company) have multiple IP addresses?
e. Use the ARIN whois database to determine the IP address range used by youruniversity.
f. Describe how an attacker can use whois databases and the nslookup tool to performreconnaissance on an institution before launching an attack.
g. Discuss why whois databases should be publicly available.
P19.
In this problem, we use the useful dig tool available on Unix and Linux hosts to explore the hierarchy of DNS servers.
Recall that in Figure 2.19 , a DNS server in the DNS hierarchy delegates a DNS query to a DNS server lower in the hierarchy, by sending back to the DNS client the name of that lower-level DNS server.
First read the man page for dig, and then answer the following questions.
a. Starting with a root DNS server (from one of the root servers [a-m].root-servers.net), initiate a sequence of queries for the IP address for your department’s Web server by using dig.
Show the list of the names of DNS servers in the delegation chain in answering your query.
b. Repeat part (a) for several popular Web sites, such as google.com , yahoo.com, or amazon.com.
P20.
Suppose you can access the caches in the local DNS servers of your department.
Can you propose a way to roughly determine the Web servers (outside your department) that are most popular among the users in your department?
Explain.
P21.
Suppose that your department has a local DNS server for all computers in the department.
You are an ordinary user (i.e., not a network/system administrator).
Can you determine if an external Web site was likely accessed from a computer in your department a couple of seconds ago?
Explain.
P22.
Consider distributing a file of  Gbits to N peers.
The server has an upload rate of  Mbps, and each peer has a download rate of  Mbps and an upload rate of u. For  100, and 1,000 and  700 Kbps, and 2 Mbps, prepare a chart giving the minimum distribution time for each of the combinations of N and u for both client-server distribution and P2P distribution.
P23.
Consider distributing a file of F bits to N peers using a client-server architecture.
Assume a fluid model where the server can simultaneously transmit to multiple peers, transmitting to eachpeer at different rates, as long as the combined rate does not exceed u. a. Suppose that  Specify a distribution scheme that has a distribution time of NF/u .
b. Suppose that  Specify a distribution scheme that has a distribution time of F/d .
c. Conclude that the minimum distribution time is in general given by  P24.
Consider distributing a file of F bits to N peers using a P2P architecture.
Assume a fluid model.
For simplicity assume that dmin is very large, so that peer download bandwidth is never a bottleneck.
a. Suppose that  Specify a distribution scheme that has a distribution time of F/u .
b. Suppose that  Specify a distribution scheme that has a distribution time of  c. Conclude that the minimum distribution time is in general given by P25.
Consider an overlay network with N active peers, with each pair of peers having an active TCP connection.
Additionally, suppose that the TCP connections pass through a total of M routers.
How many nodes and edges are there in the corresponding overlay network?
P26.
Suppose Bob joins a BitTorrent torrent, but he does not want to upload any data to any other peers (so called free-riding).
a. Bob claims that he can receive a complete copy of the file that is shared by the swarm.
IsBob’s claim possible?
Why or why not?
b. Bob further claims that he can further make his “free-riding” more efficient by using acollection of multiple computers (with distinct IP addresses) in the computer lab in his department.
How can he do that?
P27.
Consider a DASH system for which there are N video versions (at N different rates and qualities) and N audio versions (at N different rates and qualities).
Suppose we want to allow theF=15 us=30 di=2 N=10, u=300 Kbps, s us/N≤dmin.
s us/N≥dmin.
min max{NF/us, F/dmin}.
us≤(us+u1+…+uN)/N. s us≥(us+u1+…+uN)/N. NF/(us+u1+…+uN).
max{F/us, NF/(us+u1+…+uN)}.
Socket Programming Assignments The Companion Website includes six socket programming assignments.
The first four assignments are summarized below.
The fifth assignment makes use of the ICMP protocol and is summarized at the end of Chapter 5.
The sixth assignment employs multimedia protocols and is summarized at the end of Chapter 9.
It is highly recommended that students complete several, if not all, of these assignments.
Students can find full details of these assignments, as well as important snippets of the Python code, at the Web site www.pearsonhighered.com/ cs-resources .
Assignment 1: Web Serverplayer to choose at any time any of the N video versions and any of the N audio versions.
a. If we create files so that the audio is mixed in with the video, so server sends only one media stream at given time, how many files will the server need to store (each a different URL)?
b. If the server instead sends the audio and video streams separately and has the client synchronize the streams, how many files will the server need to store?
P28.
Install and compile the Python programs TCPClient and UDPClient on one host and TCPServer and UDPServer on another host.
a. Suppose you run TCPClient before you run TCPServer.
What happens?
Why?
b. Suppose you run UDPClient before you run UDPServer.
What happens?
Why?
c. What happens if you use different port numbers for the client and server sides?
P29.
Suppose that in UDPClient.py, after we create the socket, we add the line: clientSocket.bind((’’, 5432)) Will it become necessary to change UDPServer.py?
What are the port numbers for the sockets in UDPClient and UDPServer?
What were they before making this change?
P30.
Can you configure your browser to open multiple simultaneous connections to a Web site?
What are the advantages and disadvantages of having a large number of simultaneous TCP connections?
P31.
We have seen that Internet TCP sockets treat the data being sent as a byte stream but UDP sockets recognize message boundaries.
What are one advantage and one disadvantage of byte-oriented API versus having the API explicitly recognize and preserve application-definedmessage boundaries?
P32.
What is the Apache Web server?
How much does it cost?
What functionality does it currently have?
You may want to look at Wikipedia to answer this question.
In this assignment, you will develop a simple Web server in Python that is capable of processing only one request.
Specifically, your Web server will (i) create a connection socket when contacted by a client(browser); (ii) receive the HTTP request from this connection; (iii) parse the request to determine thespecific file being requested; (iv) get the requested file from the server’s file system; (v) create an HTTPresponse message consisting of the requested file preceded by header lines; and (vi) send the response over the TCP connection to the requesting browser.
If a browser requests a file that is not present in your server, your server should return a “404 Not Found” error message.
In the Companion Website, we provide the skeleton code for your server.
Your job is to complete the code, run your server, and then test your server by sending requests from browsers running on differenthosts.
If you run your server on a host that already has a Web server running on it, then you should usea different port than port 80 for your Web server.
Assignment 2: UDP Pinger In this programming assignment, you will write a client ping program in Python.
Your client will send a simple ping message to a server, receive a corresponding pong message back from the server, anddetermine the delay between when the client sent the ping message and received the pong message.
This delay is called the Round Trip Time (RTT).
The functionality provided by the client and server issimilar to the functionality provided by standard ping program available in modern operating systems.
However, standard ping programs use the Internet Control Message Protocol (ICMP) (which we will study in Chapter 5).
Here we will create a nonstandard (but simple!)
UDP-based ping program.
Your ping program is to send 10 ping messages to the target server over UDP.
For each message, your client is to determine and print the RTT when the corresponding pong message is returned.
BecauseUDP is an unreliable protocol, a packet sent by the client or server may be lost.
For this reason, theclient cannot wait indefinitely for a reply to a ping message.
You should have the client wait up to onesecond for a reply from the server; if no reply is received, the client should assume that the packet waslost and print a message accordingly.
In this assignment, you will be given the complete code for the server (available in the Companion Website).
Your job is to write the client code, which will be very similar to the server code.
It is recommended that you first study carefully the server code.
You can then write your client code, liberally cutting and pasting lines from the server code.
Assignment 3: Mail Client The goal of this programming assignment is to create a simple mail client that sends e-mail to any recipient.
Your client will need to establish a TCP connection with a mail server (e.g., a Google mailserver), dialogue with the mail server using the SMTP protocol, send an e-mail message to a recipient
(e.g., your friend) via the mail server, and finally close the TCP connection with the mail server.
For this assignment, the Companion Website provides the skeleton code for your client.
Your job is to complete the code and test your client by sending e-mail to different user accounts.
You may also trysending through different servers (for example, through a Google mail server and through youruniversity mail server).
Assignment 4: Multi-Threaded Web Proxy In this assignment, you will develop a Web proxy.
When your proxy receives an HTTP request for an object from a browser, it generates a new HTTP request for the same object and sends it to the originserver.
When the proxy receives the corresponding HTTP response with the object from the originserver, it creates a new HTTP response, including the object, and sends it to the client.
This proxy willbe multi-threaded, so that it will be able to handle multiple requests at the same time.
For this assignment, the Companion Website provides the skeleton code for the proxy server.
Your job is to complete the code, and then test it by having different browsers request Web objects via your proxy.
Wireshark Lab: HTTP Having gotten our feet wet with the Wireshark packet sniffer in Lab 1, we’re now ready to use Wireshark to investigate protocols in operation.
In this lab, we’ll explore several aspects of the HTTP protocol: thebasic GET/reply interaction, HTTP message formats, retrieving large HTML files, retrieving HTML fileswith embedded URLs, persistent and non-persistent connections, and HTTP authentication and security.
As is the case with all Wireshark labs, the full description of this lab is available at this book’s Web site, www.pearsonhighered.com/ cs-resources .
Wireshark Lab: DNS In this lab, we take a closer look at the client side of the DNS, the protocol that translates Internet hostnames to IP addresses.
Recall from Section 2.5 that the client’s role in the DNS is relatively simple —a client sends a query to its local DNS server and receives a response back.
Much can go on under the covers, invisible to the DNS clients, as the hierarchical DNS servers communicate with each other toeither recursively or iteratively resolve the client’s DNS query.
From the DNS client’s standpoint,however, the protocol is quite simple—a query is formulated to the local DNS server and a response isreceived from that server.
We observe DNS in action in this lab.
As is the case with all Wireshark labs, the full description of this lab is available at this book’s Web site, www.pearsonhighered.com/ cs-resources .
An Interview With… Marc Andreessen Marc Andreessen is the co-creator of Mosaic, the Web browser that popularized the World Wide Web in 1993.
Mosaic had a clean, easily understood interface and was the first browser todisplay images in-line with text.
In 1994, Marc Andreessen and Jim Clark founded Netscape,whose browser was by far the most popular browser through the mid-1990s.
Netscape alsodeveloped the Secure Sockets Layer (SSL) protocol and many Internet server products,including mail servers and SSL-based Web servers.
He is now a co-founder and general partnerof venture capital firm Andreessen Horowitz, overseeing portfolio development with holdings that include Facebook, Foursquare, Groupon, Jawbone, Twitter, and Zynga.
He serves on numerous boards, including Bump, eBay, Glam Media, Facebook, and Hewlett-Packard.
He holds a BS inComputer Science from the University of Illinois at Urbana-Champaign.
How did you become interested in computing?
Did you always know that you wanted to work in information technology?
The video game and personal computing revolutions hit right when I was growing up—personal computing was the new technology frontier in the late 70’s and early 80’s.
And it wasn’t justApple and the IBM PC, but hundreds of new companies like Commodore and Atari as well.
Itaught myself to program out of a book called “Instant Freeze-Dried BASIC” at age 10, and gotmy first computer (a TRS-80 Color Computer—look it up!)
at age 12.
Please describe one or two of the most exciting projects you have worked on during your career.
What were the biggest challenges?
Undoubtedly the most exciting project was the original Mosaic web browser in ’92–’93—and the biggest challenge was getting anyone to take it seriously back then.
At the time, everyonethought the interactive future would be delivered as “interactive television” by huge companies,not as the Internet by startups.
What excites you about the future of networking and the Internet?
What are your biggest concerns?
The most exciting thing is the huge unexplored frontier of applications and services that programmers and entrepreneurs are able to explore—the Internet has unleashed creativity at a level that I don’t think we’ve ever seen before.
My biggest concern is the principle of unintended consequences—we don’t always know the implications of what we do, such as the Internetbeing used by governments to run a new level of surveillance on citizens.
Is there anything in particular students should be aware of as Web technology advances?
The rate of change—the most important thing to learn is how to learn—how to flexibly adapt to changes in the specific technologies, and how to keep an open mind on the new opportunitiesand possibilities as you move through your career.
What people inspired you professionally?
Vannevar Bush, Ted Nelson, Doug Engelbart, Nolan Bushnell, Bill Hewlett and Dave Packard, Ken Olsen, Steve Jobs, Steve Wozniak, Andy Grove, Grace Hopper, Hedy Lamarr, Alan Turing,Richard Stallman.
What are your recommendations for students who want to pursue careers in computing and information technology?
Go as deep as you possibly can on understanding how technology is created, and then complement with learning how business works.
Can technology solve the world’s problems?
No, but we advance the standard of living of people through economic growth, and most economic growth throughout history has come from technology—so that’s as good as it gets.
Chapter 3 Transport Layer Residing between the application and network layers, the transport layer is a central piece of the layered network architecture.
It has the critical role of providing communication services directly to theapplication processes running on different hosts.
The pedagogic approach we take in this chapter is toalternate between discussions of transport-layer principles and discussions of how these principles areimplemented in existing protocols; as usual, particular emphasis will be given to Internet protocols, in particular the TCP and UDP transport-layer protocols.
We’ll begin by discussing the relationship between the transport and network layers.
This sets the stage for examining the first critical function of the transport layer—extending the network layer’s deliveryservice between two end systems to a delivery service between two application-layer processes runningon the end systems.
We’ll illustrate this function in our coverage of the Internet’s connectionlesstransport protocol, UDP.
We’ll then return to principles and confront one of the most fundamental problems in computer networking—how two entities can communicate reliably over a medium that may lose and corrupt data.
Through a series of increasingly complicated (and realistic!)
scenarios, we’ll build up an array oftechniques that transport protocols use to solve this problem.
We’ll then show how these principles areembodied in TCP, the Internet’s connection-oriented transport protocol.
We’ll next move on to a second fundamentally important problem in networking—controlling the transmission rate of transport-layer entities in order to avoid, or recover from, congestion within thenetwork.
We’ll consider the causes and consequences of congestion, as well as commonly used congestion-control techniques.
After obtaining a solid understanding of the issues behind congestion control, we’ll study TCP’s approach to congestion control.
3.1 Introduction and Transport-Layer Services In the previous two chapters we touched on the role of the transport layer and the services that it provides.
Let’s quickly review what we have already learned about the transport layer.
A transport-layer protocol provides for logical communication  between application processes running on different hosts.
By logical communication , we mean that from an application’s perspective, it is as if the hosts running the processes were directly connected; in reality, the hosts may be on opposite sides of the planet, connected via numerous routers and a wide range of link types.
Application processes usethe logical communication provided by the transport layer to send messages to each other, free from the worry of the details of the physical infrastructure used to carry these messages.
Figure 3.1 illustrates the notion of logical communication.
As shown in Figure 3.1, transport-layer protocols are implemented in the end systems but not in network routers.
On the sending side, the transport layer converts the application-layer messages it receives from a sending application process into transport-layer packets, known as transport-layer segments in Internet terminology.
This is done by (possibly) breaking the application messages intosmaller chunks and adding a transport-layer header to each chunk to create the transport-layersegment.
The transport layer then passes the segment to the network layer at the sending end system,where the segment is encapsulated within a network-layer packet (a datagram) and sent to thedestination.
It’s important to note that network routers act only on the network-layer fields of thedatagram; that is, they do not examine the fields of the transport-layer segment encapsulated with thedatagram.
On the receiving side, the network layer extracts the transport-layer segment from thedatagram and passes the segment up to the transport layer.
The transport layer then processes the received segment, making the data in the segment available to the receiving application.
More than one transport-layer protocol may be available to network applications.
For example, the Internet has two protocols—TCP and UDP.
Each of these protocols provides a different set of transport-layer services to the invoking application.
3.1.1 Relationship Between Transport and Network Layers Recall that the transport layer lies just above the network layer in the protocol stack.
Whereas atransport-layer protocol provides logical communication between
Figure 3.1 The transport layer provides logical rather than physical communication between application processes processes  running on different hosts, a network-layer protocol provides logical-communication between hosts.
This distinction is subtle but important.
Let’s examine this distinction with the aid of a household analogy.
Consider two houses, one on the East Coast and the other on the West Coast, with each house being home to a dozen kids.
The kids in the East Coast household are cousins of the kids in the West Coast
household.
The kids in the two households love to write to each other—each kid writes each cousin every week, with each letter delivered by the traditional postal service in a separate envelope.
Thus, each household sends 144 letters to the other household every week. (
These kids would save a lot ofmoney if they had e-mail!)
In each of the households there is one kid—Ann in the West Coast houseand Bill in the East Coast house—responsible for mail collection and mail distribution.
Each week Annvisits all her brothers and sisters, collects the mail, and gives the mail to a postal-service mail carrier, who makes daily visits to the house.
When letters arrive at the West Coast house, Ann also has the job of distributing the mail to her brothers and sisters.
Bill has a similar job on the East Coast.
In this example, the postal service provides logical communication between the two houses—the postal service moves mail from house to house, not from person to person.
On the other hand, Ann and Billprovide logical communication among the cousins—Ann and Bill pick up mail from, and deliver mail to, their brothers and sisters.
Note that from the cousins’ perspective, Ann and Bill are the mail service, even though Ann and Bill are only a part (the end-system part) of the end-to-end delivery process.
This household example serves as a nice analogy for explaining how the transport layer relates to the network layer: application messages  letters in envelopes processes  cousins hosts (also called end systems)  houses transport-layer protocol  Ann and Bill network-layer protocol  postal service (including mail carriers) Continuing with this analogy, note that Ann and Bill do all their work within their respective homes; they are not involved, for example, in sorting mail in any intermediate mail center or in moving mail from onemail center to another.
Similarly, transport-layer protocols live in the end systems.
Within an end system,a transport protocol moves messages from application processes to the network edge (that is, thenetwork layer) and vice versa, but it doesn’t have any say about how the messages are moved within the network core.
In fact, as illustrated in Figure 3.1, intermediate routers neither act on, nor recognize, any information that the transport layer may have added to the application messages.
Continuing with our family saga, suppose now that when Ann and Bill go on vacation, another cousin pair—say, Susan and Harvey—substitute for them and provide the household-internal collection anddelivery of mail.
Unfortunately for the two families, Susan and Harvey do not do the collection anddelivery in exactly the same way as Ann and Bill.
Being younger kids, Susan and Harvey pick up and drop off the mail less frequently and occasionally lose letters (which are sometimes chewed up by thefamily dog).
Thus, the cousin-pair Susan and Harvey do not provide the same set of services (that is,the same service model) as Ann and Bill.
In an analogous manner, a computer network may make= = = = =
available multiple transport protocols, with each protocol offering a different service model to applications.
The possible services that Ann and Bill can provide are clearly constrained by the possible services that the postal service provides.
For example, if the postal service doesn’t provide a maximum bound on howlong it can take to deliver mail between the two houses (for example, three days), then there is no way that Ann and Bill can guarantee a maximum delay for mail delivery between any of the cousin pairs.
In a similar manner, the services that a transport protocol can provide are often constrained by the servicemodel of the underlying network-layer protocol.
If the network-layer protocol cannot provide delay orbandwidth guarantees for transport-layer segments sent between hosts, then the transport-layerprotocol cannot provide delay or bandwidth guarantees for application messages sent betweenprocesses.
Nevertheless, certain services can be offered by a transport protocol even when the underlying network protocol doesn’t offer the corresponding service at the network layer.
For example, as we’ll see in this chapter, a transport protocol can offer reliable data transfer service to an application even when theunderlying network protocol is unreliable, that is, even when the network protocol loses, garbles, or duplicates packets.
As another example (which we’ll explore in Chapter 8 when we discuss network security), a transport protocol can use encryption to guarantee that application messages are not read by intruders, even when the network layer cannot guarantee the confidentiality of transport-layersegments.
3.1.2 Overview of the Transport Layer in the Internet Recall that the Internet makes two distinct transport-layer protocols available to the application layer.
One of these protocols is UDP (User Datagram Protocol), which provides an unreliable, connectionlessservice to the invoking application.
The second of these protocols is TCP (Transmission Control Protocol), which provides a reliable, connection-oriented service to the invoking application.
Whendesigning a network application, the application developer must specify one of these two transport protocols.
As we saw in Section 2.7, the application developer selects between UDP and TCP when creating sockets.
To simplify terminology, we refer to the transport-layer packet as a segment.
We mention, however, that the Internet literature (for example, the RFCs) also refers to the transport-layer packet for TCP as a segment but often refers to the packet for UDP as a datagram.
But this same Internet literature also uses the term datagram  for the network-layer packet!
For an introductory book on computer networking such as this, we believe that it is less confusing to refer to both TCP and UDP packets as segments,and reserve the term datagram  for the network-layer packet.
Before proceeding with our brief introduction of UDP and TCP, it will be useful to say a few words about the Internet’s network layer. (
We’ll learn about the network layer in detail in Chapters 4 and 5.)
The Internet’s network-layer protocol has a name—IP, for Internet Protocol.
IP provides logical communication between hosts.
The IP service model is a best-effort delivery service .
This means that IP makes its “best effort” to deliver segments between communicating hosts, but it makes no guarantees.
 In particular, it does not guarantee segment delivery, it does not guarantee orderly delivery of segments, and it does not guarantee the integrity of the data in the segments.
For these reasons, IPis said to be an unreliable service.
We also mention here that every host has at least one network- layer address, a so-called IP address.
We’ll examine IP addressing in detail in Chapter 4; for this chapter we need only keep in mind that each host has an IP address .
Having taken a glimpse at the IP service model, let’s now summarize the service models provided by UDP and TCP.
The most fundamental responsibility of UDP and TCP is to extend IP’s delivery servicebetween two end systems to a delivery service between two processes running on the end systems.
Extending host-to-host delivery to process-to-process delivery is called transport-layer multiplexing and demultiplexing .
We’ll discuss transport-layer multiplexing and demultiplexing in the next section.
UDP and TCP also provide integrity checking by including error-detection fields in their segments’ headers.
These two minimal transport-layer services—process-to-process data delivery and errorchecking—are the only two services that UDP provides!
In particular, like IP, UDP is an unreliableservice—it does not guarantee that data sent by one process will arrive intact (or at all!)
to the destination process.
UDP is discussed in detail in Section 3.3.
TCP, on the other hand, offers several additional services to applications.
First and foremost, it provides reliable data transfer.
Using flow control, sequence numbers, acknowledgments, and timers(techniques we’ll explore in detail in this chapter), TCP ensures that data is delivered from sendingprocess to receiving process, correctly and in order.
TCP thus converts IP’s unreliable service betweenend systems into a reliable data transport service between processes.
TCP also provides congestion control.
Congestion control is not so much a service provided to the invoking application as it is aservice for the Internet as a whole, a service for the general good.
Loosely speaking, TCP congestioncontrol prevents any one TCP connection from swamping the links and routers between communicatinghosts with an excessive amount of traffic.
TCP strives to give each connection traversing a congested link an equal share of the link bandwidth.
This is done by regulating the rate at which the sending sides of TCP connections can send traffic into the network.
UDP traffic, on the other hand, is unregulated.
Anapplication using UDP transport can send at any rate it pleases, for as long as it pleases.
A protocol that provides reliable data transfer and congestion control is necessarily complex.
We’ll need several sections to cover the principles of reliable data transfer and congestion control, and additional sections to cover the TCP protocol itself.
These topics are investigated in Sections 3.4 through 3.8.
The approach taken in this chapter is to alternate between basic principles and the TCP protocol.
For example, we’ll first discuss reliable data transfer in a general setting and then discuss how TCP
specifically provides reliable data transfer.
Similarly, we’ll first discuss congestion control in a general setting and then discuss how TCP performs congestion control.
But before getting into all this good stuff, let’s first look at transport-layer multiplexing and demultiplexing.
3.2 Multiplexing and Demultiplexing In this section, we discuss transport-layer multiplexing and demultiplexing, that is, extending the host-to- host delivery service provided by the network layer to a process-to-process delivery service forapplications running on the hosts.
In order to keep the discussion concrete, we’ll discuss this basictransport-layer service in the context of the Internet.
We emphasize, however, that amultiplexing/demultiplexing service is needed for all computer networks.
At the destination host, the transport layer receives segments from the network layer just below.
The transport layer has the responsibility of delivering the data in these segments to the appropriateapplication process running in the host.
Let’s take a look at an example.
Suppose you are sitting in frontof your computer, and you are downloading Web pages while running one FTP session and two Telnetsessions.
You therefore have four network application processes running—two Telnet processes, oneFTP process, and one HTTP process.
When the transport layer in your computer receives data from thenetwork layer below, it needs to direct the received data to one of these four processes.
Let’s now examine how this is done.
First recall from Section 2.7 that a process (as part of a network application) can have one or more sockets , doors through which data passes from the network to the process and through which data passes from the process to the network.
Thus, as shown in Figure 3.2, the transport layer in the receiving host does not actually deliver data directly to a process, but instead to an intermediary socket.
Because at any given time there can be more than one socket in the receiving host, each socket has aunique identifier.
The format of the identifier depends on whether the socket is a UDP or a TCP socket,as we’ll discuss shortly.
Now let’s consider how a receiving host directs an incoming transport-layer segment to the appropriate socket.
Each transport-layer segment has a set of fields in the segment for this purpose.
At the receivingend, the transport layer examines these fields to identify the receiving socket and then directs thesegment to that socket.
This job of delivering the data in a transport-layer segment to the correct socketis called demultiplexing .
The job of gathering data chunks at the source host from different sockets, encapsulating each data chunk with header information (that will later be used in demultiplexing) tocreate segments, and passing the segments to the network layer is called multiplexing .
Note that the transport layer in the middle host
Figure 3.2 Transport-layer multiplexing and demultiplexing in Figure 3.2 must demultiplex segments arriving from the network layer below to either process P  or P above; this is done by directing the arriving segment’s data to the corresponding process’s socket.
The transport layer in the middle host must also gather outgoing data from these sockets, form transport-layer segments, and pass these segments down to the network layer.
Although we have introducedmultiplexing and demultiplexing in the context of the Internet transport protocols, it’s important to realizethat they are concerns whenever a single protocol at one layer (at the transport layer or elsewhere) isused by multiple protocols at the next higher layer.
To illustrate the demultiplexing job, recall the household analogy in the previous section.
Each of the kids is identified by his or her name.
When Bill receives a batch of mail from the mail carrier, heperforms a demultiplexing operation by observing to whom the letters are addressed and then handdelivering the mail to his brothers and sisters.
Ann performs a multiplexing operation when she collectsletters from her brothers and sisters and gives the collected mail to the mail person.
Now that we understand the roles of transport-layer multiplexing and demultiplexing, let us examine how it is actually done in a host.
From the discussion above, we know that transport-layer multiplexing requires (1) that sockets have unique identifiers, and (2) that each segment have special fields that indicate the socket to which the segment is to be delivered.
These special fields, illustrated in Figure 3.3, are the source port number field  and the destination port number field . (
The UDP and TCP segments have other fields as well, as discussed in the subsequent sections of this chapter.)
Each port number is a 16-bit number, ranging from 0 to 65535.
The port numbers ranging from 0 to 1023 arecalled well-known port numbers  and are restricted, which means that they are reserved for use by well-known1 2
Figure 3.3 Source and destination port-number fields in a transport-layer segment application protocols such as HTTP (which uses port number 80) and FTP (which uses port number 21).
The list of well-known port numbers is given in RFC 1700 and is updated at http:/ /www.iana.org  [RFC 3232] .
When we develop a new application (such as the simple application developed in Section 2.7), we must assign the application a port number.
It should now be clear how the transport layer could  implement the demultiplexing service: Each socket in the host could be assigned a port number, and when a segment arrives at the host, the transport layer examines the destination port number in the segment and directs the segment to the correspondingsocket.
The segment’s data then passes through the socket into the attached process.
As we’ll see, thisis basically how UDP does it.
However, we’ll also see that multiplexing/demultiplexing in TCP is yet more subtle.
Connectionless Multiplexing and DemultiplexingRecall from Section 2.7.1 that the Python program running in a host can create a UDP socket with the line clientSocket = socket(AF_INET, SOCK_DGRAM) When a UDP socket is created in this manner, the transport layer automatically assigns a port number to the socket.
In particular, the transport layer assigns a port number in the range 1024 to 65535 that is currently not being used by any other UDP port in the host.
Alternatively, we can add a line into our Python program after we create the socket to associate a specific port number (say, 19157) to this UDP socket via the socket bind()  method: clientSocket.bind((’’, 19157))
If the application developer writing the code were implementing the server side of a “well-known protocol,” then the developer would have to assign the corresponding well-known port number.
Typically, the client side of the application lets the transport layer automatically (and transparently)assign the port number, whereas the server side of the application assigns a specific port number.
With port numbers assigned to UDP sockets, we can now precisely describe UDP multiplexing/demultiplexing.
Suppose a process in Host A, with UDP port 19157, wants to send a chunkof application data to a process with UDP port 46428 in Host B. The transport layer in Host A creates atransport-layer segment that includes the application data, the source port number (19157), thedestination port number (46428), and two other values (which will be discussed later, but areunimportant for the current discussion).
The transport layer then passes the resulting segment to thenetwork layer.
The network layer encapsulates the segment in an IP datagram and makes a best-effortattempt to deliver the segment to the receiving host.
If the segment arrives at the receiving Host B, the transport layer at the receiving host examines the destination port number in the segment (46428) and delivers the segment to its socket identified by port 46428.
Note that Host B could be running multipleprocesses, each with its own UDP socket and associated port number.
As UDP segments arrive fromthe network, Host B directs (demultiplexes) each segment to the appropriate socket by examining thesegment’s destination port number.
It is important to note that a UDP socket is fully identified by a two-tuple consisting of a destination IP address and a destination port number.
As a consequence, if two UDP segments have different source IP addresses and/or source port numbers, but have the same destination  IP address and destination port number, then the two segments will be directed to the same destination process via the same destination socket.
You may be wondering now, what is the purpose of the source port number?
As shown in Figure 3.4, in the A-to-B segment the source port number serves as part of a “return address”—when B wants to send a segment back to A, the destination port in the B-to-A segment will take its value from the source portvalue of the A-to-B segment. (
The complete return address is A’s IP address and the source port number.)
As an example, recall the UDP server program studied in Section 2.7.
In UDPServer.py , the server uses the recvfrom()  method to extract the client-side (source) port number from the segment it receives from the client; it then sends a new segment to the client, with the extracted source port number serving as the destination port number in this new segment.
Connection-Oriented Multiplexing and Demultiplexing In order to understand TCP demultiplexing, we have to take a close look at TCP sockets and TCP connection establishment.
One subtle difference between a TCP socket and a UDP socket is that a TCP
socket is identified by a four-tuple: (source IP address, source port number, destination IP address, destination port number).
Thus, when a TCP segment arrives from the network to a host, the host uses all four values to direct (demultiplex) the segment to the appropriate socket.
Figure 3.4 The inversion of source and destination port numbers In particular, and in contrast with UDP, two arriving TCP segments with different source IP addresses or source port numbers will (with the exception of a TCP segment carrying the original connection- establishment request) be directed to two different sockets.
To gain further insight, let’s reconsider the TCP client-server programming example in Section 2.7.2: The TCP server application has a “welcoming socket,” that waits for connection-establishment requests from TCP clients (see Figure 2.29) on port number 12000.
The TCP client creates a socket and sends a connection establishment request segment with the lines: clientSocket = socket(AF_INET, SOCK_STREAM)                clientSocket.connect((serverName,12000)) A connection-establishment request is nothing more than a TCP segment with destination port number 12000 and a special connection-establishment bit set in the TCP header (discussed in Section 3.5).
The segment also includes a source port number that was chosen by the client.
When the host operating system of the computer running the server process receives the incoming
connection-request segment with destination port 12000, it locates the server process that is waiting to accept a connection on port number 12000.
The server process then creates a new socket: connectionSocket, addr = serverSocket.accept() Also, the transport layer at the server notes the following four values in the connection-request segment: (1) the source port number in the segment, (2) the IP address of the source host, (3) the destination port number in the segment, and (4) its own IP address.
The newly created connectionsocket is identified by these four values; all subsequently arriving segments whose source port, source IP address, destination port, and destination IP address match these four values will be demultiplexed to this socket.
With the TCP connection now in place, the client and server can nowsend data to each other.
The server host may support many simultaneous TCP connection sockets, with each socket attached toa process, and with each socket identified by its own four-tuple.
When a TCP segment arrives at thehost, all four fields (source IP address, source port, destination IP address, destination port) are used todirect (demultiplex) the segment to the appropriate socket.
FOCUS ON SECURITY Port Scanning We’ve seen that a server process waits patiently on an open port for contact by a remote client.
Some ports are reserved for well-known applications (e.g., Web, FTP, DNS, and SMTP servers);other ports are used by convention by popular applications (e.g., the Microsoft 2000 SQL serverlistens for requests on UDP port 1434).
Thus, if we determine that a port is open on a host, wemay be able to map that port to a specific application running on the host.
This is very useful forsystem administrators, who are often interested in knowing which network applications arerunning on the hosts in their networks.
But attackers, in order to “case the joint,” also want to know which ports are open on target hosts.
If a host is found to be running an application with a known security flaw (e.g., a SQL server listening on port 1434 was subject to a buffer overflow,allowing a remote user to execute arbitrary code on the vulnerable host, a flaw exploited by the Slammer worm [CERT 2003–04] ), then that host is ripe for attack.
Determining which applications are listening on which ports is a relatively easy task.
Indeed there are a number of public domain programs, called port scanners, that do just that.
Perhaps the most widely used of these is nmap, freely available at http:/ /nmap.org and included in most Linux distributions.
For TCP, nmap sequentially scans ports, looking for ports that are accepting TCP connections.
For UDP, nmap again sequentially scans ports, looking for UDP ports that respond to transmitted UDP segments.
In both cases, nmap returns a list of open, closed, or unreachable ports.
A host running nmap can attempt to scan any target host anywhere  in the
Internet.
We’ll revisit nmap in Section 3.5.6, when we discuss TCP connection management.
Figure 3.5 Two clients, using the same destination port number (80) to communicate with the same Web server application The situation is illustrated in Figure 3.5, in which Host C initiates two HTTP sessions to server B, and Host A initiates one HTTP session to B. Hosts A and C and server B each have their own unique IP address—A, C, and B, respectively.
Host C assigns two different source port numbers (26145 and 7532)to its two HTTP connections.
Because Host A is choosing source port numbers independently of C, itmight also assign a source port of 26145 to its HTTP connection.
But this is not a problem—server B willstill be able to correctly demultiplex the two connections having the same source port number, since thetwo connections have different source IP addresses.
Web Servers and TCP Before closing this discussion, it’s instructive to say a few additional words about Web servers and how they use port numbers.
Consider a host running a Web server, such as an Apache Web server, on port 80.
When clients (for example, browsers) send segments to the server, all segments will have destination port 80.
In particular, both the initial connection-establishment segments and the segments carrying HTTP request messages will have destination port 80.
As we have just described, the serverdistinguishes the segments from the different clients using source IP addresses and source port
numbers.
Figure 3.5 shows a Web server that spawns a new process for each connection.
As shown in Figure 3.5, each of these processes has its own connection socket through which HTTP requests arrive and HTTP responses are sent.
We mention, however, that there is not always a one-to-one correspondence between connection sockets and processes.
In fact, today’s high-performing Web servers often use only one process, and create a new thread with a new connection socket for each new client connection. (
A thread can be viewed as a lightweight subprocess.)
If you did the first programming assignment in Chapter 2, you built a Web server that does just this.
For such a server, at any given time there may be many connection sockets (with different identifiers) attached to the same process.
If the client and server are using persistent HTTP, then throughout the duration of the persistent connection the client and server exchange HTTP messages via the same server socket.
However, if theclient and server use non-persistent HTTP, then a new TCP connection is created and closed for everyrequest/response, and hence a new socket is created and later closed for every request/response.
This frequent creating and closing of sockets can severely impact the performance of a busy Web server (although a number of operating system tricks can be used to mitigate the problem).
Readers interestedin the operating system issues surrounding persistent and non-persistent HTTP are encouraged to see [Nielsen 1997 ; Nahum 2002] .
Now that we’ve discussed transport-layer multiplexing and demultiplexing, let’s move on and discuss one of the Internet’s transport protocols, UDP.
In the next section we’ll see that UDP adds little more tothe network-layer protocol than a multiplexing/demultiplexing service.
3.3 Connectionless Transport: UDP In this section, we’ll take a close look at UDP, how it works, and what it does.
We encourage you to refer back to Section 2.1, which includes an overview of the UDP service model, and to Section 2.7.1, which discusses socket programming using UDP.
To motivate our discussion about UDP, suppose you were interested in designing a no-frills, bare-bones transport protocol.
How might you go about doing this?
You might first consider using a vacuous transport protocol.
In particular, on the sending side, you might consider taking the messages from theapplication process and passing them directly to the network layer; and on the receiving side, you mightconsider taking the messages arriving from the network layer and passing them directly to theapplication process.
But as we learned in the previous section, we have to do a little more than nothing!
At the very least, the transport layer has to provide a multiplexing/demultiplexing service in order to passdata between the network layer and the correct application-level process.
UDP, defined in [RFC 768], does just about as little as a transport protocol can do.
Aside from the multiplexing/demultiplexing function and some light error checking, it adds nothing to IP.
In fact, if the application developer chooses UDP instead of TCP, then the application is almost directly talking withIP.
UDP takes messages from the application process, attaches source and destination port numberfields for the multiplexing/demultiplexing service, adds two other small fields, and passes the resultingsegment to the network layer.
The network layer encapsulates the transport-layer segment into an IPdatagram and then makes a best-effort attempt to deliver the segment to the receiving host.
If thesegment arrives at the receiving host, UDP uses the destination port number to deliver the segment’sdata to the correct application process.
Note that with UDP there is no handshaking between sending and receiving transport-layer entities before sending a segment.
For this reason, UDP is said to be connectionless.
DNS is an example of an application-layer protocol that typically uses UDP.
When the DNS application in a host wants to make a query, it constructs a DNS query message and passes the message to UDP.Without performing any handshaking with the UDP entity running on the destination end system, thehost-side UDP adds header fields to the message and passes the resulting segment to the networklayer.
The network layer encapsulates the UDP segment into a datagram and sends the datagram to a name server.
The DNS application at the querying host then waits for a reply to its query.
If it doesn’t receive a reply (possibly because the underlying network lost the query or the reply), it might tryresending the query, try sending the query to another name server, or inform the invoking applicationthat it can’t get a reply.
Now you might be wondering why an application developer would ever choose to build an application over UDP rather than over TCP.
Isn’t TCP always preferable, since TCP provides a reliable datatransfer service, while UDP does not?
The answer is no, as some applications are better suited for UDPfor the following reasons: Finer application-level control over what data is sent, and when.
 Under UDP, as soon as an application process passes data to UDP, UDP will package the data inside a UDP segment andimmediately pass the segment to the network layer.
TCP, on the other hand, has a congestion-control mechanism that throttles the transport-layer TCP sender when one or more links between the source and destination hosts become excessively congested.
TCP will also continue to resend a segment until the receipt of the segment has been acknowledged by the destination, regardless ofhow long reliable delivery takes.
Since real-time applications often require a minimum sending rate,do not want to overly delay segment transmission, and can tolerate some data loss, TCP’s servicemodel is not particularly well matched to these applications’ needs.
As discussed below, theseapplications can use UDP and implement, as part of the application, any additional functionality thatis needed beyond UDP’s no-frills segment-delivery service.
No connection establishment.
 As we’ll discuss later, TCP uses a three-way handshake before it starts to transfer data.
UDP just blasts away without any formal preliminaries.
Thus UDP does notintroduce any delay to establish a connection.
This is probably the principal reason why DNS runsover UDP rather than TCP—DNS would be much slower if it ran over TCP.
HTTP uses TCP rather than UDP, since reliability is critical for Web pages with text.
But, as we briefly discussed in Section 2.2, the TCP connection-establishment delay in HTTP is an important contributor to the delays associated with downloading Web documents.
Indeed, the QUIC protocol (Quick UDP Internet Connection, [Iyengar 2015] ), used in Google’s Chrome browser, uses UDP as its underlying transport protocol and implements reliability in an application-layer protocol on top of UDP.
No connection state.
 TCP maintains connection state in the end systems.
This connection state includes receive and send buffers, congestion-control parameters, and sequence andacknowledgment number parameters.
We will see in Section 3.5 that this state information is needed to implement TCP’s reliable data transfer service and to provide congestion control.
UDP, on the other hand, does not maintain connection state and does not track any of these parameters.
For this reason, a server devoted to a particular application can typically support many more activeclients when the application runs over UDP rather than TCP.
Small packet header overhead.
The TCP segment has 20 bytes of header overhead in every segment, whereas UDP has only 8 bytes of overhead.
Figure 3.6 lists popular Internet applications and the transport protocols that they use.
As we expect, e- mail, remote terminal access, the Web, and file transfer run over TCP—all these applications need the reliable data transfer service of TCP.
Nevertheless, many important applications run over UDP rather than TCP.
For example, UDP is used to carry network management (SNMP; see Section 5.7) data.
UDP is preferred to TCP in this case, since network management applications must often run when the
network is in a stressed state—precisely when reliable, congestion-controlled data transfer is difficult to achieve.
Also, as we mentioned earlier, DNS runs over UDP, thereby avoiding TCP’s connection- establishment delays.
As shown in Figure 3.6, both UDP and TCP are somtimes used today with multimedia applications, such as Internet phone, real-time video conferencing, and streaming of stored audio and video.
We’ll take a close look at these applications in Chapter 9.
We just mention now that all of these applications can tolerate a small amount of packet loss, so that reliable data transfer is not absolutely critical for the application’s success.
Furthermore, real-time applications, like Internet phone and video conferencing,react very poorly to TCP’s congestion control.
For these reasons, developers of multimedia applicationsmay choose to run their applications over UDP instead of TCP.
When packet loss rates are low, and with some organizations blocking UDP traffic for security reasons (see Chapter 8), TCP becomes an increasingly attractive protocol for streaming media transport.
Figure 3.6 Popular Internet applications and their underlying transport protocols Although commonly done today, running multimedia applications over UDP is controversial.
As we mentioned above, UDP has no congestion control.
But congestion control is needed to prevent the network from entering a congested state in which very little useful work is done.
If everyone were to startstreaming high-bit-rate video without using any congestion control, there would be so much packetoverflow at routers that very few UDP packets would successfully traverse the source-to-destinationpath.
Moreover, the high loss rates induced by the uncontrolled UDP senders would cause the TCP senders (which, as we’ll see, do decrease their sending rates in the face of congestion) to dramatically decrease their rates.
Thus, the lack of congestion control in UDP can result in high loss rates between aUDP sender and receiver, and the crowding out of TCP sessions—a potentially serious problem [Floyd
1999] .
Many researchers have proposed new mechanisms to force all sources, including UDP sources, to perform adaptive congestion control [Mahdavi 1997; Floyd 2000; Kohler 2006: RFC 4340] .
Before discussing the UDP segment structure, we mention that it is possible for an application to have reliable data transfer when using UDP.
This can be done if reliability is built into the application itself (for example, by adding acknowledgment and retransmission mechanisms, such as those we’ll study in the next section).
We mentioned earlier that the QUIC protocol [Iyengar 2015]  used in Google’s Chrome browser implements reliability in an application-layer protocol on top of UDP.
But this is a nontrivial task that would keep an application developer busy debugging for a long time.
Nevertheless, buildingreliability directly into the application allows the application to “have its cake and eat it too.
That is, application processes can communicate reliably without being subjected to the transmission-rateconstraints imposed by TCP’s congestion-control mechanism.
3.3.1 UDP Segment Structure The UDP segment structure, shown in Figure 3.7, is defined in RFC 768.
The application data occupies the data field of the UDP segment.
For example, for DNS, the data field contains either a querymessage or a response message.
For a streaming audio application, audio samples fill the data field.
The UDP header has only four fields, each consisting of two bytes.
As discussed in the previous section, the port numbers allow the destination host to pass the application data to the correct process running on the destination end system (that is, to perform the demultiplexing function).
The length field specifiesthe number of bytes in the UDP segment (header plus data).
An explicit length value is needed since thesize of the data field may differ from one UDP segment to the next.
The checksum is used by thereceiving host to check whether errors have been introduced into the segment.
In truth, the checksum isalso calculated over a few of the fields in the IP header in addition to the UDP segment.
But we ignorethis detail in order to see the forest through the trees.
We’ll discuss the checksum calculation below.
Basic principles of error detection are described in Section 6.2.
The length field specifies the length of the UDP segment, including the header, in bytes.
3.3.2 UDP Checksum The UDP checksum provides for error detection.
That is, the checksum is used to determine whether bits within the UDP segment have been altered (for example, by noise in the links or while stored in arouter) as it moved from source to destination.
Figure 3.7 UDP segment structure UDP at the sender side performs the 1s complement of the sum of all the 16-bit words in the segment, with any overflow encountered during the sum being wrapped around.
This result is put in the checksum field of the UDP segment.
Here we give a simple example of the checksum calculation.
You can finddetails about efficient implementation of the calculation in RFC 1071 and performance over real data in [Stone 1998; Stone 2000].
As an example, suppose that we have the following three 16-bit words: 0110011001100000 01010101010101011000111100001100 The sum of first two of these 16-bit words is 0110011001100000 0101010101010101 1011101110110101 Adding the third word to the above sum gives1011101110110101 1000111100001100 0100101011000010 Note that this last addition had overflow, which was wrapped around.
The 1s complement is obtained by converting all the 0s to 1s and converting all the 1s to 0s.
Thus the 1s complement of the sum0100101011000010 is 1011010100111101, which becomes the checksum.
At the receiver, all four 16-
bit words are added, including the checksum.
If no errors are introduced into the packet, then clearly the sum at the receiver will be 1111111111111111.
If one of the bits is a 0, then we know that errors have been introduced into the packet.
You may wonder why UDP provides a checksum in the first place, as many link-layer protocols (including the popular Ethernet protocol) also provide error checking.
The reason is that there is no guarantee that all the links between source and destination provide error checking; that is, one of the links may use a link-layer protocol that does not provide error checking.
Furthermore, even if segmentsare correctly transferred across a link, it’s possible that bit errors could be introduced when a segment isstored in a router’s memory.
Given that neither link-by-link reliability nor in-memory error detection is guaranteed, UDP must provide error detection at the transport layer, on an end-end basis , if the end- end data transfer service is to provide error detection.
This is an example of the celebrated end-end principle  in system design [Saltzer 1984] , which states that since certain functionality (error detection, in this case) must be implemented on an end-end basis: “functions placed at the lower levels may be redundant or of little value when compared to the cost of providing them at the higher level.”
Because IP is supposed to run over just about any layer-2 protocol, it is useful for the transport layer to provide error checking as a safety measure.
Although UDP provides error checking, it does not doanything to recover from an error.
Some implementations of UDP simply discard the damaged segment;others pass the damaged segment to the application with a warning.
That wraps up our discussion of UDP.
We will soon see that TCP offers reliable data transfer to its applications as well as other services that UDP doesn’t offer.
Naturally, TCP is also more complex than UDP.
Before discussing TCP, however, it will be useful to step back and first discuss the underlying principles of reliable data transfer.
3.4 Principles of Reliable Data Transfer In this section, we consider the problem of reliable data transfer in a general context.
This is appropriate since the problem of implementing reliable data transfer occurs not only at the transport layer, but alsoat the link layer and the application layer as well.
The general problem is thus of central importance tonetworking.
Indeed, if one had to identify a “top-ten” list of fundamentally important problems in all ofnetworking, this would be a candidate to lead the list.
In the next section we’ll examine TCP and show, in particular, that TCP exploits many of the principles that we are about to describe.
Figure 3.8 illustrates the framework for our study of reliable data transfer.
The service abstraction provided to the upper-layer entities is that of a reliable channel through which data can be transferred.
With a reliable channel, no transferred data bits are corrupted (flipped from 0 to 1, or vice versa) or lost,and all are delivered in the order in which they were sent.
This is precisely the service model offered byTCP to the Internet applications that invoke it.
It is the responsibility of a reliable data transfer protocol to implement this service abstraction.
This task is made difficult by the fact that the layer below  the reliable data transfer protocol may be unreliable.
For example, TCP is a reliable data transfer protocol that is implemented on top of an unreliable (IP) end-to-end network layer.
More generally, the layer beneath the two reliablycommunicating end points might consist of a single physical link (as in the case of a link-level datatransfer protocol) or a global internetwork (as in the case of a transport-level protocol).
For ourpurposes, however, we can view this lower layer simply as an unreliable point-to-point channel.
In this section, we will incrementally develop the sender and receiver sides of a reliable data transfer protocol, considering increasingly complex models of the underlying channel.
For example, we’ll consider what protocol mechanisms are
Figure 3.8 Reliable data transfer: Service model and service implementation
needed when the underlying channel can corrupt bits or lose entire packets.
One assumption we’ll adopt throughout our discussion here is that packets will be delivered in the order in which they were sent, with some packets possibly being lost; that is, the underlying channel will not reorder packets.
Figure 3.8(b) illustrates the interfaces for our data transfer protocol.
The sending side of the data transfer protocol willbe invoked from above by a call to rdt_send() .
It will pass the data to be delivered to the upper layer at the receiving side. (
Here rdt stands for reliable data transfer  protocol and _send  indicates that the sending side of rdt is being called.
The first step in developing any protocol is to choose a good name!)
On the receiving side, rdt_rcv()  will be called when a packet arrives from the receiving side of the channel.
When the rdt protocol wants to deliver data to the upper layer, it will do so by calling deliver_data() .
In the following we use the terminology “packet” rather than transport-layer “segment.”
Because the theory developed in this section applies to computer networks in general and not just to the Internet transport layer, the generic term “packet” is perhaps more appropriate here.
In this section we consider only the case of unidirectional data transfer, that is, data transfer from the sending to the receiving side.
The case of reliable bidirectional  (that is, full-duplex) data transfer  is conceptually no more difficult but considerably more tedious to explain.
Although we consider only unidirectional data transfer, it is important to note that the sending and receiving sides of our protocol will nonetheless need to transmit packets in both directions, as indicated in Figure 3.8.
We will see shortly that, in addition to exchanging packets containing the data to be transferred, the sending andreceiving sides of rdt will also need to exchange control packets back and forth.
Both the send and receive sides of rdt send packets to the other side by a call to udt_send()  (where udt stands for unreliable data transfer ).
3.4.1 Building a Reliable Data Transfer Protocol We now step through a series of protocols, each one becoming more complex, arriving at a flawless, reliable data transfer protocol.
Reliable Data Transfer over a Perfectly Reliable Channel: rdt1.0 We first consider the simplest case, in which the underlying channel is completely reliable.
The protocol itself, which we’ll call rdt1.0 , is trivial.
The finite-state machine (FSM)  definitions for the rdt1.0 sender and receiver are shown in Figure 3.9.
The FSM in Figure 3.9(a) defines the operation of the sender, while the FSM in Figure 3.9(b) defines the operation of the receiver.
It is important to note that there are separate  FSMs for the sender and for the receiver.
The sender and receiver FSMs in Figure 3.9 each have just one state.
The arrows in the FSM description indicate the transition of the protocol from one state to another. (
Since each FSM in Figure 3.9 has just one state, a transition is necessarily from the one state back to itself; we’ll see more complicated state diagrams shortly.)
The event causing
the transition is shown above the horizontal line labeling the transition, and the actions taken when the event occurs are shown below the horizontal line.
When no action is taken on an event, or no event occurs and an action is taken, we’ll use the symbol Λ below or above the horizontal, respectively, to explicitly denote the lack of an action or event.
The initial state of the FSM is indicated by the dashed arrow.
Although the FSMs in Figure 3.9 have but one state, the FSMs we will see shortly have multiple states, so it will be important to identify the initial state of each FSM.
The sending side of rdt simply accepts data from the upper layer via the rdt_send(data)  event, creates a packet containing the data (via the action make_pkt(data) ) and sends the packet into the channel.
In practice, the rdt_send(data)  event would result from a procedure call (for example, to rdt_send() ) by the upper-layer application.
Figure 3.9 rdt1.0  – A protocol for a completely reliable channel On the receiving side, rdt receives a packet from the underlying channel via the rdt_rcv(packet) event, removes the data from the packet (via the action extract (packet, data) ) and passes the data up to the upper layer (via the action deliver_data(data) ).
In practice, the rdt_rcv(packet)  event would result from a procedure call (for example, to rdt_rcv() ) from the lower-layer protocol.
In this simple protocol, there is no difference between a unit of data and a packet.
Also, all packet flow is from the sender to receiver; with a perfectly reliable channel there is no need for the receiver side to provide any feedback to the sender since nothing can go wrong!
Note that we have also assumed that
the receiver is able to receive data as fast as the sender happens to send data.
Thus, there is no need for the receiver to ask the sender to slow down!
Reliable Data Transfer over a Channel with Bit Errors: rdt2.0 A more realistic model of the underlying channel is one in which bits in a packet may be corrupted.
Such bit errors typically occur in the physical components of a network as a packet is transmitted, propagates, or is buffered.
We’ll continue to assume for the moment that all transmitted packets are received(although their bits may be corrupted) in the order in which they were sent.
Before developing a protocol for reliably communicating over such a channel, first consider how people might deal with such a situation.
Consider how you yourself might dictate a long message over the phone.
In a typical scenario, the message taker might say “OK” after each sentence has been heard,understood, and recorded.
If the message taker hears a garbled sentence, you’re asked to repeat thegarbled sentence.
This message-dictation protocol uses both positive acknowledgments (“OK”) and negative acknowledgments (“Please repeat that.”).
These control messages allow the receiver to letthe sender know what has been received correctly, and what has been received in error and thusrequires repeating.
In a computer network setting, reliable data transfer protocols based on suchretransmission are known as ARQ (Automatic Repeat reQuest) protocols.
Fundamentally, three additional protocol capabilities are required in ARQ protocols to handle thepresence of bit errors: Error detection.
First, a mechanism is needed to allow the receiver to detect when bit errors have occurred.
Recall from the previous section that UDP uses the Internet checksum field for exactly this purpose.
In Chapter 6 we’ll examine error-detection and -correction techniques in greater detail; these techniques allow the receiver to detect and possibly correct packet bit errors.
For now, we need only know that these techniques require that extra bits (beyond the bits of original data to be transferred) be sent from the sender to the receiver; these bits will be gathered into the packet checksum field of the rdt2.0  data packet.
Receiver feedback.
 Since the sender and receiver are typically executing on different end systems, possibly separated by thousands of miles, the only way for the sender to learn of the receiver’s view of the world (in this case, whether or not a packet was received correctly) is for the receiver toprovide explicit feedback to the sender.
The positive (ACK) and negative (NAK) acknowledgment replies in the message-dictation scenario are examples of such feedback.
Our rdt2.0  protocol will similarly send ACK and NAK packets back from the receiver to the sender.
In principle, these packets need only be one bit long; for example, a 0 value could indicate a NAK and a value of 1could indicate an ACK.
Retransmission.
A packet that is received in error at the receiver will be retransmitted by the sender.
Figure 3.10 shows the FSM representation of rdt2.0 , a data transfer protocol employing error detection, positive acknowledgments, and negative acknowledgments.
The send side of rdt2.0  has two states.
In the leftmost state, the send-side protocol is waiting for data to be passed down from the upper layer.
When the rdt_send(data)  event occurs, the sender will create a packet ( sndpkt ) containing the data to be sent, along with a packet checksum (for example, as discussed in Section 3.3.2 for the case of a UDP segment), and then send the packet via the udt_send(sndpkt)  operation.
In the rightmost state, the sender protocol is waiting for an ACK or a NAK packet from the receiver.
If an ACK packet is received Figure 3.10 rdt2.0  – A protocol for a channel with bit errors (the notation rdt_rcv(rcvpkt) && isACK (rcvpkt)  in Figure 3.10 corresponds to this event), the sender knows that the most recently transmitted packet has been received correctly and thus the protocol returns to the state of waiting for data from the upper layer.
If a NAK is received, the protocol retransmits the last packet and waits for an ACK or NAK to be returned by the receiver in response to
the retransmitted data packet.
It is important to note that when the sender is in the wait-for-ACK-or-NAK state, it cannot get more data from the upper layer; that is, the rdt_send()  event can not occur; that will happen only after the sender receives an ACK and leaves this state.
Thus, the sender will not send a new piece of data until it is sure that the receiver has correctly received the current packet.
Because of this behavior, protocols such as rdt2.0  are known as stop-and-wait  protocols.
The receiver-side FSM for rdt2.0  still has a single state.
On packet arrival, the receiver replies with either an ACK or a NAK, depending on whether or not the received packet is corrupted.
In Figure 3.10, the notation rdt_rcv(rcvpkt) && corrupt(rcvpkt)  corresponds to the event in which a packet is received and is found to be in error.
Protocol rdt2.0  may look as if it works but, unfortunately, it has a fatal flaw.
In particular, we haven’t accounted for the possibility that the ACK or NAK packet could be corrupted! (
Before proceeding on, you should think about how this problem may be fixed.)
Unfortunately, our slight oversight is not asinnocuous as it may seem.
Minimally, we will need to add checksum bits to ACK/NAK packets in orderto detect such errors.
The more difficult question is how the protocol should recover from errors in ACKor NAK packets.
The difficulty here is that if an ACK or NAK is corrupted, the sender has no way ofknowing whether or not the receiver has correctly received the last piece of transmitted data.
Consider three possibilities for handling corrupted ACKs or NAKs: For the first possibility, consider what a human might do in the message-dictation scenario.
If the speaker didn’t understand the “OK” or “Please repeat that” reply from the receiver, the speaker would probably ask, “What did you say?” (
thus introducing a new type of sender-to-receiver packetto our protocol).
The receiver would then repeat the reply.
But what if the speaker’s “What did you say?”
is corrupted?
The receiver, having no idea whether the garbled sentence was part of the dictation or a request to repeat the last reply, would probably then respond with “What did you say?”
And then, of course, that response might be garbled.
Clearly, we’re heading down a difficult path.
A second alternative is to add enough checksum bits to allow the sender not only to detect, but also to recover from, bit errors.
This solves the immediate problem for a channel that can corrupt packets but not lose them.
A third approach is for the sender simply to resend the current data packet when it receives a garbled ACK or NAK packet.
This approach, however, introduces duplicate packets into the sender-to-receiver channel.
The fundamental difficulty with duplicate packets is that the receiver doesn’t know whether the ACK or NAK it last sent was received correctly at the sender.
Thus, it cannot know a priori  whether an arriving packet contains new data or is a retransmission!
A simple solution to this new problem (and one adopted in almost all existing data transfer protocols, including TCP) is to add a new field to the data packet and have the sender number its data packets byputting a sequence number into this field.
The receiver then need only check this sequence number to
determine whether or not the received packet is a retransmission.
For this simple case of a stop-and- wait protocol, a 1-bit sequence number will suffice, since it will allow the receiver to know whether the sender is resending the previously transmitted packet (the sequence number of the received packet has the same sequence number as the most recently received packet) or a new packet (the sequencenumber changes, moving “forward” in modulo-2 arithmetic).
Since we are currently assuming a channelthat does not lose packets, ACK and NAK packets do not themselves need to indicate the sequence number of the packet they are acknowledging.
The sender knows that a received ACK or NAK packet (whether garbled or not) was generated in response to its most recently transmitted data packet.
Figures 3.11 and 3.12 show the FSM description for rdt2.1 , our fixed version of rdt2.0 .
The rdt2.1  sender and receiver FSMs each now have twice as many states as before.
This is because the protocol state must now reflect whether the packet currently being sent (by the sender) or expected (at the receiver) should have a sequence number of 0 or 1.
Note that the actions in those states where a 0-numbered packet is being sent or expected are mirror images of those where a 1-numbered packet isbeing sent or expected; the only differences have to do with the handling of the sequence number.
Protocol rdt2.1  uses both positive and negative acknowledgments from the receiver to the sender.
When an out-of-order packet is received, the receiver sends a positive acknowledgment for the packet it has received.
When a corrupted packet Figure 3.11 rdt2.1  sender
Figure 3.12 rdt2.1  receiver is received, the receiver sends a negative acknowledgment.
We can accomplish the same effect as a NAK if, instead of sending a NAK, we send an ACK for the last correctly received packet.
A sender that receives two ACKs for the same packet (that is, receives duplicate ACKs) knows that the receiver did not correctly receive the packet following the packet that is being ACKed twice.
Our NAK-free reliable data transfer protocol for a channel with bit errors is rdt2.2 , shown in Figures 3.13 and 3.14.
One subtle change between rtdt2.1  and rdt2.2  is that the receiver must now include the sequence number of the packet being acknowledged by an ACK message (this is done by including the ACK, 0 or ACK, 1 argument in make_pkt()  in the receiver FSM), and the sender must now check the sequence number of the packet being acknowledged by a received ACK message (this is done byincluding the 0 or 1 argument in isACK()  in the sender FSM).
Reliable Data Transfer over a Lossy Channel with Bit Errors: rdt3.0 Suppose now that in addition to corrupting bits, the underlying channel can lose packets as well, a not- uncommon event in today’s computer networks (including the Internet).
Two additional concerns must now be addressed by the protocol: how to detect packet loss and what to do when packet loss occurs.
The use of checksumming, sequence numbers, ACK packets, and retransmissions—the techniques
Figure 3.13 rdt2.2  sender already developed in rdt2.2 —will allow us to answer the latter concern.
Handling the first concern will require adding a new protocol mechanism.
There are many possible approaches toward dealing with packet loss (several more of which are explored in the exercises at the end of the chapter).
Here, we’ll put the burden of detecting andrecovering from lost packets on the sender.
Suppose that the sender transmits a data packet and eitherthat packet, or the receiver’s ACK of that packet, gets lost.
In either case, no reply is forthcoming at the sender from the receiver.
If the sender is willing to wait long enough so that it is certain that a packet has been lost, it can simply retransmit the data packet.
You should convince yourself that this protocol does indeed work.
But how long must the sender wait to be certain that something has been lost?
The sender must clearly wait at least as long as a round-trip delay between the sender and receiver (which may include bufferingat intermediate routers) plus whatever amount of time is needed to process a packet at the receiver.
Inmany networks, this worst-case maximum delay is very difficult even to estimate, much less know withcertainty.
Moreover, the protocol should ideally recover from packet loss as soon as possible; waiting for a worst-case delay could mean a long wait until error recovery
Figure 3.14 rdt2.2  receiver is initiated.
The approach thus adopted in practice is for the sender to judiciously choose a time value such that packet loss is likely, although not guaranteed, to have happened.
If an ACK is not received within this time, the packet is retransmitted.
Note that if a packet experiences a particularly large delay,the sender may retransmit the packet even though neither the data packet nor its ACK have been lost.
This introduces the possibility of duplicate data packets in the sender-to-receiver channel.
Happily, protocol rdt2.2  already has enough functionality (that is, sequence numbers) to handle the case of duplicate packets.
From the sender’s viewpoint, retransmission is a panacea.
The sender does not know whether a data packet was lost, an ACK was lost, or if the packet or ACK was simply overly delayed.
In all cases, theaction is the same: retransmit.
Implementing a time-based retransmission mechanism requires acountdown timer  that can interrupt the sender after a given amount of time has expired.
The sender will thus need to be able to (1) start the timer each time a packet (either a first-time packet or aretransmission) is sent, (2) respond to a timer interrupt (taking appropriate actions), and (3) stop thetimer.
Figure 3.15 shows the sender FSM for rdt3.0 , a protocol that reliably transfers data over a channel that can corrupt or lose packets; in the homework problems, you’ll be asked to provide the receiver FSM for rdt3.0 .
Figure 3.16 shows how the protocol operates with no lost or delayed packets and how it handles lost data packets.
In Figure 3.16, time moves forward from the top of the diagram toward the bottom of the
Figure 3.15 rdt3.0  sender diagram; note that a receive time for a packet is necessarily later than the send time for a packet as a result of transmission and propagation delays.
In Figures 3.16(b)–(d), the send-side brackets indicate the times at which a timer is set and later times out.
Several of the more subtle aspects of this protocol are explored in the exercises at the end of this chapter.
Because packet sequence numbers alternate between 0 and 1, protocol rdt3.0  is sometimes known as the alternating-bit protocol .
We have now assembled the key elements of a data transfer protocol.
Checksums, sequence numbers, timers, and positive and negative acknowledgment packets each play a crucial and necessary role in the operation of the protocol.
We now have a working reliable data transfer protocol!
Developing a protocol and FSM representation for a simple application-layer protocol
3.4.2 Pipelined Reliable Data Transfer Protocols Protocol rdt3.0  is a functionally correct protocol, but it is unlikely that anyone would be happy with its performance, particularly in today’s high-speed networks.
At the heart of rdt3.0 ’s performance problem is the fact that it is a stop-and-wait protocol.
Figure 3.16 Operation of rdt3.0 , the alternating-bit protocol
Figure 3.17 Stop-and-wait versus pipelined protocol To appreciate the performance impact of this stop-and-wait behavior, consider an idealized case of two hosts, one located on the West Coast of the United States and the other located on the East Coast, as shown in Figure 3.17.
The speed-of-light round-trip propagation delay between these two end systems, RTT, is approximately 30 milliseconds.
Suppose that they are connected by a channel with atransmission rate, R, of 1 Gbps (10  bits per second).
With a packet size, L, of 1,000 bytes (8,000 bits) per packet, including both header fields and data, the time needed to actually transmit the packet into the 1 Gbps link is Figure 3.18(a) shows that with our stop-and-wait protocol, if the sender begins sending the packet at  then at  microseconds, the last bit enters the channel at the sender side.
The packet then makes its 15-msec cross-country journey, with the last bit of the packet emerging at the receiver at  15.008 msec.
Assuming for simplicity that ACK packets are extremely small (so that we can ignore their transmission time) and that the receiver can send an ACK as soon as the last bit of a data packet is received, the ACK emerges back at the sender at  At this point, the sender can now transmit the next message.
Thus, in 30.008 msec, the sender was sending for only0.008 msec.
If we define the utilization  of the sender (or the channel) as the fraction of time the sender is actually busy sending bits into the channel, the analysis in Figure 3.18(a) shows that the stop-and- wait protocol has a rather dismal sender utilization, U , of9 dtrans=LR=8000  bits/packet109 bits/sec=8 microseconds t=0, t=L/R=8 t=RTT/2+L/R= t=RTT+L/R=30.008  msec.
sender Usender =L/RRTT+L/R =.00830.008 =0.00027
Figure 3.18 Stop-and-wait and pipelined sending That is, the sender was busy only 2.7 hundredths of one percent of the time!
Viewed another way, the sender was able to send only 1,000 bytes in 30.008 milliseconds, an effective throughput of only 267kbps—even though a 1 Gbps link was available!
Imagine the unhappy network manager who just paid afortune for a gigabit capacity link but manages to get a throughput of only 267 kilobits per second!
Thisis a graphic example of how network protocols can limit the capabilities provided by the underlying network hardware.
Also, we have neglected lower-layer protocol-processing times at the sender and receiver, as well as the processing and queuing delays that would occur at any intermediate routers
between the sender and receiver.
Including these effects would serve only to further increase the delay and further accentuate the poor performance.
The solution to this particular performance problem is simple: Rather than operate in a stop-and-wait manner, the sender is allowed to send multiple packets without waiting for acknowledgments, as illustrated in Figure 3.17(b).
Figure 3.18(b) shows that if the sender is allowed to transmit three packets before having to wait for acknowledgments, the utilization of the sender is essentially tripled.
Since the many in-transit sender-to-receiver packets can be visualized as filling a pipeline, this technique is knownas pipelining .
Pipelining has the following consequences for reliable data transfer protocols: The range of sequence numbers must be increased, since each in-transit packet (not counting retransmissions) must have a unique sequence number and there may be multiple, in-transit, unacknowledged packets.
The sender and receiver sides of the protocols may have to buffer more than one packet.
Minimally, the sender will have to buffer packets that have been transmitted but not yet acknowledged.
Buffering of correctly received packets may also be needed at the receiver, as discussed below.
The range of sequence numbers needed and the buffering requirements will depend on the manner in which a data transfer protocol responds to lost, corrupted, and overly delayed packets.
Two basic approaches toward pipelined error recovery can be identified: Go-Back-N and selective repeat .
3.4.3 Go-Back-N (GBN) In a Go-Back-N (GBN) protocol, the sender is allowed to transmit multiple packets (when available)without waiting for an acknowledgment, but is constrained to have no more than some maximum allowable number, N, of unacknowledged packets in the pipeline.
We describe the GBN protocol in some detail in this section.
But before reading on, you are encouraged to play with the GBN applet (an awesome applet!)
at the companion Web site.
Figure 3.19 shows the sender’s view of the range of sequence numbers in a GBN protocol.
If we define base  to be the sequence number of the oldest unacknowledged Figure 3.19 Sender’s view of sequence numbers in Go-Back-N
packet and nextseqnum  to be the smallest unused sequence number (that is, the sequence number of the next packet to be sent), then four intervals in the range of sequence numbers can be identified.
Sequence numbers in the interval [ 0, base-1 ] correspond to packets that have already been transmitted and acknowledged.
The interval [base, nextseqnum-1]  corresponds to packets that have been sent but not yet acknowledged.
Sequence numbers in the interval [nextseqnum, base+N-1]  can be used for packets that can be sent immediately, should data arrive from the upper layer.
Finally, sequence numbers greater than or equal to base+N  cannot be used until an unacknowledged packet currently in the pipeline (specifically, the packet with sequence number base ) has been acknowledged.
As suggested by Figure 3.19, the range of permissible sequence numbers for transmitted but not yet acknowledged packets can be viewed as a window of size N over the range of sequence numbers.
As the protocol operates, this window slides forward over the sequence number space.
For this reason, N is often referred to as the window size  and the GBN protocol itself as a sliding-window protocol .
You might be wondering why we would even limit the number of outstanding, unacknowledged packets to a value of N in the first place.
Why not allow an unlimited number of such packets?
We’ll see in Section 3.5 that flow control is one reason to impose a limit on the sender.
We’ll examine another reason to do so in Section 3.7, when we study TCP congestion control.
In practice, a packet’s sequence number is carried in a fixed-length field in the packet header.
If k is the number of bits in the packet sequence number field, the range of sequence numbers is thus  With a finite range of sequence numbers, all arithmetic involving sequence numbers must then be done using modulo 2  arithmetic. (
That is, the sequence number space can be thought of as a ring of size 2 , where sequence number  is immediately followed by sequence number 0.)
Recall that rdt3.0 had a 1-bit sequence number and a range of sequence numbers of [0,1].
Several of the problems at the end of this chapter explore the consequences of a finite range of sequence numbers.
We will see in Section 3.5 that TCP has a 32-bit sequence number field, where TCP sequence numbers count bytes in the byte stream rather than packets.
Figures 3.20 and 3.21 give an extended FSM description of the sender and receiver sides of an ACK- based, NAK-free, GBN protocol.
We refer to this FSM[0,2k−1].
k k 2k−1
Figure 3.20 Extended FSM description of the GBN sender Figure 3.21 Extended FSM description of the GBN receiver description as an extended FSM because we have added variables (similar to programming-language variables) for base  and nextseqnum , and added operations on these variables and conditional actions involving these variables.
Note that the extended FSM specification is now beginning to look somewhat like a programming-language specification. [
Bochman 1984]  provides an excellent survey of
additional extensions to FSM techniques as well as other programming-language-based techniques for specifying protocols.
The GBN sender must respond to three types of events: Invocation from above.
 When rdt_send()  is called from above, the sender first checks to see if the window is full, that is, whether there are N outstanding, unacknowledged packets.
If the window is not full, a packet is created and sent, and variables are appropriately updated.
If the window is full, the sender simply returns the data back to the upper layer, an implicit indication that the window isfull.
The upper layer would presumably then have to try again later.
In a real implementation, thesender would more likely have either buffered (but not immediately sent) this data, or would have asynchronization mechanism (for example, a semaphore or a flag) that would allow the upper layer to call rdt_send()  only when the window is not full.
Receipt of an ACK.
In our GBN protocol, an acknowledgment for a packet with sequence number n will be taken to be a cumulative acknowledgment, indicating that all packets with a sequence number up to and including n have been correctly received at the receiver.
We’ll come back to this issue shortly when we examine the receiver side of GBN.
A timeout event.
The protocol’s name, “Go-Back-N,” is derived from the sender’s behavior in the presence of lost or overly delayed packets.
As in the stop-and-wait protocol, a timer will again be used to recover from lost data or acknowledgment packets.
If a timeout occurs, the sender resends all packets that have been previously sent but that have not yet been acknowledged.
Our sender in Figure 3.20 uses only a single timer, which can be thought of as a timer for the oldest transmitted but not yet acknowledged packet.
If an ACK is received but there are still additional transmitted but not yet acknowledged packets, the timer is restarted.
If there are no outstanding, unacknowledgedpackets, the timer is stopped.
The receiver’s actions in GBN are also simple.
If a packet with sequence number n is received correctly and is in order (that is, the data last delivered to the upper layer came from a packet with sequence number ), the receiver sends an ACK for packet n and delivers the data portion of the packet to the upper layer.
In all other cases, the receiver discards the packet and resends an ACK for the most recently received in-order packet.
Note that since packets are delivered one at a time to the upper layer, if packet k has been received and delivered, then all packets with a sequence number lower than k have also been delivered.
Thus, the use of cumulative acknowledgments is a natural choice for GBN.
In our GBN protocol, the receiver discards out-of-order packets.
Although it may seem silly and wasteful to discard a correctly received (but out-of-order) packet, there is some justification for doing so.
Recall that the receiver must deliver data in order to the upper layer.
Suppose now that packet n is expected, but packet  arrives.
Because data must be delivered in order, the receiver could  buffer (save) packet  and then deliver this packet to the upper layer after it had later received and delivered packet n. However, if packet n is lost, both it and packet  will eventually be retransmitted as a result of then−1 n+1 n+1 n+1
GBN retransmission rule at the sender.
Thus, the receiver can simply discard packet .
The advantage of this approach is the simplicity of receiver buffering—the receiver need not buffer any out- of-order packets.
Thus, while the sender must maintain the upper and lower bounds of its window and the position of nextseqnum  within this window, the only piece of information the receiver need maintain is the sequence number of the next in-order packet.
This value is held in the variable expectedseqnum , shown in the receiver FSM in Figure 3.21.
Of course, the disadvantage of throwing away a correctly received packet is that the subsequent retransmission of that packet might be lost or garbled and thus even more retransmissions would be required.
Figure 3.22 shows the operation of the GBN protocol for the case of a window size of four packets.
Because of this window size limitation, the sender sends packets 0 through 3 but then must wait for one or more of these packets to be acknowledged before proceeding.
As each successive ACK (for example, ACK0  and ACK1 ) is received, the window slides forward and the sender can transmit one new packet (pkt4 and pkt5, respectively).
On the receiver side, packet 2 is lost and thus packets 3, 4, and 5 are found to be out of order and are discarded.
Before closing our discussion of GBN, it is worth noting that an implementation of this protocol in a protocol stack would likely have a structure similar to that of the extended FSM in Figure 3.20.
The implementation would also likely be in the form of various procedures that implement the actions to be taken in response to the various events that can occur.
In such event-based programming, the various procedures are called (invoked) either by other procedures in the protocol stack, or as the result of aninterrupt.
In the sender, these events would be (1) a call from the upper-layer entity to invoke rdt_send() , (2) a timer interrupt, and (3) a call from the lower layer to invoke rdt_rcv()  when a packet arrives.
The programming exercises at the end of this chapter will give you a chance to actuallyimplement these routines in a simulated, but realistic, network setting.
We note here that the GBN protocol incorporates almost all of the techniques that we will encounter when we study the reliable data transfer components of TCP in Section 3.5.
These techniques include the use of sequence numbers, cumulative acknowledgments, checksums, and a timeout/retransmit operation.n+1
Figure 3.22 Go-Back-N in operation 3.4.4 Selective Repeat (SR) The GBN protocol allows the sender to potentially “fill the pipeline” in Figure 3.17 with packets, thus avoiding the channel utilization problems we noted with stop-and-wait protocols.
There are, however, scenarios in which GBN itself suffers from performance problems.
In particular, when the window sizeand bandwidth-delay product are both large, many packets can be in the pipeline.
A single packet error can thus cause GBN to retransmit a large number of packets, many unnecessarily.
As the probability of channel errors increases, the pipeline can become filled with these unnecessary retransmissions.
Imagine, in our message-dictation scenario, that if every time a word was garbled, the surrounding 1,000 words (for example, a window size of 1,000 words) had to be repeated.
The dictation would be
slowed by all of the reiterated words.
As the name suggests, selective-repeat protocols avoid unnecessary retransmissions by having the sender retransmit only those packets that it suspects were received in error (that is, were lost orcorrupted) at the receiver.
This individual, as-needed, retransmission will require that the receiver individually  acknowledge correctly received packets.
A window size of N will again be used to limit the number of outstanding, unacknowledged packets in the pipeline.
However, unlike GBN, the sender will have already received ACKs for some of the packets in the window.
Figure 3.23 shows the SR sender’s view of the sequence number space.
Figure 3.24 details the various actions taken by the SR sender.
The SR receiver will acknowledge a correctly received packet whether or not it is in order.
Out-of-order packets are buffered until any missing packets (that is, packets with lower sequence numbers) are received, at which point a batch of packets can be delivered in order to the upper layer.
Figure 3.25 itemizes the various actions taken by the SR receiver.
Figure 3.26 shows an example of SR operation in the presence of lost packets.
Note that in Figure 3.26, the receiver initially buffers packets 3, 4, and 5, and delivers them together with packet 2 to the upper layer when packet 2 is finally received.
Figure 3.23 Selective-repeat (SR) sender and receiver views of sequence-number space
Figure 3.24 SR sender events and actions Figure 3.25 SR receiver events and actions It is important to note that in Step 2 in Figure 3.25, the receiver reacknowledges (rather than ignores) already received packets with certain sequence numbers below  the current window base.
You should convince yourself that this reacknowledgment is indeed needed.
Given the sender and receiver sequence number spaces in Figure 3.23, for example, if there is no ACK for packet send_base propagating from the
Figure 3.26 SR operation receiver to the sender, the sender will eventually retransmit packet send_base , even though it is clear (to us, not the sender!)
that the receiver has already received that packet.
If the receiver were not to acknowledge this packet, the sender’s window would never move forward!
This example illustrates animportant aspect of SR protocols (and many other protocols as well).
The sender and receiver will notalways have an identical view of what has been received correctly and what has not.
For SR protocols,this means that the sender and receiver windows will not always coincide.
The lack of synchronization between sender and receiver windows has important consequences when we are faced with the reality of a finite range of sequence numbers.
Consider what could happen, forexample, with a finite range of four packet sequence numbers, 0, 1, 2, 3, and a window size of three.
Suppose packets 0 through 2 are transmitted and correctly received and acknowledged at the receiver.
At this point, the receiver’s window is over the fourth, fifth, and sixth packets, which have sequence numbers 3, 0, and 1, respectively.
Now consider two scenarios.
In the first scenario, shown in Figure 3.27(a), the ACKs for the first three packets are lost and the sender retransmits these packets.
The receiver thus next receives a packet with sequence number 0—a copy of the first packet sent.
In the second scenario, shown in Figure 3.27(b), the ACKs for the first three packets are all delivered correctly.
The sender thus moves its window forward and sends the fourth, fifth, and sixth packets, with sequence numbers 3, 0, and 1, respectively.
The packet with sequence number 3 is lost, but the packet with sequence number 0 arrives—a packet containing new data.
Now consider the receiver’s viewpoint in Figure 3.27, which has a figurative curtain between the sender and the receiver, since the receiver cannot “see” the actions taken by the sender.
All the receiver observes is the sequence of messages it receives from the channel and sends into the channel.
As far as it is concerned, the two scenarios in Figure 3.27 are identical.
 There is no way of distinguishing the retransmission of the first packet from an original transmission of the fifth packet.
Clearly, a window size that is 1 less than the size of the sequence number space won’t work.
But how small must the windowsize be?
A problem at the end of the chapter asks you to show that the window size must be less thanor equal to half the size of the sequence number space for SR protocols.
At the companion Web site, you will find an applet that animates the operation of the SR protocol.
Try performing the same experiments that you did with the GBN applet.
Do the results agree with what youexpect?
This completes our discussion of reliable data transfer protocols.
We’ve covered a lot of ground and introduced numerous mechanisms that together provide for reliable data transfer.
Table 3.1 summarizes these mechanisms.
Now that we have seen all of these mechanisms in operation and can see the “big picture,” we encourage you to review this section again to see how these mechanisms wereincrementally added to cover increasingly complex (and realistic) models of the channel connecting thesender and receiver, or to improve the performance of the protocols.
Let’s conclude our discussion of reliable data transfer protocols by considering one remaining assumption in our underlying channel model.
Recall that we have assumed that packets cannot bereordered within the channel between the sender and receiver.
This is generally a reasonableassumption when the sender and receiver are connected by a single physical wire.
However, when the“channel” connecting the two is a network, packet reordering can occur.
One manifestation of packetreordering is that old copies of a packet with a sequence or acknowledgment
Figure 3.27 SR receiver dilemma with too-large windows: A new packet or a retransmission?
Table 3.1 Summary of reliable data transfer mechanisms and their use Mechanism Use, Comments Checksum Used to detect bit errors in a transmitted packet.
Timer Used to timeout/retransmit a packet, possibly because the packet (or its ACK) was lost within the channel.
Because timeouts can occur when a packet isdelayed but not lost (premature timeout), or when a packet has been receivedby the receiver but the receiver-to-sender ACK has been lost, duplicate copies
of a packet may be received by a receiver.
Sequence numberUsed for sequential numbering of packets of data flowing from sender toreceiver.
Gaps in the sequence numbers of received packets allow the receiverto detect a lost packet.
Packets with duplicate sequence numbers allow thereceiver to detect duplicate copies of a packet.
Acknowledgment Used by the receiver to tell the sender that a packet or set of packets has beenreceived correctly.
Acknowledgments will typically carry the sequence numberof the packet or packets being acknowledged.
Acknowledgments may beindividual or cumulative, depending on the protocol.
NegativeacknowledgmentUsed by the receiver to tell the sender that a packet has not been receivedcorrectly.
Negative acknowledgments will typically carry the sequence number of the packet that was not received correctly.
Window, pipeliningThe sender may be restricted to sending only packets with sequence numbersthat fall within a given range.
By allowing multiple packets to be transmitted butnot yet acknowledged, sender utilization can be increased over a stop-and-waitmode of operation.
We’ll see shortly that the window size may be set on thebasis of the receiver’s ability to receive and buffer messages, or the level ofcongestion in the network, or both.
number of x can appear, even though neither the sender’s nor the receiver’s window contains x. With packet reordering, the channel can be thought of as essentially buffering packets and spontaneously emitting these packets at any point in the future.
Because sequence numbers may be reused, some care must be taken to guard against such duplicate packets.
The approach taken in practice is to ensure that a sequence number is not reused until the sender is “sure” that any previously sent packets with sequence number x are no longer in the network.
This is done by assuming that a packet cannot “live” in the network for longer than some fixed maximum amount of time.
A maximum packet lifetime ofapproximately three minutes is assumed in the TCP extensions for high-speed networks [RFC 1323] . [
Sunshine 1978] describes a method for using sequence numbers such that reordering problems can be completely avoided.
3.5 Connection-Oriented Transport: TCP Now that we have covered the underlying principles of reliable data transfer, let’s turn to TCP—the Internet’s transport-layer, connection-oriented, reliable transport protocol.
In this section, we’ll see that inorder to provide reliable data transfer, TCP relies on many of the underlying principles discussed in theprevious section, including error detection, retransmissions, cumulative acknowledgments, timers, andheader fields for sequence and acknowledgment numbers.
TCP is defined in RFC 793, RFC 1122, RFC 1323, RFC 2018, and RFC 2581.
3.5.1 The TCP Connection TCP is said to be connection-oriented  because before one application process can begin to send data to another, the two processes must first “handshake” with each other—that is, they must send some preliminary segments to each other to establish the parameters of the ensuing data transfer.
As part ofTCP connection establishment, both sides of the connection will initialize many TCP state variables (many of which will be discussed in this section and in Section 3.7) associated with the TCP connection.
The TCP “connection” is not an end-to-end TDM or FDM circuit as in a circuit-switched network.
Instead, the “connection” is a logical one, with common state residing only in the TCPs in the two communicatingend systems.
Recall that because the TCP protocol runs only in the end systems and not in theintermediate network elements (routers and link-layer switches), the intermediate network elements donot maintain TCP connection state.
In fact, the intermediate routers are completely oblivious to TCPconnections; they see datagrams, not connections.
A TCP connection provides a full-duplex service: If there is a TCP connection between Process A on one host and Process B on another host, then application-layer data can flow from Process A to Process B at the same time as application-layer data flows from Process B to Process A. A TCPconnection is also always point-to-point , that is, between a single sender and a single receiver.
So- called “multicasting” (see the online supplementary materials for this text)—the transfer of data from onesender to many receivers in a single send operation—is not possible with TCP.
With TCP, two hosts arecompany and three are a crowd!
Let’s now take a look at how a TCP connection is established.
Suppose a process running in one host wants to initiate a connection with another process in another host.
Recall that the process that is
initiating the connection is called the client process , while the other process is called the server process .
The client application process first informs the client transport layer that it wants to establish a connection CASE HISTORY Vinton Cerf, Robert Kahn, and TCP/IP In the early 1970s, packet-switched networks began to proliferate, with the ARPAnet—the precursor of the Internet—being just one of many networks.
Each of these networks had its ownprotocol.
Two researchers, Vinton Cerf and Robert Kahn, recognized the importance ofinterconnecting these networks and invented a cross-network protocol called TCP/IP, whichstands for Transmission Control Protocol/Internet Protocol.
Although Cerf and Kahn began byseeing the protocol as a single entity, it was later split into its two parts, TCP and IP, which operated separately.
Cerf and Kahn published a paper on TCP/IP in May 1974 in IEEE Transactions on Communications Technology  [Cerf 1974] .
The TCP/IP protocol, which is the bread and butter of today’s Internet, was devised before PCs, workstations, smartphones, and tablets, before the proliferation of Ethernet, cable, and DSL,WiFi, and other access network technologies, and before the Web, social media, and streamingvideo.
Cerf and Kahn saw the need for a networking protocol that, on the one hand, providesbroad support for yet-to-be-defined applications and, on the other hand, allows arbitrary hostsand link-layer protocols to interoperate.
In 2004, Cerf and Kahn received the ACM’s Turing Award, considered the “Nobel Prize of Computing” for “pioneering work on internetworking, including the design and implementation of the Internet’s basic communications protocols, TCP/IP, and for inspired leadership in networking.”
to a process in the server.
Recall from Section 2.7.2, a Python client program does this by issuing the command clientSocket.connect((serverName, serverPort)) where serverName  is the name of the server and serverPort  identifies the process on the server.
TCP in the client then proceeds to establish a TCP connection with TCP in the server.
At the end of thissection we discuss in some detail the connection-establishment procedure.
For now it suffices to knowthat the client first sends a special TCP segment; the server responds with a second special TCPsegment; and finally the client responds again with a third special segment.
The first two segments carryno payload, that is, no application-layer data; the third of these segments may carry a payload.
Because
three segments are sent between the two hosts, this connection-establishment procedure is often referred to as a three-way handshake.
Once a TCP connection is established, the two application processes can send data to each other.
Let’s consider the sending of data from the client process to the server process.
The client process passes a stream of data through the socket (the door of the process), as described in Section 2.7.
Once the data passes through the door, the data is in the hands of TCP running in the client.
As shown in Figure 3.28, TCP directs this data to the connection’s send buffer, which is one of the buffers that is set aside during the initial three-way handshake.
From time to time, TCP will grab chunks of data from the send bufferand pass the data to the network layer.
Interestingly, the TCP specification [RFC 793] is very laid back about specifying when TCP should actually send buffered data, stating that TCP should “send that data in segments at its own convenience.”
The maximum amount of data that can be grabbed and placed ina segment is limited by the maximum segment size (MSS).
The MSS is typically set by first determining the length of the largest link-layer frame that can be sent by the local sending host (the so- called maximum transmission unit, MTU ), and then setting the MSS to ensure that a TCP segment (when encapsulated in an IP datagram) plus the TCP/IP header length (typically 40 bytes) will fit into a single link-layer frame.
Both Ethernet and PPP link-layer protocols have an MTU of 1,500 bytes.
Thus atypical value of MSS is 1460 bytes.
Approaches have also been proposed for discovering the path MTU —the largest link-layer frame that can be sent on all links from source to destination [RFC 1191] —and setting the MSS based on the path MTU value.
Note that the MSS is the maximum amount of application-layer data in the segment, not the maximum size of the TCP segment including headers.(This terminology is confusing, but we have to live with it, as it is well entrenched.)
TCP pairs each chunk of client data with a TCP header, thereby forming TCP segments.
The segments are passed down to the network layer, where they are separately encapsulated within network-layer IP datagrams.
The IP datagrams are then sent into the network.
When TCP receives a segment at the other end, the segment’s data is placed in the TCP connection’s receive buffer, as shown in Figure 3.28.
The application reads the stream of data from this buffer.
Each side of the connection has Figure 3.28 TCP send and receive buffers
its own send buffer and its own receive buffer. (
You can see the online flow-control applet at http://www.awl.com/kurose-ross , which provides an animation of the send and receive buffers.)
We see from this discussion that a TCP connection consists of buffers, variables, and a socket connection to a process in one host, and another set of buffers, variables, and a socket connection to a process in another host.
As mentioned earlier, no buffers or variables are allocated to the connection in the network elements (routers, switches, and repeaters) between the hosts.
3.5.2 TCP Segment Structure Having taken a brief look at the TCP connection, let’s examine the TCP segment structure.
The TCPsegment consists of header fields and a data field.
The data field contains a chunk of application data.
As mentioned above, the MSS limits the maximum size of a segment’s data field.
When TCP sends alarge file, such as an image as part of a Web page, it typically breaks the file into chunks of size MSS(except for the last chunk, which will often be less than the MSS).
Interactive applications, however, often transmit data chunks that are smaller than the MSS; for example, with remote login applications like Telnet, the data field in the TCP segment is often only one byte.
Because the TCP header istypically 20 bytes (12 bytes more than the UDP header), segments sent by Telnet may be only 21 bytesin length.
Figure 3.29 shows the structure of the TCP segment.
As with UDP, the header includes source and destination port numbers , which are used for multiplexing/demultiplexing data from/to upper-layer applications.
Also, as with UDP, the header includes a checksum field.
A TCP segment header also contains the following fields: The 32-bit sequence number field  and the 32-bit acknowledgment number field  are used by the TCP sender and receiver in implementing a reliable data transfer service, as discussed below.
The 16-bit receive window  field is used for flow control.
We will see shortly that it is used to indicate the number of bytes that a receiver is willing to accept.
The 4-bit header length field  specifies the length of the TCP header in 32-bit words.
The TCP header can be of variable length due to the TCP options field. (
Typically, the options field is empty, so that the length of the typical TCP header is 20 bytes.)
The optional and variable-length options field  is used when a sender and receiver negotiate the maximum segment size (MSS) or as a window scaling factor for use in high-speed networks.
A time-stamping option is also defined.
See RFC 854 and RFC 1323 for additional details.
The flag field  contains 6 bits.
The ACK bit  is used to indicate that the value carried in the acknowledgment field is valid; that is, the segment contains an acknowledgment for a segment thathas been successfully received.
The RST,
Figure 3.29 TCP segment structure SYN, and FIN bits are used for connection setup and teardown, as we will discuss at the end of this section.
The CWR and ECE bits are used in explicit congestion notification, as discussed in Section 3.7.2.
Setting the PSH bit indicates that the receiver should pass the data to the upper layer immediately.
Finally, the URG bit is used to indicate that there is data in this segment that the sending-side upper-layer entity has marked as “urgent.”
The location of the last byte of this urgent data is indicated by the 16-bit urgent data pointer field .
TCP must inform the receiving-side upper- layer entity when urgent data exists and pass it a pointer to the end of the urgent data. (
In practice,the PSH, URG, and the urgent data pointer are not used.
However, we mention these fields forcompleteness.)
Our experience as teachers is that our students sometimes find discussion of packet formats rather dryand perhaps a bit boring.
For a fun and fanciful look at TCP header fields, particularly if you love Legos™ as we do, see [Pomeranz 2010] .
Sequence Numbers and Acknowledgment Numbers Two of the most important fields in the TCP segment header are the sequence number field and the acknowledgment number field.
These fields are a critical part of TCP’s reliable data transfer service.
Butbefore discussing how these fields are used to provide reliable data transfer, let us first explain what exactly TCP puts in these fields.
Figure 3.30 Dividing file data into TCP segments TCP views data as an unstructured, but ordered, stream of bytes.
TCP’s use of sequence numbers reflects this view in that sequence numbers are over the stream of transmitted bytes and not over the series of transmitted segments.
The sequence number for a segment is therefore the byte-stream number of the first byte in the segment.
Let’s look at an example.
Suppose that a process in Host Awants to send a stream of data to a process in Host B over a TCP connection.
The TCP in Host A willimplicitly number each byte in the data stream.
Suppose that the data stream consists of a file consistingof 500,000 bytes, that the MSS is 1,000 bytes, and that the first byte of the data stream is numbered 0.
As shown in Figure 3.30, TCP constructs 500 segments out of the data stream.
The first segment gets assigned sequence number 0, the second segment gets assigned sequence number 1,000, the third segment gets assigned sequence number 2,000, and so on.
Each sequence number is inserted in the sequence number field in the header of the appropriate TCP segment.
Now let’s consider acknowledgment numbers.
These are a little trickier than sequence numbers.
Recall that TCP is full-duplex, so that Host A may be receiving data from Host B while it sends data to Host B(as part of the same TCP connection).
Each of the segments that arrive from Host B has a sequence number for the data flowing from B to A. The acknowledgment number that Host A puts in its segment is the sequence number of the next byte Host A is expecting from Host B.  It is good to look at a few examples to understand what is going on here.
Suppose that Host A has received all bytes numbered 0 through 535 from B and suppose that it is about to send a segment to Host B. Host A is waiting for byte 536 and all the subsequent bytes in Host B’s data stream.
So Host A puts 536 in the acknowledgmentnumber field of the segment it sends to B. As another example, suppose that Host A has received one segment from Host B containing bytes 0 through 535 and another segment containing bytes 900 through 1,000.
For some reason Host A has notyet received bytes 536 through 899.
In this example, Host A is still waiting for byte 536 (and beyond) inorder to re-create B’s data stream.
Thus, A’s next segment to B will contain 536 in the acknowledgmentnumber field.
Because TCP only acknowledges bytes up to the first missing byte in the stream, TCP is said to provide cumulative acknowledgments.
This last example also brings up an important but subtle issue.
Host A received the third segment (bytes 900 through 1,000) before receiving the second segment (bytes 536 through 899).
Thus, the third segment arrived out of order.
The subtle issue is: What does a host do when it receives out-of-ordersegments in a TCP connection?
Interestingly, the TCP RFCs do not impose any rules here and leavethe decision up to the programmers implementing a TCP implementation.
There are basically twochoices: either (1) the receiver immediately discards out-of-order segments (which, as we discussed earlier, can simplify receiver design), or (2) the receiver keeps the out-of-order bytes and waits for the missing bytes to fill in the gaps.
Clearly, the latter choice is more efficient in terms of network bandwidth,and is the approach taken in practice.
In Figure 3.30, we assumed that the initial sequence number was zero.
In truth, both sides of a TCP connection randomly choose an initial sequence number.
This is done to minimize the possibility that a segment that is still present in the network from an earlier, already-terminated connection between twohosts is mistaken for a valid segment in a later connection between these same two hosts (which also happen to be using the same port numbers as the old connection) [Sunshine 1978].
Telnet: A Case Study for Sequence and Acknowledgment Numbers Telnet, defined in RFC 854, is a popular application-layer protocol used for remote login.
It runs over TCP and is designed to work between any pair of hosts.
Unlike the bulk data transfer applications discussed in Chapter 2, Telnet is an interactive application.
We discuss a Telnet example here, as it nicely illustrates TCP sequence and acknowledgment numbers.
We note that many users now prefer to use the SSH protocol rather than Telnet, since data sent in a Telnet connection (including passwords!)
are not encrypted, making Telnet vulnerable to eavesdropping attacks (as discussed in Section 8.7).
Suppose Host A initiates a Telnet session with Host B. Because Host A initiates the session, it is labeled the client, and Host B is labeled the server.
Each character typed by the user (at the client) will be sentto the remote host; the remote host will send back a copy of each character, which will be displayed onthe Telnet user’s screen.
This “echo back” is used to ensure that characters seen by the Telnet userhave already been received and processed at the remote site.
Each character thus traverses thenetwork twice between the time the user hits the key and the time the character is displayed on theuser’s monitor.
Now suppose the user types a single letter, ‘C,’ and then grabs a coffee.
Let’s examine the TCP segments that are sent between the client and server.
As shown in Figure 3.31, we suppose the starting sequence numbers are 42 and 79 for the client and server, respectively.
Recall that the sequence number of a segment is the sequence number of the first byte in the data field.
Thus, the first segmentsent from the client will have sequence number 42; the first segment sent from the server will havesequence number 79.
Recall that the acknowledgment number is the sequence
Figure 3.31 Sequence and acknowledgment numbers for a simple Telnet application over TCP number of the next byte of data that the host is waiting for.
After the TCP connection is established but before any data is sent, the client is waiting for byte 79 and the server is waiting for byte 42.
As shown in Figure 3.31, three segments are sent.
The first segment is sent from the client to the server, containing the 1-byte ASCII representation of the letter ‘C’ in its data field.
This first segment also has 42 in its sequence number field, as we just described.
Also, because the client has not yet receivedany data from the server, this first segment will have 79 in its acknowledgment number field.
The second segment is sent from the server to the client.
It serves a dual purpose.
First it provides an acknowledgment of the data the server has received.
By putting 43 in the acknowledgment field, the server is telling the client that it has successfully received everything up through byte 42 and is now waiting for bytes 43 onward.
The second purpose of this segment is to echo back the letter ‘C.’ Thus,the second segment has the ASCII representation of ‘C’ in its data field.
This second segment has thesequence number 79, the initial sequence number of the server-to-client data flow of this TCPconnection, as this is the very first byte of data that the server is sending.
Note that the acknowledgmentfor client-to-server data is carried in a segment carrying server-to-client data; this acknowledgment is said to be piggybacked on the server-to-client data segment.
The third segment is sent from the client to the server.
Its sole purpose is to acknowledge the data it has received from the server. (
Recall that the second segment contained data—the letter ‘C’—from the server to the client.)
This segment has an empty data field (that is, the acknowledgment is not beingpiggybacked with any client-to-server data).
The segment has 80 in the acknowledgment number fieldbecause the client has received the stream of bytes up through byte sequence number 79 and it is nowwaiting for bytes 80 onward.
You might think it odd that this segment also has a sequence number since the segment contains no data.
But because TCP has a sequence number field, the segment needs to have some sequence number.
3.5.3 Round-Trip Time Estimation and Timeout TCP, like our rdt protocol in Section 3.4, uses a timeout/retransmit mechanism to recover from lost segments.
Although this is conceptually simple, many subtle issues arise when we implement a timeout/retransmit mechanism in an actual protocol such as TCP.
Perhaps the most obvious question is the length of the timeout intervals.
Clearly, the timeout should be larger than the connection’s round-triptime (RTT), that is, the time from when a segment is sent until it is acknowledged.
Otherwise,unnecessary retransmissions would be sent.
But how much larger?
How should the RTT be estimated inthe first place?
Should a timer be associated with each and every unacknowledged segment?
So many questions!
Our discussion in this section is based on the TCP work in [Jacobson 1988] and the current IETF recommendations for managing TCP timers [RFC 6298] .
Estimating the Round-Trip Time Let’s begin our study of TCP timer management by considering how TCP estimates the round-trip time between sender and receiver.
This is accomplished as follows.
The sample RTT, denoted SampleRTT , for a segment is the amount of time between when the segment is sent (that is, passed to IP) and whenan acknowledgment for the segment is received.
Instead of measuring a SampleRTT  for every transmitted segment, most TCP implementations take only one SampleRTT  measurement at a time.
That is, at any point in time, the SampleRTT  is being estimated for only one of the transmitted but currently unacknowledged segments, leading to a new value of SampleRTT  approximately once every RTT.
Also, TCP never computes a SampleRTT  for a segment that has been retransmitted; it only measures SampleRTT  for segments that have been transmitted once [Karn 1987] . (
A problem at the end of the chapter asks you to consider why.)
Obviously, the SampleRTT  values will fluctuate from segment to segment due to congestion in the routers and to the varying load on the end systems.
Because of this fluctuation, any given SampleRTT value may be atypical.
In order to estimate a typical RTT, it is therefore natural to take some sort of average of the SampleRTT  values.
TCP maintains an average, called EstimatedRTT , of the
SampleRTT  values.
Upon obtaining a new SampleRTT , TCP updates EstimatedRTT  according to the following formula: The formula above is written in the form of a programming-language statement—the new value of EstimatedRTT is a weighted combination of the previous value of EstimatedRTT and the new value for SampleRTT.
The recommended value of α is α = 0.125 (that is, 1/8) [RFC 6298] , in which case the formula above becomes: Note that EstimatedRTT is a weighted average of the SampleRTT values.
As discussed in a homework problem at the end of this chapter, this weighted average puts more weight on recent samples than on old samples.
This is natural, as the more recent samples better reflect the current congestion in thenetwork.
In statistics, such an average is called an exponential weighted moving average (EWMA) .
The word “exponential” appears in EWMA because the weight of a given SampleRTT  decays exponentially fast as the updates proceed.
In the homework problems you will be asked to derive the exponential term in EstimatedRTT .
Figure 3.32 shows the SampleRTT  values and EstimatedRTT  for a value of α = 1/8 for a TCP connection between gaia.cs.umass.edu  (in Amherst, Massachusetts) to fantasia.eurecom.fr (in the south of France).
Clearly, the variations in the SampleRTT  are smoothed out in the computation of the EstimatedRTT .
In addition to having an estimate of the RTT, it is also valuable to have a measure of the variability of theRTT. [
RFC 6298]  defines the RTT variation, DevRTT , as an estimate of how much SampleRTT typically deviates from EstimatedRTT : Note that DevRTT  is an EWMA of the difference between SampleRTT  and EstimatedRTT .
If the SampleRTT  values have little fluctuation, then DevRTT  will be small; on the other hand, if there is a lot of fluctuation, DevRTT  will be large.
The recommended value of β is 0.25.EstimatedRTT =(1−α)⋅EstimatedRTT +α⋅SampleRTT EstimatedRTT =0.875⋅EstimatedRTT +0.125⋅SampleRTT DevRTT=(1−β)⋅DevRTT+β⋅|SampleRTT −EstimatedRTT |
Setting and Managing the Retransmission Timeout Interval Given values of EstimatedRTT  and DevRTT , what value should be used for TCP’s timeout interval?
Clearly, the interval should be greater than or equal to PRINCIPLES IN PRACTICE TCP provides reliable data transfer by using positive acknowledgments and timers in much the same way that we studied in Section 3.4.
TCP acknowledges data that has been received correctly, and it then retransmits segments when segments or their corresponding acknowledgments are thought to be lost or corrupted.
Certain versions of TCP also have an implicit NAK mechanism—with TCP’s fast retransmit mechanism, the receipt of three duplicateACKs for a given segment serves as an implicit NAK for the following segment, triggeringretransmission of that segment before timeout.
TCP uses sequences of numbers to allow thereceiver to identify lost or duplicate segments.
Just as in the case of our reliable data transfer protocol, rdt3.0 , TCP cannot itself tell for certain if a segment, or its ACK, is lost, corrupted, or overly delayed.
At the sender, TCP’s response will be the same: retransmit the segment in question.
TCP also uses pipelining, allowing the sender to have multiple transmitted but yet-to-be- acknowledged segments outstanding at any given time.
We saw earlier that pipelining cangreatly improve a session’s throughput when the ratio of the segment size to round-trip delay issmall.
The specific number of outstanding, unacknowledged segments that a sender can have isdetermined by TCP’s flow-control and congestion-control mechanisms.
TCP flow control is discussed at the end of this section; TCP congestion control is discussed in Section 3.7.
For the time being, we must simply be aware that the TCP sender uses pipelining.
EstimatedRTT , or unnecessary retransmissions would be sent.
But the timeout interval should not be too much larger than EstimatedRTT ; otherwise, when a segment is lost, TCP would not quickly retransmit the segment, leading to large data transfer delays.
It is therefore desirable to set the timeoutequal to the EstimatedRTT  plus some margin.
The margin should be large when there is a lot of fluctuation in the SampleRTT  values; it should be small when there is little fluctuation.
The value of DevRTT  should thus come into play here.
All of these considerations are taken into account in TCP’s method for determining the retransmission timeout interval: An initial TimeoutInterval  value of 1 second is recommended [RFC 6298] .
Also, when a timeout occurs, the value of TimeoutInterval  is doubled to avoid a premature timeout occurring for aTimeoutInterval =EstimatedRTT +4⋅DevRTT
subsequent segment that will soon be acknowledged.
However, as soon as a segment is received and EstimatedRTT  is updated, the TimeoutInterval  is again computed using the formula above.
Figure 3.32 RTT samples and RTT estimates 3.5.4 Reliable Data Transfer Recall that the Internet’s network-layer service (IP service) is unreliable.
IP does not guarantee datagram delivery, does not guarantee in-order delivery of datagrams, and does not guarantee theintegrity of the data in the datagrams.
With IP service, datagrams can overflow router buffers and neverreach their destination, datagrams can arrive out of order, and bits in the datagram can get corrupted(flipped from 0 to 1 and vice versa).
Because transport-layer segments are carried across the network by IP datagrams, transport-layer segments can suffer from these problems as well.
TCP creates a reliable data transfer service  on top of IP’s unreliable best-effort service.
TCP’s reliable data transfer service ensures that the data stream that a process reads out of its TCP receive buffer is uncorrupted, without gaps, without duplication, and in sequence; that is, the byte stream is exactly thesame byte stream that was sent by the end system on the other side of the connection.
How TCP provides a reliable data transfer involves many of the principles that we studied in Section 3.4.
In our earlier development of reliable data transfer techniques, it was conceptually easiest to assume
that an individual timer is associated with each transmitted but not yet acknowledged segment.
While this is great in theory, timer management can require considerable overhead.
Thus, the recommended TCP timer management procedures [RFC 6298]  use only a single  retransmission timer, even if there are multiple transmitted but not yet acknowledged segments.
The TCP protocol described in this section follows this single-timer recommendation.
We will discuss how TCP provides reliable data transfer in two incremental steps.
We first present a highly simplified description of a TCP sender that uses only timeouts to recover from lost segments; wethen present a more complete description that uses duplicate acknowledgments in addition to timeouts.
In the ensuing discussion, we suppose that data is being sent in only one direction, from Host A to HostB, and that Host A is sending a large file.
Figure 3.33 presents a highly simplified description of a TCP sender.
We see that there are three major events related to data transmission and retransmission in the TCP sender: data received from application above; timer timeout; and ACK Figure 3.33 Simplified TCP sender
receipt.
Upon the occurrence of the first major event, TCP receives data from the application, encapsulates the data in a segment, and passes the segment to IP.
Note that each segment includes asequence number that is the byte-stream number of the first data byte in the segment, as described in Section 3.5.2.
Also note that if the timer is already not running for some other segment, TCP starts the timer when the segment is passed to IP. (
It is helpful to think of the timer as being associated with the oldest unacknowledged segment.)
The expiration interval for this timer is the TimeoutInterval , which is calculated from EstimatedRTT  and DevRTT , as described in Section 3.5.3.
The second major event is the timeout.
TCP responds to the timeout event by retransmitting the segment that caused the timeout.
TCP then restarts the timer.
The third major event that must be handled by the TCP sender is the arrival of an acknowledgment segment (ACK) from the receiver (more specifically, a segment containing a valid ACK field value).
On the occurrence of this event, TCP compares the ACK value y with its variable SendBase .
The TCP state variable SendBase  is the sequence number of the oldest unacknowledged byte. (
Thus SendBase–1  is the sequence number of the last byte that is known to have been received correctly and in order at the receiver.)
As indicated earlier, TCP uses cumulative acknowledgments, so that y acknowledges the receipt of all bytes before byte number y. If y > SendBase , then the ACK is acknowledging one or more previously unacknowledged segments.
Thus the sender updates its SendBase  variable; it also restarts the timer if there currently are any not-yet-acknowledged segments.
A Few Interesting Scenarios We have just described a highly simplified version of how TCP provides reliable data transfer.
But even this highly simplified version has many subtleties.
To get a good feeling for how this protocol works, let’s now walk through a few simple scenarios.
Figure 3.34 depicts the first scenario, in which Host A sends one segment to Host B. Suppose that this segment has sequence number 92 and contains 8 bytes of data.
After sending this segment, Host A waits for a segment from B with acknowledgment number 100.
Although the segment from A is received at B, the acknowledgment from B to A gets lost.
In this case, the timeout event occurs, and Host A retransmits the same segment.
Of course, when Host B receivesthe retransmission, it observes from the sequence number that the segment contains data that hasalready been received.
Thus, TCP in Host B will discard the bytes in the retransmitted segment.
In a second scenario, shown in Figure 3.35, Host A sends two segments back to back.
The first segment has sequence number 92 and 8 bytes of data, and the second segment has sequence number 100 and 20 bytes of data.
Suppose that both segments arrive intact at B, and B sends two separate acknowledgments for each of these segments.
The first of these acknowledgments has acknowledgment number 100; the second has acknowledgment number 120.
Suppose now that neither of the acknowledgments arrives at Host A before the timeout.
When the timeout event occurs, Host
Figure 3.34 Retransmission due to a lost acknowledgment A resends the first segment with sequence number 92 and restarts the timer.
As long as the ACK for the second segment arrives before the new timeout, the second segment will not be retransmitted.
In a third and final scenario, suppose Host A sends the two segments, exactly as in the second example.
The acknowledgment of the first segment is lost in the network, but just before the timeoutevent, Host A receives an acknowledgment with acknowledgment number 120.
Host A therefore knows that Host B has received everything  up through byte 119; so Host A does not resend either of the two segments.
This scenario is illustrated in Figure 3.36.
Doubling the Timeout Interval We now discuss a few modifications that most TCP implementations employ.
The first concerns the length of the timeout interval after a timer expiration.
In this modification, whenever the timeout eventoccurs, TCP retransmits the not-yet-acknowledged segment with the smallest sequence number, asdescribed above.
But each time TCP retransmits, it sets the next timeout interval to twice the previousvalue,
Figure 3.35 Segment 100 not retransmitted rather than deriving it from the last EstimatedRTT  and DevRTT  (as described in Section 3.5.3).
For example, suppose TimeoutInterval  associated with the oldest not yet acknowledged segment is .75 sec when the timer first expires.
TCP will then retransmit this segment and set the new expiration time to 1.5 sec.
If the timer expires again 1.5 sec later, TCP will again retransmit this segment, nowsetting the expiration time to 3.0 sec.
Thus the intervals grow exponentially after each retransmission.
However, whenever the timer is started after either of the two other events (that is, data received from application above, and ACK received), the TimeoutInterval  is derived from the most recent values of EstimatedRTT  and DevRTT .
This modification provides a limited form of congestion control. (
More comprehensive forms of TCPcongestion control will be studied in Section 3.7.)
The timer expiration is most likely caused by congestion in the network, that is, too many packets arriving at one (or more) router queues in the path between the source and destination, causing packets to be dropped and/or long queuing delays.
Intimes of congestion, if the sources continue to retransmit packets persistently, the congestion
Figure 3.36 A cumulative acknowledgment avoids retransmission of the first segment may get worse.
Instead, TCP acts more politely, with each sender retransmitting after longer and longer intervals.
We will see that a similar idea is used by Ethernet when we study CSMA/CD in Chapter 6.
Fast Retransmit One of the problems with timeout-triggered retransmissions is that the timeout period can be relatively long.
When a segment is lost, this long timeout period forces the sender to delay resending the lostpacket, thereby increasing the end-to-end delay.
Fortunately, the sender can often detect packet losswell before the timeout event occurs by noting so-called duplicate ACKs.
A duplicate ACK is an ACK that reacknowledges a segment for which the sender has already received an earlier acknowledgment.
To understand the sender’s response to a duplicate ACK, we must look at why the receiver sends a duplicate ACK in the first place.
Table 3.2 summarizes the TCP receiver’s ACK generation policy [RFC 5681] .
When a TCP receiver receives Table 3.2 TCP ACK Generation Recommendation [RFC 5681] Event TCP Receiver Action
Arrival of in-order segment with expected sequence number.
All data up to expectedsequence number already acknowledged.
Delayed ACK.
Wait up to 500 msec for arrival ofanother in-order segment.
If next in-order segmentdoes not arrive in this interval, send an ACK.
Arrival of in-order segment with expectedsequence number.
One other in-ordersegment waiting for ACK transmission.
One Immediately send single cumulative ACK,ACKing both in-order segments.
Arrival of out-of-order segment with higher-than-expected sequence number.
Gapdetected.
Immediately send duplicate ACK, indicatingsequence number of next expected byte (which isthe lower end of the gap).
Arrival of segment that partially or completelyfills in gap in received data.
Immediately send ACK, provided that segmentstarts at the lower end of gap.
a segment with a sequence number that is larger than the next, expected, in-order sequence number, itdetects a gap in the data stream—that is, a missing segment.
This gap could be the result of lost orreordered segments within the network.
Since TCP does not use negative acknowledgments, thereceiver cannot send an explicit negative acknowledgment back to the sender.
Instead, it simplyreacknowledges (that is, generates a duplicate ACK for) the last in-order byte of data it has received. (
Note that Table 3.2 allows for the case that the receiver does not discard out-of-order segments.)
Because a sender often sends a large number of segments back to back, if one segment is lost, there will likely be many back-to-back duplicate ACKs.
If the TCP sender receives three duplicate ACKs forthe same data, it takes this as an indication that the segment following the segment that has beenACKed three times has been lost. (
In the homework problems, we consider the question of why thesender waits for three duplicate ACKs, rather than just a single duplicate ACK.)
In the case that three duplicate ACKs are received, the TCP sender performs a fast retransmit  [RFC 5681] , retransmitting the missing segment before that segment’s timer expires.
This is shown in Figure 3.37, where the second segment is lost, then retransmitted before its timer expires.
For TCP with fast retransmit, the following code snippet replaces the ACK received event in Figure 3.33: event: ACK received, with ACK field value of y             if (y > SendBase) {             SendBase=y            if (there are currently any not yet                       acknowledged segments)                start timer
} Figure 3.37 Fast retransmit: retransmitting the missing segment before the segment’s timer expires             else {/* a duplicate ACK for already ACKed                    segment */               increment number of duplicate ACKs                   received for y               if (number of duplicate ACKS received                   for y==3)                   /* TCP fast retransmit */                    resend segment with sequence number y                }           break;
We noted earlier that many subtle issues arise when a timeout/retransmit mechanism is implemented in an actual protocol such as TCP.
The procedures above, which have evolved as a result of more than 20years of experience with TCP timers, should convince you that this is indeed the case!
Go-Back-N or Selective Repeat?
Let us close our study of TCP’s error-recovery mechanism by considering the following question: Is TCP a GBN or an SR protocol?
Recall that TCP acknowledgments are cumulative and correctly received but out-of-order segments are not individually ACKed by the receiver.
Consequently, as shown in Figure 3.33 (see also Figure 3.19), the TCP sender need only maintain the smallest sequence number of a transmitted but unacknowledged byte ( SendBase ) and the sequence number of the next byte to be sent (NextSeqNum ).
In this sense, TCP looks a lot like a GBN-style protocol.
But there are some striking differences between TCP and Go-Back-N. Many TCP implementations will buffer correctly received but out-of-order segments [Stevens 1994] .
Consider also what happens when the sender sends a sequence of segments 1, 2, . . .,
N, and all of the segments arrive in order without error at the receiver.
Further suppose that the acknowledgment for packet  gets lost, but the remaining  acknowledgments arrive at the sender before their respective timeouts.
In this example, GBN would retransmit not only packet n, but also all of the subsequent packets  TCP, on the other hand, would retransmit at most one segment, namely, segment n. Moreover, TCP would not even retransmit segment n if the acknowledgment for segment  arrived before the timeout for segment n. A proposed modification to TCP, the so-called selective acknowledgment  [RFC 2018] , allows a TCP receiver to acknowledge out-of-order segments selectively rather than just cumulatively acknowledging the last correctly received, in-order segment.
When combined with selective retransmission—skippingthe retransmission of segments that have already been selectively acknowledged by the receiver—TCPlooks a lot like our generic SR protocol.
Thus, TCP’s error-recovery mechanism is probably best categorized as a hybrid of GBN and SR protocols.
3.5.5 Flow Control Recall that the hosts on each side of a TCP connection set aside a receive buffer for the connection.
When the TCP connection receives bytes that are correct and in sequence, it places the data in thereceive buffer.
The associated application process will read data from this buffer, but not necessarily atthe instant the data arrives.
Indeed, the receiving application may be busy with some other task andmay not even attempt to read the data until long after it has arrived.
If the application is relatively slow at reading the data, the sender can very easily overflow the connection’s receive buffer by sending too much data too quickly.n<N N−1 n+1,n+2,…,N. n+1
TCP provides a flow-control service to its applications to eliminate the possibility of the sender overflowing the receiver’s buffer.
Flow control is thus a speed-matching service—matching the rate atwhich the sender is sending against the rate at which the receiving application is reading.
As notedearlier, a TCP sender can also be throttled due to congestion within the IP network; this form of sender control is referred to as congestion control , a topic we will explore in detail in Sections 3.6 and 3.7.
Even though the actions taken by flow and congestion control are similar (the throttling of the sender), they are obviously taken for very different reasons.
Unfortunately, many authors use the termsinterchangeably, and the savvy reader would be wise to distinguish between them.
Let’s now discusshow TCP provides its flow-control service.
In order to see the forest for the trees, we supposethroughout this section that the TCP implementation is such that the TCP receiver discards out-of-ordersegments.
TCP provides flow control by having the sender  maintain a variable called the receive window .
Informally, the receive window is used to give the sender an idea of how much free buffer space is available at the receiver.
Because TCP is full-duplex, the sender at each side of the connectionmaintains a distinct receive window.
Let’s investigate the receive window in the context of a file transfer.
Suppose that Host A is sending a large file to Host B over a TCP connection.
Host B allocates a receive buffer to this connection; denote its size by RcvBuffer .
From time to time, the application process in Host B reads from the buffer.
Define the following variables: LastByteRead : the number of the last byte in the data stream read from the buffer by the application process in B LastByteRcvd : the number of the last byte in the data stream that has arrived from the network and has been placed in the receive buffer at B Because TCP is not permitted to overflow the allocated buffer, we must have The receive window, denoted rwnd  is set to the amount of spare room in the buffer: Because the spare room changes with time, rwnd  is dynamic.
The variable rwnd  is illustrated in Figure 3.38.LastByteRcvd −LastByteRead ≤RcvBuffer rwnd=RcvBuffer −[LastByteRcvd −LastByteRead ]
How does the connection use the variable rwnd  to provide the flow-control service?
Host B tells Host A how much spare room it has in the connection buffer by placing its current value of rwnd  in the receive window field of every segment it sends to A. Initially, Host B sets rwnd = RcvBuffer .
Note that to pull this off, Host B must keep track of several connection-specific variables.
Host A in turn keeps track of two variables, LastByteSent  and LastByteAcked , which have obvious meanings.
Note that the difference between these two variables, LastByteSent – LastByteAcked , is the amount of unacknowledged data that A has sent into the connection.
By keeping the amount of unacknowledged data less than the value of rwnd , Host A is assured that it is not Figure 3.38 The receive window (rwnd)  and the receive buffer (RcvBuffer) overflowing the receive buffer at Host B. Thus, Host A makes sure throughout the connection’s life that There is one minor technical problem with this scheme.
To see this, suppose Host B’s receive buffer becomes full so that rwnd  = 0.
After advertising rwnd  = 0 to Host A, also suppose that B has nothing to send to A. Now consider what happens.
As the application process at B empties the buffer, TCP doesnot send new segments with new rwnd  values to Host A; indeed, TCP sends a segment to Host A only if it has data to send or if it has an acknowledgment to send.
Therefore, Host A is never informed that some space has opened up in Host B’s receive buffer—Host A is blocked and can transmit no moredata!
To solve this problem, the TCP specification requires Host A to continue to send segments withone data byte when B’s receive window is zero.
These segments will be acknowledged by the receiver.
Eventually the buffer will begin to empty and the acknowledgments will contain a nonzero rwnd  value.
LastByteSent −LastByteAcked ≤rwnd
The online site at http://www.awl.com/kurose-ross  for this book provides an interactive Java applet that illustrates the operation of the TCP receive window.
Having described TCP’s flow-control service, we briefly mention here that UDP does not provide flow control and consequently, segments may be lost at the receiver due to buffer overflow.
For example,consider sending a series of UDP segments from a process on Host A to a process on Host B. For a typical UDP implementation, UDP will append the segments in a finite-sized buffer that “precedes” the corresponding socket (that is, the door to the process).
The process reads one entire segment at a timefrom the buffer.
If the process does not read the segments fast enough from the buffer, the buffer willoverflow and segments will get dropped.
3.5.6 TCP Connection Management In this subsection we take a closer look at how a TCP connection is established and torn down.
Although this topic may not seem particularly thrilling, it is important because TCP connectionestablishment can significantly add to perceived delays (for example, when surfing the Web).Furthermore, many of the most common network attacks—including the incredibly popular SYN floodattack—exploit vulnerabilities in TCP connection management.
Let’s first take a look at how a TCP connection is established.
Suppose a process running in one host (client) wants to initiate a connection with another process in another host (server).
The client application process first informs the client TCPthat it wants to establish a connection to a process in the server.
The TCP in the client then proceeds toestablish a TCP connection with the TCP in the server in the following manner: Step 1.
The client-side TCP first sends a special TCP segment to the server-side TCP.
This special segment contains no application-layer data.
But one of the flag bits in the segment’s header (see Figure 3.29), the SYN bit, is set to 1.
For this reason, this special segment is referred to as a SYN segment.
In addition, the client randomly chooses an initial sequence number ( client_isn ) and puts this number in the sequence number field of the initial TCP SYN segment.
This segment isencapsulated within an IP datagram and sent to the server.
There has been considerable interest in properly randomizing the choice of the client_isn  in order to avoid certain security attacks [CERT 2001–09] .
Step 2.
Once the IP datagram containing the TCP SYN segment arrives at the server host (assuming it does arrive!),
the server extracts the TCP SYN segment from the datagram, allocates the TCP buffers and variables to the connection, and sends a connection-granted segment to the client TCP. (
We’ll see in Chapter 8 that the allocation of these buffers and variables before completing the third step of the three-way handshake makes TCP vulnerable to a denial-of-service attack known as SYN flooding.)
This connection-granted segment also contains no application-layerdata.
However, it does contain three important pieces of information in the segment header.
First,the SYN bit is set to 1.
Second, the acknowledgment field of the TCP segment header is set to
client_isn+1 .
Finally, the server chooses its own initial sequence number ( server_isn ) and puts this value in the sequence number field of the TCP segment header.
This connection-granted segment is saying, in effect, “I received your SYN packet to start a connection with your initial sequence number, client_isn .
I agree to establish this connection.
My own initial sequence number is server_isn .”
The connection-granted segment is referred to as a SYNACK segment .
Step 3.
Upon receiving the SYNACK segment, the client also allocates buffers and variables to the connection.
The client host then sends the server yet another segment; this last segment acknowledges the server’s connection-granted segment (the client does so by putting the value server_isn+1  in the acknowledgment field of the TCP segment header).
The SYN bit is set to zero, since the connection is established.
This third stage of the three-way handshake may carryclient-to-server data in the segment payload.
Once these three steps have been completed, the client and server hosts can send segmentscontaining data to each other.
In each of these future segments, the SYN bit will be set to zero.
Notethat in order to establish the connection, three packets are sent between the two hosts, as illustrated in Figure 3.39.
For this reason, this connection-establishment procedure is often referred to as a three- way handshake.
Several aspects of the TCP three-way handshake are explored in the homework problems (Why are initial sequence numbers needed?
Why is a three-way handshake, as opposed to atwo-way handshake, needed?).
It’s interesting to note that a rock climber and a belayer (who isstationed below the rock climber and whose job it is to handle the climber’s safety rope) use a three-way-handshake communication protocol that is identical to TCP’s to ensure that both sides are readybefore the climber begins ascent.
All good things must come to an end, and the same is true with a TCP connection.
Either of the two processes participating in a TCP connection can end the connection.
When a connection ends, the “resources” (that is, the buffers and variables)
Figure 3.39 TCP three-way handshake: segment exchange
Figure 3.40 Closing a TCP connection in the hosts are deallocated.
As an example, suppose the client decides to close the connection, as shown in Figure 3.40.
The client application process issues a close command.
This causes the client TCP to send a special TCP segment to the server process.
This special segment has a flag bit in thesegment’s header, the FIN bit (see Figure 3.29), set to 1.
When the server receives this segment, it sends the client an acknowledgment segment in return.
The server then sends its own shutdown segment, which has the FIN bit set to 1.
Finally, the client acknowledges the server’s shutdownsegment.
At this point, all the resources in the two hosts are now deallocated.
During the life of a TCP connection, the TCP protocol running in each host makes transitions through various TCP states .
Figure 3.41 illustrates a typical sequence of TCP states that are visited by the client TCP.
The client TCP begins in the CLOSED state.
The application on the client side initiates a new TCP connection (by creating a Socket object in our Java examples as in the Python examples from Chapter 2).
This causes TCP in the client to send a SYN segment to TCP in the server.
After having sent the SYN segment, the client TCP enters the SYN_SENT state.
While in the SYN_SENT state, the client TCP waits for a segment from the server TCP that includes an acknowledgment for the client’sprevious segment and Figure 3.41 A typical sequence of TCP states visited by a client TCP
has the SYN bit set to 1.
Having received such a segment, the client TCP enters the ESTABLISHED state.
While in the ESTABLISHED state, the TCP client can send and receive TCP segments containing payload (that is, application-generated) data.
Suppose that the client application decides it wants to close the connection. (
Note that the server could also choose to close the connection.)
This causes the client TCP to send a TCP segment with the FIN bit set to 1 and to enter the FIN_WAIT_1 state.
While in the FIN_WAIT_1 state, the client TCP waits for a TCP segment from the server with an acknowledgment.
When it receives this segment, the client TCPenters the FIN_WAIT_2 state.
While in the FIN_WAIT_2 state, the client waits for another segment fromthe server with the FIN bit set to 1; after receiving this segment, the client TCP acknowledges theserver’s segment and enters the TIME_WAIT state.
The TIME_WAIT state lets the TCP client resendthe final acknowledgment in case the ACK is lost.
The time spent in the TIME_WAIT state isimplementation-dependent, but typical values are 30 seconds, 1 minute, and 2 minutes.
After the wait,the connection formally closes and all resources on the client side (including port numbers) are released.
Figure 3.42 illustrates the series of states typically visited by the server-side TCP, assuming the client begins connection teardown.
The transitions are self-explanatory.
In these two state-transition diagrams, we have only shown how a TCP connection is normally established and shut down.
We have notdescribed what happens in certain pathological scenarios, for example, when both sides of a connectionwant to initiate or shut down at the same time.
If you are interested in learning about Figure 3.42 A typical sequence of TCP states visited by a server-side TCP
this and other advanced issues concerning TCP, you are encouraged to see Stevens’ comprehensive book [Stevens 1994] .
Our discussion above has assumed that both the client and server are prepared to communicate, i.e., that the server is listening on the port to which the client sends its SYN segment.
Let’s consider what happens when a host receives a TCP segment whose port numbers or source IP address do not match with any of the ongoing sockets in the host.
For example, suppose a host receives a TCP SYN packetwith destination port 80, but the host is not accepting connections on port 80 (that is, it is not running aWeb server on port 80).
Then the host will send a special reset segment to the source.
This TCP segment has the RST flag bit (see Section 3.5.2) set to 1.
Thus, when a host sends a reset segment, it is telling the source “I don’t have a socket for that segment.
Please do not resend the segment.”
When a host receives a UDP packet whose destination port number doesn’t match with an ongoing UDP socket, the host sends a special ICMP datagram, as discussed in Chapter 5.
Now that we have a good understanding of TCP connection management, let’s revisit the nmap port- scanning tool and examine more closely how it works.
To explore a specific TCP port, say port 6789, ona target host, nmap will send a TCP SYN segment with destination port 6789 to that host.
There arethree possible outcomes: The source host receives a TCP SYNACK segment from the target host.
Since this means that an application is running with TCP port 6789 on the target post, nmap returns “open.”
FOCUS ON SECURITY The Syn Flood Attack We’ve seen in our discussion of TCP’s three-way handshake that a server allocates and initializes connection variables and buffers in response to a received SYN.
The server then sends a SYNACK in response, and awaits an ACK segment from the client.
If the client doesnot send an ACK to complete the third step of this 3-way handshake, eventually (often aftera minute or more) the server will terminate the half-open connection and reclaim theallocated resources.
This TCP connection management protocol sets the stage for a classic Denial of Service (DoS) attack known as the SYN flood attack.
In this attack, the attacker(s) send a largenumber of TCP SYN segments, without completing the third handshake step.
With this deluge of SYN segments, the server’s connection resources become exhausted as they are allocated (but never used!)
for half-open connections; legitimate clients are then denied service.
Such SYN flooding attacks were among the first documented DoS attacks [CERT SYN 1996] .
Fortunately, an effective defense known as SYN cookies  [RFC 4987]  are now deployed in most major operating systems.
SYN cookies work as follows: When the server receives a SYN segment, it does not know if the segment is coming
from a legitimate user or is part of a SYN flood attack.
So, instead of creating a half-open TCP connection for this SYN, the server creates an initial TCP sequence number that is a complicated function (hash function) of source and destination IP addresses and portnumbers of the SYN segment, as well as a secret number only known to the server.
Thiscarefully crafted initial sequence number is the so-called “cookie.”
The server then sends the client a SYNACK packet with this special initial sequence number.
Importantly, the server does not remember the cookie or any other state information corresponding to the SYN.
A legitimate client will return an ACK segment.
When the server receives this ACK, it must verify that the ACK corresponds to some SYN sent earlier.
But how is this done if the server maintains no memory about SYN segments?
As you may have guessed, it isdone with the cookie.
Recall that for a legitimate ACK, the value in the acknowledgment field is equal to the initial sequence number in the SYNACK (the cookie value in this case) plus one (see Figure 3.39).
The server can then run the same hash function using the source and destination IP address and port numbers in the SYNACK (which are the same as in the original SYN) and the secret number.
If the result of the function plus oneis the same as the acknowledgment (cookie) value in the client’s SYNACK, the serverconcludes that the ACK corresponds to an earlier SYN segment and is hence valid.
Theserver then creates a fully open connection along with a socket.
On the other hand, if the client does not return an ACK segment, then the original SYN has done no harm at the server, since the server hasn’t yet allocated any resources in response to the original bogus SYN.
The source host receives a TCP RST segment from the target host.
This means that the SYN segment reached the target host, but the target host is not running an application with TCP port 6789.
But the attacker at least knows that the segments destined to the host at port 6789 are notblocked by any firewall on the path between source and target hosts. (
Firewalls are discussed in Chapter 8.)
The source receives nothing.
 This likely means that the SYN segment was blocked by an intervening firewall and never reached the target host.
Nmap is a powerful tool that can “case the joint” not only for open TCP ports, but also for open UDP ports, for firewalls and their configurations, and even for the versions of applications and operating systems.
Most of this is done by manipulating TCP connection-management segments [Skoudis 2006].
You can download nmap from www.nmap.org .
This completes our introduction to error control and flow control in TCP.
In Section 3.7 we’ll return to TCP and look at TCP congestion control in some depth.
Before doing so, however, we first step back and examine congestion-control issues in a broader context.
3.6 Principles of Congestion Control In the previous sections, we examined both the general principles and specific TCP mechanisms used to provide for a reliable data transfer service in the face of packet loss.
We mentioned earlier that, inpractice, such loss typically results from the overflowing of router buffers as the network becomescongested.
Packet retransmission thus treats a symptom of network congestion (the loss of a specifictransport-layer segment) but does not treat the cause of network congestion—too many sources attempting to send data at too high a rate.
To treat the cause of network congestion, mechanisms are needed to throttle senders in the face of network congestion.
In this section, we consider the problem of congestion control in a general context, seeking to understand why congestion is a bad thing, how network congestion is manifested in the performancereceived by upper-layer applications, and various approaches that can be taken to avoid, or react to,network congestion.
This more general study of congestion control is appropriate since, as with reliabledata transfer, it is high on our “top-ten” list of fundamentally important problems in networking.
The following section contains a detailed study of TCP’s congestion-control algorithm.
3.6.1 The Causes and the Costs of Congestion Let’s begin our general study of congestion control by examining three increasingly complex scenarios in which congestion occurs.
In each case, we’ll look at why congestion occurs in the first place and at the cost of congestion (in terms of resources not fully utilized and poor performance received by the endsystems).
We’ll not (yet) focus on how to react to, or avoid, congestion but rather focus on the simplerissue of understanding what happens as hosts increase their transmission rate and the network becomes congested.
Scenario 1: Two Senders, a Router with Infinite Buffers We begin by considering perhaps the simplest congestion scenario possible: Two hosts (A and B) each have a connection that shares a single hop between source and destination, as shown in Figure 3.43.
Let’s assume that the application in Host A is sending data into the connection (for example, passingdata to the transport-level protocol via a socket) at an average rate of λ bytes/sec.
These data are original in the sense that each unit of data is sent into the socket only once.
The underlying transport- level protocol is a simple one.
Data is encapsulated and sent; no error recovery (for example, in
retransmission), flow control, or congestion control is performed.
Ignoring the additional overhead due to adding transport- and lower-layer header information, the rate at which Host A offers traffic to the router in this first scenario is thus λ  bytes/sec.
Host B operates in a similar manner, and we assume for simplicity that it too is sending at a rate of λ  bytes/sec.
Packets from Hosts A and B pass through a router and over a shared outgoing link of capacity R. The router has buffers that allow it to store incoming packets when the packet-arrival rate exceeds the outgoing link’s capacity.
In this first scenario, we assume that the router has an infinite amount of buffer space.
Figure 3.44 plots the performance of Host A’s connection under this first scenario.
The left graph plots the per-connection throughput  (number of bytes per Figure 3.43 Congestion scenario 1: Two connections sharing a single hop with infinite buffers Figure 3.44 Congestion scenario 1: Throughput and delay as a function of host sending ratein in
second at the receiver) as a function of the connection-sending rate.
For a sending rate between 0 and R/2, the throughput at the receiver equals the sender’s sending rate—everything sent by the sender is received at the receiver with a finite delay.
When the sending rate is above R/2, however, the throughput is only R/2.
This upper limit on throughput is a consequence of the sharing of link capacity between two connections.
The link simply cannot deliver packets to a receiver at a steady-state rate that exceeds R/2.
No matter how high Hosts A and B set their sending rates, they will each never see a throughput higher than R/2.
Achieving a per-connection throughput of R/2 might actually appear to be a good thing, because the link is fully utilized in delivering packets to their destinations.
The right-hand graph in Figure 3.44, however, shows the consequence of operating near link capacity.
As the sending rate approaches R/2 (from the left), the average delay becomes larger and larger.
When the sending rate exceeds R/2, the average number of queued packets in the router is unbounded, and the average delay between source and destination becomes infinite (assuming that the connections operate at these sending rates for an infinite period of time and there is an infinite amount of buffering available).
Thus, while operating at an aggregate throughput of near R may be ideal from a throughput standpoint, it is far from ideal from a delay standpoint.
Even in this (extremely) idealized scenario, we’ve already found one cost of a congested network—large queuing delays are experienced as the packet-arrival rate nears the link capacity.
Scenario 2: Two Senders and a Router with Finite Buffers Let’s now slightly modify scenario 1 in the following two ways (see Figure 3.45).
First, the amount of router buffering is assumed to be finite.
A consequence of this real-world assumption is that packets will be dropped when arriving to an already-full buffer.
Second, we assume that each connection is reliable.
If a packet containing
Figure 3.45 Scenario 2: Two hosts (with retransmissions) and a router with finite buffers a transport-level segment is dropped at the router, the sender will eventually retransmit it.
Because packets can be retransmitted, we must now be more careful with our use of the term sending rate .
Specifically, let us again denote the rate at which the application sends original data into the socket byλ bytes/sec.
The rate at which the transport layer sends segments (containing original data and retransmitted data) into the network will be denoted  bytes/sec.
 is sometimes referred to as the offered load to the network.
The performance realized under scenario 2 will now depend strongly on how retransmission is performed.
First, consider the unrealistic case that Host A is able to somehow (magically!)
determinewhether or not a buffer is free in the router and thus sends a packet only when a buffer is free.
In this case, no loss would occur, λ would be equal to , and the throughput of the connection would be equal to λ .
This case is shown in Figure 3.46(a).
From a throughput standpoint, performance is ideal— everything that is sent is received.
Note that the average host sending rate cannot exceed R/2 under this scenario, since packet loss is assumed never to occur.
Consider next the slightly more realistic case that the sender retransmits only when a packet is known for certain to be lost. (
Again, this assumption is a bit of a stretch.
However, it is possible that the sendinghost might set its timeout large enough to be virtually assured that a packet that has not beenacknowledged has been lost.)
In this case, the performance might look something like that shown in Figure 3.46(b).
To appreciate what is happening here, consider the case that the offered load,  (the rate of original data transmission plus retransmissions), equals R/2.
According to Figure 3.46(b), at this value of the offered load, the rate at which datain λ′in λ′in in λ′in in λ′in
Figure 3.46 Scenario 2 performance with finite buffers are delivered to the receiver application is R/3.
Thus, out of the 0.5 R units of data transmitted, 0.333R bytes/sec (on average) are original data and 0.166 R bytes/sec (on average) are retransmitted data.
We see here another cost of a congested network—the sender must perform retransmissions in order to compensate for dropped (lost) packets due to buffer overflow.
Finally, let us consider the case that the sender may time out prematurely and retransmit a packet that has been delayed in the queue but not yet lost.
In this case, both the original data packet and theretransmission may reach the receiver.
Of course, the receiver needs but one copy of this packet andwill discard the retransmission.
In this case, the work done by the router in forwarding the retransmittedcopy of the original packet was wasted, as the receiver will have already received the original copy of this packet.
The router would have better used the link transmission capacity to send a different packet instead.
Here then is yet another cost of a congested network—unneeded retransmissions by the sender in the face of large delays may cause a router to use its link bandwidth to forward unneededcopies of a packet.
Figure 3.46 (c) shows the throughput versus offered load when each packet is assumed to be forwarded (on average) twice by the router.
Since each packet is forwarded twice, the throughput will have an asymptotic value of R/4 as the offered load approaches R/2.
Scenario 3: Four Senders, Routers with Finite Buffers, and Multihop Paths In our final congestion scenario, four hosts transmit packets, each over overlapping two-hop paths, as shown in Figure 3.47.
We again assume that each host uses a timeout/retransmission mechanism to implement a reliable data transfer service, that all hosts have the same value of λ, and that all router links have capacity R bytes/sec.
in
Figure 3.47 Four senders, routers with finite buffers, and multihop paths Let’s consider the connection from Host A to Host C, passing through routers R1 and R2.
The A–C connection shares router R1 with the D–B connection and shares router R2 with the B–D connection.
For extremely small values of λ, buffer overflows are rare (as in congestion scenarios 1 and 2), and the throughput approximately equals the offered load.
For slightly larger values of λ, the corresponding throughput is also larger, since more original data is being transmitted into the network and delivered to the destination, and overflows are still rare.
Thus, for small values of λ, an increase in λ results in an increase in λ.
Having considered the case of extremely low traffic, let’s next examine the case that λ (and hence ) is extremely large.
Consider router R2.
The A–C traffic arriving to router R2 (which arrives at R2 after being forwarded from R1) can have an arrival rate at R2 that is at most R, the capacity of the link from R1 to R2, regardless of the value of λ.
If  is extremely large for all connections (including thein in in in out in λ′in inλ′in
Figure 3.48 Scenario 3 performance with finite buffers and multihop paths B–D connection), then the arrival rate of B–D traffic at R2 can be much larger than that of the A–C traffic.
Because the A–C and B–D traffic must compete at router R2 for the limited amount of buffer space, the amount of A–C traffic that successfully gets through R2 (that is, is not lost due to bufferoverflow) becomes smaller and smaller as the offered load from B–D gets larger and larger.
In the limit,as the offered load approaches infinity, an empty buffer at R2 is immediately filled by a B–D packet, and the throughput of the A–C connection at R2 goes to zero.
This, in turn, implies that the A–C end-to-end throughput goes to zero  in the limit of heavy traffic.
These considerations give rise to the offered load versus throughput tradeoff shown in Figure 3.48.
The reason for the eventual decrease in throughput with increasing offered load is evident when one considers the amount of wasted work done by the network.
In the high-traffic scenario outlined above, whenever a packet is dropped at a second-hop router, the work done by the first-hop router inforwarding a packet to the second-hop router ends up being “wasted.”
The network would have beenequally well off (more accurately, equally bad off) if the first router had simply discarded that packet andremained idle.
More to the point, the transmission capacity used at the first router to forward the packetto the second router could have been much more profitably used to transmit a different packet. (
Forexample, when selecting a packet for transmission, it might be better for a router to give priority to packets that have already traversed some number of upstream routers.)
So here we see yet another cost of dropping a packet due to congestion—when a packet is dropped along a path, the transmission capacity that was used at each of the upstream links to forward that packet to the point at which it is dropped ends up having been wasted.
3.6.2 Approaches to Congestion Control In Section 3.7, we’ll examine TCP’s specific approach to congestion control in great detail.
Here, we identify the two broad approaches to congestion control that are taken in practice and discuss specific
network architectures and congestion-control protocols embodying these approaches.
At the highest level, we can distinguish among congestion-control approaches by whether the network layer provides explicit assistance to the transport layer for congestion-control purposes: End-to-end congestion control.
 In an end-to-end approach to congestion control, the network layer provides no explicit support to the transport layer for congestion-control purposes.
Even thepresence of network congestion must be inferred by the end systems based only on observed network behavior (for example, packet loss and delay).
We’ll see shortly in Section 3.7.1 that TCP takes this end-to-end approach toward congestion control, since the IP layer is not required to provide feedback to hosts regarding network congestion.
TCP segment loss (as indicated by atimeout or the receipt of three duplicate acknowledgments) is taken as an indication of networkcongestion, and TCP decreases its window size accordingly.
We’ll also see a more recent proposalfor TCP congestion control that uses increasing round-trip segment delay as an indicator ofincreased network congestion Network-assisted congestion control.
 With network-assisted congestion control, routers provide explicit feedback to the sender and/or receiver regarding the congestion state of the network.
Thisfeedback may be as simple as a single bit indicating congestion at a link – an approach taken in the early IBM SNA [Schwartz 1982], DEC DECnet [Jain 1989; Ramakrishnan 1990]  architectures, and ATM [Black 1995]  network architectures.
More sophisticated feedback is also possible.
For example, in ATM Available Bite Rate (ABR)  congestion control, a router informs the sender of the maximum host sending rate it (the router) can support on an outgoing link.
As noted above, the Internet-default versions of IP and TCP adopt an end-to-end approach towards congestion control.
We’ll see, however, in Section 3.7.2 that, more recently, IP and TCP may also optionally implement network-assisted congestion control.
For network-assisted congestion control, congestion information is typically fed back from the network tothe sender in one of two ways, as shown in Figure 3.49.
Direct feedback may be sent from a network router to the sender.
This form of notification typically takes the form of a choke packet (essentially saying, “I’m congested!”).
The second and more common form of notification occurs when a routermarks/updates a field in a packet flowing from sender to receiver to indicate congestion.
Upon receipt ofa marked packet, the receiver then notifies the sender of the congestion indication.
This latter form ofnotification takes a full round-trip time.
Figure 3.49 Two feedback pathways for network-indicated congestion information
3.7 TCP Congestion Control In this section we return to our study of TCP.
As we learned in Section 3.5, TCP provides a reliable transport service between two processes running on different hosts.
Another key component of TCP is its congestion-control mechanism.
As indicated in the previous section, TCP must use end-to-endcongestion control rather than network-assisted congestion control, since the IP layer provides no explicit feedback to the end systems regarding network congestion.
The approach taken by TCP is to have each sender limit the rate at which it sends traffic into its connection as a function of perceived network congestion.
If a TCP sender perceives that there is littlecongestion on the path between itself and the destination, then the TCP sender increases its send rate;if the sender perceives that there is congestion along the path, then the sender reduces its send rate.
But this approach raises three questions.
First, how does a TCP sender limit the rate at which it sendstraffic into its connection?
Second, how does a TCP sender perceive that there is congestion on thepath between itself and the destination?
And third, what algorithm should the sender use to change its send rate as a function of perceived end-to-end congestion?
Let’s first examine how a TCP sender limits the rate at which it sends traffic into its connection.
In Section 3.5 we saw that each side of a TCP connection consists of a receive buffer, a send buffer, and several variables ( LastByteRead , rwnd , and so on).
The TCP congestion-control mechanism operating at the sender keeps track of an additional variable, the congestion window .
The congestion window, denoted cwnd , imposes a constraint on the rate at which a TCP sender can send traffic into the network.
Specifically, the amount of unacknowledged data at a sender may not exceed the minimum of cwnd  and rwnd , that is: In order to focus on congestion control (as opposed to flow control), let us henceforth assume that the TCP receive buffer is so large that the receive-window constraint can be ignored; thus, the amount of unacknowledged data at the sender is solely limited by cwnd .
We will also assume that the sender always has data to send, i.e., that all segments in the congestion window are sent.
The constraint above limits the amount of unacknowledged data at the sender and therefore indirectly limits the sender’s send rate.
To see this, consider a connection for which loss and packet transmissiondelays are negligible.
Then, roughly, at the beginning of every RTT, the constraint permits the sender toLastByteSent −LastByteAcked ≤min{cwnd, rwnd}
send cwnd  bytes of data into the connection; at the end of the RTT the sender receives acknowledgments for the data.
Thus the sender’s send rate is roughly cwnd/RTT bytes/sec.
By adjusting the value of cwnd , the sender can therefore adjust the rate at which it sends data into its connection.
Let’s next consider how a TCP sender perceives that there is congestion on the path between itself and the destination.
Let us define a “loss event” at a TCP sender as the occurrence of either a timeout or the receipt of three duplicate ACKs from the receiver. (
Recall our discussion in Section 3.5.4 of the timeout event in Figure 3.33 and the subsequent modification to include fast retransmit on receipt of three duplicate ACKs.)
When there is excessive congestion, then one (or more) router buffers along the path overflows, causing a datagram (containing a TCP segment) to be dropped.
The dropped datagram, inturn, results in a loss event at the sender—either a timeout or the receipt of three duplicate ACKs—which is taken by the sender to be an indication of congestion on the sender-to-receiver path.
Having considered how congestion is detected, let’s next consider the more optimistic case when the network is congestion-free, that is, when a loss event doesn’t occur.
In this case, acknowledgments for previously unacknowledged segments will be received at the TCP sender.
As we’ll see, TCP will take the arrival of these acknowledgments as an indication that all is well—that segments being transmittedinto the network are being successfully delivered to the destination—and will use acknowledgments toincrease its congestion window size (and hence its transmission rate).
Note that if acknowledgmentsarrive at a relatively slow rate (e.g., if the end-end path has high delay or contains a low-bandwidth link),then the congestion window will be increased at a relatively slow rate.
On the other hand, ifacknowledgments arrive at a high rate, then the congestion window will be increased more quickly.
Because TCP uses acknowledgments to trigger (or clock) its increase in congestion window size, TCPis said to be self-clocking.
Given the mechanism  of adjusting the value of cwnd  to control the sending rate, the critical question remains: How should a TCP sender determine the rate at which it should send?
If TCP senders collectively send too fast, they can congest the network, leading to the type of congestion collapse that we saw in Figure 3.48.
Indeed, the version of TCP that we’ll study shortly was developed in response to observed Internet congestion collapse [Jacobson 1988] under earlier versions of TCP.
However, if TCP senders are too cautious and send too slowly, they could under utilize the bandwidth in the network; thatis, the TCP senders could send at a higher rate without congesting the network.
How then do the TCPsenders determine their sending rates such that they don’t congest the network but at the same timemake use of all the available bandwidth?
Are TCP senders explicitly coordinated, or is there adistributed approach in which the TCP senders can set their sending rates based only on localinformation?
TCP answers these questions using the following guiding principles: A lost segment implies congestion, and hence, the TCP sender’s rate should be decreasedwhen a segment is lost.
 Recall from our discussion in Section 3.5.4, that a timeout event or the
receipt of four acknowledgments for a given segment (one original ACK and then three duplicate ACKs) is interpreted as an implicit “loss event” indication of the segment following the quadruply ACKed segment, triggering a retransmission of the lost segment.
From a congestion-controlstandpoint, the question is how the TCP sender should decrease its congestion window size, andhence its sending rate, in response to this inferred loss event.
An acknowledged segment indicates that the network is delivering the sender’s segments tothe receiver, and hence, the sender’s rate can be increased when an ACK arrives for apreviously unacknowledged segment.
 The arrival of acknowledgments is taken as an implicit indication that all is well—segments are being successfully delivered from sender to receiver, and the network is thus not congested.
The congestion window size can thus be increased.
Bandwidth probing.
 Given ACKs indicating a congestion-free source-to-destination path and loss events indicating a congested path, TCP’s strategy for adjusting its transmission rate is to increase its rate in response to arriving ACKs until a loss event occurs, at which point, the transmission rate isdecreased.
The TCP sender thus increases its transmission rate to probe for the rate that at which congestion onset begins, backs off from that rate, and then to begins probing again to see if the congestion onset rate has changed.
The TCP sender’s behavior is perhaps analogous to the childwho requests (and gets) more and more goodies until finally he/she is finally told “No!”,
backs off abit, but then begins making requests again shortly afterwards.
Note that there is no explicit signalingof congestion state by the network—ACKs and loss events serve as implicit signals—and that eachTCP sender acts on local information asynchronously from other TCP senders.
Given this overview of TCP congestion control, we’re now in a position to consider the details of the celebrated TCP congestion-control algorithm , which was first described in [Jacobson 1988] and is standardized in [RFC 5681] .
The algorithm has three major components: (1) slow start, (2) congestion avoidance, and (3) fast recovery.
Slow start and congestion avoidance are mandatory components ofTCP, differing in how they increase the size of cwnd  in response to received ACKs.
We’ll see shortly that slow start increases the size of cwnd  more rapidly (despite its name!)
than congestion avoidance.
Fast recovery is recommended, but not required, for TCP senders.
Slow StartWhen a TCP connection begins, the value of cwnd  is typically initialized to a small value of 1 MSS [RFC 3390] , resulting in an initial sending rate of roughly MSS/RTT.
For example, if MSS = 500 bytes and RTT = 200 msec, the resulting initial sending rate is only about 20 kbps.
Since the available bandwidth to the TCP sender may be much larger than MSS/RTT, the TCP sender would like to find the amount of available bandwidth quickly.
Thus, in the slow-start  state, the value of cwnd  begins at 1 MSS and increases by 1 MSS every time a transmitted segment is first acknowledged.
In the example ofFigure 3.50, TCP sends the first segment into the network
Figure 3.50 TCP slow start and waits for an acknowledgment.
When this acknowledgment arrives, the TCP sender increases the congestion window by one MSS and sends out two maximum-sized segments.
These segments are then acknowledged, with the sender increasing the congestion window by 1 MSS for each of theacknowledged segments, giving a congestion window of 4 MSS, and so on.
This process results in adoubling of the sending rate every RTT.
Thus, the TCP send rate starts slow but grows exponentiallyduring the slow start phase.
But when should this exponential growth end?
Slow start provides several answers to this question.
First, if there is a loss event (i.e., congestion) indicated by a timeout, the TCP sender sets the value of cwnd  to 1 and begins the slow start process anew.
It also sets the value of a second state variable, ssthresh  (shorthand for “slow start threshold”) to cwnd/2 —half of the value of the congestion window value when congestion was detected.
The second way in which slow start may end is directly tied to the value of ssthresh .
Since ssthresh  is half the value of cwnd  when congestion was last detected, it might be a bit reckless to keep doubling cwnd  when it reaches or surpasses the value of ssthresh .
Thus, when the value of cwnd  equals ssthresh , slow start ends and TCP transitions into congestion avoidance mode.
As we’ll see, TCP increases cwnd  more cautiously when in congestion-avoidance mode.
The final way in which slow start can end is if three duplicate ACKs are
detected, in which case TCP performs a fast retransmit (see Section 3.5.4) and enters the fast recovery state, as discussed below.
TCP’s behavior in slow start is summarized in the FSM description of TCP congestion control in Figure 3.51.
The slow-start algorithm traces it roots to [Jacobson 1988]; an approach similar to slow start was also proposed independently in [Jain 1986].
Congestion Avoidance On entry to the congestion-avoidance state, the value of cwnd  is approximately half its value when congestion was last encountered—congestion could be just around the corner!
Thus, rather than doubling the value of cwnd  every RTT, TCP adopts a more conservative approach and increases the value of cwnd  by just a single MSS every RTT [RFC 5681] .
This can be accomplished in several ways.
A common approach is for the TCP sender to increase cwnd  by MSS bytes (MSS/ cwnd ) whenever a new acknowledgment arrives.
For example, if MSS is 1,460 bytes and cwnd  is 14,600 bytes, then 10 segments are being sent within an RTT.
Each arriving ACK (assuming one ACK per segment) increases the congestion window size by 1/10 MSS, and thus, the value of the congestion window will haveincreased by one MSS after ACKs when all 10 segments have been received.
But when should congestion avoidance’s linear increase (of 1 MSS per RTT) end?
TCP’s congestion- avoidance algorithm behaves the same when a timeout occurs.
As in the case of slow start: The value of cwnd  is set to 1 MSS, and the value of ssthresh  is updated to half the value of cwnd  when the loss event occurred.
Recall, however, that a loss event also can be triggered by a triple duplicate ACK event.
Figure 3.51 FSM description of TCP congestion control In this case, the network is continuing to deliver segments from sender to receiver (as indicated by the receipt of duplicate ACKs).
So TCP’s behavior to this type of loss event should be less drastic than with a timeout-indicated loss: TCP halves the value of cwnd  (adding in 3 MSS for good measure to account for the triple duplicate ACKs received) and records the value of ssthresh  to be half the value of cwnd when the triple duplicate ACKs were received.
The fast-recovery state is then entered.
Fast Recovery In fast recovery, the value of cwnd  is increased by 1 MSS for every duplicate ACK received for the missing segment that caused TCP to enter the fast-recovery state.
Eventually, when an ACK arrives for the missing segment, TCP enters the
Examining the behavior of TCP PRINCIPLES IN PRACTICE TCP SPLITTING: OPTIMIZING THE PERFORMANCE OF CLOUD SERVICES For cloud services such as search, e-mail, and social networks, it is desirable to provide a high- level of responsiveness, ideally giving users the illusion that the services are running within theirown end systems (including their smartphones).
This can be a major challenge, as users areoften located far away from the data centers responsible for serving the dynamic contentassociated with the cloud services.
Indeed, if the end system is far from a data center, then the RTT will be large, potentially leading to poor response time performance due to TCP slow start.
As a case study, consider the delay in receiving a response for a search query.
Typically, the server requires three TCP windows during slow start to deliver the response [Pathak 2010] .
Thus the time from when an end system initiates a TCP connection until the time when it receives the last packet of the response is roughly  (one RTT to set up the TCP connection plus three RTTs for the three windows of data) plus the processing time in the datacenter.
These RTT delays can lead to a noticeable delay in returning search results for asignificant fraction of queries.
Moreover, there can be significant packet loss in access networks, leading to TCP retransmissions and even larger delays.
One way to mitigate this problem and improve user-perceived performance is to (1) deploy front- end servers closer to the users, and (2) utilize TCP splitting  by breaking the TCP connection at the front-end server.
With TCP splitting, the client establishes a TCP connection to the nearbyfront-end, and the front-end maintains a persistent TCP connection to the data center with a very large TCP congestion window [Tariq 2008, Pathak 2010 , Chen 2011].
With this approach, the response time roughly becomes  processing time, where RTT  is the round- trip time between client and front-end server, and RTT  is the round-trip time between the front- end server and the data center (back-end server).
If the front-end server is close to client, thenthis response time approximately becomes RTT plus processing time, since RTT  is negligibly small and RTT  is approximately RTT.
In summary, TCP splitting can reduce the networking delay roughly from  to RTT, significantly improving user-perceived performance, particularly for users who are far from the nearest data center.
TCP splitting also helps reduce TCP retransmission delays caused by losses in access networks.
Google and Akamai have made extensive use of their CDN servers in access networks (recall our discussion in Section 2.6) to perform TCP splitting for the cloud services they support [Chen 2011].4⋅RTT 4⋅RTTFE+RTTBE+ FE BE FE BE 4⋅RTT
congestion-avoidance state after deflating cwnd .
If a timeout event occurs, fast recovery transitions to the slow-start state after performing the same actions as in slow start and congestion avoidance: The value of cwnd  is set to 1 MSS, and the value of ssthresh  is set to half the value of cwnd  when the loss event occurred.
Fast recovery is a recommended, but not required, component of TCP [RFC 5681] .
It is interesting that an early version of TCP, known as TCP Tahoe, unconditionally cut its congestion window to 1 MSS and entered the slow-start phase after either a timeout-indicated or triple-duplicate-ACK-indicated loss event.
The newer version of TCP, TCP Reno, incorporated fast recovery.
Figure 3.52 illustrates the evolution of TCP’s congestion window for both Reno and Tahoe.
In this figure, the threshold is initially equal to 8 MSS.
For the first eight transmission rounds, Tahoe and Reno take identical actions.
The congestion window climbs exponentially fast during slow start and hits the threshold at the fourth round of transmission.
The congestion window then climbs linearly until a tripleduplicate- ACK event occurs, just after transmission round 8.
Note that the congestion window is  when this loss event occurs.
The value of ssthresh  is then set to  cwnd   Under TCP Reno, the congestion window is set to cwnd    and then grows linearly.
Under TCP Tahoe, the congestion window is set to 1 MSS and grows exponentially until it reaches the value of ssthresh , at which point it grows linearly.
Figure 3.51 presents the complete FSM description of TCP’s congestion-control algorithms—slow start, congestion avoidance, and fast recovery.
The figure also indicates where transmission of new segments or retransmitted segments can occur.
Although it is important to distinguish between TCP errorcontrol/retransmission and TCP congestion control, it’s also important to appreciate how these twoaspects of TCP are inextricably linked.
TCP Congestion Control: Retrospective Having delved into the details of slow start, congestion avoidance, and fast recovery, it’s worthwhile to now step back and view the forest from the trees.
Ignoring the12⋅MSS 0.5⋅ =6⋅MSS.
=9⋅MSS
Figure 3.52 Evolution of TCP’s congestion window (Tahoe and Reno) Figure 3.53 Additive-increase, multiplicative-decrease congestion control initial slow-start period when a connection begins and assuming that losses are indicated by triple duplicate ACKs rather than timeouts, TCP’s congestion control consists of linear (additive) increase in cwnd  of 1 MSS per RTT and then a halving (multiplicative decrease) of cwnd  on a triple duplicate-ACK event.
For this reason, TCP congestion control is often referred to as an additive-increase, multiplicative-decrease (AIMD)  form of congestion control.
AIMD congestion control gives rise to the “saw tooth” behavior shown in Figure 3.53, which also nicely illustrates our earlier intuition of TCP “probing” for bandwidth—TCP linearly increases its congestion window size (and hence its transmissionrate) until a triple duplicate-ACK event occurs.
It then decreases its congestion window size by a factor of two but then again begins increasing it linearly, probing to see if there is additional availablebandwidth.
As noted previously, many TCP implementations use the Reno algorithm [Padhye 2001] .
Many variations of the Reno algorithm have been proposed [RFC 3782 ; RFC 2018] .
The TCP Vegas algorithm [Brakmo 1995 ; Ahn 1995] attempts to avoid congestion while maintaining good throughput.
The basic idea of Vegas is to (1) detect congestion in the routers between source and destination before packet loss occurs, and (2) lower the rate linearly when this imminent packet loss is detected.
Imminent packet loss is predicted by observing the RTT.
The longer the RTT of the packets, the greater the congestion in the routers.
As of late 2015, the Ubuntu Linux implementation of TCP provided slowstart,congestion avoidance, fast recovery, fast retransmit, and SACK, by default; alternative congestion control algorithms, such as TCP Vegas and BIC [Xu 2004], are also provided.
For a survey of the many flavors of TCP, see [Afanasyev 2010] .
TCP’s AIMD algorithm was developed based on a tremendous amount of engineering insight and experimentation with congestion control in operational networks.
Ten years after TCP’s development, theoretical analyses showed that TCP’s congestion-control algorithm serves as a distributed asynchronous-optimization algorithm that results in several important aspects of user and network performance being simultaneously optimized [Kelly 1998] .
A rich theory of congestion control has since been developed [Srikant 2004] .
Macroscopic Description of TCP Throughput Given the saw-toothed behavior of TCP, it’s natural to consider what the average throughput (that is, the average rate) of a long-lived TCP connection might be.
In this analysis we’ll ignore the slow-start phasesthat occur after timeout events. (
These phases are typically very short, since the sender grows out of the phase exponentially fast.)
During a particular round-trip interval, the rate at which TCP sends data is a function of the congestion window and the current RTT.
When the window size is w bytes and the current round-trip time is RTT seconds, then TCP’s transmission rate is roughly w/RTT.
TCP then probes for additional bandwidth by increasing w by 1 MSS each RTT until a loss event occurs.
Denote by W the value of w when a loss event occurs.
Assuming that RTT and W are approximately constant over the duration of the connection, the TCP transmission rate ranges from W/(2 · RTT) to W/RTT .
These assumptions lead to a highly simplified macroscopic model for the steady-state behavior of TCP.The network drops a packet from the connection when the rate increases to W/RTT;  the rate is then cut in half and then increases by MSS/ RTT every RTT until it again reaches W/RTT .
This process repeats itself over and over again.
Because TCP’s throughput (that is, rate) increases linearly between the two extreme values, we have Using this highly idealized model for the steady-state dynamics of TCP, we can also derive an interesting expression that relates a connection’s loss rate to its available bandwidth [Mahdavi 1997].
average throughput of a connection =0.75⋅WRTT
This derivation is outlined in the homework problems.
A more sophisticated model that has been found empirically to agree with measured data is [Padhye 2000] .
TCP Over High-Bandwidth Paths It is important to realize that TCP congestion control has evolved over the years and indeed continues to evolve.
For a summary of current TCP variants and discussion of TCP evolution, see [Floyd 2001, RFC 5681 , Afanasyev 2010] .
What was good for the Internet when the bulk of the TCP connections carried SMTP, FTP, and Telnet traffic is not necessarily good for today’s HTTP-dominated Internet or for a future Internet with services that are still undreamed of.
The need for continued evolution of TCP can be illustrated by considering the high-speed TCP connections that are needed for grid- and cloud-computing applications.
For example, consider a TCP connection with 1,500-byte segments and a 100 ms RTT, and suppose we want to send data through this connection at 10 Gbps.
Following [RFC 3649] , we note that using the TCP throughput formula above, in order to achieve a 10 Gbps throughput, the average congestion window size would need to be83,333 segments.
That’s a lot of segments, leading us to be rather concerned that one of these 83,333 in-flight segments might be lost.
What would happen in the case of a loss?
Or, put another way, what fraction of the transmitted segments could be lost that would allow the TCP congestion-control algorithm specified in Figure 3.51 still to achieve the desired 10 Gbps rate?
In the homework questions for this chapter, you are led through the derivation of a formula relating the throughput of a TCP connection as afunction of the loss rate (L), the round-trip time (RTT), and the maximum segment size (MSS): Using this formula, we can see that in order to achieve a throughput of 10 Gbps, today’s TCP congestion-control algorithm can only tolerate a segment loss probability of 2 · 10  (or equivalently, one loss event for every 5,000,000,000 segments)—a very low rate.
This observation has led a number of researchers to investigate new versions of TCP that are specifically designed for such high-speed environments; see [Jin 2004; Kelly 2003 ; Ha 2008 ; RFC 7323]  for discussions of these efforts.
3.7.1 Fairness Consider K TCP connections, each with a different end-to-end path, but all passing through a bottleneck link with transmission rate R bps. (
By bottleneck link , we mean that for each connection, all the other links along the connection’s path are not congested and have abundant transmission capacity as compared with the transmission capacity of the bottleneck link.)
Suppose each connection is transferring a large file and there is no UDP traffic passing through the bottleneck link.
A congestion-control mechanism is said to be fair if the average transmission rate of each connection is approximately R/K;average throughput of a connection =1.22⋅MSSRTTL –10
that is, each connection gets an equal share of the link bandwidth.
Is TCP’s AIMD algorithm fair, particularly given that different TCP connections may start at different times and thus may have different window sizes at a given point in time? [
Chiu 1989] provides an elegant and intuitive explanation of why TCP congestion control converges to provide an equal share of a bottleneck link’s bandwidth among competing TCP connections.
Let’s consider the simple case of two TCP connections sharing a single link with transmission rate R, as shown in Figure 3.54.
Assume that the two connections Figure 3.54 Two TCP connections sharing a single bottleneck link have the same MSS and RTT (so that if they have the same congestion window size, then they have the same throughput), that they have a large amount of data to send, and that no other TCP connections or UDP datagrams traverse this shared link.
Also, ignore the slow-start phase of TCP and assume the TCPconnections are operating in CA mode (AIMD) at all times.
Figure 3.55 plots the throughput realized by the two TCP connections.
If TCP is to share the link bandwidth equally between the two connections, then the realized throughput should fall along the 45- degree arrow (equal bandwidth share) emanating from the origin.
Ideally, the sum of the two throughputs should equal R. (Certainly, each connection receiving an equal, but zero, share of the link capacity is not a desirable situation!)
So the goal should be to have the achieved throughputs fall somewhere near the intersection of the equal bandwidth share line and the full bandwidth utilization line in Figure 3.55.
Suppose that the TCP window sizes are such that at a given point in time, connections 1 and 2 realize throughputs indicated by point A in Figure 3.55.
Because the amount of link bandwidth jointly consumed by the two connections is less than R, no loss will occur, and both connections will increase their window by 1 MSS per RTT as a result of TCP’s congestion-avoidance algorithm.
Thus, the joint throughput of the two connections proceeds along a 45-degree line (equal increase for both
connections) starting from point A. Eventually, the link bandwidth jointly consumed by the two connections will be greater than R, and eventually packet loss will occur.
Suppose that connections 1 and 2 experience packet loss when they realize throughputs indicated by point B. Connections 1 and 2 then decrease their windows by a factor of two.
The resulting throughputs realized are thus at point C, halfway along a vector starting at B and ending at the origin.
Because the joint bandwidth use is less than R at point C, the two connections again increase their throughputs along a 45-degree line starting from C. Eventually, loss will again occur, for example, at point D, and the two connections again decrease their window sizes by a factor of two, and so on.
You should convince yourself that the bandwidth realized by the two connections eventually fluctuates along the equal bandwidth share line.
You should also convince Figure 3.55 Throughput realized by TCP connections 1 and 2 yourself that the two connections will converge to this behavior regardless of where they are in the two- dimensional space!
Although a number of idealized assumptions lie behind this scenario, it still provides an intuitive feel for why TCP results in an equal sharing of bandwidth among connections.
In our idealized scenario, we assumed that only TCP connections traverse the bottleneck link, that the connections have the same RTT value, and that only a single TCP connection is associated with a host-destination pair.
In practice, these conditions are typically not met, and client-server applications canthus obtain very unequal portions of link bandwidth.
In particular, it has been shown that when multipleconnections share a common bottleneck, those sessions with a smaller RTT are able to grab theavailable bandwidth at that link more quickly as it becomes free (that is, open their congestion windows faster) and thus will enjoy higher throughput than those connections with larger RTTs  [Lakshman
1997] .
Fairness and UDP We have just seen how TCP congestion control regulates an application’s transmission rate via the congestion window mechanism.
Many multimedia applications, such as Internet phone and video conferencing, often do not run over TCP for this very reason—they do not want their transmission ratethrottled, even if the network is very congested.
Instead, these applications prefer to run over UDP,which does not have built-in congestion control.
When running over UDP, applications can pump their audio and video into the network at a constant rate and occasionally lose packets, rather than reducetheir rates to “fair” levels at times of congestion and not lose any packets.
From the perspective of TCP,the multimedia applications running over UDP are not being fair—they do not cooperate with the otherconnections nor adjust their transmission rates appropriately.
Because TCP congestion control will decrease its transmission rate in the face of increasing congestion (loss), while UDP sources need not, it is possible for UDP sources to crowd out TCP traffic.
An area of research today is thus thedevelopment of congestion-control mechanisms for the Internet that prevent UDP traffic from bringing the Internet’s throughput to a grinding halt [Floyd 1999; Floyd 2000; Kohler 2006; RFC 4340] .
Fairness and Parallel TCP Connections But even if we could force UDP traffic to behave fairly, the fairness problem would still not be completely solved.
This is because there is nothing to stop a TCP-based application from using multiple parallel connections.
For example, Web browsers often use multiple parallel TCP connections to transfer the multiple objects within a Web page. (
The exact number of multiple connections is configurable in mostbrowsers.)
When an application uses multiple parallel connections, it gets a larger fraction of the bandwidth in a congested link.
As an example, consider a link of rate R supporting nine ongoing client- server applications, with each of the applications using one TCP connection.
If a new application comes along and also uses one TCP connection, then each application gets approximately the same transmission rate of R/10.
But if this new application instead uses 11 parallel TCP connections, then the new application gets an unfair allocation of more than R/2.
Because Web traffic is so pervasive in the Internet, multiple parallel connections are not uncommon.
3.7.2 Explicit Congestion Notification (ECN): Network-assisted Congestion Control Since the initial standardization of slow start and congestion avoidance in the late 1980’s [RFC 1122] , TCP has implemented the form of end-end congestion control that we studied in Section 3.7.1: a TCP sender receives no explicit congestion indications from the network layer, and instead infers congestion through observed packet loss.
More recently, extensions to both IP and TCP [RFC 3168]  have been proposed, implemented, and deployed that allow the network to explicitly signal congestion to a TCP
sender and receiver.
This form of network-assisted congestion control is known as Explicit Congestion Notification .
As shown in Figure 3.56, the TCP and IP protocols are involved.
At the network layer, two bits (with four possible values, overall) in the Type of Service field of the IP datagram header (which we’ll discuss in Section 4.3) are used for ECN.
One setting of the ECN bits is used by a router to indicate that it (the Figure 3.56 Explicit Congestion Notification: network-assisted congestion control router) is experiencing congestion.
This congestion indication is then carried in the marked IP datagramto the destination host, which then informs the sending host, as shown in Figure 3.56.
RFC 3168 does not provide a definition of when a router is congested; that decision is a configuration choice made possible by the router vendor, and decided by the network operator.
However, RFC 3168 doesrecommend that an ECN congestion indication be set only in the face of persistent congestion.
Asecond setting of the ECN bits is used by the sending host to inform routers that the sender and receiverare ECN-capable, and thus capable of taking action in response to ECN-indicated network congestion.
As shown in Figure 3.56, when the TCP in the receiving host receives an ECN congestion indication via a received datagram, the TCP in the receiving host informs the TCP in the sending host of the congestion indication by setting the ECE (Explicit Congestion Notification Echo) bit (see Figure 3.29) in a receiver-to-sender TCP ACK segment.
The TCP sender, in turn, reacts to an ACK with an ECE congestion indication by halving the congestion window, as it would react to a lost segment using fastretransmit, and sets the CWR (Congestion Window Reduced) bit in the header of the next transmittedTCP sender-to-receiver segment.
Other transport-layer protocols besides TCP may also make use of network-layer-signaled ECN.
The Datagram Congestion Control Protocol (DCCP) [RFC 4340]  provides a low-overhead, congestion- controlled UDP-like unreliable service that utilizes ECN.
DCTCP (Data Center TCP) [Alizadeh 2010], a version of TCP designed specifically for data center networks, also makes use of ECN.
3.8 Summary We began this chapter by studying the services that a transport-layer protocol can provide to network applications.
At one extreme, the transport-layer protocol can be very simple and offer a no-frills serviceto applications, providing only a multiplexing/demultiplexing function for communicating processes.
TheInternet’s UDP protocol is an example of such a no-frills transport-layer protocol.
At the other extreme, atransport-layer protocol can provide a variety of guarantees to applications, such as reliable delivery of data, delay guarantees, and bandwidth guarantees.
Nevertheless, the services that a transport protocol can provide are often constrained by the service model of the underlying network-layer protocol.
If thenetwork-layer protocol cannot provide delay or bandwidth guarantees to transport-layer segments, thenthe transport-layer protocol cannot provide delay or bandwidth guarantees for the messages sentbetween processes.
We learned in Section 3.4 that a transport-layer protocol can provide reliable data transfer even if the underlying network layer is unreliable.
We saw that providing reliable data transfer has many subtle points, but that the task can be accomplished by carefully combining acknowledgments, timers, retransmissions, and sequence numbers.
Although we covered reliable data transfer in this chapter, we should keep in mind that reliable data transfer can be provided by link-, network-, transport-, or application-layer protocols.
Any of the upperfour layers of the protocol stack can implement acknowledgments, timers, retransmissions, andsequence numbers and provide reliable data transfer to the layer above.
In fact, over the years,engineers and computer scientists have independently designed and implemented link-, network-,transport-, and application-layer protocols that provide reliable data transfer (although many of these protocols have quietly disappeared).
In Section 3.5, we took a close look at TCP, the Internet’s connection-oriented and reliable transport- layer protocol.
We learned that TCP is complex, involving connection management, flow control, and round-trip time estimation, as well as reliable data transfer.
In fact, TCP is actually more complex thanour description—we intentionally did not discuss a variety of TCP patches, fixes, and improvements thatare widely implemented in various versions of TCP.
All of this complexity, however, is hidden from thenetwork application.
If a client on one host wants to send data reliably to a server on another host, it simply opens a TCP socket to the server and pumps data into that socket.
The client-server application is blissfully unaware of TCP’s complexity.
In Section 3.6, we examined congestion control from a broad perspective, and in Section 3.7, we showed how TCP implements congestion control.
We learned that congestion control is imperative for
the well-being of the network.
Without congestion control, a network can easily become gridlocked, with little or no data being transported end-to-end.
In Section 3.7 we learned that TCP implements an end- to-end congestion-control mechanism that additively increases its transmission rate when the TCP connection’s path is judged to be congestion-free, and multiplicatively decreases its transmission rate when loss occurs.
This mechanism also strives to give each TCP connection passing through a congested link an equal share of the link bandwidth.
We also examined in some depth the impact of TCP connection establishment and slow start on latency.
We observed that in many importantscenarios, connection establishment and slow start significantly contribute to end-to-end delay.
Weemphasize once more that while TCP congestion control has evolved over the years, it remains an areaof intensive research and will likely continue to evolve in the upcoming years.
Our discussion of specific Internet transport protocols in this chapter has focused on UDP and TCP—the two “work horses” of the Internet transport layer.
However, two decades of experience with these two protocols has identified circumstances in which neither is ideally suited.
Researchers have thus been busy developing additional transport-layer protocols, several of which are now IETF proposedstandards.
The Datagram Congestion Control Protocol (DCCP) [RFC 4340]  provides a low-overhead, message- oriented, UDP-like unreliable service, but with an application-selected form of congestion control that is compatible with TCP.
If reliable or semi-reliable data transfer is needed by an application, then thiswould be performed within the application itself, perhaps using the mechanisms we have studied in Section 3.4.
DCCP is envisioned for use in applications such as streaming media (see Chapter 9) that can exploit the tradeoff between timeliness and reliability of data delivery, but that want to be responsive to network congestion.
Google’s QUIC (Quick UDP Internet Connections) protocol [Iyengar 2016], implemented in Google’s Chromium browser, provides reliability via retransmission as well as error correction, fast-connectionsetup, and a rate-based congestion control algorithm that aims to be TCP friendly—all implemented asan application-level protocol on top of UDP.
In early 2015, Google reported that roughly half of allrequests from Chrome to Google servers are served over QUIC.
DCTCP (Data Center TCP) [Alizadeh 2010] is a version of TCP designed specifically for data center networks, and uses ECN to better support the mix of short- and long-lived flows that characterize data center workloads.
The Stream Control Transmission Protocol (SCTP) [RFC 4960 , RFC 3286]  is a reliable, message- oriented protocol that allows several different application-level “streams” to be multiplexed through a single SCTP connection (an approach known as “multi-streaming”).
From a reliability standpoint, thedifferent streams within the connection are handled separately, so that packet loss in one stream does not affect the delivery of data in other streams.
QUIC provides similar multi-stream semantics.
SCTP
also allows data to be transferred over two outgoing paths when a host is connected to two or more networks, optional delivery of out-of-order data, and a number of other features.
SCTP’s flow- and congestion-control algorithms are essentially the same as in TCP.
The TCP-Friendly Rate Control (TFRC) protocol [RFC 5348]  is a congestion-control protocol rather than a full-fledged transport-layer protocol.
It specifies a congestion-control mechanism that could be used in another transport protocol such as DCCP (indeed one of the two application-selectable protocols available in DCCP is TFRC).
The goal of TFRC is to smooth out the “saw tooth” behavior (see Fig­ure 3.53) in TCP congestion control, while maintaining a long-term sending rate that is “reasonably” close to that of TCP.
With a smoother sending rate than TCP, TFRC is well-suited for multimedia applications such as IP telephony or streaming media where such a smooth rate is important.
TFRC is an “equation- based” protocol that uses the measured packet loss rate as input to an equation [Padhye 2000]  that estimates what TCP’s throughput would be if a TCP session experiences that loss rate.
This rate is then taken as TFRC’s target sending rate.
Only the future will tell whether DCCP, SCTP, QUIC, or TFRC will see widespread deployment.
While these protocols clearly provide enhanced capabilities over TCP and UDP, TCP and UDP have proventhemselves “good enough” over the years.
Whether “better” wins out over “good enough” will depend ona complex mix of technical, social, and business considerations.
In Chapter 1, we said that a computer network can be partitioned into the “network edge” and the “network core.”
The network edge covers everything that happens in the end systems.
Having now covered the application layer and the transport layer, our discussion of the network edge is complete.
It is time to explore the network core!
This journey begins in the next two chapters, where we’ll study the network layer, and continues into Chapter 6, where we’ll study the link layer.
Homework Problems and Questions Chapter 3 Review Questions SECTIONS 3.1–3.3 R1.
Suppose the network layer provides the following service.
The network layer in the source host accepts a segment of maximum size 1,200 bytes and a destination host address from the transport layer.
The network layer then guarantees to deliver the segment to the transport layerat the destination host.
Suppose many network application processes can be running at thedestination host.
a. Design the simplest possible transport-layer protocol that will get application data to the desired process at the destination host.
Assume the operating system in the destination host has assigned a 4-byte port number to each running application process.
b. Modify this protocol so that it provides a “return address” to the destination process.
c. In your protocols, does the transport layer “have to do anything” in the core of the computer network?
R2.
Consider a planet where everyone belongs to a family of six, every family lives in its own house, each house has a unique address, and each person in a given house has a unique name.
Suppose this planet has a mail service that delivers letters from source house todestination house.
The mail service requires that (1) the letter be in an envelope, and that (2) theaddress of the destination house (and nothing more) be clearly written on the envelope.
Suppose each family has a delegate family member who collects and distributes letters for theother family members.
The letters do not necessarily provide any indication of the recipients ofthe letters.
a. Using the solution to Problem R1 above as inspiration, describe a protocol that the delegates can use to deliver letters from a sending family member to a receiving family member.
b. In your protocol, does the mail service ever have to open the envelope and examine the letter in order to provide its service?
R3.
Consider a TCP connection between Host A and Host B. Suppose that the TCP segments traveling from Host A to Host B have source port number x and destination port number y. What are the source and destination port numbers for the segments traveling from Host B to Host A?
SECTION 3.4 SECTION 3.5R4.
Describe why an application developer might choose to run an application over UDP rather than TCP.
R5.
Why is it that voice and video traffic is often sent over TCP rather than UDP in today’s Internet? (
Hint: The answer we are looking for has nothing to do with TCP’s congestion-control mechanism.)
R6.
Is it possible for an application to enjoy reliable data transfer even when the application runs over UDP?
If so, how?
R7.
Suppose a process in Host C has a UDP socket with port number 6789.
Suppose both Host A and Host B each send a UDP segment to Host C with destination port number 6789.
Will both of these segments be directed to the same socket at Host C?
If so, how will the process at HostC know that these two segments originated from two different hosts?
R8.
Suppose that a Web server runs in Host C on port 80.
Suppose this Web server uses persistent connections, and is currently receiving requests from two different Hosts, A and B. Are all of the requests being sent through the same socket at Host C?
If they are being passed through different sockets, do both of the sockets have port 80?
Discuss and explain.
R9.
In our rdt protocols, why did we need to introduce sequence numbers?
R10.
In our rdt protocols, why did we need to introduce timers?
R11.
Suppose that the roundtrip delay between sender and receiver is constant and known to the sender.
Would a timer still be necessary in protocol rdt 3.0 , assuming that packets can be lost?
Explain.
R12.
Visit the Go-Back-N Java applet at the companion Web site.
a. Have the source send five packets, and then pause the animation before any of the five packets reach the destination.
Then kill the first packet and resume the animation.
Describe what happens.
b. Repeat the experiment, but now let the first packet reach the destination and kill the first acknowledgment.
Describe again what happens.
c. Finally, try sending six packets.
What happens?
R13.
Repeat R12, but now with the Selective Repeat Java applet.
How are Selective Repeat andGo-Back-N different?
R14.
True or false?
a. Host A is sending Host B a large file over a TCP connection.
Assume Host B has no data to send Host A. Host B will not send acknowledgments to Host A because Host B cannot piggyback the acknowledgments on data.
SECTION 3.7 Problemsb.
The size of the TCP rwnd  never changes throughout the duration of the connection.
c. Suppose Host A is sending Host B a large file over a TCP connection.
The number of unacknowledged bytes that A sends cannot exceed the size of the receive buffer.
d. Suppose Host A is sending a large file to Host B over a TCP connection.
If the sequence number for a segment of this connection is m, then the sequence number for the subsequent segment will necessarily be .
e. The TCP segment has a field in its header for rwnd .
f. Suppose that the last SampleRTT  in a TCP connection is equal to 1 sec.
The current value of TimeoutInterval  for the connection will necessarily be  sec.
g. Suppose Host A sends one segment with sequence number 38 and 4 bytes of data over a TCP connection to Host B. In this same segment the acknowledgment number is necessarily 42.
R15.
Suppose Host A sends two TCP segments back to back to Host B over a TCP connection.
The first segment has sequence number 90; the second has sequence number 110.
a. How much data is in the first segment?
b. Suppose that the first segment is lost but the second segment arrives at B. In the acknowledgment that Host B sends to Host A, what will be the acknowledgment number?
R16.
Consider the Telnet example discussed in Section 3.5 .
A few seconds after the user types the letter ‘C,’ the user types the letter ‘R.’ After typing the letter ‘R,’ how many segments are sent, and what is put in the sequence number and acknowledgment fields of the segments?m+1 ≥1 R17.
Suppose two TCP connections are present over some bottleneck link of rate R bps.
Both connections have a huge file to send (in the same direction over the bottleneck link).
Thetransmissions of the files start at the same time.
What transmission rate would TCP like to giveto each of the connections?
R18.
True or false?
Consider congestion control in TCP.
When the timer expires at the sender, the value of ssthresh  is set to one half of its previous value.
R19.
In the discussion of TCP splitting in the sidebar in Section 3.7 , it was claimed that the response time with TCP splitting is approximately  Justify this claim.4⋅RTTFE+RTTBE+processing time .
P1.
Suppose Client A initiates a Telnet session with Server S. At about the same time, Client B
also initiates a Telnet session with Server S. Provide possible source and destination port numbers for a. The segments sent from A to S. b. The segments sent from B to S. c. The segments sent from S to A. d. The segments sent from S to B. e. If A and B are different hosts, is it possible that the source port number in the segments from A to S is the same as that from B to S?
f. How about if they are the same host?
P2.
Consider Figure 3.5 .
What are the source and destination port values in the segments flowing from the server back to the clients’ processes?
What are the IP addresses in the network-layer datagrams carrying the transport-layer segments?
P3.
UDP and TCP use 1s complement for their checksums.
Suppose you have the following three 8-bit bytes: 01010011, 01100110, 01110100.
What is the 1s complement of the sum of these 8-bit bytes? (
Note that although UDP and TCP use 16-bit words in computing thechecksum, for this problem you are being asked to consider 8-bit sums.)
Show all work.
Why is itthat UDP takes the 1s complement of the sum; that is, why not just use the sum?
With the 1scomplement scheme, how does the receiver detect errors?
Is it possible that a 1-bit error will goundetected?
How about a 2-bit error?
P4.
a. Suppose you have the following 2 bytes: 01011100 and 01100101.
What is the 1s complement of the sum of these 2 bytes?
b. Suppose you have the following 2 bytes: 11011010 and 01100101.
What is the 1scomplement of the sum of these 2 bytes?
c. For the bytes in part (a), give an example where one bit is flipped in each of the 2 bytesand yet the 1s complement doesn’t change.
P5.
Suppose that the UDP receiver computes the Internet checksum for the received UDP segment and finds that it matches the value carried in the checksum field.
Can the receiver be absolutely certain that no bit errors have occurred?
Explain.
P6.
Consider our motivation for correcting protocol rdt2.1 .
Show that the receiver, shown in Figure 3.57 , when operating with the sender shown in Figure 3.11 , can lead the sender and receiver to enter into a deadlock state, where each is waiting for an event that will never occur.
P7.
In protocol rdt3.0 , the ACK packets flowing from the receiver to the sender do not have sequence numbers (although they do have an ACK field that contains the sequence number of the packet they are acknowledging).
Why is it that our ACK packets do not require sequencenumbers?
Figure 3.57 An incorrect receiver for protocol rdt 2.1 P8.
Draw the FSM for the receiver side of protocol rdt3.0 .
P9.
Give a trace of the operation of protocol rdt3.0  when data packets and acknowledgment packets are garbled.
Your trace should be similar to that used in Figure 3.16 .
P10.
Consider a channel that can lose packets but has a maximum delay that is known.
Modify protocol rdt2.1  to include sender timeout and retransmit.
Informally argue why your protocol can communicate correctly over this channel.
P11.
Consider the rdt2.2  receiver in Figure 3.14 , and the creation of a new packet in the self-transition (i.e., the transition from the state back to itself) in the Wait-for-0-from-below andthe Wait-for-1-from-below states: sndpkt=make_pkt(ACK, 1, checksum)  and sndpkt=make_pkt(ACK, 0, checksum) .
Would the protocol work correctly if this action were removed from the self-transition in the Wait-for-1-from-below state?
Justify your answer.
What if this event were removed from the self-transition in the Wait-for-0-from-below state? [
Hint: In this latter case, consider what would happen if the first sender-to-receiver packet were corrupted.]
P12.
The sender side of rdt3.0  simply ignores (that is, takes no action on) all received packets that are either in error or have the wrong value in the acknum  field of an acknowledgment packet.
Suppose that in such circumstances, rdt3.0  were simply to retransmit the current data packet.
Would the protocol still work? (
Hint: Consider what would happen if there were only bit errors; there are no packet losses but premature timeouts canoccur.
Consider how many times the nth packet is sent, in the limit as n approaches infinity.)
P13.
Consider the rdt 3.0 protocol.
Draw a diagram showing that if the network connection between the sender and receiver can reorder messages (that is, that two messages propagating in the medium between the sender and receiver can be reordered), then the alternating-bitprotocol will not work correctly (make sure you clearly identify the sense in which it will not workcorrectly).
Your diagram should have the sender on the left and the receiver on the right, with thetime axis running down the page, showing data (D) and acknowledgment (A) message exchange.
Make sure you indicate the sequence number associated with any data or acknowledgment segment.
P14.
Consider a reliable data transfer protocol that uses only negative acknowledgments.
Suppose the sender sends data only infrequently.
Would a NAK-only protocol be preferable to a protocol that uses ACKs?
Why?
Now suppose the sender has a lot of data to send and the end-to-end connection experiences few losses.
In this second case, would a NAK-only protocol bepreferable to a protocol that uses ACKs?
Why?
P15.
Consider the cross-country example shown in Figure 3.17 .
How big would the window size have to be for the channel utilization to be greater than 98 percent?
Suppose that the size of a packet is 1,500 bytes, including both header fields and data.
P16.
Suppose an application uses rdt 3.0  as its transport layer protocol.
As the stop-and-wait protocol has very low channel utilization (shown in the cross-country example), the designers of this application let the receiver keep sending back a number (more than two) of alternating ACK0 and ACK 1 even if the corresponding data have not arrived at the receiver.
Would this application design increase the channel utilization?
Why?
Are there any potential problems with this approach?
Explain.
P17.
Consider two network entities, A and B, which are connected by a perfect bi-directional channel (i.e., any message sent will be received correctly; the channel will not corrupt, lose, or re-order packets).
A and B are to deliver data messages to each other in an alternating manner:First, A must deliver a message to B, then B must deliver a message to A, then A must deliver amessage to B and so on.
If an entity is in a state where it should not attempt to deliver a message to the other side, and there is an event like rdt_send(data)  call from above that attempts to pass data down for transmission to the other side, this call from above can simply beignored with a call to rdt_unable_to_send(data) , which informs the higher layer that it is currently not able to send data. [
Note: This simplifying assumption is made so you don’t have to worry about buffering data.]
Draw a FSM specification for this protocol (one FSM for A, and one FSM for B!).
Note that you do not have to worry about a reliability mechanism here; the main point of this question is tocreate a FSM specification that reflects the synchronized behavior of the two entities.
You should use the following events and actions that have the same meaning as protocol rdt1.0 in Figure 3.9 : rdt_send(data), packet = make_pkt(data) , udt_send(packet), rdt_rcv(packet) , extract (packet, data), deliver_data(data) .
Make sure your protocol reflects the strict alternation of sending between A and B. Also, make sure to indicate the initial states for A and B in your FSM descriptions.
P18.
In the generic SR protocol that we studied in Section 3.4.4 , the sender transmits a message as soon as it is available (if it is in the window) without waiting for an acknowledgment.
Suppose now that we want an SR protocol that sends messages two at a time.
That is, thesender will send a pair of messages and will send the next pair of messages only when it knowsthat both messages in the first pair have been received correctly.
Suppose that the channel may lose messages but will not corrupt or reorder messages.
Design an error-control protocol for the unidirectional reliable transfer of messages.
Give an FSM description of the sender and receiver.
Describe the format of the packets sent between sender and receiver, and vice versa.
If you use any procedure calls other than those in Section 3.4 (for example, udt_send() , start_timer() , rdt_rcv() , and so on), clearly state their actions.
Give an example (a timeline trace of sender and receiver) showing how your protocol recovers from a lost packet.
P19.
Consider a scenario in which Host A wants to simultaneously send packets to Hosts B and C. A is connected to B and C via a broadcast channel—a packet sent by A is carried by the channel to both B and C. Suppose that the broadcast channel connecting A, B, and C canindependently lose and corrupt packets (and so, for example, a packet sent from A might becorrectly received by B, but not by C).
Design a stop-and-wait-like error-control protocol forreliably transferring packets from A to B and C, such that A will not get new data from the upperlayer until it knows that both B and C have correctly received the current packet.
Give FSM descriptions of A and C. (Hint: The FSM for B should be essentially the same as for C.) Also, give a description of the packet format(s) used.
P20.
Consider a scenario in which Host A and Host B want to send messages to Host C. Hosts A and C are connected by a channel that can lose and corrupt (but not reorder) messages.
Hosts B and C are connected by another channel (independent of the channel connecting A andC) with the same properties.
The transport layer at Host C should alternate in deliveringmessages from A and B to the layer above (that is, it should first deliver the data from a packetfrom A, then the data from a packet from B, and so on).
Design a stop-and-wait-like error-controlprotocol for reliably transferring packets from A and B to C, with alternating delivery at C as described above.
Give FSM descriptions of A and C. ( Hint: The FSM for B should be essentially the same as for A.) Also, give a description of the packet format(s) used.
P21.
Suppose we have two network entities, A and B. B has a supply of data messages that will be sent to A according to the following conventions.
When A gets a request from the layer above to get the next data (D) message from B, A must send a request (R) message to B on the A-to-Bchannel.
Only when B receives an R message can it send a data (D) message back to A on theB-to-A channel.
A should deliver exactly one copy of each D message to the layer above.
Rmessages can be lost (but not corrupted) in the A-to-B channel; D messages, once sent, arealways delivered correctly.
The delay along both channels is unknown and variable.
Design (give an FSM description of) a protocol that incorporates the appropriate mechanisms to compensate for the loss-prone A-to-B channel and implements message passing to the layerabove at entity A, as discussed above.
Use only those mechanisms that are absolutely
necessary.
P22.
Consider the GBN protocol with a sender window size of 4 and a sequence number range of 1,024.
Suppose that at time t, the next in-order packet that the receiver is expecting has asequence number of k. Assume that the medium does not reorder messages.
Answer the following questions: a. What are the possible sets of sequence numbers inside the sender’s window at time t?
Justify your answer.
b. What are all possible values of the ACK field in all possible messages currentlypropagating back to the sender at time t?
Justify your answer.
P23.
Consider the GBN and SR protocols.
Suppose the sequence number space is of size k. What is the largest allowable sender window that will avoid the occurrence of problems such as that in Figure 3.27 for each of these protocols?
P24.
Answer true or false to the following questions and briefly justify your answer: a. With the SR protocol, it is possible for the sender to receive an ACK for a packet that falls outside of its current window.
b. With GBN, it is possible for the sender to receive an ACK for a packet that falls outside ofits current window.
c. The alternating-bit protocol is the same as the SR protocol with a sender and receiverwindow size of 1.
d. The alternating-bit protocol is the same as the GBN protocol with a sender and receiverwindow size of 1.
P25.
We have said that an application may choose UDP for a transport protocol because UDP offers finer application control (than TCP) of what data is sent in a segment and when.
a. Why does an application have more control of what data is sent in a segment?
b. Why does an application have more control on when the segment is sent?
P26.
Consider transferring an enormous file of L bytes from Host A to Host B. Assume an MSS of 536 bytes.
a. What is the maximum value of L such that TCP sequence numbers are not exhausted?
Recall that the TCP sequence number field has 4 bytes.
b. For the L you obtain in (a), find how long it takes to transmit the file.
Assume that a totalof 66 bytes of transport, network, and data-link header are added to each segment before the resulting packet is sent out over a 155 Mbps link.
Ignore flow control andcongestion control so A can pump out the segments back to back and continuously.
P27.
Host A and B are communicating over a TCP connection, and Host B has already received from A all bytes up through byte 126.
Suppose Host A then sends two segments to Host B back- to-back.
The first and second segments contain 80 and 40 bytes of data, respectively.
In the first
segment, the sequence number is 127, the source port number is 302, and the destination port number is 80.
Host B sends an acknowledgment whenever it receives a segment from Host A. a. In the second segment sent from Host A to B, what are the sequence number, source port number, and destination port number?
b. If the first segment arrives before the second segment, in the acknowledgment of the firstarriving segment, what is the acknowledgment number, the source port number, and the destination port number?
c. If the second segment arrives before the first segment, in the acknowledgment of the first arriving segment, what is the acknowledgment number?
d. Suppose the two segments sent by A arrive in order at B. The first acknowledgment islost and the second acknowledgment arrives after the first timeout interval.
Draw a timing diagram, showing these segments and all other segments and acknowledgments sent.(Assume there is no additional packet loss.)
For each segment in your figure, provide the sequence number and the number of bytes of data; for each acknowledgment that you add, provide the acknowledgment number.
P28.
Host A and B are directly connected with a 100 Mbps link.
There is one TCP connection between the two hosts, and Host A is sending to Host B an enormous file over this connection.
Host A can send its application data into its TCP socket at a rate as high as 120 Mbps but HostB can read out of its TCP receive buffer at a maximum rate of 50 Mbps.
Describe the effect ofTCP flow control.
P29.
SYN cookies were discussed in Section 3.5.6 .
a. Why is it necessary for the server to use a special initial sequence number in the SYNACK?
b. Suppose an attacker knows that a target host uses SYN cookies.
Can the attacker createhalf-open or fully open connections by simply sending an ACK packet to the target?
Why or why not?
c. Suppose an attacker collects a large amount of initial sequence numbers sent by the server.
Can the attacker cause the server to create many fully open connections by sending ACKs with those initial sequence numbers?
Why?
P30.
Consider the network shown in Scenario 2 in Section 3.6.1 .
Suppose both sending hosts A and B have some fixed timeout values.
a. Argue that increasing the size of the finite buffer of the router might possibly decreasethe throughput ( λ).
b. Now suppose both hosts dynamically adjust their timeout values (like what TCP does) based on the buffering delay at the router.
Would increasing the buffer size help to increase the throughput?
Why?
P31.
Suppose that the five measured SampleRTT  values (see Section 3.5.3 ) are 106 ms, 120out
ms, 140 ms, 90 ms, and 115 ms.
Compute the EstimatedRTT  after each of these SampleRTT values is obtained, using a value of  and assuming that the value of EstimatedRTT was 100 ms just before the first of these five samples were obtained.
Compute also the DevRTT after each sample is obtained, assuming a value of  and assuming the value of DevRTT was 5 ms just before the first of these five samples was obtained.
Last, compute the TCP TimeoutInterval  after each of these samples is obtained.
P32.
Consider the TCP procedure for estimating RTT.
Suppose that .
Let SampleRTT  be the most recent sample RTT, let SampleRTT  be the next most recent sample RTT, and so on.
a. For a given TCP connection, suppose four acknowledgments have been returned with corresponding sample RTTs: SampleRTT , SampleRTT , SampleRTT , and SampleRTT .
Express EstimatedRTT  in terms of the four sample RTTs.
b. Generalize your formula for n sample RTTs.
c. For the formula in part (b) let n approach infinity.
Comment on why this averaging procedure is called an exponential moving average.
P33.
In Section 3.5.3 , we discussed TCP’s estimation of RTT.
Why do you think TCP avoids measuring the SampleRTT  for retransmitted segments?
P34.
What is the relationship between the variable SendBase  in Section 3.5.4 and the variable LastByteRcvd  in Section 3.5.5 ?
P35.
What is the relationship between the variable LastByteRcvd  in Section 3.5.5 and the variable y in Section 3.5.4?
P36.
In Section 3.5.4 , we saw that TCP waits until it has received three duplicate ACKs before performing a fast retransmit.
Why do you think the TCP designers chose not to perform a fast retransmit after the first duplicate ACK for a segment is received?
P37.
Compare GBN, SR, and TCP (no delayed ACK).
Assume that the timeout values for all three protocols are sufficiently long such that 5 consecutive data segments and their corresponding ACKs can be received (if not lost in the channel) by the receiving host (Host B)and the sending host (Host A) respectively.
Suppose Host A sends 5 data segments to Host B,and the 2nd segment (sent from A) is lost.
In the end, all 5 data segments have been correctlyreceived by Host B. a. How many segments has Host A sent in total and how many ACKs has Host B sent in total?
What are their sequence numbers?
Answer this question for all three protocols.
b. If the timeout values for all three protocol are much longer than 5 RTT, then whichprotocol successfully delivers all five data segments in shortest time interval?
P38.
In our description of TCP in Figure 3.53 , the value of the threshold, ssthresh , is set as ssthresh=cwnd/2  in several places and ssthresh  value is referred to as being set to half the window size when a loss event occurred.
Must the rate at which the sender is sending when the loss event occurred be approximately equal to cwnd  segments per RTT?
Explain yourα=0.125 β=0.25 α=0.1 1 2 4 3 2 1
answer.
If your answer is no, can you suggest a different manner in which ssthresh  should be set?
P39.
Consider Figure 3.46(b) .
If  increases beyond R/2, can λ  increase beyond R/3?
Explain.
Now consider Figure 3.46(c) .
If  increases beyond R/2, can λ  increase beyond R/4 under the assumption that a packet will be forwarded twice on average from the router to the receiver?
Explain.
P40.
Consider Figure 3.58 .
Assuming TCP Reno is the protocol experiencing the behavior shown above, answer the following questions.
In all cases, you should provide a short discussion justifying your answer.
Examining the behavior of TCP a. Identify the intervals of time when TCP slow start is operating.
b. Identify the intervals of time when TCP congestion avoidance is operating.
c. After the 16th transmission round, is segment loss detected by a triple duplicate ACK or by a timeout?
d. After the 22nd transmission round, is segment loss detected by a triple duplicate ACK orby a timeout?
Figure 3.58 TCP window size as a function of timeλ′in out λ′inout
e. What is the initial value of ssthresh  at the first transmission round?
f. What is the value of ssthresh  at the 18th transmission round?
g. What is the value of ssthresh  at the 24th transmission round?
h. During what transmission round is the 70th segment sent?
i. Assuming a packet loss is detected after the 26th round by the receipt of a triple duplicate ACK, what will be the values of the congestion window size and of ssthresh ?
j. Suppose TCP Tahoe is used (instead of TCP Reno), and assume that triple duplicate ACKs are received at the 16th round.
What are the ssthresh  and the congestion window size at the 19th round?
k. Again suppose TCP Tahoe is used, and there is a timeout event at 22nd round.
How many packets have been sent out from 17th round till 22nd round, inclusive?
P41.
Refer to Figure 3.55 , which illustrates the convergence of TCP’s AIMD algorithm.
Suppose that instead of a multiplicative decrease, TCP decreased the window size by a constant amount.
Would the resulting AIAD algorithm converge to an equal share algorithm?
Justify your answer using a diagram similar to Figure 3.55 .
P42.
In Section 3.5.4 , we discussed the doubling of the timeout interval after a timeout event.
This mechanism is a form of congestion control.
Why does TCP need a window-based congestion-control mechanism (as studied in Section 3.7 ) in addition to this doubling-timeout- interval mechanism?P43.
Host A is sending an enormous file to Host B over a TCP connection.
Over this connection there is never any packet loss and the timers never expire.
Denote the transmission rate of the link connecting Host A to the Internet by R bps.
Suppose that the process in Host A is capable of sending data into its TCP socket at a rate S bps, where  Further suppose that the TCP receive buffer is large enough to hold the entire file, and the send buffer can hold only one percent of the file.
What would prevent the process in Host A from continuously passing data to its TCP socket at rate S bps?
TCP flow control?
TCP congestion control?
Or something else?
Elaborate.
P44.
Consider sending a large file from a host to another over a TCP connection that has no loss.
a. Suppose TCP uses AIMD for its congestion control without slow start.
Assuming cwnd increases by 1 MSS every time a batch of ACKs is received and assuming approximately constant round-trip times, how long does it take for cwnd  increase from 6 MSS to 12 MSS (assuming no loss events)?
b. What is the average throughout (in terms of MSS and RTT) for this connection up through ?
P45.
Recall the macroscopic description of TCP throughput.
In the period of time from when theS=10⋅R. time=6 RTT
connection’s rate varies from W/(2 · RTT) to W/RTT , only one packet is lost (at the very end of the period).
a. Show that the loss rate (fraction of packets lost) is equal to b. Use the result above to show that if a connection has loss rate L, then its average rate is approximately given by P46.
Consider that only a single TCP (Reno) connection uses one 10Mbps link which does not buffer any data.
Suppose that this link is the only congested link between the sending and receiving hosts.
Assume that the TCP sender has a huge file to send to the receiver, and thereceiver’s receive buffer is much larger than the congestion window.
We also make the followingassumptions: each TCP segment size is 1,500 bytes; the two-way propagation delay of thisconnection is 150 msec; and this TCP connection is always in congestion avoidance phase, thatis, ignore slow start.
a. What is the maximum window size (in segments) that this TCP connection can achieve?
b. What is the average window size (in segments) and average throughput (in bps) of this TCP connection?
c. How long would it take for this TCP connection to reach its maximum window again afterrecovering from a packet loss?
P47.
Consider the scenario described in the previous problem.
Suppose that the 10Mbps link can buffer a finite number of segments.
Argue that in order for the link to always be busy sending data, we would like to choose a buffer size that is at least the product of the link speed C and the two-way propagation delay between the sender and the receiver.
P48.
Repeat Problem 46, but replacing the 10 Mbps link with a 10 Gbps link.
Note that in your answer to part c, you will realize that it takes a very long time for the congestion window size to reach its maximum window size after recovering from a packet loss.
Sketch a solution to solvethis problem.
P49.
Let T (measured by RTT) denote the time interval that a TCP connection takes to increase its congestion window size from W/2 to W, where W is the maximum congestion window size.
Argue that T is a function of TCP’s average throughput.
P50.
Consider a simplified TCP’s AIMD algorithm where the congestion window size is measured in number of segments, not in bytes.
In additive increase, the congestion window size increases by one segment in each RTT.
In multiplicative decrease, the congestion window sizedecreases by half (if the result is not an integer, round down to the nearest integer).
Suppose that two TCP connections, C  and C , share a single congested link of speed 30 segments per second.
Assume that both C  and C  are in the congestion avoidance phase.
Connection C ’s RTT is 50 msec and connection C ’s RTT is 100 msec.
Assume that when the data rate in theL=loss rate=138W2+34W ≈1.22⋅MSSRTTL 1 2 1 2 1 2
link exceeds the link’s speed, all TCP connections experience data segment loss.
a. If both C  and C  at time t  have a congestion window of 10 segments, what are their congestion window sizes after 1000 msec?
b. In the long run, will these two connections get the same share of the bandwidth of the congested link?
Explain.
P51.
Consider the network described in the previous problem.
Now suppose that the two TCP connections, C1 and C2, have the same RTT of 100 msec.
Suppose that at time t , C1’s congestion window size is 15 segments but C2’s congestion window size is 10 segments.
a. What are their congestion window sizes after 2200 msec?
b. In the long run, will these two connections get about the same share of the bandwidth of the congested link?
c. We say that two connections are synchronized, if both connections reach their maximumwindow sizes at the same time and reach their minimum window sizes at the same time.
In the long run, will these two connections get synchronized eventually?
If so, what aretheir maximum window sizes?
d. Will this synchronization help to improve the utilization of the shared link?
Why?
Sketch some idea to break this synchronization.
P52.
Consider a modification to TCP’s congestion control algorithm.
Instead of additive increase, we can use multiplicative increase.
A TCP sender increases its window size by a small positive constant  whenever it receives a valid ACK.
Find the functional relationship between loss rate L and maximum congestion window W. Argue that for this modified TCP, regardless ofTCP’s average throughput, a TCP connection always spends the same amount of time to increase its congestion window size from W/2 to W. P53.
In our discussion of TCP futures in Section 3.7 , we noted that to achieve a throughput of 10 Gbps, TCP could only tolerate a segment loss probability of  (or equivalently, one loss event for every 5,000,000,000 segments).
Show the derivation for the values of  (1 out of 5,000,000) for the RTT and MSS values given in Section 3.7 .
If TCP needed to support a 100 Gbps connection, what would the tolerable loss be?
P54.
In our discussion of TCP congestion control in Section 3.7 , we implicitly assumed that the TCP sender always had data to send.
Consider now the case that the TCP sender sends a largeamount of data and then goes idle (since it has no more data to send) at t. TCP remains idle for a relatively long period of time and then wants to send more data at t. What are the advantages and disadvantages of having TCP use the cwnd  and ssthresh  values from t  when starting to send data at t ?
What alternative would you recommend?
Why?
P55.
In this problem we investigate whether either UDP or TCP provides a degree of end-point authentication.
a. Consider a server that receives a request within a UDP packet and responds to thatrequest within a UDP packet (for example, as done by a DNS server).
If a client with IP1 2 0 0 a(0<a<1) 2⋅10−10 2⋅10−10 1 2 1 2
Programming Assignments Implementing a Reliable Transport Protocol In this laboratory programming assignment, you will be writing the sending and receiving transport-level code for implementing a simple reliable data transfer protocol.
There are two versions of this lab, thealternating-bit-protocol version and the GBN version.
This lab should be fun—your implementation will differ very little from what would be required in a real-world situation.
Since you probably don’t have standalone machines (with an OS that you can modify), your code will have to execute in a simulated hardware/software environment.
However, the programming interfaceprovided to your routines—the code that would call your entities from above and from below—is veryclose to what is done in an actual UNIX environment. (
Indeed, the software interfaces described in thisprogramming assignment are much more realistic than the infinite loop senders and receivers that manytexts describe.)
Stopping and starting timers are also simulated, and timer interrupts will cause your timer handling routine to be activated.
The full lab assignment, as well as code you will need to compile with your own code, are available at this book’s Web site: www.pearsonhighered.com/ cs-resources .
Wireshark Lab: Exploring TCPaddress X spoofs its address with address Y, where will the server send its response?
b. Suppose a server receives a SYN with IP source address Y, and after responding with a SYNACK, receives an ACK with IP source address Y with the correct acknowledgment number.
Assuming the server chooses a random initial sequence number and there is no“man-in-the-middle,” can the server be certain that the client is indeed at Y (and not at some other address X that is spoofing Y)?
P56.
In this problem, we consider the delay introduced by the TCP slow-start phase.
Consider a client and a Web server directly connected by one link of rate R. Suppose the client wants to retrieve an object whose size is exactly equal to 15 S, where S is the maximum segment size (MSS).
Denote the round-trip time between client and server as RTT (assumed to be constant).Ignoring protocol headers, determine the time to retrieve the object (including TCP connectionestablishment) when a.  b.  c. .4 S/R>S/R+RTT>2S/RS/R+RTT>4 S/RS/R>RTT
In this lab, you’ll use your Web browser to access a file from a Web server.
As in earlier Wireshark labs, you’ll use Wireshark to capture the packets arriving at your computer.
Unlike earlier labs, you’ll also be able to download a Wireshark-readable packet trace from the Web server from which you downloaded the file.
In this server trace, you’ll find the packets that were generated by your own access of the Webserver.
You’ll analyze the client- and server-side traces to explore aspects of TCP.
In particular, you’ll evaluate the performance of the TCP connection between your computer and the Web server.
You’ll trace TCP’s window behavior, and infer packet loss, retransmission, flow control and congestion controlbehavior, and estimated roundtrip time.
As is the case with all Wireshark labs, the full description of this lab is available at this book’s Web site, www.pearsonhighered.com/ cs-resources .
Wireshark Lab: Exploring UDP In this short lab, you’ll do a packet capture and analysis of your favorite application that uses UDP (for example, DNS or a multimedia application such as Skype).
As we learned in Section 3.3, UDP is a simple, no-frills transport protocol.
In this lab, you’ll investigate the header fields in the UDP segment as well as the checksum calculation.
As is the case with all Wireshark labs, the full description of this lab is available at this book’s Web site, www.pearsonhighered.com/ cs-resources .
AN INTERVIEW WITH... Van Jacobson Van Jacobson works at Google and was previously a Research Fellow at PARC.
Prior to that, hewas co-founder and Chief Scientist of Packet Design.
Before that, he was Chief Scientist atCisco.
Before joining Cisco, he was head of the Network Research Group at Lawrence BerkeleyNational Laboratory and taught at UC Berkeley and Stanford.
Van received the ACM SIGCOMMAward in 2001 for outstanding lifetime contribution to the field of communication networks and the IEEE Kobayashi Award in 2002 for “contributing to the understanding of network congestion and developing congestion control mechanisms that enabled the successful scaling of theInternet”.
He was elected to the U.S. National Academy of Engineering in 2004.
Please describe one or two of the most exciting projects you have worked on during your career.
What were the biggest challenges?
School teaches us lots of ways to find answers.
In every interesting problem I’ve worked on, the challenge has been finding the right question.
When Mike Karels and I started looking at TCPcongestion, we spent months staring at protocol and packet traces asking “Why is it failing?”.
One day in Mike’s office, one of us said “The reason I can’t figure out why it fails is because Idon’t understand how it ever worked to begin with.”
That turned out to be the right question and it forced us to figure out the “ack clocking” that makes TCP work.
After that, the rest was easy.
More generally, where do you see the future of networking and the Internet?
For most people, the Web is the Internet.
Networking geeks smile politely since we know the Web is an application running over the Internet but what if they’re right?
The Internet is aboutenabling conversations between pairs of hosts.
The Web is about distributed informationproduction and consumption. “
Information propagation” is a very general view of communicationof which “pairwise conversation” is a tiny subset.
We need to move into the larger tent.
Networking today deals with broadcast media (radios, PONs, etc.)
by pretending it’s a point-to-point wire.
That’s massively inefficient.
Terabits-per-second of data are being exchanged all overthe World via thumb drives or smart phones but we don’t know how to treat that as “networking”.
ISPs are busily setting up caches and CDNs to scalably distribute video and audio.
Caching is a necessary part of the solution but there’s no part of today’s networking—from Information,Queuing or Traffic Theory down to the Internet protocol specs—that tells us how to engineer and deploy it.
I think and hope that over the next few years, networking will evolve to embrace themuch larger vision of communication that underlies the Web.
What people inspired you professionally?
When I was in grad school, Richard Feynman visited and gave a colloquium.
He talked about a piece of Quantum theory that I’d been struggling with all semester and his explanation was so simple and lucid that what had been incomprehensible gibberish to me became obvious andinevitable.
That ability to see and convey the simplicity that underlies our complex world seemsto me a rare and wonderful gift.
What are your recommendations for students who want careers in computer science and networking?
It’s a wonderful field—computers and networking have probably had more impact on society than any invention since the book.
Networking is fundamentally about connecting stuff, and studying it helps you make intellectual connections: Ant foraging & Bee dances demonstrate protocol design better than RFCs, traffic jams or people leaving a packed stadium are theessence of congestion, and students finding flights back to school in a post-Thanksgivingblizzard are the core of dynamic routing.
If you’re interested in lots of stuff and want to have animpact, it’s hard to imagine a better field.
Chapter 4 The Network Layer: Data Plane We learned in the previous chapter that the transport layer provides various forms of process-to-process communication by relying on the network layer’s host-to-host communication service.
We also learnedthat the transport layer does so without any knowledge about how the network layer actually implementsthis service.
So perhaps you’re now wondering, what’s under the hood of the host-to-hostcommunication service, what makes it tick?
In this chapter and the next, we’ll learn exactly how the network layer can provide its host-to-host communication service.
We’ll see that unlike the transport and application layers, there is a piece of the network layer in each and every host and router in the network.
 Because of this, network-layer protocols are among the most challenging (and therefore among the most interesting!)
in the protocol stack.
Since the network layer is arguably the most complex layer in the protocol stack, we’ll have a lot of ground to cover here.
Indeed, there is so much to cover that we cover the network layer in two chapters.
We’ll see that the network layer can be decomposed into two interacting parts, the data plane and the control plane .
In Chapter 4, we’ll first cover the data plane functions of the network layer—the per- router functions in the network layer that determine how a datagram (that is, a network-layer packet) arriving on one of a router’s input links is forwarded to one of that router’s output links.
We’ll cover both traditional IP forwarding (where forwarding is based on a datagram’s destination address) andgeneralized forwarding (where forwarding and other functions may be performed using values in severaldifferent fields in the datagram’s header).
We’ll study the IPv4 and IPv6 protocols and addressing in detail.
In Chapter 5, we’ll cover the control plane functions of the network layer—the network-wide  logic that controls how a datagram is routed among routers along an end-to-end path from the source host to the destination host.
We’ll cover routing algorithms, as well as routing protocols, such as OSPF and BGP, that are in widespread use in today’s Internet.
Traditionally, these control-plane routing protocolsand data-plane forwarding functions have been implemented together, monolithically, within a router.
Software-defined networking (SDN) explicitly separates the data plane and control plane byimplementing these control plane functions as a separate service, typically in a remote “controller.”
We’ll also cover SDN controllers in Chapter 5.
This distinction between data-plane and control-plane functions in the network layer is an important concept to keep in mind as you learn about the network layer  —it will help structure your thinking about
the network layer and reflects a modern view of the network layer’s role in computer networking.
4.1 Overview of Network Layer Figure 4.1 shows a simple network with two hosts, H1 and H2, and several routers on the path between H1 and H2.
Let’s suppose that H1 is sending information to H2, and consider the role of the network layer in these hosts and in the intervening routers.
The network layer in H1 takes segments from thetransport layer in H1, encapsulates each segment into a datagram, and then sends the datagrams to its nearby router, R1.
At the receiving host, H2, the network layer receives the datagrams from its nearby router R2, extracts the transport-layer segments, and delivers the segments up to the transport layer atH2.
The primary data-plane role of each router is to forward datagrams from its input links to its outputlinks; the primary role of the network control plane is to coordinate these local, per-router forwardingactions so that datagrams are ultimately transferred end-to-end, along paths of routers between source and destination hosts.
Note that the routers in Figure 4.1 are shown with a truncated protocol stack, that is, with no upper layers above the network layer, because routers do not run application- and transport-layer protocols such as those we examined in Chapters 2 and 3.
4.1.1 Forwarding and Routing: The Data and Control Planes The primary role of the network layer is deceptively simple—to move packets from a sending host to a receiving host.
To do so, two important network-layer functions can be identified: Forwarding.
 When a packet arrives at a router’s input link, the router must move the packet to the appropriate output link.
For example, a packet arriving from Host H1 to Router R1 in Figure 4.1 must be forwarded to the next router on a path to H2.
As we will see, forwarding is but one function (albeitthe most
Figure 4.1 The network layer common and important one!)
implemented in the data plane.
In the more general case, which we’ll cover in Section 4.4, a packet might also be blocked from exiting a router (e.g., if the packet originated at a known malicious sending host, or if the packet were destined to a forbidden destination host), or might be duplicated and sent over multiple outgoing links.
Routing.
 The network layer must determine the route or path taken by packets as they flow from a sender to a receiver.
The algorithms that calculate these paths are referred to as routing algorithms .
A routing algorithm would determine, for example, the path along which packets flow
from H1 to H2 in Figure 4.1.
Routing is implemented in the control plane of the network layer.
The terms forwarding  and routing  are often used interchangeably by authors discussing the network layer.
We’ll use these terms much more precisely in this book.
Forwarding  refers to the router-local action of transferring a packet from an input link interface to the appropriate output link interface.
Forwarding takes place at very short timescales (typically a few nanoseconds), and thus is typicallyimplemented in hardware.
Routing  refers to the network-wide process that determines the end-to-end paths that packets take from source to destination.
Routing takes place on much longer timescales(typically seconds), and as we will see is often implemented in software.
Using our driving analogy, consider the trip from Pennsylvania to Florida undertaken by our traveler back in Section 1.3.1.
During this trip, our driver passes through many interchanges en route to Florida.
We can think of forwarding as the process of getting through a single interchange: A car enters the interchange from one road anddetermines which road it should take to leave the interchange.
We can think of routing as the process of planning the trip from Pennsylvania to Florida: Before embarking on the trip, the driver has consulted a map and chosen one of many paths possible, with each path consisting of a series of road segmentsconnected at interchanges.
A key element in every network router is its forwarding table .
A router forwards a packet by examining the value of one or more fields in the arriving packet’s header, and then using these header values to index into its forwarding table.
The value stored in the forwarding table entry for those values indicates the outgoing link interface at that router to which that packet is to be forwarded.
For example, in Figure 4.2, a packet with header field value of 0110  arrives to a router.
The router indexes into its forwarding table and determines that the output link interface for this packet is interface 2.
The router then internallyforwards the packet to interface 2.
In Section 4.2, we’ll look inside a router and examine the forwarding function in much greater detail.
Forwarding is the key function performed by the data-plane functionality of the network layer.
Control Plane: The Traditional Approach But now you are undoubtedly wondering how a router’s forwarding tables are configured in the first place.
This is a crucial issue, one that exposes the important interplay between forwarding (in dataplane) and routing (in control plane).
As shown
Figure 4.2 Routing algorithms determine values in forward tables in Figure 4.2, the routing algorithm determines the contents of the routers’ forwarding tables.
In this example, a routing algorithm runs in each and every router and both forwarding and routing functions are contained within a router.
As we’ll see in Sections 5.3 and 5.4, the routing algorithm function in one router communicates with the routing algorithm function in other routers to compute the values for its forwarding table.
How is this communication performed?
By exchanging routing messages containingrouting information according to a routing protocol!
We’ll cover routing algorithms and protocols in Sections 5.2 through 5.4.
The distinct and different purposes of the forwarding and routing functions can be further illustrated by considering the hypothetical (and unrealistic, but technically feasible) case of a network in which all forwarding tables are configured directly by human network operators physically present at the routers.
In this case, no routing protocols would be required!
Of course, the human operators would need to interact with each other to ensure that the forwarding tables were configured in such a way that packets reached their intended destinations.
It’s also likely that human configuration would be more error-proneand much slower to respond to changes in the network topology than a routing protocol.
We’re thus fortunate that all networks have both a forwarding and a routing function!
Control Plane: The SDN Approach The approach to implementing routing functionality shown in Figure 4.2—with each router having a routing component that communicates with the routing component of other routers—has been the
traditional approach adopted by routing vendors in their products, at least until recently.
Our observation that humans could manually configure forwarding tables does suggest, however, that there may be other ways for control-plane functionality to determine the contents of the data-plane forwarding tables.
Figure 4.3 shows an alternate approach in which a physically separate (from the routers), remote controller computes and distributes the forwarding tables to be used by each and every router.
Note that the data plane components of Figures 4.2 and 4.3 are identical.
In Figure 4.3, however, control-plane routing functionality is separated Figure 4.3 A remote controller determines and distributes values in ­forwarding tables from the physical router—the routing device performs forwarding only, while the remote controller computes and distributes forwarding tables.
The remote controller might be implemented in a remote data center with high reliability and redundancy, and might be managed by the ISP or some third party.
How might the routers and the remote controller communicate?
By exchanging messages containing forwarding tables and other pieces of routing information.
The control-plane approach shown in Figure 4.3 is at the heart of software-defined networking (SDN) , where the network is “software-defined” because the controller that computes forwarding tables and interacts with routers is implemented in software.
Increasingly, these software implementations are also open, i.e., similar to Linux OS code, the
code is publically available, allowing ISPs (and networking researchers and students!)
to innovate and propose changes to the software that controls network-layer functionality.
We will cover the SDN control plane in Section 5.5.
4.1.2 Network Service Model Before delving into the network layer’s data plane, let’s wrap up our introduction by taking the broader view and consider the different types of service that might be offered by the network layer.
When thetransport layer at a sending host transmits a packet into the network (that is, passes it down to thenetwork layer at the sending host), can the transport layer rely on the network layer to deliver the packetto the destination?
When multiple packets are sent, will they be delivered to the transport layer in the receiving host in the order in which they were sent?
Will the amount of time between the sending of two sequential packet transmissions be the same as the amount of time between their reception?
Will thenetwork provide any feedback about congestion in the network?
The answers to these questions andothers are determined by the service model provided by the network layer.
The network service model defines the characteristics of end-to-end delivery of packets between sending and receiving hosts.
Let’s now consider some possible services that the network layer could provide.
These services could include: Guaranteed delivery.
This service guarantees that a packet sent by a source host will eventually arrive at the destination host.
Guaranteed delivery with bounded delay.
 This service not only guarantees delivery of the packet, but delivery within a specified host-to-host delay bound (for example, within 100 msec).
In-order packet delivery.
This service guarantees that packets arrive at the destination in the orderthat they were sent.
Guaranteed minimal bandwidth.
 This network-layer service emulates the behavior of a transmission link of a specified bit rate (for example, 1 Mbps) between sending and receiving hosts.
As long as the sending host transmits bits (as part of packets) at a rate below the specified bit rate, then all packets are eventually delivered to the destination host.
Security.
The network layer could encrypt all datagrams at the source and decrypt them at the destination, thereby providing confidentiality to all transport-layer segments.
This is only a partial list of services that a network layer could provide—there are countless variations possible.
The Internet’s network layer provides a single service, known as best-effort service .
With best-effort service, packets are neither guaranteed to be received in the order in which they were sent, nor is their eventual delivery even guaranteed.
There is no guarantee on the end-to-end delay nor is there a
minimal bandwidth guarantee.
It might appear that best-effort service is a euphemism for no service at all—a network that delivered no packets to the destination would satisfy the definition of best-effort delivery service!
Other network architectures have defined and implemented service models that go beyond the Internet’s best-effort service.
For example, the ATM network architecture [MFA Forum 2016 , Black 1995]  provides for guaranteed in-order delay, bounded delay, and guaranteed minimal bandwidth.
There have also been proposed service model extensions to the Internet architecture; forexample, the Intserv architecture [RFC 1633]  aims to provide end-end delay guarantees and congestion-free communication.
Interestingly, in spite of these well-developed alternatives, the Internet’s basic best-effort service model combined with adequate bandwidth provisioning have arguably proven tobe more than “good enough” to enable an amazing range of applications, including streaming videoservices such as Netflix and voice-and-video-over-IP, real-time conferencing applications such as Skypeand Facetime.
An Overview of Chapter 4 Having now provided an overview of the network layer, we’ll cover the data-plane component of the network layer in the following sections in this chapter.
In Section 4.2, we’ll dive down into the internal hardware operations of a router, including input and output packet processing, the router’s internalswitching mechanism, and packet queueing and scheduling.
In Section 4.3, we’ll take a look at traditional IP forwarding, in which packets are forwarded to output ports based on their destination IPaddresses.
We’ll encounter IP addressing, the celebrated IPv4 and IPv6 protocols and more.
In Section 4.4, we’ll cover more generalized forwarding, where packets may be forwarded to output ports based on a large number of header values (i.e., not only based on destination IP address).
Packets may be blocked or duplicated at the router, or may have certain header field values rewritten—all under softwarecontrol.
This more generalized form of packet forwarding is a key component of a modern network dataplane, including the data plane in software-defined networks (SDN).
We mention here in passing that the terms forwarding  and switching  are often used interchangeably by computer-networking researchers and practitioners; we’ll use both terms interchangeably in this textbook as well.
While we’re on the topic of terminology, it’s also worth mentioning two other terms that are often used interchangeably, but that we will use more carefully.
We’ll reserve the term packet switch to mean a general packet-switching device that transfers a packet from input link interface to output link interface, according to values in a packet’s header fields.
Some packet switches, called link-layer switches  (examined in Chapter 6), base their forwarding decision on values in the fields of the link- layer frame; switches are thus referred to as link-layer (layer 2) devices.
Other packet switches, calledrouters, base their forwarding decision on header field values in the network-layer datagram.
Routersare thus network-layer (layer 3) devices. (
To fully appreciate this important distinction, you might want to review Section 1.5.2, where we discuss network-layer datagrams and link-layer frames and their relationship.)
Since our focus in this chapter is on the network layer, we’ll mostly use the term router in place of packet switch.
4.2 What’s Inside a Router?
Now that we’ve overviewed the data and control planes within the network layer, the important distinction between forwarding and routing, and the services and functions of the network layer, let’s turnour attention to its forwarding function—the actual transfer of packets from a router’s incoming links tothe appropriate outgoing links at that router.
A high-level view of a generic router architecture is shown in Figure 4.4.
Four router components can be identified: Figure 4.4 Router architecture Input ports.
 An input port  performs several key functions.
It performs the physical layer function of terminating an incoming physical link at a router; this is shown in the leftmost box of an input port and the rightmost box of an output port in Figure 4.4.
An input port also performs link-layer functions needed to interoperate with the link layer at the other side of the incoming link; this is represented by the middle boxes in the input and output ports.
Perhaps most crucially, a lookup function is also performed at the input port; this will occur in the rightmost box of the input port.
It is here that theforwarding table is consulted to determine the router output port to which an arriving packet will beforwarded via the switching fabric.
Control packets (for example, packets carrying routing protocolinformation) are forwarded from an input port to the routing processor.
Note that the term “port” here—referring to the physical input and output router interfaces—is distinctly different from the software
ports associated with network applications and sockets discussed in Chapters 2 and 3.
In practice, the number of ports supported by a router can range from a relatively small number in enterprise routers, to hundreds of 10 Gbps ports in a router at an ISP’s edge, where the number of incominglines tends to be the greatest.
The Juniper MX2020, edge router, for example, supports up to 960 10 Gbps Ethernet ports, with an overall router system capacity of 80 Tbps [Juniper MX 2020 2016].
Switching fabric.
 The switching fabric connects the router’s input ports to its output ports.
This switching fabric is completely contained within the router—a network inside of a network router!
Output ports.
 An output port  stores packets received from the switching fabric and transmits these packets on the outgoing link by performing the necessary link-layer and physical-layer functions.
When a link is bidirectional (that is, carries traffic in both directions), an output port will typically bepaired with the input port for that link on the same line card.
Routing processor.
 The routing processor performs control-plane functions.
In traditional routers, it executes the routing protocols (which we’ll study in Sections 5.3 and 5.4), maintains routing tables and attached link state information, and computes the forwarding table for the router.
In SDNrouters, the routing processor is responsible for communicating with the remote controller in order to (among other activities) receive forwarding table entries computed by the remote controller, and install these entries in the router’s input ports.
The routing processor also performs the network management functions that we’ll study in Section 5.7.
A router’s input ports, output ports, and switching fabric are almost always implemented in hardware, asshown in Figure 4.4.
To appreciate why a hardware implementation is needed, consider that with a 10 Gbps input link and a 64-byte IP datagram, the input port has only 51.2 ns to process the datagrambefore another datagram may arrive.
If N ports are combined on a line card (as is often done in practice), the datagram-processing pipeline must operate N times faster—far too fast for software implementation.
Forwarding hardware can be implemented either using a router vendor’s own hardware designs, or constructed using purchased merchant-silicon chips (e.g., as sold by companies such asIntel and Broadcom).
While the data plane operates at the nanosecond time scale, a router’s control functions—executing the routing protocols, responding to attached links that go up or down, communicating with the remotecontroller (in the SDN case) and performing management functions—operate at the millisecond orsecond timescale.
These control plane  functions are thus usually implemented in software and execute on the routing processor (typically a traditional CPU).
Before delving into the details of router internals, let’s return to our analogy from the beginning of this chapter, where packet forwarding was compared to cars entering and leaving an interchange.
Let’ssuppose that the interchange is a roundabout, and that as a car enters the roundabout, a bit ofprocessing is required.
Let’s consider what information is required for this processing: Destination-based forwarding.
 Suppose the car stops at an entry station and indicates its final
destination (not at the local roundabout, but the ultimate destination of its journey).
An attendant at the entry station looks up the final destination, determines the roundabout exit that leads to that final destination, and tells the driver which roundabout exit to take.
Generalized forwarding.
 The attendant could also determine the car’s exit ramp on the basis of many other factors besides the destination.
For example, the selected exit ramp might depend onthe car’s origin, for example the state that issued the car’s license plate.
Cars from a certain set ofstates might be directed to use one exit ramp (that leads to the destination via a slow road), while cars from other states might be directed to use a different exit ramp (that leads to the destination via superhighway).
The same decision might be made based on the model, make and year of the car.
Or a car not deemed roadworthy might be blocked and not be allowed to pass through theroundabout.
In the case of generalized forwarding, any number of factors may contribute to theattendant’s choice of the exit ramp for a given car.
Once the car enters the roundabout (which may be filled with other cars entering from other input roadsand heading to other roundabout exits), it eventually leaves at the prescribed roundabout exit ramp,where it may encounter other cars leaving the roundabout at that exit.
We can easily recognize the principal router components in Figure 4.4 in this analogy—the entry road and entry station correspond to the input port (with a lookup function to determine to local outgoing port); the roundabout corresponds to the switch fabric; and the roundabout exit road corresponds to the outputport.
With this analogy, it’s instructive to consider where bottlenecks might occur.
What happens if carsarrive blazingly fast (for example, the roundabout is in Germany or Italy!)
but the station attendant isslow?
How fast must the attendant work to ensure there’s no backup on an entry road?
Even with ablazingly fast attendant, what happens if cars traverse the roundabout slowly—can backups still occur?
And what happens if most of the cars entering at all of the roundabout’s entrance ramps all want toleave the roundabout at the same exit ramp—can backups occur at the exit ramp or elsewhere?
Howshould the roundabout operate if we want to assign priorities to different cars, or block certain cars fromentering the roundabout in the first place?
These are all analogous to critical questions faced by routerand switch designers.
In the following subsections, we’ll look at router functions in more detail. [
Iyer 2008 , Chao 2001; Chuang 2005; Turner 1988; McKeown 1997a ; Partridge 1998; Sopranos 2011] provide a discussion of specific router architectures.
For concreteness and simplicity, we’ll initially assume in this section that forwarding decisions are based only on the packet’s destination address, rather than on a generalized set of packet header fields.
We will cover the case of more generalized packet forwarding in Section 4.4.
4.2.1 Input Port Processing and Destination-Based Forwarding
A more detailed view of input processing is shown in Figure 4.5.
As just discussed, the input port’s line- termination function and link-layer processing implement the physical and link layers for that individual input link.
The lookup performed in the input port is central to the router’s operation—it is here that therouter uses the forwarding table to look up the output port to which an arriving packet will be forwardedvia the switching fabric.
The forwarding table is either computed and updated by the routing processor(using a routing protocol to interact with the routing processors in other network routers) or is received from a remote SDN controller.
The forwarding table is copied from the routing processor to the line cards over a separate bus (e.g., a PCI bus) indicated by the dashed line from the routing processor to the input line cards in Figure 4.4.
With such a shadow copy at each line card, forwarding decisions can be made locally, at each input port, without invoking the centralized routing processor on a per-packet basis and thus avoiding a centralized processing bottleneck.
Let’s now consider the “simplest” case that the output port to which an incoming packet is to be switched is based on the packet’s destination address.
In the case of 32-bit IP addresses, a brute-force implementation of the forwarding table would have one entry for every possible destination address.
Since there are more than 4 billion possible addresses, this option is totally out of the question.
Figure 4.5 Input port processing As an example of how this issue of scale can be handled, let’s suppose that our router has four links, numbered 0 through 3, and that packets are to be forwarded to the link interfaces as follows: Destination Address Range Link Interface 11001000 00010111 00010000 00000000 through 11001000 00010111 00010111 111111110 11001000 00010111 00011000 00000000 1
through 11001000 00010111 00011000 11111111 11001000 00010111 00011001 00000000 through 11001000 00010111 00011111 111111112 Otherwise 3 Clearly, for this example, it is not necessary to have 4 billion entries in the router’s forwarding table.
We could, for example, have the following forwarding table with just four entries: Prefix Link Interface 11001000 00010111 00010 0 11001000 00010111 00011000 1 11001000 00010111 00011 2 Otherwise 3 With this style of forwarding table, the router matches a prefix of the packet’s destination address with the entries in the table; if there’s a match, the router forwards the packet to a link associated with the match.
For example, suppose the packet’s destination address is 11001000 00010111 00010110 10100001 ; because the 21-bit prefix of this address matches the first entry in the table, the router forwards the packet to link interface 0.
If a prefix doesn’t match any of the first three entries, then the router forwards the packet to the default interface 3.
Although this sounds simple enough, there’s a veryimportant subtlety here.
You may have noticed that it is possible for a destination address to match more than one entry.
For example, the first 24 bits of the address 11001000 00010111 00011000 10101010  match the second entry in the table, and the first 21 bits of the address match the third entry in the table.
When there are multiple matches, the router uses the longest prefix matching rule ; that is, it finds the longest matching entry in the table and forwards the packet to the link interface associatedwith the longest prefix match.
We’ll see exactly why this longest prefix-matching rule is used when we study Internet addressing in more detail in Section 4.3.
Given the existence of a forwarding table, lookup is conceptually simple— ­hardware logic just searches through the forwarding table looking for the longest prefix match.
But at Gigabit transmission rates, this lookup must be performed in nanoseconds (recall our earlier example of a 10 Gbps link and a 64-byte IPdatagram).
Thus, not only must lookup be performed in hardware, but techniques beyond a simple linear search through a large table are needed; surveys of fast lookup algorithms can be found in [Gupta 2001 , Ruiz-Sanchez 2001].
Special attention must also be paid to memory access times, resulting in designs with embedded on-chip DRAM and faster SRAM (used as a DRAM cache) memories.
Inpractice, Ternary Content Addressable Memories (TCAMs) are also often used for lookup [Yu 2004].
With a TCAM, a 32-bit IP address is presented to the memory, which returns the content of the forwarding table entry for that address in essentially constant time.
The Cisco Catalyst 6500 and 7600 Series routers and switches can hold upwards of a million TCAM forwarding table entries [Cisco TCAM 2014] .
Once a packet’s output port has been determined via the lookup, the packet can be sent into the switching fabric.
In some designs, a packet may be temporarily blocked from entering the switchingfabric if packets from other input ports are currently using the fabric.
A blocked packet will be queued atthe input port and then scheduled to cross the fabric at a later point in time.
We’ll take a closer look atthe blocking, queuing, and scheduling of packets (at both input ports and output ports) shortly.
Although“lookup” is arguably the most important action in input port processing, many other actions must betaken: (1) physical- and link-layer processing must occur, as discussed previously; (2) the packet’s version number, checksum and time-to-live field—all of which we’ll study in Section 4.3—must be checked and the latter two fields rewritten; and (3) counters used for network management (such as the number of IP datagrams received) must be updated.
Let’s close our discussion of input port processing by noting that the input port steps of looking up a destination IP address (“match”) and then sending the packet into the switching fabric to the specifiedoutput port (“action”) is a specific case of a more general “match plus action” abstraction that is performed in many networked devices, not just routers.
In link-layer switches (covered in Chapter 6), link-layer destination addresses are looked up and several actions may be taken in addition to sendingthe frame into the switching fabric towards the output port.
In firewalls (covered in Chapter 8)—devices that filter out selected incoming packets—an incoming packet whose header matches a given criteria (e.g., a combination of source/destination IP addresses and transport-layer port numbers) may be dropped (action).
In a network address translator (NAT, covered in Section 4.3), an incoming packet whose transport-layer port number matches a given value will have its port number rewritten before forwarding (action).
Indeed, the “match plus action” abstraction is both powerful and prevalent in network devices today, and is central to the notion of generalized forwarding that we’ll study in Section 4.4.
4.2.2 Switching The switching fabric is at the very heart of a router, as it is through this fabric that the packets are actually switched (that is, forwarded) from an input port to an output port.
Switching can be accomplished in a number of ways, as shown in Figure 4.6: Switching via memory.
 The simplest, earliest routers were traditional computers, with switching between input and output ports being done under direct control of the CPU (routing processor).
Input and output ports functioned as traditional I/O devices in a traditional operating system.
An input portwith an arriving packet first signaled the routing processor via an interrupt.
The packet was then copied from the input port into processor memory.
The routing processor then extracted the destination address from the header, looked up the appropriate output port in the forwarding table,and copied the packet to the output port’s buffers.
In this scenario, if the memory bandwidth is such that a maximum of B packets per second can be written into, or read from, memory, then the overall forwarding throughput (the total rate at which packets are transferred from input ports to output ports) must be less than B/2.
Note also that two packets cannot be forwarded Figure 4.6 Three switching techniques
at the same time, even if they have different destination ports, since only one memory read/write can be done at a time over the shared system bus.
Some modern routers switch via memory.
A major difference from early routers, however, is that the lookup of the destination address and the storing of the packet into the appropriate memory location are performed by processing on the input line cards.
In some ways, routers that switch via memory look very much like shared-memory multiprocessors, with the processing on a line card switching(writing) packets into the memory of the appropriate output port.
Cisco’s Catalyst 8500 series switches [Cisco 8500 2016]  internally switches packets via a shared memory.
Switching via a bus.
 In this approach, an input port transfers a packet directly to the output port over a shared bus, without intervention by the routing processor.
This is typically done by having the input port pre-pend a switch-internal label (header) to the packet indicating the local output port towhich this packet is being transferred and transmitting the packet onto the bus.
All output ports receive the packet, but only the port that matches the label will keep the packet.
The label is then removed at the output port, as this label is only used within the switch to cross the bus.
If multiplepackets arrive to the router at the same time, each at a different input port, all but one must waitsince only one packet can cross the bus at a time.
Because every packet must cross the single bus,the switching speed of the router is limited to the bus speed; in our roundabout analogy, this is as ifthe roundabout could only contain one car at a time.
Nonetheless, switching via a bus is oftensufficient for routers that operate in small local area and enterprise networks.
The Cisco 6500 router [Cisco 6500 2016]  internally switches packets over a 32-Gbps-backplane bus.
Switching via an interconnection network.
 One way to overcome the bandwidth limitation of a single, shared bus is to use a more sophisticated interconnection network, such as those that have been used in the past to interconnect processors in a multiprocessor computer architecture.
A crossbar switch is an interconnection network consisting of 2 N buses that connect N input ports to N output ports, as shown in Figure 4.6.
Each vertical bus intersects each horizontal bus at a crosspoint, which can be opened or closed at any time by the switch fabric controller (whose logic is
part of the switching fabric itself).
When a packet arrives from port A and needs to be forwarded to port Y, the switch controller closes the crosspoint at the intersection of busses A and Y, and port A then sends the packet onto its bus, which is picked up (only) by bus Y. Note that a packet from portB can be forwarded to port X at the same time, since the A-to-Y and B-to-X packets use differentinput and output busses.
Thus, unlike the previous two switching approaches, crossbar switches arecapable of forwarding multiple packets in parallel.
A crossbar switch is non-blocking —a packet being forwarded to an output port will not be blocked from reaching that output port as long as noother packet is currently being forwarded to that output port.
However, if two packets from twodifferent input ports are destined to that same output port, then one will have to wait at the input, since only one packet can be sent over any given bus at a time.
Cisco 12000 series switches [Cisco 12000 2016]  use a crossbar switching network; the Cisco 7600 series can be configured to use either a bus or crossbar switch [Cisco 7600 2016] .
More sophisticated interconnection networks use multiple stages of switching elements to allow packets from different input ports to proceed towards the same output port at the same time through the multi-stage switching fabric.
See [Tobagi 1990] for a survey of switch architectures.
The Cisco CRS employs a three-stage non-blocking switching strategy.
A router’s switching capacity can also be scaled by running multiple switching fabrics in parallel.
In this approach, input ports and output ports are connected to N switching fabrics that operate in parallel.
An input port breaks a packet into K smaller chunks, and sends (“sprays”) the chunks through K of these N switching fabrics to the selected output port, which reassembles the K chunks back into the original packet.
4.2.3 Output Port Processing Output port processing, shown in Figure 4.7, takes packets that have been stored in the output port’s memory and transmits them over the output link.
This includes selecting and de-queueing packets fortransmission, and performing the needed link-layer and physical-layer transmission functions.
4.2.4 Where Does Queuing Occur?
If we consider input and output port functionality and the configurations shown in Figure 4.6, it’s clear that packet queues may form at both the input ports and the output ports, just as we identified cases where cars may wait at the inputs and outputs of the traffic intersection in our roundabout analogy.
The location and extent of queueing (either at the input port queues or the output port queues) will depend on the traffic load, the relative speed of the switching fabric, and the line speed.
Let’s now consider these queues in a bit more detail, since as these queues grow large, the router’s memory can eventuallybe exhausted and packet loss will occur when no memory is available to store arriving packets.
Recall that in our earlier ­discussions, we said that packets were “lost within the network” or “dropped at a
router.”
It is here, at these queues within a router, where such packets are actually dropped and lost.
Figure 4.7 Output port processing Suppose that the input and output line speeds (transmission rates) all have an identical transmission rate of R  packets per second, and that there are N input ports and N output ports.
To further simplify the discussion, let’s assume that all packets have the same fixed length, and that packets arrive to input ports in a synchronous manner.
That is, the time to send a packet on any link is equal to the time toreceive a packet on any link, and during such an interval of time, either zero or one packets can arrive on an input link.
Define the switching fabric transfer rate R  as the rate at which packets can be moved from input port to output port.
If R  is N times faster than R , then only negligible queuing will occur at the input ports.
This is because even in the worst case, where all N input lines are receiving packets, and all packets are to be forwarded to the same output port, each batch of N packets (one packet per input port) can be cleared through the switch fabric before the next batch arrives.
Input Queueing But what happens if the switch fabric is not fast enough (relative to the input line speeds) to transfer all arriving packets through the fabric without delay?
In this case, packet queuing can also occur at the input ports, as packets must join input port queues to wait their turn to be transferred through theswitching fabric to the output port.
To illustrate an important consequence of this queuing, consider acrossbar switching fabric and suppose that (1) all link speeds are identical, (2) that one packet can betransferred from any one input port to a given output port in the same amount of time it takes for apacket to be received on an input link, and (3) packets are moved from a given input queue to their desired output queue in an FCFS manner.
Multiple packets can be transferred in parallel, as long as their output ports are different.
However, if two packets at the front of two input queues are destined forthe same output queue, then one of the packets will be blocked and must wait at the input queue—theswitching fabric can transfer only one packet to a given output port at a time.
Figure 4.8 shows an example in which two packets (darkly shaded) at the front of their input queues are destined for the same upper-right output port.
Suppose that the switch fabric chooses to transfer the packet from the front of the upper-left queue.
In this case, the darkly shaded packet in the lower-left queue must wait.
But not only must this darkly shaded packet wait, so too must the lightly shadedline switch switch line
packet that is queued behind that packet in the lower-left queue, even though there is no contention for the middle-right output port (the destination for the lightly shaded packet).
This phenomenon is known as head-of-the-line  (HOL) blocking  in an input-queued switch—a queued packet in an input queue must wait for transfer through the fabric (even though its output port is free) because it is blocked by another packet at the head of the line. [
Karol 1987]  shows that due to HOL blocking, the input queue will grow to unbounded length (informally, this is equivalent to saying that significant packet loss will occur) under certain assumptions as soon as the packet arrival rate on the input links reaches only 58 percent of their capacity.
A number of solutions to HOL blocking are discussed in [McKeown 1997].
Figure 4.8 HOL blocking at and input-queued switch Output Queueing Let’s next consider whether queueing can occur at a switch’s output ports.
Suppose that R  is again N times faster than R  and that packets arriving at each of the N input ports are destined to the same output port.
In this case, in the time it takes to send a single packet onto the outgoing link, N new packets will arrive at this output port (one from each of the N input ports).
Since the output port canswitch line
transmit only a single packet in a unit of time (the packet transmission time), the N arriving packets will have to queue (wait) for transmission over the outgoing link.
Then N more packets can possibly arrive in the time it takes to transmit just one of the N packets that had just previously been queued.
And so on.
Thus, packet queues can form at the output ports even when the switching fabric is N times faster than the port line speeds.
Eventually, the number of queued packets can grow large enough to exhaust available memory at the output port.
Figure 4.9 Output port queueing When there is not enough memory to buffer an incoming packet, a decision must be made to either dropthe arriving packet (a policy known as drop-tail ) or remove one or more already-queued packets to make room for the newly arrived packet.
In some cases, it may be advantageous to drop (or mark the header of) a packet before the buffer is full in order to provide a congestion signal to the sender.
A number of proactive packet-dropping and -marking policies (which collectively have become known as active queue management (AQM)  algorithms) have been proposed and analyzed [Labrador 1999, Hollot 2002].
One of the most widely studied and implemented AQM algorithms is the Random Early Detection (RED)  algorithm [Christiansen 2001; Floyd 2016].
Output port queuing is illustrated in Figure 4.9.
At time t, a packet has arrived at each of the incoming input ports, each destined for the uppermost outgoing port.
Assuming identical line speeds and a switchoperating at three times the line speed, one time unit later (that is, in the time needed to receive or send
a packet), all three original packets have been transferred to the outgoing port and are queued awaiting transmission.
In the next time unit, one of these three packets will have been transmitted over the outgoing link.
In our example, two new packets have arrived at the incoming side of the switch; one of these packets is destined for this uppermost output port.
A consequence of such queuing is that a packet scheduler at the output port must choose one packet, among those queued, for transmission— a topic we’ll cover in the following section.
Given that router buffers are needed to absorb the fluctuations in traffic load, a natural question to ask is how much buffering is required.
For many years, the rule of thumb [RFC 3439]  for buffer sizing was that the amount of buffering (B) should be equal to an average round-trip time ( RTT, say 250 msec) times the link capacity (C).
This result is based on an analysis of the queueing dynamics of a relatively small number of TCP flows [Villamizar 1994].
Thus, a 10 Gbps link with an RTT of 250 msec would need an amount of buffering equal to B 5 RTT · C 5 2.5 Gbits of buffers.
More recent theoretical and experimental efforts [Appenzeller 2004], however, suggest that when there are a large number of TCP flows (N) passing through a link, the amount of buffering needed is  With a large number of flows typically passing through large backbone router links (see, e.g., [Fraleigh 2003]), the value of N can be large, with the decrease in needed buffer size becoming quite significant. [
Appenzeller 2004; Wischik 2005; Beheshti 2008]  provide very readable discussions of the buffer-sizing problem from a theoretical, implementation, and operational standpoint.
4.2.5 Packet Scheduling Let’s now return to the question of determining the order in which queued packets are transmitted overan outgoing link.
Since you yourself have undoubtedly had to wait in long lines on many occasions andobserved how waiting customers are served, you’re no doubt familiar with many of the queueingdisciplines commonly used in routers.
There is first-come-first-served (FCFS, also known as first-in-first-out, FIFO).
The British are famous for patient and orderly FCFS queueing at bus stops and in the marketplace (“Oh, are you queueing?”).
Other countries operate on a priority basis, with one class of waiting customers given priority service over other waiting customers.
There is also round-robinqueueing, where customers are again divided into classes (as in priority queueing) but each class ofcustomer is given service in turn.
First-in-First-Out (FIFO)Figure 4.10 shows the queuing model abstraction for the FIFO link-scheduling discipline.
Packets arriving at the link output queue wait for transmission if the link is currently busy transmitting another packet.
If there is not sufficient buffering space to hold the arriving packet, the queue’s packet- discarding policy then determines whether the packet will be dropped (lost) or whether other packets willbe removed from the queue to make space for the arriving packet, as discussed above.
In ourB=RTI⋅C/N.
discussion below, we’ll ignore packet discard.
When a packet is completely transmitted over the outgoing link (that is, receives service) it is removed from the queue.
The FIFO (also known as first-come-first-served, or FCFS) scheduling discipline selects packets for link transmission in the same order in which they arrived at the output link queue.
We’re all familiar withFIFO queuing from service centers, where Figure 4.10 FIFO queueing abstraction arriving customers join the back of the single waiting line, remain in order, and are then served when they reach the front of the line.
Figure 4.11 shows the FIFO queue in operation.
Packet arrivals are indicated by numbered arrows above the upper timeline, with the number indicating the order in which the packet arrived.
Individual packet departures are shown below the lower timeline.
The time that apacket spends in service (being transmitted) is indicated by the shaded rectangle between the twotimelines.
In our examples here, let’s assume that each packet takes three units of time to betransmitted.
Under the FIFO discipline, packets leave in the same order in which they arrived.
Note thatafter the departure of packet 4, the link remains idle (since packets 1 through 4 have been transmittedand removed from the queue) until the arrival of packet 5.
Priority Queuing Under priority queuing, packets arriving at the output link are classified into priority classes upon arrival at the queue, as shown in Figure 4.12.
In practice, a network operator may configure a queue so that packets carrying network management information (e.g., as indicated by the source or destination TCP/UDP port number) receive priority over user traffic; additionally, real-time voice-over-IP packetsmight receive priority over non-real traffic such as SMTP or IMAP e-mail packets.
Each
Figure 4.11 The FIFO queue in operation Figure 4.12 The priority queueing model priority class typically has its own queue.
When choosing a packet to transmit, the priority queuing discipline will transmit a packet from the highest priority class that has a nonempty queue (that is, has packets waiting for transmission).
The choice among packets in the same priority class is typically donein a FIFO manner.
Figure 4.13 illustrates the operation of a priority queue with two priority classes.
Packets 1, 3, and 4 belong to the high-priority class, and packets 2 and 5 belong to the low-priority class.
Packet 1 arrives and, finding the link idle, begins transmission.
During the transmission of packet 1, packets 2 and 3arrive and are queued in the low- and high-priority queues, respectively.
After the transmission of packet1, packet 3 (a high-priority packet) is selected for transmission over packet 2 (which, even though it arrived earlier, is a low-priority packet).
At the end of the transmission of packet 3, packet 2 then begins transmission.
Packet 4 (a high-priority packet) arrives during the transmission of packet 2 (a low-prioritypacket).
Under a non-preemptive priority queuing  discipline, the transmission of a packet is not interrupted once it has
Figure 4.13 The priority queue in operation Figure 4.14 The two-class robin queue in operation begun.
In this case, packet 4 queues for transmission and begins being transmitted after the transmission of packet 2 is completed.
Round Robin and Weighted Fair Queuing (WFQ) Under the round robin queuing discipline, packets are sorted into classes as with priority queuing.
However, rather than there being a strict service priority among classes, a round robin scheduleralternates service among the classes.
In the simplest form of round robin scheduling, a class 1 packet istransmitted, followed by a class 2 packet, followed by a class 1 packet, followed by a class 2 packet,and so on.
A so-called work-conserving queuing  discipline will never allow the link to remain idle whenever there are packets (of any class) queued for transmission.
A work-conserving round robindiscipline that looks for a packet of a given class but finds none will immediately check the next class inthe round robin sequence.
Figure 4.14 illustrates the operation of a two-class round robin queue.
In this example, packets 1, 2, and
4 belong to class 1, and packets 3 and 5 belong to the second class.
Packet 1 begins transmission immediately upon arrival at the output queue.
Packets 2 and 3 arrive during the transmission of packet 1 and thus queue for transmission.
After the transmission of packet 1, the link scheduler looks for a class2 packet and thus transmits packet 3.
After the transmission of packet 3, the scheduler looks for a class1 packet and thus transmits packet 2.
After the transmission of packet 2, packet 4 is the only queuedpacket; it is thus transmitted immediately after packet 2.
A generalized form of round robin queuing that has been widely implemented in routers is the so-called weighted fair queuing (WFQ) discipline  [Demers 1990 ; Parekh 1993 ; Cisco QoS 2016].
WFQ is illustrated in Figure 4.15.
Here, arriving packets are classified and queued in the appropriate per-class waiting area.
As in round robin scheduling, a WFQ scheduler will serve classes in a circular manner—first serving class 1, then serving class 2, then serving class 3, and then (assuming there are threeclasses) repeating the service pattern.
WFQ is also a work-conserving Figure 4.15 Weighted fair queueing queuing discipline and thus will immediately move on to the next class in the service sequence when it finds an empty class queue.
WFQ differs from round robin in that each class may receive a differential amount of service in any interval of time.
Specifically, each class, i, is assigned a weight, w. Under WFQ, during any interval of time during which there are class i packets to send, class i will then be guaranteed to receive a fraction of service equal to  where the sum in the denominator is taken over all classes that also have packets queued for transmission.
In the worst case, even if all classes have queued packets, class i will still be guaranteed to receive a fraction  of the bandwidth, where in this worst case the sum in the denominator is over all classes.
Thus, for a link with transmission rate R, class i will always achieve a throughput of at least  Our description of WFQ has been idealized, as we have not considered the fact that packets are discrete and a packet’s transmission will not be interrupted to begintransmission of another packet; [Demers 1990 ; Parekh 1993]  discuss this packetization issue.i wi/(∑wj), wi/(∑wj) R⋅wi/(∑wj).
4.3 The Internet Protocol (IP): IPv4, Addressing, IPv6, and More Our study of the network layer thus far in Chapter 4—the notion of the data and control plane component of the network layer, our distinction between forwarding and routing, the identification of various network service models, and our look inside a router—have often been without reference to anyspecific computer network architecture or protocol.
In this section we’ll focus on key aspects of the network layer on today’s Internet and the celebrated Internet Protocol (IP).
There are two versions of IP in use today.
We’ll first examine the widely deployed IP protocol version 4, which is usually referred to simply as IPv4 [RFC 791] Figure 4.16 IPv4 datagram format in Section 4.3.1.
We’ll examine IP version 6 [RFC 2460 ; RFC 4291] , which has been proposed to replace IPv4, in Section 4.3.5.
In between, we’ll primarily cover Internet addressing—a topic that might seem rather dry and detail-oriented but we’ll see is crucial to understanding how the Internet’s network layer works.
To master IP addressing is to master the Internet’s network layer itself!
4.3.1 IPv4 Datagram Format Recall that the Internet’s network-layer packet is referred to as a datagram .
We begin our study of IP with an overview of the syntax and semantics of the IPv4 datagram.
You might be thinking that nothing could be drier than the syntax and semantics of a packet’s bits.
Nevertheless, the datagram plays acentral role in the Internet—every networking student and professional needs to see it, absorb it, and master it. (
And just to see that protocol headers can indeed be fun to study, check out [Pomeranz 2010] ).
The IPv4 datagram format is shown in Figure 4.16.
The key fields in the IPv4 datagram are the following: Version number.
These 4 bits specify the IP protocol version of the datagram.
By looking at the version number, the router can determine how to interpret the remainder of the IP datagram.
Different versions of IP use different datagram formats.
The datagram format for IPv4 is shown in Figure 4.16.
The datagram format for the new version of IP (IPv6) is discussed in Section 4.3.5.
Header length.
Because an IPv4 datagram can contain a variable number of options (which are included in the IPv4 datagram header), these 4 bits are needed to determine where in the IP datagram the payload (e.g., the transport-layer segment being encapsulated in this datagram)actually begins.
Most IP datagrams do not contain options, so the typical IP datagram has a 20-byte header.
Type of service.
The type of service (TOS) bits were included in the IPv4 header to allow different types of IP datagrams to be distinguished from each other.
For example, it might be useful to distinguish real-time datagrams (such as those used by an IP telephony application) from non-real-time traffic (for example, FTP).
The specific level of service to be provided is a policy issue determined and configured by the network administrator for that router.
We also learned in Section 3.7.2 that two of the TOS bits are used for Explicit Congestion ­Notification.
Datagram length.
This is the total length of the IP datagram (header plus data), measured in bytes.
Since this field is 16 bits long, the theoretical maximum size of the IP datagram is 65,535 bytes.
However, datagrams are rarely larger than 1,500 bytes, which allows an IP datagram to fit in thepayload field of a maximally sized Ethernet frame.
Identifier, flags, fragmentation offset.
 These three fields have to do with so-called IP fragmentation, a topic we will consider shortly.
Interestingly, the new version of IP, IPv6, does notallow for fragmentation.
Time-to-live.
The time-to-live (TTL) field is included to ensure that datagrams do not circulate forever (due to, for example, a long-lived routing loop) in the network.
This field is decremented by one each time the datagram is processed by a router.
If the TTL field reaches 0, a router must dropthat datagram.
Protocol.
 This field is typically used only when an IP datagram reaches its final destination.
The value of this field indicates the specific transport-layer protocol to which the data portion of this IPdatagram should be passed.
For example, a value of 6 indicates that the data portion is passed toTCP, while a value of 17 indicates that the data is passed to UDP.
For a list of all possible values,
see [IANA Protocol Numbers 2016] .
Note that the protocol number in the IP datagram has a role that is analogous to the role of the port number field in the transport-layer segment.
The protocol number is the glue that binds the network and transport layers together, whereas the port number is the glue that binds the transport and application layers together.
We’ll see in Chapter 6 that the link- layer frame also has a special field that binds the link layer to the network layer.
Header checksum.
 The header checksum aids a router in detecting bit errors in a received IP datagram.
The header checksum is computed by treating each 2 bytes in the header as a numberand summing these numbers using 1s complement arithmetic.
As discussed in Section 3.3, the 1s complement of this sum, known as the Internet checksum, is stored in the checksum field.
A router computes the header checksum for each received IP datagram and detects an error condition if the checksum carried in the datagram header does not equal the computed checksum.
Routers typically discard datagrams for which an error has been detected.
Note that the checksum must berecomputed and stored again at each router, since the TTL field, and possibly the options field aswell, will change.
An interesting discussion of fast algorithms for computing the Internet checksum is [RFC 1071] .
A question often asked at this point is, why does TCP/IP perform error checking at both the transport and network layers?
There are several reasons for this repetition.
First, note that only the IP header is checksummed at the IP layer, while the TCP/UDP checksum is computed over theentire TCP/UDP segment.
Second, TCP/UDP and IP do not necessarily both have to belong to the same protocol stack.
TCP can, in principle, run over a different network-layer protocol (for example, ATM) [Black 1995] ) and IP can carry data that will not be passed to TCP/UDP.
Source and destination IP addresses.
 When a source creates a datagram, it inserts its IP address into the source IP address field and inserts the address of the ultimate destination into the destination IP address field.
Often the source host determines the destination address via a DNS lookup, as discussed in Chapter 2.
We’ll discuss IP addressing in detail in Section 4.3.3.
Options.
 The options fields allow an IP header to be extended.
Header options were meant to be used rarely—hence the decision to save overhead by not including the information in options fields in every datagram header.
However, the mere existence of options does complicate matters—sincedatagram headers can be of variable length, one cannot determine a priori where the data field will start.
Also, since some datagrams may require options processing and others may not, the amount of time needed to process an IP datagram at a router can vary greatly.
These considerationsbecome particularly important for IP processing in high-performance routers and hosts.
For these reasons and others, IP options were not included in the IPv6 header, as discussed in Section 4.3.5.
Data (payload).
Finally, we come to the last and most important field—the raison d’etre  for the datagram in the first place!
In most circumstances, the data field of the IP datagram contains the transport-layer segment (TCP or UDP) to be delivered to the destination.
However, the data field can carry other types of data, such as ICMP messages (discussed in Section 5.6).
Note that an IP datagram has a total of 20 bytes of header (assuming no options).
If the datagram carries a TCP segment, then each (non-fragmented) datagram carries a total of 40 bytes of header (20bytes of IP header plus 20 bytes of TCP header) along with the application-layer message.
4.3.2 IPv4 Datagram Fragmentation We’ll see in Chapter 6 that not all link-layer protocols can carry network-layer packets of the same size.
Some protocols can carry big datagrams, whereas other protocols can carry only little datagrams.
For example, Ethernet frames can carry up to 1,500 bytes of data, whereas frames for some wide-area links can carry no more than 576 bytes.
The maximum amount of data that a link-layer frame can carry is called the maximum transmission unit (MTU).
Because each IP datagram is encapsulated within the link-layer frame for transport from one router to the next router, the MTU of the link-layer protocol places a hard limit on the length of an IP datagram.
Having a hard limit on the size of an IP datagram is notmuch of a problem.
What is a problem is that each of the links along the route between sender anddestination can use different link-layer protocols, and each of these protocols can have different MTUs.
To understand the forwarding issue better, imagine that you are a router that interconnects several links, each running different link-layer protocols with different MTUs.
Suppose you receive an IP datagram from one link.
You check your forwarding table to determine the outgoing link, and this outgoing link has an MTU that is smaller than the length of the IP datagram.
Time to panic—how are you going to squeeze this oversized IP datagram into the payload field of the link-layer frame?
The solution is tofragment the payload in the IP datagram into two or more smaller IP datagrams, encapsulate each ofthese smaller IP datagrams in a separate link-layer frame; and send these frames over the outgoing link.
Each of these smaller datagrams is referred to as a fragment.
Fragments need to be reassembled before they reach the transport layer at the destination.
Indeed,both TCP and UDP are expecting to receive complete, unfragmented segments from the network layer.
The designers of IPv4 felt that reassembling datagrams in the routers would introduce significant complication into the protocol and put a damper on router performance. (
If you were a router, would youwant to be reassembling fragments on top of everything else you had to do?)
Sticking to the principle ofkeeping the network core simple, the designers of IPv4 decided to put the job of datagram reassemblyin the end systems rather than in network routers.
When a destination host receives a series of datagrams from the same source, it needs to determine whether any of these datagrams are fragments of some original, larger datagram.
If some datagramsare fragments, it must further determine when it has received the last fragment and how the fragments it has received should be pieced back together to form the original datagram.
To allow the destination host to perform these reassembly tasks, the designers of IP (version 4) put identification, flag,  and fragmentation offset fields in the IP datagram header.
When a datagram is created, the sending host stamps the datagram with an identification number as well as source and destination addresses.
Typically, the sending host increments the identification number for each datagram it sends.
When arouter needs to fragment a datagram, each resulting datagram (that is, fragment) is stamped with the
source address, destination address, and identification number of the original datagram.
When the destination receives a series of datagrams from the same sending host, it can examine the identification numbers of the datagrams to determine which of the datagrams are actually fragments of the samelarger datagram.
Because IP is an unreliable service, one or more of the fragments may never arrive atthe destination.
For this reason, in order for the destination host to be absolutely sure it has received thelast fragment of Figure 4.17 IP fragmentation and reassembly the original datagram, the last fragment has a flag bit set to 0, whereas all the other fragments have this flag bit set to 1.
Also, in order for the destination host to determine whether a fragment is missing (and also to be able to reassemble the fragments in their proper order), the offset field is used to specifywhere the fragment fits within the original IP datagram.
Figure 4.17 illustrates an example.
A datagram of 4,000 bytes (20 bytes of IP header plus 3,980 bytes of IP payload) arrives at a router and must be forwarded to a link with an MTU of 1,500 bytes.
This implies that the 3,980 data bytes in the original datagram must be allocated to three separate fragments(each of which is also an IP datagram).
The online material for this book, and the problems at the end of this chapter will allow you to explore fragmentation in more detail.
Also, on this book’s Web site, we provide a Java applet that generatesfragments.
You provide the incoming datagram size, the MTU, and the incoming datagram identification.
The applet automatically generates the fragments for you.
See http:/ /www.pearsonhighered.com/ cs- resources/ .
4.3.3 IPv4 Addressing We now turn our attention to IPv4 addressing.
Although you may be thinking that addressing must be a straightforward topic, hopefully by the end of this section you’ll be convinced that Internet addressing is not only a juicy, subtle, and interesting topic but also one that is of central importance to the Internet.
An excellent treatment of IPv4 addressing can be found in the first chapter in [Stewart 1999] .
Before discussing IP addressing, however, we’ll need to say a few words about how hosts and routers are connected into the Internet.
A host typically has only a single link into the network; when IP in thehost wants to send a datagram, it does so over this link.
The boundary between the host and thephysical link is called an interface.
Now consider a router and its interfaces.
Because a router’s job is to receive a datagram on one link and forward the datagram on some other link, a router necessarily hastwo or more links to which it is connected.
The boundary between the router and any one of its links isalso called an interface.
A router thus has multiple interfaces, one for each of its links.
Because everyhost and router is capable of sending and receiving IP datagrams, IP requires each host and router interface to have its own IP address.
Thus, an IP address is technically associated with an interface, rather than with the host or router containing that interface.
Each IP address is 32 bits long (equivalently, 4 bytes), and there are thus a total of 2  (or approximately 4 billion) possible IP addresses.
These addresses are typically written in so-called dotted-decimal notation , in which each byte of the address is written in its decimal form and is separated by a period (dot) from other bytes in the address.
For example, consider the IP address 193.32.216.9.
The 193 is the decimal equivalent of the first 8 bits of the address; the 32 is the decimal equivalent of the second 8bits of the address, and so on.
Thus, the address 193.32.216.9 in binary notation is 11000001 00100000 11011000 00001001 Each interface on every host and router in the global Internet must have an IP address that is globally unique (except for interfaces behind NATs, as discussed in Section 4.3.4).
These addresses cannot be chosen in a willy-nilly manner, however.
A portion of an interface’s IP address will be determined by the subnet to which it is connected.
Figure 4.18 provides an example of IP addressing and interfaces.
In this figure, one router (with three interfaces) is used to interconnect seven hosts.
Take a close look at the IP addresses assigned to the host and router interfaces, as there are several things to notice.
The three hosts in the upper-left portion of Figure 4.18, and the router interface to which they are connected, all have an IP address of the form32
223.1.1.xxx.
That is, they all have the same leftmost 24 bits in their IP address.
These four interfaces are also interconnected to each other by a network that contains no routers .
This network could be interconnected by an Ethernet LAN, in which case the interfaces would be interconnected by an Ethernet switch (as we’ll discuss in Chapter 6), or by a wireless access point (as we’ll discuss in Chapter 7).
We’ll represent this routerless network connecting these hosts as a cloud for now, and dive into the internals of such networks in Chapters 6 and 7.
In IP terms, this network interconnecting three host interfaces and one router interface forms a subnet [RFC 950]. (
A subnet is also called an IP network or simply Figure 4.18 Interface addresses and subnets a network in the Internet literature.)
IP addressing assigns an address to this subnet: 223.1.1.0/24, where the /24 (“slash-24”) notation, sometimes known as a subnet mask, indicates that the leftmost 24 bits of the 32-bit quantity define the subnet address.
The 223.1.1.0/24 subnet thus consists of the three host interfaces (223.1.1.1, 223.1.1.2, and 223.1.1.3) and one router interface (223.1.1.4).
Any additional hosts attached to the 223.1.1.0/24 subnet would be required  to have an address of the form 223.1.1.xxx.
There are two additional subnets shown in Figure 4.18: the 223.1.2.0/24 network and the 223.1.3.0/24 subnet.
Figure 4.19 illustrates the three IP subnets present in Figure 4.18.
The IP definition of a subnet is not restricted to Ethernet segments that connect multiple hosts to arouter interface.
To get some insight here, consider Figure 4.20, which shows three routers that are interconnected with each other by point-to-point links.
Each router has three interfaces, one for each point-to-point link and one for the broadcast link that directly connects the router to a pair of hosts.
What
subnets are present here?
Three subnets, 223.1.1.0/24, 223.1.2.0/24, and 223.1.3.0/24, are similar to the subnets we encountered in Figure 4.18.
But note that there are three additional subnets in this example as well: one subnet, 223.1.9.0/24, for the interfaces that connect routers R1 and R2; another subnet, 223.1.8.0/24, for the interfaces that connect routers R2 and R3; and a third subnet,223.1.7.0/24, for the interfaces that connect routers R3 and R1.
For a general interconnected system of routers and hosts, we can use the following recipe to define the subnets in the system: Figure 4.19 Subnet addresses To determine the subnets, detach each interface from its host or router, creating islands of isolated networks, with interfaces terminating the end points of the isolated networks.
Each of these isolatednetworks is called a subnet.
If we apply this procedure to the interconnected system in Figure 4.20, we get six islands or subnets.
From the discussion above, it’s clear that an organization (such as a company or academic institution)with multiple Ethernet segments and point-to-point links will have multiple subnets, with all of thedevices on a given subnet having the same subnet address.
In principle, the different subnets couldhave quite different subnet addresses.
In practice, however, their subnet addresses often have much in common.
To understand why, let’s next turn our attention to how addressing is handled in the global Internet.
The Internet’s address assignment strategy is known as Classless Interdomain Routing  (CIDR— pronounced cider) [RFC 4632] .
CIDR generalizes the notion of subnet addressing.
As with subnet
addressing, the 32-bit IP address is divided into two parts and again has the dotted-decimal form a.b.c.d/x, where x indicates the number of bits in the first part of the address.
The x most significant bits of an address of the form a.b.c.d/x constitute the network portion of the IP address, and are often referred to as the prefix  (or network prefix ) of the address.
An organization is typically assigned a block of contiguous addresses, that is, a range of addresses with a common prefix (see the Principles in Practice feature).
In this case, the IP addresses of devices within the organization will share the common prefix.
When we cover the Internet’s BGP routing protocol in Figure 4.20 Three routers interconnecting six subnets Section 5.4, we’ll see that only these x leading prefix bits are considered by routers outside the organization’s network.
That is, when a router outside the organization forwards a datagram whose destination address is inside the organization, only the leading x bits of the address need be considered.
This considerably reduces the size of the forwarding table in these routers, since a single  entry of the form a.b.c.d/x will be sufficient to forward packets to any destination within the organization.
The remaining 32- x bits of an address can be thought of as distinguishing among the devices within  the organization, all of which have the same network prefix.
These are the bits that will be considered whenforwarding packets at routers within  the organization.
These lower-order bits may (or may not) have an
additional subnetting structure, such as that discussed above.
For example, suppose the first 21 bits of the CIDRized address a.b.c.d/21 specify the organization’s network prefix and are common to the IP addresses of all devices in that organization.
The remaining 11 bits then identify the specific hosts in theorganization.
The organization’s internal structure might be such that these 11 rightmost bits are usedfor subnetting within the organization, as discussed above.
For example, a.b.c.d/24 might refer to aspecific subnet within the organization.
Before CIDR was adopted, the network portions of an IP address were constrained to be 8, 16, or 24 bits in length, an addressing scheme known as classful addressing, since subnets with 8-, 16-, and 24-bit subnet addresses were known as class A, B, and C networks, respectively.
The requirement thatthe subnet portion of an IP address be exactly 1, 2, or 3 bytes long turned out to be problematic forsupporting the rapidly growing number of organizations with small and medium-sized subnets.
A class C (/24) subnet could accommodate only up to 2  − 2 = 254 hosts (two of the 2  = 256 addresses are reserved for special use)—too small for many organizations.
However, a class B (/16) subnet, which supports up to 65,634 hosts, was too large.
Under classful addressing, an organization with, say, 2,000hosts was typically allocated a class B (/16) subnet address.
This led to a rapid depletion of the class Baddress space and poor utilization of the assigned address space.
For example, the organization thatused a class B address for its 2,000 hosts was allocated enough of the address space for up to 65,534interfaces—leaving more than 63,000 addresses that could not be used by other organizations.
PRINCIPLES IN PRACTICE This example of an ISP that connects eight organizations to the Internet nicely illustrates how carefully allocated CIDRized addresses facilitate routing.
Suppose, as shown in Figure 4.21, that the ISP (which we’ll call Fly-By-Night-ISP) advertises to the outside world that it should be sent any datagrams whose first 20 address bits match 200.23.16.0/20.
The rest of the worldneed not know that within the address block 200.23.16.0/20 there are in fact eight otherorganizations, each with its own subnets.
This ability to use a single prefix to advertise multiple networks is often referred to as address aggregation (also route aggregation  or route summarization).
Address aggregation works extremely well when addresses are allocated in blocks to ISPs and then from ISPs to client organizations.
But what happens when addresses are not allocated insuch a hierarchical manner?
What would happen, for example, if Fly-By-Night-ISP acquiresISPs-R-Us and then has Organization 1 connect to the Internet through its subsidiary ISPs-R- Us?
As shown in Figure 4.21, the subsidiary ISPs-R-Us owns the address block 199.31.0.0/16, but Organization 1’s IP addresses are unfortunately outside of this address block.
What should be done here?
Certainly, Organization 1 could renumber all of its routers and hosts to have addresses within the ISPs-R-Us address block.
But this is a costly solution, and Organization 1might well be reassigned to another subsidiary in the future.
The solution typically adopted is for Organization 1 to keep its IP addresses in 200.23.18.0/23.
In this case, as shown in Figure 4.22,8 8
Fly-By-Night-ISP continues to advertise the address block 200.23.16.0/20 and ISPs-R-Us continues to advertise 199.31.0.0/16.
However, ISPs-R-Us now also advertises the block of addresses for Organization 1, 200.23.18.0/23.
When other routers in the larger Internet see the address blocks 200.23.16.0/20 (from Fly-By-Night-ISP) and 200.23.18.0/23 (from ISPs-R-Us) and want to route to an address in the block 200.23.18.0/23, they will use longest prefix matching  (see Section 4.2.1), and route toward ISPs-R-Us, as it advertises the longest (i.e., most-specific) address prefix that matches the destination address.
Figure 4.21 Hierarchical addressing and route aggregation
Figure 4.22 ISPs-R-Us has a more specific route to Organization 1 We would be remiss if we did not mention yet another type of IP address, the IP broadcast address 255.255.255.255.
When a host sends a datagram with destination address 255.255.255.255, the message is delivered to all hosts on the same subnet.
Routers optionally forward the message intoneighboring subnets as well (although they usually don’t).
Having now studied IP addressing in detail, we need to know how hosts and subnets get their addresses in the first place.
Let’s begin by looking at how an organization gets a block of addresses forits devices, and then look at how a device (such as a host) is assigned an address from within theorganization’s block of addresses.
Obtaining a Block of Addresses In order to obtain a block of IP addresses for use within an organization’s subnet, a network administrator might first contact its ISP, which would provide addresses from a larger block of addressesthat had already been allocated to the ISP.
For example, the ISP may itself have been allocated theaddress block 200.23.16.0/20.
The ISP, in turn, could divide its address block into eight equal-sizedcontiguous address blocks and give one of these address blocks out to each of up to eight organizationsthat are supported by this ISP, as shown below. (
We have underlined the subnet part of theseaddresses for your convenience.)
ISP’s block:     200.23.16.0/20      11001000 00010111 0001 0000 00000000
Organization 0    200.23.16.0/23      11001000 00010111 0001000 0 00000000 Organization 1    200.23.18.0/23      11001000 00010111 0001001 0 00000000 Organization 2    200.23.20.0/23      11001000 00010111 0001010 0 00000000     …   …                                    … Organization 7    200.23.30.0/23      11001000 00010111 0001111 0 00000000 While obtaining a set of addresses from an ISP is one way to get a block of addresses, it is not the only way.
Clearly, there must also be a way for the ISP itself to get a block of addresses.
Is there a global authority that has ultimate responsibility for managing the IP address space and allocating address blocks to ISPs and other organizations?
Indeed there is!
IP addresses are managed under the authority of the Internet Corporation for Assigned Names and Numbers (ICANN) [ICANN 2016] , based on guidelines set forth in [RFC 7020] .
The role of the nonprofit ICANN organization [NTIA 1998] is not only to allocate IP addresses, but also to manage the DNS root servers.
It also has the very contentious job of assigning domain names and resolving domain name disputes.
The ICANN allocates addresses toregional Internet registries (for example, ARIN, RIPE, APNIC, and LACNIC, which together form the Address Supporting Organization of ICANN [ASO-ICANN 2016] ), and handle the allocation/management of addresses within their regions.
Obtaining a Host Address: The Dynamic Host Configuration Protocol Once an organization has obtained a block of addresses, it can assign individual IP addresses to the host and router interfaces in its organization.
A system administrator will typically manually configure theIP addresses into the router (often remotely, with a network management tool).
Host addresses can alsobe configured manually, but typically this is done using the Dynamic Host Configuration Protocol (DHCP)  [RFC 2131] .
DHCP allows a host to obtain (be allocated) an IP address automatically.
A network administrator can configure DHCP so that a given host receives the same IP address each time it connects to the network, or a host may be assigned a temporary IP address  that will be different each time the host connects to the network.
In addition to host IP address assignment, DHCP also allows a host to learn additional information, such as its subnet mask, the address of its first-hop router(often called the default gateway), and the address of its local DNS server.
Because of DHCP’s ability to automate the network-related aspects of connecting a host into a network, it is often referred to as a plug-and-play  or zeroconf (zero-configuration) protocol.
This capability makes it very attractive to the network administrator who would otherwise have to perform these tasks manually!
DHCP is also enjoying widespread use in residential Internet access networks, enterprise
networks, and in wireless LANs, where hosts join and leave the network frequently.
Consider, for example, the student who carries a laptop from a dormitory room to a library to a classroom.
It is likely that in each location, the student will be connecting into a new subnet and hence will need a new IPaddress at each location.
DHCP is ideally suited to this situation, as there are many users coming andgoing, and addresses are needed for only a limited amount of time.
The value of DHCP’s plug-and-playcapability is clear, since it’s unimaginable that a system administrator would be able to reconfigure laptops at each location, and few students (except those taking a computer networking class!)
would have the expertise to configure their laptops manually.
DHCP is a client-server protocol.
A client is typically a newly arriving host wanting to obtain network configuration information, including an IP address for itself.
In the simplest case, each subnet (in the addressing sense of Figure 4.20) will have a DHCP server.
If no server is present on the subnet, a DHCP relay agent (typically a router) that knows the address of a DHCP server for that network isneeded.
Figure 4.23 shows a DHCP server attached to subnet 223.1.2/24, with the router serving as the relay agent for arriving clients attached to subnets 223.1.1/24 and 223.1.3/24.
In our discussion below, we’ll assume that a DHCP server is available on the subnet.
For a newly arriving host, the DHCP protocol is a four-step process, as shown in Figure 4.24 for the network setting shown in Figure 4.23.
In this figure, yiaddr  (as in “your Internet address”) indicates the address being allocated to the newly arriving client.
The four steps are: Figure 4.23 DHCP client and server
DHCP server discovery.
 The first task of a newly arriving host is to find a DHCP server with which to interact.
This is done using a DHCP discover message , which a client sends within a UDP packet to port 67.
The UDP packet is encapsulated in an IP datagram.
But to whom should this datagram be sent?
The host doesn’t even know the IP address of the network to which it is attaching, much less the address of a DHCP server for this network.
Given this, the DHCP client creates an IP datagram containing its DHCP discover message along with the broadcast destinationIP address of 255.255.255.255 and a “this host” source IP address of 0.0.0.0.
The DHCP clientpasses the IP datagram to the link layer, which then broadcasts this frame to all nodes attached to the subnet (we will cover the details of link-layer broadcasting in Section 6.4).
DHCP server offer(s).
 A DHCP server receiving a DHCP discover message responds to the client with a DHCP offer message  that is broadcast to all nodes on the subnet, again using the IP broadcast address of 255.255.255.255. (
You might want to think about why this server reply must also be broadcast).
Since several DHCP servers can be present on the subnet, the client may find itself in the enviable position of being able to choose from among several offers.
Each
Figure 4.24 DHCP client-server interaction server offer message contains the transaction ID of the received discover message, the proposed IP address for the client, the network mask, and an IP address lease time —the amount of time for which the IP address will be valid.
It is common for the server to set the lease time to several hours or days [Droms 2002] .
DHCP request.
The newly arriving client will choose from among one or more server offers and respond to its selected offer with a DHCP request message , echoing back the configuration parameters.
DHCP ACK.
 The server responds to the DHCP request message with a DHCP ACK message , confirming the requested parameters.
Once the client receives the DHCP ACK, the interaction is complete and the client can use the DHCP- allocated IP address for the lease duration.
Since a client may want to use its address beyond the
lease’s expiration, DHCP also provides a mechanism that allows a client to renew its lease on an IP address.
From a mobility aspect, DHCP does have one very significant shortcoming.
Since a new IP address is obtained from DHCP each time a node connects to a new subnet, a TCP connection to a remote application cannot be maintained as a mobile node moves between subnets.
In Chapter 6, we will examine mobile IP—an extension to the IP infrastructure that allows a mobile node to use a single permanent address as it moves between subnets.
Additional details about DHCP can be found in [Droms 2002]  and [dhc 2016].
An open source reference implementation of DHCP is available from the Internet Systems Consortium [ISC 2016].
4.3.4 Network Address Translation (NAT) Given our discussion about Internet addresses and the IPv4 datagram format, we’re now well aware that every IP-capable device needs an IP address.
With the proliferation of small office, home office (SOHO)subnets, this would seem to imply that whenever a SOHO wants to install a LAN to connect multiplemachines, a range of addresses would need to be allocated by the ISP to cover all of the SOHO’s IPdevices (including phones, tablets, gaming devices, IP TVs, printers and more).
If the subnet grew bigger, a larger block of addresses would have to be allocated.
But what if the ISP had already allocated the contiguous portions of the SOHO network’s current address range?
And what typical homeownerwants (or should need) to know how to manage IP addresses in the first place?
Fortunately, there is asimpler approach to address allocation that has found increasingly widespread use in such scenarios:network address translation (NAT)  [RFC 2663 ; RFC 3022 ; Huston 2004, Zhang 2007; Cisco NAT 2016] .
Figure 4.25 shows the operation of a NAT-enabled router.
The NAT-enabled router, residing in thehome, has an interface that is part of the home network on the right of Figure 4.25.
Addressing within the home network is exactly as we have seen above—all four interfaces in the home network have thesame subnet address of 10.0.0/24.
The address space 10.0.0.0/8 is one of three portions of the IP address space that is reserved in [RFC 1918]  for a private network  or a realm with private addresses , such as the home network in Figure 4.25.
A realm with private addresses refers to a network whose addresses only have meaning to devices within that network.
To see why this is important, consider the fact that there are hundreds of thousands of home networks, many using thesame address space, 10.0.0.0/24.
Devices within a given home network can send packets to each other using 10.0.0.0/24 addressing.
However, packets forwarded beyond  the home network into the larger global Internet clearly cannot use these addresses (as either a source or a destination address) because there are hundreds of thousands of networks using this block of addresses.
That is, the10.0.0.0/24 addresses can only have meaning within the
Figure 4.25 Network address translation given home network.
But if private addresses only have meaning within a given network, how is addressing handled when packets are sent to or received from the global Internet, where addresses are necessarily unique?
The answer lies in understanding NAT.
The NAT-enabled router does not look like a router to the outside world.
Instead the NAT router behaves to the outside world as a single  device with a single  IP address.
In Figure 4.25, all traffic leaving the home router for the larger Internet has a source IP address of 138.76.29.7, and all traffic entering the home router must have a destination address of 138.76.29.7.
In essence, the NAT-enabledrouter is hiding the details of the home network from the outside world. (
As an aside, you might wonderwhere the home network computers get their addresses and where the router gets its single IP address.
Often, the answer is the same—DHCP!
The router gets its address from the ISP’s DHCP server, and the router runs a DHCP server to provide addresses to computers within the NAT-DHCP-router-controlled home network’s address space.)
If all datagrams arriving at the NAT router from the WAN have the same destination IP address (specifically, that of the WAN-side interface of the NAT router), then how does the router know theinternal host to which it should forward a given datagram?
The trick is to use a NAT translation table at the NAT router, and to include port numbers as well as IP addresses in the table entries.
Consider the example in Figure 4.25.
Suppose a user sitting in a home network behind host 10.0.0.1 requests a Web page on some Web server (port 80) with IP address 128.119.40.186.
The host 10.0.0.1 assigns the (arbitrary) source port number 3345 and sends the datagram into the LAN.
The NAT routerreceives the datagram, generates a new source port number 5001 for the datagram, replaces the
source IP address with its WAN-side IP address 138.76.29.7, and replaces the original source port number 3345 with the new source port number 5001.
When generating a new source port number, the NAT router can select any source port number that is not currently in the NAT translation table. (
Notethat because a port number field is 16 bits long, the NAT protocol can support over 60,000 simultaneousconnections with a single WAN-side IP address for the router!)
NAT in the router also adds an entry toits NAT translation table.
The Web server, blissfully unaware that the arriving datagram containing the HTTP request has been manipulated by the NAT router, responds with a datagram whose destination address is the IP address of the NAT router, and whose destination port number is 5001.
When thisdatagram arrives at the NAT router, the router indexes the NAT translation table using the destination IPaddress and destination port number to obtain the appropriate IP address (10.0.0.1) and destination portnumber (3345) for the browser in the home network.
The router then rewrites the datagram’s destinationaddress and destination port number, and forwards the datagram into the home network.
NAT has enjoyed widespread deployment in recent years.
But NAT is not without detractors.
First, one might argue that, port numbers are meant to be used for addressing processes, not for addressing hosts.
This violation can indeed cause problems for servers running on the home network, since, as we have seen in Chapter 2, server processes wait for incoming requests at well-known port numbers and peers in a P2P protocol need to accept incoming connections when acting as servers.
Technicalsolutions to these problems include NAT traversal  tools [RFC 5389]  and Universal Plug and Play (UPnP), a protocol that allows a host to discover and configure a nearby NAT [UPnP Forum 2016].
More “philosophical” arguments have also been raised against NAT by architectural purists.
Here, the concern is that routers are meant to be layer 3 (i.e., network-layer) devices, and should process packets only up to the network layer.
NAT violates this principle that hosts should be talking directly with each other, without interfering nodes modifying IP addresses, much less port numbers.
But like it or not, NAT has not become an important component of the Internet, as have other so-called middleboxes  [Sekar 2011]  that operate at the network layer but have functions that are quite different from routers.
Middleboxes do not perform traditional datagram forwarding, but instead perform functions such as NAT, load balancing of traffic flows, traffic firewalling (see accompanying sidebar), and more.
The generalized forwarding paradigm that we’ll study shortly in Section 4.4 allows a number of these middlebox functions, as well as traditional router forwarding, to be accomplished in a common, integrated manner.
FOCUS ON SECURITY INSPECTING DATAGRAMS: FIREWALLS AND INTRUSION DETECTION SYSTEMS Suppose you are assigned the task of administering a home, departmental, university, or corporate network.
Attackers, knowing the IP address range of your network, can easily send IPdatagrams to addresses in your range.
These datagrams can do all kinds of devious things,including mapping your network with ping sweeps and port scans, crashing vulnerable hosts with
malformed packets, scanning for open TCP/UDP ports on servers in your network, and infecting hosts by including malware in the packets.
As the network administrator, what are you going to do about all those bad guys out there, each capable of sending malicious packets into yournetwork?
Two popular defense mechanisms to malicious packet attacks are firewalls andintrusion detection systems (IDSs).
As a network administrator, you may first try installing a firewall between your network and the Internet. (
Most access routers today have firewall capability.)
Firewalls inspect the datagram andsegment header fields, denying suspicious datagrams entry into the internal network.
For example, a firewall may be configured to block all ICMP echo request packets (see Section 5.6), thereby preventing an attacker from doing a traditional port scan across your IP address range.
Firewalls can also block packets based on source and destination IP addresses and portnumbers.
Additionally, firewalls can be configured to track TCP connections, granting entry onlyto datagrams that belong to approved connections.
Additional protection can be provided with an IDS.
An IDS, typically situated at the network boundary, performs “deep packet inspection,” examining not only header fields but also thepayloads in the datagram (including application-layer data).
An IDS has a database of packetsignatures that are known to be part of attacks.
This database is automatically updated as newattacks are discovered.
As packets pass through the IDS, the IDS attempts to match headerfields and payloads to the signatures in its signature database.
If such a match is found, an alertis created.
An intrusion prevention system (IPS) is similar to an IDS, except that it actually blocks packets in addition to creating alerts.
In Chapter 8, we’ll explore firewalls and IDSs in more detail.
Can firewalls and IDSs fully shield your network from all attacks?
The answer is clearly no, as attackers continually find new attacks for which signatures are not yet available.
But firewallsand traditional signature-based IDSs are useful in protecting your network from known attacks.
4.3.5 IPv6 In the early 1990s, the Internet Engineering Task Force began an effort to develop a successor to theIPv4 protocol.
A prime motivation for this effort was the realization that the 32-bit IPv4 address spacewas beginning to be used up, with new subnets and IP nodes being attached to the Internet (and being allocated unique IP addresses) at a breathtaking rate.
To respond to this need for a large IP addressspace, a new IP protocol, IPv6, was developed.
The designers of IPv6 also took this opportunity to tweak and augment other aspects of IPv4, based on the accumulated operational experience with IPv4.
The point in time when IPv4 addresses would be completely allocated (and hence no new networks
could attach to the Internet) was the subject of considerable debate.
The estimates of the two leaders of the IETF’s Address Lifetime Expectations working group were that addresses would become exhausted in 2008 and 2018, respectively [Solensky 1996] .
In February 2011, IANA allocated out the last remaining pool of unassigned IPv4 addresses to a regional registry.
While these registries still have available IPv4 addresses within their pool, once these addresses are exhausted, there are no more available address blocks that can be allocated from a central pool [Huston 2011a].
A recent survey of IPv4 address-space exhaustion, and the steps taken to prolong the life of the address space is [Richter 2015] .
Although the mid-1990s estimates of IPv4 address depletion suggested that a considerable amount of time might be left until the IPv4 address space was exhausted, it was realized that considerable timewould be needed to deploy a new technology on such an extensive scale, and so the process to develop IP version 6 (IPv6) [RFC 2460]  was begun [RFC 1752] . (
An often-asked question is what happened to IPv5?
It was initially envisioned that the ST-2 protocol would become IPv5, but ST-2 waslater dropped.)
An excellent source of information about IPv6 is [Huitema 1998] .
IPv6 Datagram Format The format of the IPv6 datagram is shown in Figure 4.26.
The most important changes introduced in IPv6 are evident in the datagram format: Expanded addressing capabilities.
 IPv6 increases the size of the IP address from 32 to 128 bits.
This ensures that the world won’t run out of IP addresses.
Now, every grain of sand on the planet can be IP-addressable.
In addition to unicast and multicast addresses, IPv6 has introduced a newtype of address, called an anycast address , that allows a datagram to be delivered to any one of a group of hosts. (
This feature could be used, for example, to send an HTTP GET to the nearest of anumber of mirror sites that contain a given document.)
A streamlined 40-byte header.
As discussed below, a number of IPv4 fields have been dropped or made optional.
The resulting 40-byte fixed-length header allows for faster processing of the IP datagram by a router.
A new encoding of options allows for more flexible options processing.
Flow labeling.
 IPv6 has an elusive definition of a flow.
RFC 2460  states that this allows “labeling of packets belonging to particular flows for which the sender
Figure 4.26 IPv6 datagram format requests special handling, such as a non-default quality of service or real-time service.”
For example, audio and video transmission might likely be treated as a flow.
On the other hand, the more traditional applications, such as file transfer and e-mail, might not be treated as flows.
It ispossible that the traffic carried by a high-priority user (for example, someone paying for betterservice for their traffic) might also be treated as a flow.
What is clear, however, is that the designersof IPv6 foresaw the eventual need to be able to differentiate among the flows, even if the exactmeaning of a flow had yet to be determined.
As noted above, a comparison of Figure 4.26 with Figure 4.16 reveals the simpler, more streamlined structure of the IPv6 datagram.
The following fields are defined in IPv6: Version.
This 4-bit field identifies the IP version number.
Not surprisingly, IPv6 carries a value of 6 in this field.
Note that putting a 4 in this field does not create a valid IPv4 datagram. (
If it did, life would be a lot simpler—see the discussion below regarding the transition from IPv4 to IPv6.)
Traffic class.
The 8-bit traffic class field, like the TOS field in IPv4, can be used to give priority to certain datagrams within a flow, or it can be used to give priority to datagrams from certain applications (for example, voice-over-IP) over datagrams from other applications (for example,SMTP e-mail).
Flow label.
 As discussed above, this 20-bit field is used to identify a flow of datagrams.
Payload length.
 This 16-bit value is treated as an unsigned integer giving the number of bytes in the IPv6 datagram following the fixed-length, 40-byte datagram header.
Next header.
This field identifies the protocol to which the contents (data field) of this datagram will be delivered (for example, to TCP or UDP).
The field uses the same values as the protocol field in the IPv4 header.
Hop limit.
 The contents of this field are decremented by one by each router that forwards the datagram.
If the hop limit count reaches zero, the datagram is ­discarded.
Source and destination addresses.
 The various formats of the IPv6 128-bit address are described in RFC 4291.
Data.
This is the payload portion of the IPv6 datagram.
When the datagram reaches its destination, the payload will be removed from the IP datagram and passed on to the protocol specified in the next header field.
The discussion above identified the purpose of the fields that are included in the IPv6 datagram.
Comparing the IPv6 datagram format in Figure 4.26 with the IPv4 datagram format that we saw in Figure 4.16, we notice that several fields appearing in the IPv4 datagram are no longer present in the IPv6 datagram: Fragmentation/reassembly.
IPv6 does not allow for fragmentation and reassembly at intermediaterouters; these operations can be performed only by the source and destination.
If an IPv6 datagram received by a router is too large to be forwarded over the outgoing link, the router simply drops the datagram and sends a “Packet Too Big” ICMP error message (see Section 5.6) back to the sender.
The sender can then resend the data, using a smaller IP datagram size.
Fragmentation and reassembly is a time-consuming operation; removing this functionality from the routers and placing itsquarely in the end systems considerably speeds up IP forwarding within the network.
Header checksum.
 Because the transport-layer (for example, TCP and UDP) and link-layer (for example, Ethernet) protocols in the Internet layers perform checksumming, the designers of IPprobably felt that this functionality was sufficiently redundant in the network layer that it could beremoved.
Once again, fast processing of IP packets was a central concern.
Recall from our discussion of IPv4 in Section 4.3.1 that since the IPv4 header contains a TTL field (similar to the hop limit field in IPv6), the IPv4 header checksum needed to be recomputed at every router.
As with fragmentation and reassembly, this too was a costly operation in IPv4.
Options.
 An options field is no longer a part of the standard IP header.
However, it has not gone away.
Instead, the options field is one of the possible next headers pointed to from within the IPv6header.
That is, just as TCP or UDP protocol headers can be the next header within an IP packet, sotoo can an options field.
The removal of the options field results in a fixed-length, 40-byte IP header.
Transitioning from IPv4 to IPv6 Now that we have seen the technical details of IPv6, let us consider a very practical matter: How will the public Internet, which is based on IPv4, be transitioned to IPv6?
The problem is that while new IPv6-capable systems can be made backward-compatible, that is, can send, route, and receive IPv4datagrams, already deployed IPv4-capable systems are not capable of handling IPv6 datagrams.
Several options are possible [Huston 2011b, RFC 4213] .
One option would be to declare a flag day—a given time and date when all Internet machines would be turned off and upgraded from IPv4 to IPv6.
The last major technology transition (from using NCP to
using TCP for reliable transport service) occurred almost 35 years ago.
Even back then [RFC 801], when the Internet was tiny and still being administered by a small number of “wizards,” it was realized that such a flag day was not possible.
A flag day involving billions of devices is even more unthinkabletoday.
The approach to IPv4-to-IPv6 transition that has been most widely adopted in practice involves tunneling  [RFC 4213] .
The basic idea behind tunneling —a key concept with applications in many other scenarios beyond IPv4-to-IPv6 transition, including wide use in the all-IP cellular networks that we’ll cover in Chapter  7—is the following.
Suppose two IPv6 nodes (in this example, B and E in Figure 4.27) want to interoperate using IPv6 datagrams but are connected to each other by intervening IPv4 routers.
We refer to the intervening set of IPv4 routers between two IPv6 routers as a tunnel , as illustrated in Figure 4.27.
With tunneling, the IPv6 node on the sending side of the tunnel (in this example, B) takesthe entire IPv6 datagram and puts it in the data (payload) field of an IPv4 datagram.
This IPv4 datagram is then addressed to the IPv6 node on the receiving side of the tunnel (in this example, E) and sent to the first node in the tunnel (in this example, C).
The intervening IPv4 routers in the tunnel route this IPv4 datagram among themselves, just as they would any other datagram, blissfully unaware that the IPv4datagram itself contains a complete IPv6 datagram.
The IPv6 node on the receiving side of the tunneleventually receives the IPv4 datagram (it is the destination of the IPv4 datagram!),
determines that theIPv4 datagram contains an IPv6 datagram (by observing that the protocol number field in the IPv4 datagram is 41 [RFC 4213] , indicating that the IPv4 payload is a IPv6 datagram), extracts the IPv6 datagram, and then routes the IPv6 datagram exactly as it would if it had received the IPv6 datagram from a directly connected IPv6 neighbor.
We end this section by noting that while the adoption of IPv6 was initially slow to take off [Lawton 2001; Huston 2008b], momentum has been building.
NIST [NIST IPv6 2015] reports that more than a third of US government second-level domains are IPv6-enabled.
On the client side, Google reports that only about 8 percent of the clients accessing Google services do so via IPv6 [Google IPv6 2015].
But other recent measurements [Czyz 2014]  indicate that IPv6 adoption is accelerating.
The proliferation of devices such as IP-enabled phones and other portable devices
Figure 4.27 Tunneling provides an additional push for more widespread deployment of IPv6.
Europe’s Third Generation Partnership Program [3GPP 2016] has specified IPv6 as the standard addressing scheme for mobile multimedia.
One important lesson that we can learn from the IPv6 experience is that it is enormously difficult to change network-layer protocols.
Since the early 1990s, numerous new network-layer protocols havebeen trumpeted as the next major revolution for the Internet, but most of these protocols have hadlimited penetration to date.
These protocols include IPv6, multicast protocols, and resource reservationprotocols; a discussion of these latter two protocols can be found in the online supplement to this text.
Indeed, introducing new protocols into the network layer is like replacing the foundation of a house—it is difficult to do without tearing the whole house down or at least temporarily relocating the house’s residents.
On the other hand, the Internet has witnessed rapid deployment of new protocols at theapplication layer.
The classic examples, of course, are the Web, instant messaging, streaming media,distributed games, and various forms of social media.
Introducing new application-layer protocols is likeadding a new layer of paint to a house—it is relatively easy to do, and if you choose an attractive color,others in the neighborhood will copy you.
In summary, in the future we can certainly expect to see changes in the Internet’s network layer, but these changes will likely occur on a time scale that is muchslower than the changes that will occur at the application layer.
4.4 Generalized Forwarding and SDN In Section 4.2.1, we noted that an Internet router’s forwarding decision has traditionally been based solely on a packet’s destination address.
In the previous section, however, we’ve also seen that there has been a proliferation of middleboxes that perform many layer-3 functions.
NAT boxes rewrite headerIP addresses and port numbers; firewalls block traffic based on header-field values or redirect packets for additional processing, such as deep packet inspection (DPI).
Load-balancers forward packets requesting a given service (e.g., an HTTP request) to one of a set of a set of servers that provide that service. [
RFC 3234]  lists a number of common middlebox functions.
This proliferation of middleboxes, layer-2 switches, and layer-3 routers [Qazi 2013]—each with its own specialized hardware, software and management interfaces—has undoubtedly resulted in costly headaches for many network operators.
However, recent advances in software-defined networking havepromised, and are now delivering, a unified approach towards providing many of these network-layerfunctions, and certain link-layer functions as well, in a modern, elegant, and integrated manner.
Recall that Section 4.2.1 characterized destination-based forwarding as the two steps of looking up a destination IP address (“match”), then sending the packet into the switching fabric to the specified output port (“action”).
Let’s now consider a significantly more general “match-plus-action” paradigm, where the“match” can be made over multiple header fields associated with different protocols at different layers inthe protocol stack.
The “action” can include forwarding the packet to one or more output ports (as indestination-based forwarding), load balancing packets across multiple outgoing interfaces that lead to aservice (as in load balancing), rewriting header values (as in NAT), purposefully blocking/dropping a packet (as in a firewall), sending a packet to a special server for further processing and action (as in DPI), and more.
In generalized forwarding, a match-plus-action table generalizes the notion of the destination-based forwarding table that we encountered in Section 4.2.1.
Because forwarding decisions may be made using network-layer and/or link-layer source and destination addresses, the forwarding devices shown inFigure 4.28 are more accurately described as “packet switches” rather than layer 3 “routers” or layer 2 “switches.”
Thus, in the remainder of this section, and in Section 5.5, we’ll refer
Figure 4.28 Generalized forwarding: Each packet switch contains a match-plus-action table that is computed and distributed by a remote controller to these devices as packet switches, adopting the terminology that is gaining widespread adoption in SDN literature.
Figure 4.28 shows a match-plus-action table in each packet switch, with the table being computed, installed, and updated by a remote controller.
We note that while it is possible for the control components at the individual packet switch to interact with each other (e.g., in a manner similar to that in Figure 4.2), in practice generalized match-plus-action capabilities are implemented via a remote controller that computes, installs, and updates these tables.
You might take a minute to compare Figures 4.2, 4.3 and 4.28—what similarities and differences do you notice between destination-based forwarding shown in Figure 4.2 and 4.3, and generalized forwarding shown in Figure 4.28?
Our following discussion of generalized forwarding will be based on OpenFlow [McKeown 2008, OpenFlow 2009 , Casado 2014 , Tourrilhes 2014]—a highly visible and successful standard that has pioneered the notion of the match-plus-action forwarding abstraction and controllers, as well as the SDN revolution more generally [Feamster 2013] .
We’ll primarily consider OpenFlow 1.0, which introduced key SDN abstractions and functionality in a particularly clear and concise manner.
Later versions of
OpenFlow introduced additional capabilities as a result of experience gained through implementation and use; current and earlier versions of the OpenFlow standard can be found at [ONF 2016].
Each entry in the match-plus-action forwarding table, known as a flow table  in OpenFlow, includes: A set of header field values  to which an incoming packet will be matched.
As in the case of destination-based forwarding, hardware-based matching is most rapidly performed in TCAMmemory, with more than a million destination address entries being possible [Bosshart 2013] .
A packet that matches no flow table entry can be dropped or sent to the remote controller for more processing.
In practice, a flow table may be implemented by multiple flow tables for performance or cost reasons [Bosshart 2013] , but we’ll focus here on the abstraction of a single flow table.
A set of counters that are updated as packets are matched to flow table entries.
These counters might include the number of packets that have been matched by that table entry, and the time since the table entry was last updated.
A set of actions to be taken when a packet matches a flow table entry.
These actions might be to forward the packet to a given output port, to drop the packet, makes copies of the packet and sent them to multiple output ports, and/or to rewrite selected header fields.
We’ll explore matching and actions in more detail in Sections 4.4.1 and 4.4.2, respectively.
We’ll then study how the network-wide collection of per-packet switch matching rules can be used to implement awide range of functions including routing, layer-2 switching, firewalling, load-balancing, virtual networks, and more in Section 4.4.3.
In closing, we note that the flow table is essentially an API, the abstraction through which an individual packet switch’s behavior can be programmed; we’ll see in Section 4.4.3 that network-wide behaviors can similarly be programmed by appropriately programming/configuringthese tables in a collection of network packet switches [Casado 2014] .
4.4.1 Match Figure 4.29 shows the eleven packet-header fields and the incoming port ID that can be matched in an OpenFlow 1.0 match-plus-action rule.
Recall from Figure 4.29 Packet matching fields, OpenFlow 1.0 flow table
Section 1.5.2 that a link-layer (layer 2) frame arriving to a packet switch will contain a network-layer (layer 3) datagram as its payload, which in turn will typically contain a transport-layer (layer 4) segment.
The first observation we make is that OpenFlow’s match abstraction allows for a match to be made on selected fields from three layers of protocol headers (thus rather brazenly defying the layering principle we studied in Section 1.5).
Since we’ve not yet covered the link layer, suffice it to say that the source and destination MAC addresses shown in Figure 4.29 are the link-layer addresses associated with the frame’s sending and receiving interfaces; by forwarding on the basis of Ethernet addresses rather than IP addresses, we can see that an OpenFlow-enabled device can equally perform as a router (layer-3device) forwarding datagrams as well as a switch (layer-2 device) forwarding frames.
The Ethernet typefield corresponds to the upper layer protocol (e.g., IP) to which the frame’s payload will be de-multiplexed, and the VLAN fields are concerned with so-called virtual local area networks that we’ll study in Chapter 6.
The set of twelve values that can be matched in the OpenFlow 1.0 specification has grown to 41 values in more recent OpenFlow specifications [Bosshart 2014] .
The ingress port refers to the input port at the packet switch on which a packet is received.
The packet’sIP source address, IP destination address, IP protocol field, and IP type of service fields were discussed earlier in Section 4.3.1.
The transport-layer source and destination port number fields can also be matched.
Flow table entries may also have wildcards.
For example, an IP address of 128.119.*.*
in a flow table will match the corresponding address field of any datagram that has 128.119 as the first 16 bits of its address.
Each flow table entry also has an associated priority.
If a packet matches multiple flow table entries, the selected match and corresponding action will be that of the highest priority entry with whichthe packet matches.
Lastly, we observe that not all fields in an IP header can be matched.
For example OpenFlow does not allow matching on the basis of TTL field or datagram length field.
Why are some fields allowed for matching, while others are not?
Undoubtedly, the answer has to do with the tradeoff betweenfunctionality and complexity.
The “art” in choosing an abstraction is to provide for enough functionality toaccomplish a task (in this case to implement, configure, and manage a wide range of network-layer functions that had previously been implemented through an assortment of network-layer devices), without over-burdening the abstraction with so much detail and generality that it becomes bloated and unusable.
Butler Lampson has famously noted [Lampson 1983]: Do one thing at a time, and do it well.
An interface should capture the minimum essentials of an abstraction.
Don’t generalize; generalizations are generally wrong.
Given OpenFlow’s success, one can surmise that its designers indeed chose their abstraction well.
Additional details of OpenFlow matching can be found in [OpenFlow 2009 , ONF 2016].
4.4.2 Action As shown in Figure 4.28, each flow table entry has a list of zero or more actions that determine the processing that is to be applied to a packet that matches a flow table entry.
If there are multiple actions, they are performed in the order specified in the list.
Among the most important possible actions are: Forwarding.
 An incoming packet may be forwarded to a particular physical output port, broadcast over all ports (except the port on which it arrived) or multicast over a selected set of ports.
The packet may be encapsulated and sent to the remote controller for this device.
That controller thenmay (or may not) take some action on that packet, including installing new flow table entries, and may return the packet to the device for forwarding under the updated set of flow table rules.
Dropping.
 A flow table entry with no action indicates that a matched packet should be dropped.
Modify-field.
 The values in ten packet header fields (all layer 2, 3, and 4 fields shown in Figure 4.29 except the IP Protocol field) may be re-written before the packet is forwarded to the chosen output port.
4.4.3 OpenFlow Examples of Match-plus-action in Action Having now considered both the match and action components of generalized forwarding, let’s put these ideas together in the context of the sample network shown in Figure 4.30.
The network has 6 hosts (h1, h2, h3, h4, h5 and h6) and three packet switches (s1, s2 and s3), each with four local interfaces (numbered 1 through 4).
We’ll consider a number of network-wide behaviors that we’d like to implement,and the flow table entries in s1, s2 and s3 needed to implement this behavior.
Figure 4.30 OpenFlow match-plus-action network with three packet switches, 6 hosts, and an OpenFlow controller A First Example: Simple Forwarding As a very simple example, suppose that the desired forwarding behavior is that packets from h5 or h6 destined to h3 or h4 are to be forwarded from s3 to s1, and then from s1 to s2 (thus completely avoidingthe use of the link between s3 and s2).
The flow table entry in s1 would be: s1 Flow Table (Example 1) Match Action Ingress Port = 1 ; IP Src = 10.3.*.* ;
IP Dst = 10.2.*.*
Forward(4) … … Of course, we’ll also need a flow table entry in s3 so that datagrams sent from h5 or h6 are forwarded to s1 over outgoing interface 3: s3 Flow Table (Example 1) Match Action IP Src = 10.3.*.* ;
IP Dst = 10.2.*.*
Forward(3) … … Lastly, we’ll also need a flow table entry in s2 to complete this first example, so that datagrams arriving from s1 are forwarded to their destination, either host h3 or h4: s2 Flow Table (Example 1) Match Action Ingress port = 2 ; IP Dst = 10.2.0.3 Forward(3) Ingress port = 2 ; IP Dst = 10.2.0.4 Forward(4)
… … A Second Example: Load Balancing As a second example, let’s consider a load-balancing scenario, where datagrams from h3 destined to 10.1.*.*
are to be forwarded over the direct link between s2 and s1, while datagrams from h4 destined to10.1.*.*
are to be forwarded over the link between s2 and s3 (and then from s3 to s1).
Note that this behavior couldn’t be achieved with IP’s destination-based forwarding.
In this case, the flow table in s2 would be: s2 Flow Table (Example 2) Match Action Ingress port = 3; IP Dst = 10.1.*.*
Forward(2) Ingress port = 4; IP Dst = 10.1.*.*
Forward(1) … … Flow table entries are also needed at s1 to forward the datagrams received from s2 to either h1 or h2; and flow table entries are needed at s3 to forward datagrams received on interface 4 from s2 over interface 3 towards s1.
See if you can figure out these flow table entries at s1 and s3.
A Third Example: Firewalling As a third example, let’s consider a firewall scenario in which s2 wants only to receive (on any of its interfaces) traffic sent from hosts attached to s3.
s2 Flow Table (Example 3) Match Action IP Src = 10.3.*.*
IP Dst = 10.2.0.3 Forward(3) IP Src = 10.3.*.*
IP Dst = 10.2.0.4 Forward(4) … …
If there were no other entries in s2’s flow table, then only traffic from 10.3.*.*
would be forwarded to the hosts attached to s2.
Although we’ve only considered a few basic scenarios here, the versatility and advantages of generalized forwarding are hopefully apparent.
In homework problems, we’ll explore how flow tables can be used to create many different logical behaviors, including virtual networks—two or more logically separate networks (each with their own independent and distinct forwarding behavior)—that use the same physical set of packet switches and links.
In Section 5.5, we’ll return to flow tables when we study the SDN controllers that compute and distribute the flow tables, and the protocol used for communicating between a packet switch and its controller.
4.5 Summary In this chapter we’ve covered the data plane  functions of the network layer—the per-router  functions that determine how packets arriving on one of a router’s input links are forwarded to one of that router’s output links.
We began by taking a detailed look at the internal operations of a router, studying input andoutput port functionality and destination-based forwarding, a router’s internal switching mechanism, packet queue management and more.
We covered both traditional IP forwarding (where forwarding is based on a datagram’s destination address) and generalized forwarding (where forwarding and otherfunctions may be performed using values in several different fields in the datagram’s header) and seenthe versatility of the latter approach.
 We also studied the IPv4 and IPv6 protocols in detail, and Internetaddressing, which we found to be much deeper, subtler, and more interesting than we might haveexpected.
With our newfound understanding of the network-layer’s data plane, we’re now ready to dive into the network layer’s control plane in Chapter 5!
Homework Problems and Questions Chapter 4 Review Questions SECTION 4.1 SECTION 4.2R1.
Let’s review some of the terminology used in this textbook.
Recall that the name of a transport-layer packet is segment and that the name of a link-layer packet is frame.
What is the name of a network-layer packet?
Recall that both routers and link-layer switches are called packet switches .
What is the fundamental difference between a router and link-layer switch?
R2.
We noted that network layer functionality can be broadly divided into data plane functionality and control plane functionality.
What are the main functions of the data plane?
Of the control plane?
R3.
We made a distinction between the forwarding function and the routing function performed in the network layer.
What are the key differences between routing and forwarding?
R4.
What is the role of the forwarding table within a router?
R5.
We said that a network layer’s service model “defines the characteristics of end-to-end transport of packets between sending and receiving hosts.”
What is the service model of the Internet’s network layer?
What guarantees are made by the Internet’s service model regardingthe host-to-host delivery of datagrams?
R6.
In Section 4.2 , we saw that a router typically consists of input ports, output ports, a switching fabric and a routing processor.
Which of these are implemented in hardware and which are implemented in software?
Why?
Returning to the notion of the network layer’s data planeand control plane, which are implemented in hardware and which are implemented in software?Why?
R7.
Discuss why each input port in a high-speed router stores a shadow copy of the forwarding table.
R8.
What is meant by destination-based forwarding?
How does this differ from generalized forwarding (assuming you’ve read Section 4.4 , which of the two approaches are adopted by Software-Defined Networking)?
R9.
Suppose that an arriving packet matches two or more entries in a router’s forwarding table.
With traditional destination-based forwarding, what rule does a router apply to determine which
SECTION 4.3of these rules should be applied to determine the output port to which the arriving packet should be switched?
R10.
Three types of switching fabrics are discussed in Section 4.2 .
List and briefly describe each type.
Which, if any, can send multiple packets across the fabric in parallel?R11.
Describe how packet loss can occur at input ports.
Describe how packet loss at input ports can be eliminated (without using infinite buffers).
R12.
Describe how packet loss can occur at output ports.
Can this loss be prevented by increasing the switch fabric speed?
R13.
What is HOL blocking?
Does it occur in input ports or output ports?
R14.
In Section 4.2 , we studied FIFO, Priority, Round Robin (RR), and Weighted Fair Queueing (WFQ) packet scheduling disciplines?
Which of these queueing disciplines ensure that all packets depart in the order in which they arrived?
R15.
Give an example showing why a network operator might want one class of packets to be given priority over another class of packets.
R16.
What is an essential different between RR and WFQ packet scheduling?
Is there a case (Hint: Consider the WFQ weights) where RR and WFQ will behave exactly the same?
R17.
Suppose Host A sends Host B a TCP segment encapsulated in an IP datagram.
When Host B receives the datagram, how does the network layer in Host B know it should pass the segment (that is, the payload of the datagram) to TCP rather than to UDP or to some otherupper-layer protocol?
R18.
What field in the IP header can be used to ensure that a packet is forwarded through no more than N routers?
R19.
Recall that we saw the Internet checksum being used in both transport-layer segment (inUDP and TCP headers, Figures 3.7 and 3.29 respectively) and in network-layer datagrams (IP header, Figure 4.16 ).
Now consider a transport layer segment encapsulated in an IP datagram.
Are the checksums in the segment header and datagram header computed over any common bytes in the IP datagram?
Explain your answer.
R20.
When a large datagram is fragmented into multiple smaller datagrams, where are these smaller datagrams reassembled into a single larger datagram?
R21.
Do routers have IP addresses?
If so, how many?
R22.
What is the 32-bit binary equivalent of the IP address 223.1.3.27?R23.
Visit a host that uses DHCP to obtain its IP address, network mask, default router, and IP address of its local DNS server.
List these values.
R24.
Suppose there are three routers between a source host and a destination host.
Ignoring fragmentation, an IP datagram sent from the source host to the destination host will travel over how many interfaces?
How many forwarding tables will be indexed to move the datagram fromthe source to the ­destination?
SECTION 4.4 ProblemsR25.
Suppose an application generates chunks of 40 bytes of data every 20 msec, and each chunk gets encapsulated in a TCP segment and then an IP datagram.
What percentage of each datagram will be overhead, and what percentage will be application data?
R26.
Suppose you purchase a wireless router and connect it to your cable modem.
Also suppose that your ISP dynamically assigns your connected device (that is, your wireless router) one IP address.
Also suppose that you have five PCs at home that use 802.11 to wirelessly connect to your wireless router.
How are IP addresses assigned to the five PCs?
Does the wireless router use NAT?
Why or why not?
R27.
What is meant by the term “route aggregation”?
Why is it useful for a router to perform route aggregation?
R28.
What is meant by a “plug-and-play” or “zeroconf” protocol?
R29.
What is a private network address?
Should a datagram with a private network address ever be present in the larger public Internet?
Explain.
R30.
Compare and contrast the IPv4 and the IPv6 header fields.
Do they have any fields in common?
R31.
It has been said that when IPv6 tunnels through IPv4 routers, IPv6 treats the IPv4 tunnels as link-layer protocols.
Do you agree with this statement?
Why or why not?
R32.
How does generalized forwarding differ from destination-based ­forwarding?
R33.
What is the difference between a forwarding table that we encountered in destination- based forwarding in Section 4.1 and OpenFlow’s flow table that we encountered in Section 4.4 ?
R34.
What is meant by the “match plus action” operation of a router or switch?
In the case of destination-based forwarding packet switch, what is matched and what is the action taken?
In the case of an SDN, name three fields that can be matched, and three actions that can be taken.
R35.
Name three header fields in an IP datagram that can be “matched” in OpenFlow 1.0 generalized forwarding.
What are three IP datagram header fields that cannot be “matched” in OpenFlow?
P1.
Consider the network below.
a. Show the forwarding table in router A, such that all traffic destined to host H3 is forwarded through interface 3.
b. Can you write down a forwarding table in router A, such that all traffic from H1 destinedto host H3 is forwarded through interface 3, while all traffic from H2 destined to host H3 is forwarded through interface 4? (
Hint: This is a trick question.)
P2.
Suppose two packets arrive to two different input ports of a router at exactly the same time.
Also suppose there are no other packets anywhere in the router.
a. Suppose the two packets are to be forwarded to two different output ports.
Is it possibleto forward the two packets through the switch fabric at the same time when the fabric uses a shared bus?
b. Suppose the two packets are to be forwarded to two different output ports.
Is it possible to forward the two packets through the switch fabric at the same time when the fabric uses switching via memory?
c. Suppose the two packets are to be forwarded to the same output port.
Is it possible to forward the two packets through the switch fabric at the same time when the fabric uses a crossbar?
P3.
In Section 4.2 , we noted that the maximum queuing delay is (n–1)D  if the switching fabric is n times faster than the input line rates.
Suppose that all packets are of the same length, n packets arrive at the same time to the n input ports, and all n packets want to be forwarded to different output ports.
What is the maximum delay for a packet for the (a) memory, (b) bus, and (c) crossbar switching fabrics?
P4.
Consider the switch shown below.
Suppose that all datagrams have the same fixed length, that the switch operates in a slotted, synchronous manner, and that in one time slot a datagram can be transferred from an input port to an output port.
The switch fabric is a crossbar so that atmost one datagram can be transferred to a given output port in a time slot, but different output ports can receive datagrams from different input ports in a single time slot.
What is the minimal number of time slots needed to transfer the packets shown from input ports to their output ports,assuming any input queue scheduling order you want (i.e., it need not have HOL blocking)?What is the largest number of slots needed, assuming the worst-case scheduling order you candevise, assuming that a non-empty input queue is never idle?
P5.
Consider a datagram network using 32-bit host addresses.
Suppose a router has four links, numbered 0 through 3, and packets are to be forwarded to the link interfaces as follows: Destination Address Range Link Interface 11100000 00000000 00000000 00000000 through11100000 00111111 11111111 111111110 11100000 01000000 00000000 00000000 through 11100000 01000000 11111111 111111111 11100000 01000001 00000000 00000000through11100001 01111111 11111111 111111112 otherwise 3 a. Provide a forwarding table that has five entries, uses longest prefix matching, and forwards packets to the correct link interfaces.
b. Describe how your forwarding table determines the appropriate link interface fordatagrams with destination addresses: 11001000 10010001 01010001 01010101 11100001 01000000 11000011 0011110011100001 10000000 00010001 01110111 P6.
Consider a datagram network using 8-bit host addresses.
Suppose a router uses longest prefix matching and has the following forwarding table: Prefix Match Interface
00 0 010 1 011 2 10 2 11 3 For each of the four interfaces, give the associated range of destination host addresses and the number of addresses in the range.
P7.
Consider a datagram network using 8-bit host addresses.
Suppose a router uses longest prefix matching and has the following forwarding table: Prefix Match Interface 1 0 10 1 111 2 otherwise 3 For each of the four interfaces, give the associated range of destination host addresses and the number of addresses in the range.
P8.
Consider a router that interconnects three subnets: Subnet 1, Subnet 2, and Subnet 3.
Suppose all of the interfaces in each of these three subnets are required to have the prefix 223.1.17/24.
Also suppose that Subnet 1 is required to support at least 60 interfaces, Subnet 2 isto support at least 90 interfaces, and Subnet 3 is to support at least 12 interfaces.
Provide threenetwork addresses (of the form a.b.c.d/x) that satisfy these constraints.
P9.
In Section 4.2.2 an example forwarding table (using longest prefix matching) is given.
Rewrite this forwarding table using the a.b.c.d/x notation instead of the binary string notation.
P10.
In Problem P5 you are asked to provide a forwarding table (using longest prefix matching).
Rewrite this forwarding table using the a.b.c.d/x notation instead of the binary string notation.
P11.
Consider a subnet with prefix 128.119.40.128/26.
Give an example of one IP address (of form xxx.xxx.xxx.xxx) that can be assigned to this network.
Suppose an ISP owns the block of addresses of the form 128.119.40.64/26.
Suppose it wants to create four subnets from thisblock, with each block having the same number of IP addresses.
What are the prefixes (of form
a.b.c.d/x) for the four subnets?
P12.
Consider the topology shown in Figure 4.20 .
Denote the three subnets with hosts (starting clockwise at 12:00) as Networks A, B, and C. Denote the subnets without hosts as Networks D, E, and F. a. Assign network addresses to each of these six subnets, with the following constraints: All addresses must be allocated from 214.97.254/23; Subnet A should have enough addresses to support 250 interfaces; Subnet B should have enough addresses tosupport 120 interfaces; and Subnet C should have enough addresses to support 120 interfaces.
Of course, subnets D, E and F should each be able to support two interfaces.
For each subnet, the assignment should take the form a.b.c.d/x or a.b.c.d/x – e.f.g.h/y. b. Using your answer to part (a), provide the forwarding tables (using longest prefix matching) for each of the three routers.
P13.
Use the whois service at the American Registry for Internet Numbers ( http:/ /www.arin.net/ whois ) to determine the IP address blocks for three universities.
Can the whois services be used to determine with certainty the geographical location of a specific IP address?
Use www.maxmind.com  to determine the locations of the Web servers at each of these universities.
P14.
Consider sending a 2400-byte datagram into a link that has an MTU of 700 bytes.
Suppose the original datagram is stamped with the identification number 422.
How many fragments are generated?
What are the values in the various fields in the IP datagram(s) generated related tofragmentation?
P15.
Suppose datagrams are limited to 1,500 bytes (including header) between source Host A and destination Host B. Assuming a 20-byte IP header, how many datagrams would be required to send an MP3 consisting of 5 million bytes?
Explain how you computed your answer.
P16.
Consider the network setup in Figure 4.25 .
Suppose that the ISP instead assigns the router the address 24.34.112.235 and that the network address of the home network is 192.168.1/24.
a. Assign addresses to all interfaces in the home network.
b. Suppose each host has two ongoing TCP connections, all to port 80 at host 128.119.40.86.
Provide the six corresponding entries in the NAT translation table.
P17.
Suppose you are interested in detecting the number of hosts behind a NAT.
You observe that the IP layer stamps an identification number sequentially on each IP packet.
The identification number of the first IP packet generated by a host is a random number, and theidentification numbers of the subsequent IP packets are sequentially assigned.
Assume all IPpackets generated by hosts behind the NAT are sent to the outside world.
a. Based on this observation, and assuming you can sniff all packets sent by the NAT to the outside, can you outline a simple technique that detects the number of unique hosts behind a NAT?
Justify your answer.
b. If the identification numbers are not sequentially assigned but randomly assigned, would
your technique work?
Justify your answer.
P18.
In this problem we’ll explore the impact of NATs on P2P applications.
Suppose a peer with username Arnold discovers through querying that a peer with username Bernard has a file it wants to download.
Also suppose that Bernard and Arnold are both behind a NAT.
Try to devise a technique that will allow Arnold to establish a TCP connection with Bernard without application- specific NAT configuration.
If you have difficulty devising such a technique, discuss why.
P19.
Consider the SDN OpenFlow network shown in Figure 4.30 .
Suppose that the desired forwarding behavior for datagrams arriving at s2 is as follows: any datagrams arriving on input port 1 from hosts h5 or h6 that are destined to hosts h1 or h2 should be forwarded over output port 2; any datagrams arriving on input port 2 from hosts h1 or h2 that are destined to hosts h5 orh6 should be forwarded over output port 1; any arriving datagrams on input ports 1 or 2 and destined to hosts h3 or h4 should bedelivered to the host specified; hosts h3 and h4 should be able to send datagrams to each other.
Specify the flow table entries in s2 that implement this forwarding behavior.
P20.
Consider again the SDN OpenFlow network shown in Figure 4.30 .
Suppose that the desired forwarding behavior for datagrams arriving from hosts h3 or h4 at s2 is as follows: any datagrams arriving from host h3 and destined for h1, h2, h5 or h6 should be forwarded in a clockwise direction in the network; any datagrams arriving from host h4 and destined for h1, h2, h5 or h6 should be forwarded ina counter-clockwise direction in the network.
Specify the flow table entries in s2 that implement this forwarding behavior.
P21.
Consider again the scenario from P19 above.
Give the flow tables entries at packet switches s1 and s3, such that any arriving datagrams with a source address of h3 or h4 are routed to the destination hosts specified in the destination address field in the IP datagram. (
Hint:Your forwarding table rules should include the cases that an arriving datagram is destined for adirectly attached host or should be forwarded to a neighboring router for eventual host deliverythere.)
P22.
Consider again the SDN OpenFlow network shown in Figure 4.30 .
Suppose we want switch s2 to function as a firewall.
Specify the flow table in s2 that implements the following firewall behaviors (specify a different flow table for each of the four firewalling behaviors below) for delivery of datagrams destined to h3 and h4.
You do not need to specify the forwarding behavior in s2 that forwards traffic to other routers.
Only traffic arriving from hosts h1 and h6 should be delivered to hosts h3 or h4 (i.e., that arriving traffic from hosts h2 and h5 is blocked).
Only TCP traffic is allowed to be delivered to hosts h3 or h4 (i.e., that UDP traffic is blocked).
Wireshark Lab In the Web site for this textbook, www.pearsonhighered.com/ cs-resources , you’ll find a Wireshark lab assignment that examines the operation of the IP protocol, and the IP datagram format in particular.
AN INTERVIEW WITH… Vinton G. Cerf Vinton G. Cerf is Vice President and Chief Internet Evangelist for Google.
He served for over 16years at MCI in various positions, ending up his tenure there as Senior Vice President forTechnology Strategy.
He is widely known as the co-designer of the TCP/IP protocols and the architecture of the Internet.
During his time from 1976 to 1982 at the US Department of Defense Advanced Research Projects Agency (DARPA), he played a key role leading the development ofInternet and Internet-related data packet and security techniques.
He received the USPresidential Medal of Freedom in 2005 and the US National Medal of Technology in 1997.
Heholds a BS in Mathematics from Stanford University and an MS and PhD in computer sciencefrom UCLA.
What brought you to specialize in networking?
I was working as a programmer at UCLA in the late 1960s.
My job was supported by the US Defense Advanced Research Projects Agency (called ARPA then, called DARPA now).
I wasworking in the laboratory of Professor Leonard Kleinrock on the Network Measurement Center ofthe newly created ARPAnet.
The first node of the ARPAnet was installed at UCLA on September 1, 1969.
I was responsible for programming a computer that was used to capture performance information about the ARPAnet and to report this information back for comparison withmathematical models and predictions of the performance of the network.
Several of the other graduate students and I were made responsible for working on the so-calledOnly traffic destined to h3 is to be delivered (i.e., all traffic to h4 is blocked).
Only UDP traffic from h1 and destined to h3 is to be delivered.
All other traffic is blocked.
host-level protocols of the ARPAnet—the procedures and formats that would allow many different kinds of computers on the network to interact with each other.
It was a fascinating exploration into a new world (for me) of distributed computing and communication.
Did you imagine that IP would become as pervasive as it is today when you first designed the protocol?
When Bob Kahn and I first worked on this in 1973, I think we were mostly very focused on the central question: How can we make heterogeneous packet networks interoperate with oneanother, assuming we cannot actually change the networks themselves?
We hoped that wecould find a way to permit an arbitrary collection of packet-switched networks to be interconnected in a transparent fashion, so that host computers could communicate end-to-end without having to do any translations in between.
I think we knew that we were dealing with powerful and expandable technology, but I doubt we had a clear image of what the world wouldbe like with hundreds of millions of computers all interlinked on the Internet.
What do you now envision for the future of networking and the Internet?
What major challenges/obstacles do you think lie ahead in their development?
I believe the Internet itself and networks in general will continue to proliferate.
Already there is convincing evidence that there will be billions of Internet-enabled devices on the Internet,including appliances like cell phones, refrigerators, personal digital assistants, home servers,televisions, as well as the usual array of laptops, servers, and so on.
Big challenges includesupport for mobility, battery life, capacity of the access links to the network, and ability to scalethe optical core of the network up in an unlimited fashion.
Designing an interplanetary extension of the Internet is a project in which I am deeply engaged at the Jet Propulsion Laboratory.
We will need to cut over from IPv4 [32-bit addresses] to IPv6 [128 bits].
The list is long!
Who has inspired you professionally?
My colleague Bob Kahn; my thesis advisor, Gerald Estrin; my best friend, Steve Crocker (we met in high school and he introduced me to computers in 1960!);
and the thousands ofengineers who continue to evolve the Internet today.
Do you have any advice for students entering the networking/Internet field?
Think outside the limitations of existing systems—imagine what might be possible; but then do the hard work of figuring out how to get there from the current state of affairs.
Dare to dream: Ahalf dozen colleagues and I at the Jet Propulsion Laboratory have been working on the design ofan interplanetary extension of the terrestrial Internet.
It may take decades to implement this,
mission by mission, but to paraphrase: “A man’s reach should exceed his grasp, or what are the heavens for?”
Chapter 5 The Network Layer: Control Plane In this chapter, we’ll complete our journey through the network layer by covering the control-plane component of the network layer—the network-wide  logic that controls not only how a datagram is forwarded among routers along an end-to-end path from the source host to the destination host, but also how network-layer components and services are configured and managed.
In Section 5.2, we’ll cover traditional routing algorithms for computing least cost paths in a graph; these algorithms are the basis for two widely deployed Internet routing protocols: OSPF and BGP, that we’ll cover in Sections 5.3 and 5.4, respectively.
As we’ll see, OSPF is a routing protocol that operates within a single ISP’s network.
BGP is a routing protocol that serves to interconnect all of the networks in the Internet; BGP is thus often referred to as the “glue” that holds the Internet together.
Traditionally, control-plane routingprotocols have been implemented together with data-plane forwarding functions, monolithically, within a router.
As we learned in the introduction to Chapter 4, software-defined networking (SDN) makes a clear separation between the data and control planes, implementing control-plane functions in a separate “controller” service that is distinct, and remote, from the forwarding components of the routers it controls.
We’ll cover SDN controllers in Section 5.5.
In Sections 5.6 and 5.7 we’ll cover some of the nuts and bolts of managing an IP network: ICMP (the Internet Control Message Protocol) and SNMP (the Simple Network Management Protocol).
5.1 Introduction Let’s quickly set the context for our study of the network control plane by recalling Figures 4.2 and 4.3.
There, we saw that the forwarding table (in the case of ­destination-based forwarding) and the flow table (in the case of generalized forwarding) were the principal elements that linked the network layer’s data and control planes.
We learned that these tables specify the local data-plane forwarding behavior of a router.
We saw that in the case of generalized forwarding, the actions taken ( Section 4.4.2) could include not only forwarding a packet to a router’s output port, but also dropping a packet, replicating a packet, and/or rewriting layer 2, 3 or 4 packet-header fields.
In this chapter, we’ll study how those forwarding and flow tables are computed, maintained and installed.
In our introduction to the network layer in Section 4.1, we learned that there are two possible approaches for doing so.
Per-router control.
Figure 5.1 illustrates the case where a routing algorithm runs in each and every router; both a forwarding and a routing function are contained Figure 5.1 Per-router control: Individual routing algorithm components interact in the control plane
within each router.
Each router has a routing component that communicates with the routing components in other routers to compute the values for its forwarding table.
This per-router controlapproach has been used in the Internet for decades.
The OSPF and BGP protocols that we’ll study in Sections 5.3 and 5.4 are based on this per-router approach to control.
Logically centralized control.
 Figure 5.2 illustrates the case in which a logically centralized controller computes and distributes the forwarding tables to be used by each and every router.
Aswe saw in Section 4.4, the generalized match-plus-action abstraction allows the router to perform traditional IP forwarding as well as a rich set of other functions (load sharing, firewalling, and NAT) that had been previously implemented in separate middleboxes.
Figure 5.2 Logically centralized control: A distinct, typically remote, controller interacts with local control agents (CAs) The controller interacts with a control agent (CA) in each of the routers via a well-defined protocol to configure and manage that router’s flow table.
Typically, the CA has minimum functionality; its job is tocommunicate with the controller, and to do as the controller commands.
Unlike the routing algorithms in Figure 5.1, the CAs do not directly interact with each other nor do they actively take part in computing
the forwarding table.
This is a key distinction between per-router control and logically centralized control.
By “logically centralized” control [Levin 2012] we mean that the routing control service is accessed as if it were a single central service point, even though the service is likely to be implemented via multiple servers for fault-tolerance, and performance scalability reasons.
As we will see in Section 5.5, SDN adopts this notion of a logically centralized controller—an approach that is finding increased use in production deployments.
Google uses SDN to control the routers in its internal B4 global wide-area network that interconnects its data centers [Jain 2013].
SWAN [Hong 2013], from Microsoft Research, uses a logically centralized controller to manage routing and forwarding between a wide area network and a data center network.
China Telecom and China Unicom are using SDN both within data centers and between data centers [Li 2015].
AT&T has noted [AT&T 2013] that it “supports many SDN capabilities and independently defined, proprietary mechanisms that fall under the SDN architectural framework.”
5.2 Routing Algorithms In this section we’ll study routing algorithms , whose goal is to determine good paths (equivalently, routes), from senders to receivers, through the network of routers.
Typically, a “good” path is one that has the least cost.
We’ll see that in practice, however, real-world concerns such as policy issues (for example, a rule such as “router x, belonging to organization Y, should not forward any packets originating from the network owned by organization Z ”) also come into play.
We note that whether the network control plane adopts a per-router control approach or a logically centralized approach, there must always be a well- defined sequence of routers that a packet will cross in traveling from sending to receiving host.
Thus, therouting algorithms that compute these paths are of fundamental importance, and another candidate for ourtop-10 list of fundamentally important networking concepts.
A graph is used to formulate routing problems.
Recall that a graph   is a set N of nodes and a collection E of edges, where each edge is a pair of nodes from N. In the context of network-layer routing, the nodes in the graph represent Figure 5.3 Abstract graph model of a computer network routers—the points at which packet-forwarding decisions are made—and the edges connecting these nodes represent the physical links between these routers.
Such a graph abstraction of a computer network is shown in Figure 5.3.
To view some graphs representing real network maps, see [Dodge 2016, Cheswick 2000] ; for a discussion of how well different graph-based models model the Internet, see [Zegura 1997, Faloutsos 1999, Li 2004].
As shown in Figure 5.3, an edge also has a value representing its cost.
Typically, an edge’s cost may reflect the physical length of the corresponding link (for example, a transoceanic link might have a higherG=(N, E)
cost than a short-haul terrestrial link), the link speed, or the monetary cost associated with a link.
For our purposes, we’ll simply take the edge costs as a given and won’t worry about how they are determined.
For any edge ( x, y) in E, we denote c(x, y) as the cost of the edge between nodes x and y. If the pair (x, y) does not belong to E, we set  Also, we’ll only consider undirected graphs (i.e., graphs whose edges do not have a direction) in our discussion here, so that edge ( x, y) is the same as edge (y, x) and that  however, the algorithms we’ll study can be easily extended to the case of directed links with a different cost in each direction.
Also, a node y is said to be a neighbor  of node x if (x, y) belongs to E. Given that costs are assigned to the various edges in the graph abstraction, a natural goal of a routing algorithm is to identify the least costly paths between sources and destinations.
To make this problemmore precise, recall that a path in a graph  is a sequence of nodes  such that each of the pairs  are edges in E. The cost of a path  is simply the sum of all the edge costs along the path, that is,  Given any two nodes x and y, there are typically many paths between the two nodes, with each path having a cost.
One or more of these paths is a least-cost path.
The least-cost problem is therefore clear: Find a path between the source and destination that has least cost.
In Figure 5.3, for example, the least-cost path between source node u and destination node w is (u, x, y, w) with a path cost of 3.
Note that if all edges in the graph have the same cost, the least-cost path is also the shortest path (that is, the path with the smallest number of links between the source and the destination).
As a simple exercise, try finding the least-cost path from node u to z in Figure 5.3 and reflect for a moment on how you calculated that path.
If you are like most people, you found the path from u to z by examining Figure 5.3, tracing a few routes from u to z, and somehow convincing yourself that the path you had chosen had the least cost among all possible paths. (
Did you check all of the 17 possible paths between u and z?
Probably not!)
Such a calculation is an example of a centralized routing algorithm—the routing algorithm was run in one location, your brain, with complete information about the network.
Broadly, one way in which we can classify routing algorithms is according to whether they are centralizedor decentralized.
A centralized routing algorithm  computes the least-cost path between a source and destination using complete, global knowledge about the network.
That is, the algorithm takes the connectivitybetween all nodes and all link costs as inputs.
This then requires that the algorithm somehow obtainthis information before actually performing the calculation.
The calculation itself can be run at one site (e.g., a logically centralized controller as in Figure 5.2) or could be replicated in the routing component of each and every router (e.g., as in Figure 5.1).
The key distinguishing feature here, however, is that the algorithm has complete information about connectivity and link costs.
Algorithms with global state information are often referred to as link-state (LS) algorithms, since the algorithm must be aware of the cost of each link in the network.
We’ll study LS algorithms in Section 5.2.1.
In a decentralized routing algorithm , the calculation of the least-cost path is carried out in anc(x, y)=∞. c(x, y)=c(y, x); G=(N, E) (x1,x2,⋯,xp) (x1,x2),(x2,x3),⋯,(xp−1,xp) (x1,x2,⋯, xp) c(x1,x2)+c(x2,x3)+⋯+c(xp−1,xp).
iterative, distributed manner by the routers.
No node has complete information about the costs of all network links.
Instead, each node begins with only the knowledge of the costs of its own directly attached links.
Then, through an iterative process of calculation and exchange of information with itsneighboring nodes, a node gradually calculates the least-cost path to a destination or set of destinations.
The decentralized routing algorithm we’ll study below in Section 5.2.2 is called a distance-vector (DV) algorithm, because each node maintains a vector of estimates of the costs (distances) to all other nodes in the network.
Such decentralized algorithms, with interactive message exchange between neighboring routers is perhaps more naturally suited to control planes where the routers interact directly with each other, as in Figure 5.1.
A second broad way to classify routing algorithms is according to whether they are static or dynamic.
Instatic routing algorithms , routes change very slowly over time, often as a result of human intervention (for example, a human manually editing a link costs).
Dynamic routing algorithms  change the routing paths as the network traffic loads or topology change.
A dynamic algorithm can be run either periodically or in direct response to topology or link cost changes.
While dynamic algorithms are more responsive to network changes, they are also more susceptible to problems such as routing loops and route oscillation.
A third way to classify routing algorithms is according to whether they are load-sensitive or load- insensitive.
In a load-sensitive algorithm , link costs vary dynamically to reflect the current level of congestion in the underlying link.
If a high cost is associated with a link that is currently congested, arouting algorithm will tend to choose routes around such a congested link.
While early ARPAnet routing algorithms were load-sensitive [McQuillan 1980], a number of difficulties were encountered [Huitema 1998] .
Today’s Internet routing algorithms (such as RIP, OSPF, and BGP) are load-insensitive , as a link’s cost does not explicitly reflect its current (or recent past) level of congestion.
5.2.1 The Link-State (LS) Routing Algorithm Recall that in a link-state algorithm, the network topology and all link costs are known, that is, available as input to the LS algorithm.
In practice this is accomplished by having each node broadcast link-state packets to all other nodes in the network, with each link-state packet containing the identities and costs of its attached links.
In practice (for example, with the Internet’s OSPF routing protocol, discussed in Section 5.3) this is often accomplished by a link-state broadcast  algorithm ­[Perlman 1999] .
The result of the nodes’ broadcast is that all nodes have an identical and complete view of the network.
Each node canthen run the LS algorithm and compute the same set of least-cost paths as every other node.
The link-state routing algorithm we present below is known as Dijkstra’s algorithm , named after its inventor.
A closely related algorithm is Prim’s algorithm; see [Cormen 2001]  for a general discussion of graph algorithms.
Dijkstra’s algorithm computes the least-cost path from one node (the source, which we will refer to as u) to all other nodes in the network.
Dijkstra’s algorithm is iterative and has the property that
after the kth iteration of the algorithm, the least-cost paths are known to k destination nodes, and among the least-cost paths to all destination nodes, these k paths will have the k smallest costs.
Let us define the following notation: D(v): cost of the least-cost path from the source node to destination v as of this iteration of the algorithm.
p(v): previous node (neighbor of v) along the current least-cost path from the source to v. N′: subset of nodes; v is in N′  if the least-cost path from the source to v is definitively known.
The centralized routing algorithm consists of an initialization step followed by a loop.
The number of times the loop is executed is equal to the number of nodes in the network.
Upon termination, the algorithm will have calculated the shortest paths from the source node u to every other node in the network.
Link-State (LS) Algorithm for Source Node u 1  Initialization:   2   N’ = {u} 3   for all nodes v4     if v is a neighbor of u5       then D(v) = c(u, v)6     else D(v) = ∞ 7 8  Loop 9   find w not in N’ such that D(w) is a minimum 10  add w to N’ 11  update D(v) for each neighbor v of w and not in N’:12        D(v) = min(D(v), D(w)+ c(w, v) )13   /* new cost to v is either old cost to v or known14    least path cost to w plus cost from w to v */ 15 until N’= N As an example, let’s consider the network in Figure 5.3 and compute the least-cost paths from u to all possible destinations.
A tabular summary of the algorithm’s computation is shown in Table 5.1, where each line in the table gives the values of the algorithm’s variables at the end of the iteration.
Let’s consider the few first steps in detail.
In the initialization step, the currently known least-cost paths from u to its directly attached neighbors,
v, x, and w, are initialized to 2, 1, and 5, respectively.
Note in Table 5.1 Running the link-state algorithm on the network in Figure 5.3 step N’ D (v), p (v) D (w), p (w) D (x), p (x) D (y), p (y) D (z), p (z) 0 u 2, u 5, u 1,u ∞ ∞ 1 ux 2, u 4, x 2, x ∞ 2 uxy 2, u 3, y 4, y 3 uxyv 3, y 4, y 4 uxyvw 4, y 5 uxyvwz particular that the cost to w is set to 5 (even though we will soon see that a lesser-cost path does indeed exist) since this is the cost of the direct (one hop) link from u to w. The costs to y and z are set to infinity because they are not directly connected to u. In the first iteration, we look among those nodes not yet added to the set N′ and find that node with the least cost as of the end of the previous iteration.
That node is x, with a cost of 1, and thus x is added to the set N′. Line 12 of the LS algorithm is then performed to update D(v) for all nodes v, yielding the results shown in the second line (Step 1) in Table 5.1.
The cost of the path to v is unchanged.
The cost of the path to w (which was 5 at the end of the initialization) through node x is found to have a cost of 4.
Hence this lower-cost path is selected and w’s predecessor along the shortest path from u is set to x. Similarly, the cost to y (through x) is computed to be 2, and the table is updated accordingly.
In the second iteration, nodes v and y are found to have the least-cost paths (2), and we break the tie arbitrarily and add y to the set N′ so that N′  now contains u, x, and y. The cost to the remaining nodes not yet in N′ , that is, nodes v, w, and z, are updated via line 12 of the LS algorithm, yielding the results shown in the third row in Table 5.1.
And so on . . .
When the LS algorithm terminates, we have, for each node, its predecessor along the least-cost path from the source node.
For each predecessor, we also have its predecessor, and so in this manner we can construct the entire path from the source to all destinations.
The forwarding table in a node, say node u, can then be constructed from this information by storing, for each destination, the next-hop node on theleast-cost path from u to the destination.
Figure 5.4 shows the resulting least-cost paths and forwarding table in u for the network in Figure 5.3.
Figure 5.4 Least cost path and forwarding table for node u What is the computational complexity of this algorithm?
That is, given n nodes (not counting the source), how much computation must be done in the worst case to find the least-cost paths from the source to all destinations?
In the first iteration, we need to search through all n nodes to determine the node, w, not in N′ that has the minimum cost.
In the second iteration, we need to check  nodes to determine the minimum cost; in the third iteration  nodes, and so on.
Overall, the total number of nodes we need to search through over all the iterations is  and thus we say that the preceding implementation of the LS algorithm has worst-case complexity of order n squared: O(n ). (
A more sophisticated implementation of this algorithm, using a data structure known as a heap, can find the minimum in line 9 in logarithmic rather than linear time, thus reducing the complexity.)
Before completing our discussion of the LS algorithm, let us consider a pathology that can arise.
Figure 5.5 shows a simple network topology where link costs are equal to the load carried on the link, for example, reflecting the delay that would be experienced.
In this example, link costs are not symmetric; that is, c(u, v) equals c(v, u) only if the load carried on both directions on the link ( u, v) is the same.
In this example, node z originates a unit of traffic destined for w, node x also originates a unit of traffic destined for w, and node y injects an amount of traffic equal to e, also destined for w. The initial routing is shown in Figure 5.5(a) with the link costs corresponding to the amount of traffic carried.
When the LS algorithm is next run, node y determines (based on the link costs shown in Figure 5.5(a)) that the clockwise path to w has a cost of 1, while the counterclockwise path to w (which it had been using) has a cost of  Hence y’s least-cost path to w is now clockwise.
Similarly, x determines that its new least-cost path to w is also clockwise, resulting in costs shown in Figure 5.5(b).
When the LS algorithm is run next, nodes x, y, and z all detect a zero-cost path to w in the counterclockwise direction, and all route their traffic to the counterclockwise routes.
The next time the LS algorithm is run, x, y, and z all then route their traffic to the clockwise routes.
What can be done to prevent such oscillations (which can occur in any algorithm, not just an LS algorithm, that uses a congestion or delay-based link metric)?
One solution would be to mandate that link costs notdepend on the amount of trafficn−1 n−2 n(n+1)/2, 2 1+e.
Figure 5.5 Oscillations with congestion-sensitive routing
carried—an unacceptable solution since one goal of routing is to avoid highly congested (for example, high-delay) links.
Another solution is to ensure that not all routers run the LS algorithm at the same time.
This seems a more reasonable solution, since we would hope that even if routers ran the LS algorithmwith the same periodicity, the execution instance of the algorithm would not be the same at each node.
Interestingly, researchers have found that routers in the Internet can self-synchronize among themselves [Floyd Synchronization 1994] .
That is, even though they initially execute the algorithm with the same period but at different instants of time, the algorithm execution instance can eventually become, and remain, synchronized at the routers.
One way to avoid such self-synchronization is for each router torandomize the time it sends out a link advertisement.
Having studied the LS algorithm, let’s consider the other major routing algorithm that is used in practice today—the distance-vector routing algorithm.
5.2.2 The Distance-Vector (DV) Routing Algorithm Whereas the LS algorithm is an algorithm using global information, the distance-vector  (DV) algorithm is iterative, asynchronous, and distributed.
It is distributed  in that each node receives some information from one or more of its directly attached  neighbors, performs a calculation, and then distributes the results of its calculation back to its neighbors.
It is iterative in that this process continues on until no more information is exchanged between neighbors. (
Interestingly, the algorithm is also self-terminating—there is no signal that the computation should stop; it just stops.)
The algorithm is asynchronous  in that it does not require all of the nodes to operate in lockstep with each other.
We’ll see that an asynchronous, iterative, self- terminating, distributed algorithm is much more interesting and fun than a centralized algorithm!
Before we present the DV algorithm, it will prove beneficial to discuss an important relationship that exists among the costs of the least-cost paths.
Let d (y) be the cost of the least-cost path from node x to node y. Then the least costs are related by the celebrated Bellman-Ford equation, namely,x
where the min  in the equation is taken over all of x’s neighbors.
The Bellman-Ford equation is rather intuitive.
Indeed, after traveling from x to v, if we then take the least-cost path from v to y, the path cost will be  Since we must begin by traveling to some neighbor v, the least cost from x to y is the minimum of  taken over all neighbors v. But for those who might be skeptical about the validity of the equation, let’s check it for source node u and destination node z in Figure 5.3.
The source node u has three neighbors: nodes v, x, and w. By walking along various paths in the graph, it is easy to see that  and  Plugging these values into Equation  5.1, along with the costs  and  gives  which is obviously true and which is exactly what the Dijskstra algorithm gave us for the same network.
This quick verification should help relieve any skepticism you may have.
The Bellman-Ford equation is not just an intellectual curiosity.
It actually has significant practical importance: the solution to the Bellman-Ford equation provides the entries in node x’s forwarding table.
To see this, let v* be any neighboring node that achieves the minimum in Equation  5.1.
Then, if node x wants to send a packet to node y along a least-cost path, it should first forward the packet to node v*.
Thus, node x’s forwarding table would specify node v* as the next-hop router for the ultimate destination y. Another important practical contribution of the Bellman-Ford equation is that it suggests the form of the neighbor- to-neighbor communication that will take place in the DV algorithm.
The basic idea is as follows.
Each node x begins with D(y), an estimate of the cost of the least-cost path from itself to node y, for all nodes, y, in N. Let  be node x’s distance vector, which is the vector of cost estimates from x to all other nodes, y, in N. With the DV algorithm, each node x maintains the following routing information: For each neighbor v, the cost c(x, v) from x to directly attached neighbor, v Node x’s distance vector, that is, , containing x’s estimate of its cost to all destinations, y, in N The distance vectors of each of its neighbors, that is,  for each neighbor v of x In the distributed, asynchronous algorithm, from time to time, each node sends a copy of its distance vector to each of its neighbors.
When a node x receives a new distance vector from any of its neighbors w, it saves w’s distance vector, and then uses the Bellman-Ford equation to update its own distance vector as follows: If node x’s distance vector has changed as a result of this update step, node x will then send its updateddx(y)=minv{c(x,v)+dv(y)}, (5.1) v c(x,v)+dv(y).
c(x,v)+dv(y) dv(z)=5, dx(z)=3, dw(z)=3.
c(u,v)=2, c(u,x)=1, c(u,w)=5, du(z)=min{2+5,5+3,1+3}=4, x Dx=[Dx(y): y in N] Dx=[Dx(y): y in N] Dv=[Dv(y): y in N] Dx(y)=minv{c(x,v)+Dv(y)}      for each node y in N
distance vector to each of its neighbors, which can in turn update their own distance vectors.
Miraculously enough, as long as all the nodes continue to exchange their distance vectors in an asynchronous fashion, each cost estimate D (y) converges to d (y), the actual cost of the least-cost path from node x to node y [Bertsekas 1991] !
Distance-Vector (DV) Algorithm At each node, x: 1  Initialization: 2    for all destinations y in N: 3       D (y)= c(x, y)/* if y is not a neighbor then c(x, y)= ∞ */ 4    for each neighbor w 5       D (y) = ?
for all destinations y in N 6    for each neighbor w 7       send distance vector  D = [D(y): y in N] to w 8 9  loop  10    wait  (until I see a link cost change to some neighbor w or 11            until I receive a distance vector from some neighbor w) 12 13    for each y in N: 14        D (y) = min {c(x, v) + D (y)} 15 16 if Dx(y) changed for any destination y 17       send distance vector D  = [D(y): y in N] to all neighbors 1819 forever  In the DV algorithm, a node x updates its distance-vector estimate when it either sees a cost change in one of its directly attached links or receives a distance-vector update from some neighbor.
But to update its own forwarding table for a given destination y, what node x really needs to know is not the shortest-path distance to y but instead the neighboring node v*(y) that is the next-hop router along the shortest path to y. As you might expect, the next-hop router v*(y) is the neighbor v that achieves the minimum in Line 14 of the DV algorithm. (
If there are multiple neighbors v that achieve the minimum, then v*(y) can be any of the minimizing neighbors.)
Thus, in Lines 13–14, for each destination y, node x also determines v*(y) and updates its forwarding table for destination y.x x x w x x x v v x x
Recall that the LS algorithm is a centralized algorithm in the sense that it requires each node to first obtain a complete map of the network before running the Dijkstra algorithm.
The DV algorithm is decentralized and does not use such global information.
Indeed, the only information a node will have is the costs of the links to its directly attached neighbors and information it receives from these neighbors.
Each node waitsfor an update from any neighbor (Lines 10–11), calculates its new distance vector when receiving an update (Line 14), and distributes its new distance vector to its neighbors (Lines 16–17).
DV-like algorithms are used in many routing protocols in practice, including the Internet’s RIP and BGP, ISO IDRP, NovellIPX, and the original ARPAnet.
Figure 5.6 illustrates the operation of the DV algorithm for the simple three-node network shown at the top of the figure.
The operation of the algorithm is illustrated in a synchronous manner, where all nodes simultaneously receive distance vectors from their neighbors, compute their new distance vectors, andinform their neighbors if their distance vectors have changed.
After studying this example, you shouldconvince yourself that the algorithm operates correctly in an asynchronous manner as well, with node computations and update generation/reception occurring at any time.
The leftmost column of the figure displays three initial routing tables  for each of the three nodes.
For example, the table in the upper-left corner is node x’s initial routing table.
Within a specific routing table, each row is a distance vector— specifically, each node’s routing table includes its own distance vector and that of each of its neighbors.
Thus, the first row in node x’s initial routing table is  The second and third rows in this table are the most recently received distance vectors from nodes y and z, respectively.
Because at initialization node x has not received anything from node y or z, the entries in the second and third rows are initialized to infinity.
After initialization, each node sends its distance vector to each of its two neighbors.
This is illustrated inFigure 5.6 by the arrows from the first column of tables to the second column of tables.
For example, node x sends its distance vector D   [0, 2, 7] to both nodes y and z. After receiving the updates, each node recomputes its own distance vector.
For example, node x computes The second column therefore displays, for each node, the node’s new distance vector along with distancevectors just received from its neighbors.
Note, for example, thatDx=[Dx(x),Dx(y),Dx(z)]=[0,2,7].
x= Dx(x)=0Dx(y)=min{c(x,y)+Dy(y),c(x,z)+Dz(y)}=min{2+0, 7+1}=2Dx(z)=min{c(x,y)+Dy(z),c(x,z)+Dz(z)}=min{2+1,7+0}= 3
Figure 5.6 Distance-vector (DV) algorithm in operation node x’s estimate for the least cost to node z, D (z), has changed from 7 to 3.
Also note that for node x, neighboring node y achieves the minimum in line 14 of the DV algorithm; thus at this stage of the algorithm, we have at node x that  and  After the nodes recompute their distance vectors, they again send their updated distance vectors to their neighbors (if there has been a change).
This is illustrated in Figure 5.6 by the arrows from the second column of tables to the third column of tables.
Note that only nodes x and z send updates: node y’s distance vector didn’t change so node y doesn’t send an update.
After receiving the updates, the nodes then recompute their distance vectors and update their routing tables, which are shown in the third column.x v*(y)=y v*(z)=y.
The process of receiving updated distance vectors from neighbors, recomputing routing table entries, and informing neighbors of changed costs of the least-cost path to a destination continues until no updatemessages are sent.
At this point, since no update messages are sent, no further routing table calculationswill occur and the algorithm will enter a quiescent state; that is, all nodes will be performing the wait inLines 10–11 of the DV algorithm.
The algorithm remains in the quiescent state until a link cost changes, as discussed next.
Distance-Vector Algorithm: Link-Cost Changes and Link Failure When a node running the DV algorithm detects a change in the link cost from itself to a neighbor (Lines 10–11), it updates its distance vector (Lines 13–14) and, if there’s a change in the cost of the least-cost path, informs its neighbors (Lines 16–17) of its new distance vector.
Figure 5.7(a) illustrates a scenario where the link cost from y to x changes from 4 to 1.
We focus here only on y’ and z’s distance table entries to destination x. The DV algorithm causes the following sequence of events to occur: At time t , y detects the link-cost change (the cost has changed from 4 to 1), updates its distance vector, and informs its neighbors of this change since its distance vector has changed.
At time t , z receives the update from y and updates its table.
It computes a new least cost to x (it has decreased from a cost of 5 to a cost of 2) and sends its new distance vector to its neighbors.
At time t , y receives z’s update and updates its distance table.
y’s least costs do not change and hence y does not send any message to z. The algorithm comes to a quiescent state.
Thus, only two iterations are required for the DV algorithm to reach a quiescent state.
The good news about the decreased cost between x and y has propagated quickly through the network.
Figure 5.7 Changes in link cost Let’s now consider what can happen when a link cost increases.
 Suppose that the link cost between x and y increases from 4 to 60, as shown in Figure 5.7(b).
1.
Before the link cost changes,  and  At time t, y detects the link-0 12 Dy(x)=4, Dy(z)=1, Dz(y)=1, Dz(x)=5.0
cost change (the cost has changed from 4 to 60).
y computes its new minimum-cost path to x to have a cost of Of course, with our global view of the network, we can see that this new cost via z is wrong.
But the only information node y has is that its direct cost to x is 60 and that z has last told y that z could get to x with a cost of 5.
So in order to get to x, y would now route through z, fully expecting that z will be able to get to x with a cost of 5.
As of t we have a routing loop —in order to get to x, y routes through z, and z routes through y. A routing loop is like a black hole—a packet destined for x arriving at y or z as of t  will bounce back and forth between these two nodes forever (or until the forwarding tables are changed).
2.
Since node y has computed a new minimum cost to x, it informs z of its new distance vector at time t. 3.
Sometime after t , z receives y’s new distance vector, which indicates that y’s minimum cost to x is 6.
z knows it can get to y with a cost of 1 and hence computes a new least cost to x of  Since z’s least cost to x has increased, it then informs y of its new distance vector at t .
4.
In a similar manner, after receiving z’s new distance vector, y determines  and sends z its distance vector.
z then determines  and sends y its distance vector, and so on.
How long will the process continue?
You should convince yourself that the loop will persist for 44 iterations (message exchanges between y and z)—until z eventually computes the cost of its path via y to be greater than 50.
At this point, z will (finally!)
determine that its least-cost path to x is via its direct connection to x. y will then route to x via z. The result of the bad news about the increase in link cost has indeed traveled slowly!
What would have happened if the link cost c(y, x) had changed from 4 to 10,000 and the cost c(z, x) had been 9,999?
Because of such scenarios, the problem we have seen is sometimes referred to as the count-to-infinity ­problem.
Distance-Vector Algorithm: Adding Poisoned Reverse The specific looping scenario just described can be avoided using a technique known as poisoned reverse.
The idea is simple—if z routes through y to get to destination x, then z will advertise to y that its distance to x is infinity, that is, z will advertise to y that  (even though z knows  in truth).
z will continue telling this little white lie to y as long as it routes to x via y. Since y believes that z has no path to x, y will never attempt to route to x via z, as long as z continues to route to x via y (and lies about doing so).
Let’s now see how poisoned reverse solves the particular looping problem we encountered before in Figure 5.5(b).
As a result of the poisoned reverse, y’s distance table indicates  When the cost of the (x, y) link changes from 4 to 60 at time t , y updates its table and continues to route directly to x, albeitDy(x)=min{c(y,x)+Dx(x), c(y,z)+Dz(x)}=min{60+0,1+5}=6 1 1 1 1 Dz(x)=min{50+0,1+6}=7.
2 Dy(x)=8 Dz(x)=9 Dz(x)=∞ Dz(x)=5 Dz(x)=∞. 0
at a higher cost of 60, and informs z of its new cost to x, that is,  After receiving the update at t , z immediately shifts its route to x to be via the direct (z, x) link at a cost of 50.
Since this is a new least-cost path to x, and since the path no longer passes through y, z now informs y that  at t. After receiving the update from z, y updates its distance table with  Also, since z is now on y’s least- cost path to x, y poisons the reverse path from z to x by informing z at time t  that  (even though y knows that  in truth).
Does poisoned reverse solve the general count-to-infinity problem?
It does not.
You should convinceyourself that loops involving three or more nodes (rather than simply two immediately neighboring nodes) will not be detected by the poisoned reverse technique.
A Comparison of LS and DV Routing Algorithms The DV and LS algorithms take complementary approaches toward computing routing.
In the DV algorithm, each node talks to only its directly connected neighbors, but it provides its neighbors with least- cost estimates from itself to all the nodes (that it knows about) in the network.
The LS algorithm requires global information.
Consequently, when implemented in each and every router, e.g., as in Figure 4.2 and 5.1, each node would need to communicate with all other nodes (via broadcast), but it tells them only the costs of its directly connected links.
Let’s conclude our study of LS and DV algorithms with a quick comparison of some of their attributes.
Recall that N is the set of nodes (routers) and E is the set of edges (links).
Message complexity.
We have seen that LS requires each node to know the cost of each link in the network.
This requires O(|N| |E|) messages to be sent.
Also, whenever a link cost changes, the new link cost must be sent to all nodes.
The DV algorithm requires message exchanges between directlyconnected neighbors at each iteration.
We have seen that the time needed for the algorithm to converge can depend on many factors.
When link costs change, the DV algorithm will propagate the results of the changed link cost only if the new link cost results in a changed least-cost path for one ofthe nodes attached to that link.
Speed of convergence.
We have seen that our implementation of LS is an O(|N| ) algorithm requiring O(|N| |E|)) messages.
The DV algorithm can converge slowly and can have routing loops while the algorithm is converging.
DV also suffers from the count-to-infinity problem.
Robustness.
What can happen if a router fails, misbehaves, or is sabotaged?
Under LS, a router could broadcast an incorrect cost for one of its attached links (but no others).
A node could also corrupt or drop any packets it received as part of an LS broadcast.
But an LS node is computing only its ownforwarding tables; other nodes are performing similar calculations for themselves.
This means route calculations are somewhat separated under LS, providing a degree of robustness.
Under DV, a node can advertise incorrect least-cost paths to any or all destinations. (
Indeed, in 1997, a malfunctioningrouter in a small ISP provided national backbone routers with erroneous routing information.
Thiscaused other routers to flood the malfunctioning router with traffic and caused large portions of theDy(x)=60.1 Dz(x)=502 Dy(x)=51.
3 Dy(x)=∞ Dy(x)=51 2
Internet to become disconnected for up to several hours [Neumann 1997] .)
More generally, we note that, at each iteration, a node’s calculation in DV is passed on to its neighbor and then indirectly to its neighbor’s neighbor on the next iteration.
In this sense, an incorrect node calculation can be diffusedthrough the entire network under DV.
In the end, neither algorithm is an obvious winner over the other; indeed, both algorithms are used in the Internet.
5.3 Intra-AS Routing in the Internet: OSPF In our study of routing algorithms so far, we’ve viewed the network simply as a collection of interconnected routers.
One router was indistinguishable from another in the sense that all routersexecuted the same routing algorithm to compute routing paths through the entire network.
In practice,this model and its view of a homogenous set of routers all executing the same routing algorithm issimplistic for two important reasons: Scale.
As the number of routers becomes large, the overhead involved in communicating, computing, and storing routing information becomes prohibitive.
Today’s Internet consists of hundreds of millions of routers.
Storing routing information for possible destinations at each of these routers would clearly require enormous amounts of memory.
The overhead required to broadcast connectivity and link cost updates among all of the routers would be huge!
A distance-vector algorithm that iterated among such a large number of routers would surely never converge.
Clearly,something must be done to reduce the complexity of route computation in a network as large as theInternet.
Administrative autonomy.
As described in Section 1.3, the Internet is a network of ISPs, with each ISP consisting of its own network of routers.
An ISP generally desires to operate its network as itpleases (for example, to run whatever routing algorithm it chooses within its network) or to hideaspects of its network’s internal organization from the outside.
Ideally, an organization should be able to operate and administer its network as it wishes, while still being able to connect its network to other outside networks.
Both of these problems can be solved by organizing routers into autonomous ­systems (ASs) , with each AS consisting of a group of routers that are under the same administrative control.
Often therouters in an ISP, and the links that interconnect them, constitute a single AS.
Some ISPs, however,partition their network into multiple ASs.
In particular, some tier-1 ISPs use one gigantic AS for theirentire network, whereas others break up their ISP into tens of interconnected ASs.
An autonomous system is identified by its globally unique autonomous system number (ASN) [RFC 1930] .
AS numbers, like IP addresses, are assigned by ICANN regional registries [ICANN 2016] .
Routers within the same AS all run the same routing algorithm and have information about each other.
The routing algorithm ­running within an autonomous system is called an intra-autonomous system routing ­protocol .
Open Shortest Path First (OSPF)
OSPF routing and its closely related cousin, IS-IS, are widely used for intra-AS routing in the Internet.
The Open in OSPF indicates that the routing protocol specification is publicly available (for example, as opposed to Cisco’s EIGRP protocol, which was only recently became open [Savage 2015] , after roughly 20 years as a Cisco-proprietary protocol).
The most recent version of OSPF, version 2, is defined in[RFC 2328] , a public document.
OSPF is a link-state protocol that uses flooding of link-state information and a Dijkstra’s least-cost path algorithm.
With OSPF, each router constructs a complete topological map (that is, a graph) of the entireautonomous system.
Each router then locally runs Dijkstra’s shortest-path algorithm to determine a shortest-path tree to all subnets, with itself as the root node.
Individual link costs are configured by the network administrator (see sidebar, Principles and Practice: Setting OSPF Weights ).
The administrator might choose to set all link costs to 1, PRINCIPLES IN PRACTICE SETTING OSPF LINK WEIGHTS Our discussion of link-state routing has implicitly assumed that link weights are set, a routing algorithm such as OSPF is run, and traffic flows according to the routing tables computed by theLS algorithm.
In terms of cause and effect, the link weights are given (i.e., they come first) andresult (via Dijkstra’s algorithm) in routing paths that minimize overall cost.
In this viewpoint, linkweights reflect the cost of using a link (e.g., if link weights are inversely proportional to capacity,then the use of high-capacity links would have smaller weight and thus be more attractive from arouting standpoint) and Dijsktra’s algorithm serves to minimize overall cost.
In practice, the cause and effect relationship between link weights and routing paths may be reversed, with network operators configuring link weights in order to obtain routing paths that achieve certain traffic engineering goals [Fortz 2000, Fortz 2002].
For example, suppose a network operator has an estimate of traffic flow entering the network at each ingress point and destined for each egress point.
The operator may then want to put in place a specific routing ofingress-to-egress flows that minimizes the maximum utilization over all of the network’s links.
But with a routing algorithm such as OSPF, the operator’s main “knobs” for tuning the routing offlows through the network are the link weights.
Thus, in order to achieve the goal of minimizing the maximum link utilization, the operator must find the set of link weights that achieves this goal.
This is a reversal of the cause and effect relationship—the desired routing of flows isknown, and the OSPF link weights must be found such that the OSPF routing algorithm resultsin this desired routing of flows.
thus achieving minimum-hop routing, or might choose to set the link weights to be inversely proportionalto link capacity in order to discourage traffic from using low-bandwidth links.
OSPF does not mandate a policy for how link weights are set (that is the job of the ­network administrator), but instead provides
the mechanisms (protocol) for determining least-cost path routing for the given set of link weights.
With OSPF, a router broadcasts routing information to all other routers in the autonomous system, not just to its neighboring routers.
A router broadcasts link-state information whenever there is a change in a link’s state (for example, a change in cost or a change in up/down status).
It also broadcasts a link’s state periodically (at least once every 30 minutes), even if the link’s state has not changed.
RFC 2328 notes that “this periodic updating of link state advertisements adds robustness to the link statealgorithm.”
OSPF advertisements are contained in OSPF messages that are carried directly by IP, with an upper-layer protocol of 89 for OSPF.
Thus, the OSPF protocol must itself implement functionalitysuch as reliable message transfer and link-state broadcast.
The OSPF protocol also checks that linksare operational (via a HELLO message that is sent to an attached neighbor) and allows an OSPF routerto obtain a neighboring router’s database of network-wide link state.
Some of the advances embodied in OSPF include the following: Security.
Exchanges between OSPF routers (for example, link-state updates) can be authenticated.
With authentication, only trusted routers can participate in the OSPF protocol within an AS, thus preventing malicious intruders (or networking students taking their newfound knowledge out for ajoyride) from injecting incorrect information into router tables.
By default, OSPF packets between routers are not authenticated and could be forged.
Two types of authentication can be configured— simple and MD5 (see Chapter 8 for a discussion on MD5 and authentication in general).
With simple authentication, the same password is configured on each router.
When a router sends an OSPF packet, it includes the password in plaintext.
Clearly, simple authentication is not very secure.
MD5authentication is based on shared secret keys that are configured in all the routers.
For each OSPFpacket that it sends, the router computes the MD5 hash of the content of the OSPF packet appended with the secret key. (
See the discussion of message authentication codes in Chapter 8.)
Then the router includes the resulting hash value in the OSPF packet.
The receiving router, using the preconfigured secret key, will compute an MD5 hash of the packet and compare it with the hash value that the packet carries, thus verifying the packet’s authenticity.
Sequence numbers are also used with MD5 authentication to protect against replay attacks.
Multiple same-cost paths.
When multiple paths to a destination have the same cost, OSPF allows multiple paths to be used (that is, a single path need not be chosen for carrying all traffic when multiple equal-cost paths exist).
Integrated support for unicast and multicast routing.
 Multicast OSPF (MOSPF) [RFC 1584] provides simple extensions to OSPF to provide for multicast routing.
MOSPF uses the existingOSPF link database and adds a new type of link-state advertisement to the existing OSPF link-statebroadcast mechanism.
Support for hierarchy within a single AS.
 An OSPF autonomous system can be configured hierarchically into areas.
Each area runs its own OSPF link-state routing algorithm, with each routerin an area broadcasting its link state to all other routers in that area.
Within each area, one or more
area border routers are responsible for routing packets outside the area.
Lastly, exactly one OSPF area in the AS is configured to be the backbone area.
The primary role of the backbone area is to route traffic between the other areas in the AS.
The backbone always contains all area borderrouters in the AS and may contain non-border routers as well.
Inter-area routing within the ASrequires that the packet be first routed to an area border router (intra-area routing), then routedthrough the backbone to the area border router that is in the destination area, and then routed to the final destination.
OSPF is a relatively complex protocol, and our coverage here has been necessarily brief; [Huitema 1998 ; Moy 1998; RFC 2328]  provide additional details.
5.4 Routing Among the ISPs: BGP We just learned that OSPF is an example of an intra-AS routing protocol.
When routing a packet between a source and destination within the same AS, the route the packet follows is entirelydetermined by the intra-AS routing protocol.
However, to route a packet across multiple ASs, say from asmartphone in Timbuktu to a server in a datacenter in Silicon Valley, we need an inter-autonomous system routing protocol .
Since an inter-AS routing protocol involves coordination among multiple ASs, communicating ASs must run the same inter-AS routing protocol.
In fact, in the Internet, all ASs run thesame inter-AS routing protocol, called the Border Gateway Protocol, more commonly known as BGP [RFC 4271 ; Stewart 1999] .
BGP is arguably the most important of all the Internet protocols (the only other contender would be the IP protocol that we studied in Section 4.3), as it is the protocol that glues the thousands of ISPs in the Internet together.
As we will soon see, BGP is a decentralized and asynchronous protocol in the vein ofdistance-vector routing described in Section 5.2.2.
Although BGP is a complex and challenging protocol, to understand the Internet on a deep level, we need to become familiar with its underpinnings and operation.
The time we devote to learning BGP will be well worth the effort.
5.4.1 The Role of BGP To understand the responsibilities of BGP, consider an AS and an arbitrary router in that AS.
Recall thatevery router has a forwarding table, which plays the central role in the process of forwarding arrivingpackets to outbound router links.
As we have learned, for destinations that are within the same AS, theentries in the router’s forwarding table are determined by the AS’s intra-AS routing protocol.
But whatabout destinations that are outside of the AS?
This is precisely where BGP comes to the rescue.
In BGP, packets are not routed to a specific destination address, but instead to CIDRized prefixes, with each prefix representing a subnet or a collection of subnets.
In the world of BGP, a destination may takethe form 138.16.68/22, which for this example includes 1,024 IP addresses.
Thus, a router’s forwarding table will have entries of the form ( x, I), where x is a prefix (such as 138.16.68/22) and I is an interface number for one of the router’s interfaces.
As an inter-AS routing protocol, BGP provides each router a means to: 1.
Obtain prefix reachability information from neighboring ASs.
 In particular, BGP allows each
subnet to advertise its existence to the rest of the Internet.
A subnet screams, “I exist and I am here,” and BGP makes sure that all the routers in the Internet know about this subnet.
If it weren’t for BGP, each subnet would be an isolated island—alone, unknown and unreachable bythe rest of the Internet.
2.
Determine the “best” routes to the prefixes.
A router may learn about two or more different routes to a specific prefix.
To determine the best route, the router will locally run a BGP route- selection procedure (using the prefix reachability information it obtained via neighboring routers).The best route will be determined based on policy as well as the reachability information.
Let us now delve into how BGP carries out these two tasks.
++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 5.4.2 Advertising BGP Route Information Consider the network shown in Figure 5.8.
As we can see, this simple network has three autonomous systems: AS1, AS2, and AS3.
As shown, AS3 includes a subnet with prefix x. For each AS, each router is either a gateway router or an internal router.
A gateway router is a router on the edge of an AS thatdirectly connects to one or more routers in other ASs.
An internal router connects only to hosts and routers within its own AS.
In AS1, for example, router 1c is a gateway router; routers 1a, 1b, and 1d areinternal routers.
Let’s consider the task of advertising reachability information for prefix x to all of the routers shown in Figure 5.8.
At a high level, this is straightforward.
First, AS3 sends a BGP message to AS2, saying that x exists and is in AS3; let’s denote this message as “AS3 x”.
Then AS2 sends a BGP message to AS1, saying that x exists and that you can get to x by first passing through AS2 and then going to AS3; let’sdenote that message as “AS2 AS3 x”.
In this manner, each of the autonomous systems will not onlylearn about the existence of x, but also learn about a path of autonomous systems that leads to x. Although the discussion in the above paragraph about advertising BGP reachability information should get the general idea across, it is not precise in the sense that autonomous systems do not actually sendmessages to each other, but instead routers do.
To understand this, let’s now re-examine the example in Figure 5.8.
In BGP,
Figure 5.8 Network with three autonomous systems.
AS3 includes a subnet with prefix x pairs of routers exchange routing information over semi-permanent TCP connections using port 179.
Each such TCP connection, along with all the BGP messages sent over the connection, is called a BGP connection .
Furthermore, a BGP connection that spans two ASs is called an external BGP (eBGP) connection, and a BGP session between routers in the same AS is called an internal BGP (iBGP) connection.
Examples of BGP connections for the network in Figure 5.8 are shown in Figure 5.9.
There is typically one eBGP connection for each link that directly connects gateway routers in different ASs; thus, in Figure 5.9, there is an eBGP connection between gateway routers 1c and 2a and an eBGP connection between gateway routers 2c and 3a.
There are also iBGP connections between routers within each of the ASs.
In particular, Figure 5.9 displays a common configuration of one BGP connection for each pair of routers internal to an AS, creating a mesh of TCP connections within each AS.
In Figure 5.9, the eBGP connections are shown with the long dashes; the iBGP connections are shown with the short dashes.
Note that iBGP connections do not always correspond to physical links.
In order to propagate the reachability information, both iBGP and eBGP sessions are used.
Consider again advertising the reachability information for prefix x to all routers in AS1 and AS2.
In this process,gateway router 3a first sends an eBGP message “AS3 x” to gateway router 2c.
Gateway router 2c then sends the iBGP message “AS3 x” to all of the other routers in AS2, including to gateway router 2a.
Gateway router 2a then sends the eBGP message “AS2 AS3 x” to gateway router 1c.
Figure 5.9 eBGP and iBGP connections Finally, gateway router 1c uses iBGP to send the message “AS2 AS3 x” to all the routers in AS1.
After this process is complete, each router in AS1 and AS2 is aware of the existence of x and is also aware of an AS path that leads to x. Of course, in a real network, from a given router there may be many different paths to a given destination, each through a different sequence of ASs.
For example, consider the network in Figure 5.10, which is the original network in Figure 5.8, with an additional physical link from router 1d to router 3d.
In this case, there are two paths from AS1 to x: the path “AS2 AS3 x” via router 1c; and the new path “AS3 x” via the router 1d.
5.4.3 Determining the Best Routes As we have just learned, there may be many paths from a given router to a destination subnet.
In fact, inthe Internet, routers often receive reachability information about dozens of different possible paths.
Howdoes a router choose among these paths (and then configure its forwarding table accordingly)?
Before addressing this critical question, we need to introduce a little more BGP terminology.
When a router advertises a prefix across a BGP connection, it includes with the prefix several BGP attributes.
In BGP jargon, a prefix along with its attributes is called a route.
Two of the more important attributes are AS-PATH and NEXT-HOP.
The AS-PATH attribute contains the list of ASs through which the
Figure 5.10 Network augmented with peering link between AS1 and AS3 advertisement has passed, as we’ve seen in our examples above.
To generate the AS-PATH value, when a prefix is passed to an AS, the AS adds its ASN to the existing list in the AS-PATH.
For example, in Figure 5.10, there are two routes from AS1 to subnet x: one which uses the AS-PATH “AS2 AS3”; and another that uses the AS-PATH “A3”.
BGP routers also use the AS-PATH attribute to detect and prevent looping advertisements; specifically, if a router sees that its own AS is contained in the path list,it will reject the advertisement.
Providing the critical link between the inter-AS and intra-AS routing protocols, the NEXT-HOP attribute has a subtle but important use.
The NEXT-HOP is the IP address of the router interface that begins the AS-PATH.
To gain insight into this attribute, let’s again refer to Figure 5.10.
As indicated in Figure 5.10, the NEXT-HOP attribute for the route “AS2 AS3 x” from AS1 to x that passes through AS2 is the IPaddress of the left interface on router 2a.
The NEXT-HOP attribute for the route “AS3 x” from AS1 to xthat bypasses AS2 is the IP address of the leftmost interface of router 3d.
In summary, in this toyexample, each router in AS1 becomes aware of two BGP routes to prefix x: IP address of leftmost interface for router 2a; AS2 AS3; x IP address of leftmost interface of router 3d; AS3; x Here, each BGP route is written as a list with three components: NEXT-HOP; AS-PATH; destination prefix.
In practice, a BGP route includes additional attributes, which we will ignore for the time being.
Note that the NEXT-HOP attribute is an IP address of a router that does not belong to AS1; however, the subnet that contains this IP address directly attaches to AS1.
Hot Potato Routing
We are now finally  in position to talk about BGP routing algorithms in a precise manner.
We will begin with one of the simplest routing algorithms, namely, hot potato routing .
Consider router 1b in the network in Figure 5.10.
As just described, this router will learn about two possible BGP routes to prefix x. In hot potato routing, the route chosen (from among all possible routes) is that route with the least cost to the NEXT-HOP router beginning that route.
In this example, router 1b will consult its intra-AS routing information to find the least-cost intra-AS path to NEXT-HOP router 2a and the least-cost intra-AS path to NEXT-HOP router 3d, and then select the route with the smallest ofthese least-cost paths.
For example, suppose that cost is defined as the number of links traversed.
Thenthe least cost from router 1b to router 2a is 2, the least cost from router 1b to router 2d is 3, and router2a would therefore be selected.
Router 1b would then consult its forwarding table (configured by its intra-AS algorithm) and find the interface I that is on the least-cost path to router 2a.
It then adds (x, I) to its forwarding table.
The steps for adding an outside-AS prefix in a router’s forwarding table for hot potato routing are summarized in Figure 5.11.
It is important to note that when adding an outside-AS prefix into a forwarding table, both the inter-AS routing protocol (BGP) and the intra-AS routing protocol (e.g., OSPF) are used.
The idea behind hot-potato routing is for router 1b to get packets out of its AS as quickly as possible (more specifically, with the least cost possible) without worrying about the cost of the remaining portionsof the path outside of its AS to the destination.
In the name “hot potato routing,” a packet is analogous toa hot potato that is burning in your hands.
Because it is burning hot, you want to pass it off to another person (another AS) as quickly as possible.
Hot potato routing is thus Figure 5.11 Steps in adding outside-AS destination in a router’s ­forwarding table a selfish ­algorithm—it tries to reduce the cost in its own AS while ignoring the other components of theend-to-end costs outside its AS.
Note that with hot potato routing, two routers in the same AS may choose two different AS paths to the same prefix.
For example, we just saw that router 1b would sendpackets through AS2 to reach x. However, router 1d would bypass AS2 and send packets directly toAS3 to reach x. Route-Selection Algorithm
In practice, BGP uses an algorithm that is more complicated than hot potato routing, but nevertheless incorporates hot potato routing.
For any given destination prefix, the input into BGP’s route-selectionalgorithm is the set of all routes to that prefix that have been learned and accepted by the router.
If thereis only one such route, then BGP obviously selects that route.
If there are two or more routes to thesame prefix, then BGP sequentially invokes the following elimination rules until one route remains: 1.
A route is assigned a local preference  value as one of its attributes (in addition to the AS-PATH and NEXT-HOP attributes).
The local preference of a route could have been set by the router orcould have been learned from another router in the same AS.
The value of the local preferenceattribute is a policy decision that is left entirely up to the AS’s network administrator. (
We will shortly discuss BGP policy issues in some detail.)
The routes with the highest local preference values are selected.
2.
From the remaining routes (all with the same highest local preference value), the route with the shortest AS-PATH is selected.
If this rule were the only rule for route selection, then BGP would be using a DV algorithm for path determination, where the distance metric uses the number ofAS hops rather than the number of router hops.
3.
From the remaining routes (all with the same highest local preference value and the same AS- PATH length), hot potato routing is used, that is, the route with the closest NEXT-HOP router is selected.
4.
If more than one route still remains, the router uses BGP identifiers to select the route; see[Stewart 1999] .
As an example, let’s again consider router 1b in Figure 5.10.
Recall that there are exactly two BGP routes to prefix x, one that passes through AS2 and one that bypasses AS2.
Also recall that if hot potatorouting on its own were used, then BGP would route packets through AS2 to prefix x. But in the aboveroute-selection algorithm, rule 2 is applied before rule 3, causing BGP to select the route that bypassesAS2, since that route has a shorter AS PATH.
So we see that with the above route-selection algorithm,BGP is no longer a selfish algorithm—it first looks for routes with short AS paths (thereby likely reducingend-to-end delay).
As noted above, BGP is the de facto standard for inter-AS routing for the Internet.
To see the contents of various BGP routing tables (large!)
extracted from routers in tier-1 ISPs, see http:/ / www.routeviews.org .
BGP routing tables often contain over half a million routes (that is, prefixes and corresponding attributes).
Statistics about the size and characteristics of BGP routing tables are presented in [Potaroo 2016].
5.4.4 IP-Anycast
In addition to being the Internet’s inter-AS routing protocol, BGP is often used to implement the IP- anycast service [RFC 1546 , RFC 7094] , which is commonly used in DNS.
To motivate IP-anycast, consider that in many applications, we are interested in (1) replicating the same content on different servers in many different dispersed geographical locations, and (2) having each user access the contentfrom the server that is closest.
For example, a CDN may replicate videos and other objects on servers in different countries.
Similarly, the DNS system can replicate DNS records on DNS servers throughout the world.
When a user wants to access this replicated content, it is desirable to point the user to the“nearest” server with the replicated content.
BGP’s route-selection algorithm provides an easy andnatural mechanism for doing so.
To make our discussion concrete, let’s describe how a CDN might use IP- ­anycast.
As shown in Figure 5.12, during the IP-anycast configuration stage, the CDN company assigns the same IP address to each of its servers, and uses standard BGP to advertise this IP address from each of the servers.
When a BGP router receives multiple route advertisements for this IP address, it treats these advertisements as providing different paths to the same physical location (when, in fact, the advertisements are for different paths to different physical locations).
When configuring its routing table, each router will locally use theBGP route-selection algorithm to pick the “best” (for example, closest, as determined by AS-hop counts)route to that IP address.
For example, if one BGP route (corresponding to one location) is only one AShop away from the router, and all other BGP routes (corresponding to other locations) are two or moreAS hops away, then the BGP router would choose to route packets to the location that is one hop away.
After this initial BGP address-advertisement phase, the CDN can do its main job of distributing content.
When a client requests the video, the CDN returns to the client the common IP address used by the geographically dispersed servers, no matter where the client is located.
When the client sends a request to that IP address, Internet routers then forward the request packet to the “closest” server, as defined bythe BGP route-selection algorithm.
Although the above CDN example nicely illustrates how IP-anycast can be used, in practice CDNs generally choose not to use IP-anycast because BGP routing changes can result in different packets ofthe same TCP connection arriving at different instances of the Web server.
But IP-anycast is extensively used by the DNS system to direct DNS queries to the closest root DNS server.
Recall from Section 2.4, there are currently 13 IP addresses for root DNS servers.
But corresponding
Figure 5.12 Using IP-anycast to bring users to the closest CDN server to each of these addresses, there are multiple DNS root servers, with some of these addresses having over 100 DNS root servers scattered over all corners of the world.
When a DNS query is sent to one of these 13 IP addresses, IP anycast is used to route the query to the nearest of the DNS root servers thatis responsible for that address.
5.4.5 Routing Policy When a router selects a route to a destination, the AS routing policy can trump all other considerations,such as shortest AS path or hot potato routing.
Indeed, in the route-selection algorithm, routes are firstselected according to the local-preference attribute, whose value is fixed by the policy of the local AS.
Let’s illustrate some of the basic concepts of BGP routing policy with a simple example.
Figure 5.13 shows six interconnected autonomous systems: A, B, C, W, X, and Y. It is important to note that A, B, C, W, X, and Y are ASs, not routers.
Let’s
Figure 5.13 A simple BGP policy scenario assume that autonomous systems W, X, and Y are access ISPs and that A, B, and C are backbone provider networks.
We’ll also assume that A, B, and C, directly send traffic to each other, and provide full BGP information to their customer networks.
All traffic entering an ISP access network must bedestined for that network, and all traffic leaving an ISP access network must have originated in thatnetwork.
W and Y are clearly access ISPs.
X is a multi-homed access ISP, since it is connected to therest of the network via two different providers (a scenario that is becoming increasingly common inpractice).
However, like W and Y, X itself must be the source/destination of all traffic leaving/entering X.But how will this stub network behavior be implemented and enforced?
How will X be prevented fromforwarding traffic between B and C?
This can easily be accomplished by controlling the manner in whichBGP routes are advertised.
In particular X will function as an access ISP network if it advertises (to its neighbors B and C) that it has no paths to any other destinations except itself.
That is, even though X may know of a path, say XCY, that reaches network Y, it will not advertise this path to B. Since B isunaware that X has a path to Y, B would never forward traffic destined to Y (or C) via X. This simpleexample illustrates how a selective route advertisement policy can be used to implementcustomer/provider routing relationships.
Let’s next focus on a provider network, say AS B. Suppose that B has learned (from A) that A has a path AW to W. B can thus install the route AW into its routing information base.
Clearly, B also wants to advertise the path BAW to its customer, X, so that X knows that it can route to W via B. But should B advertise the path BAW to C?
If it does so, then C could route traffic to W via BAW.
If A, B, and C are allbackbone providers, than B might rightly feel that it should not have to shoulder the burden (and cost!)of carrying transit traffic between A and C. B might rightly feel that it is A’s and C’s job (and cost!)
tomake sure that C can route to/from A’s customers via a direct connection between A and C. There arecurrently no official standards that govern how backbone ISPs route among themselves.
However, arule of thumb followed by commercial ISPs is that any traffic flowing across an ISP’s backbone networkmust have either a source or a destination (or both) in a network that is a customer of that ISP;otherwise the traffic would be getting a free ride on the ISP’s network.
Individual peering agreements (that would govern questions such as PRINCIPLES IN PRACTICE
WHY ARE THERE DIFFERENT INTER-AS AND INTRA-AS ROUTING PROTOCOLS?
Having now studied the details of specific inter-AS and intra-AS routing protocols deployed in today’s Internet, let’s conclude by considering perhaps the most fundamental question we couldask about these protocols in the first place (hopefully, you have been wondering this all along,and have not lost the forest for the trees!):
Why are different inter-AS and intra-AS routingprotocols used?
The answer to this question gets at the heart of the differences between the goals of routing within an AS and among ASs: Policy.
Among ASs, policy issues dominate.
It may well be important that traffic originating in a given AS not be able to pass through another specific AS.
Similarly, a given AS may well want to control what transit traffic it carries between other ASs.
We have seen that BGPcarries path attributes and provides for controlled distribution of routing information so that such policy-based routing decisions can be made.
Within an AS, everything is nominally under the same administrative control, and thus policy issues play a much less importantrole in choosing routes within the AS.
Scale.
The ability of a routing algorithm and its data structures to scale to handle routing to/among large numbers of networks is a critical issue in inter-AS routing.
Within an AS, scalability is less of a concern.
For one thing, if a single ISP becomes too large, it is alwayspossible to divide it into two ASs and perform inter-AS routing between the two new ASs. (
Recall that OSPF allows such a hierarchy to be built by splitting an AS into areas.)
Performance.
 Because inter-AS routing is so policy oriented, the quality (for example, performance) of the routes used is often of secondary concern (that is, a longer or more costly route that satisfies certain policy criteria may well be taken over a route that is shorterbut does not meet that criteria).
Indeed, we saw that among ASs, there is not even the notion of cost (other than AS hop count) associated with routes.
Within a single AS, however, such policy concerns are of less importance, allowing routing to focus more on the level ofperformance realized on a route.
those raised above) are typically negotiated between pairs of ISPs and are often confidential; [Huston 1999a]  provides an interesting discussion of peering agreements.
For a detailed description of how routing policy reflects commercial relationships among ISPs, see [Gao 2001; Dmitiropoulos 2007].
For a discussion of BGP routing polices from an ISP standpoint, see [Caesar 2005b] .
This completes our brief introduction to BGP.
Understanding BGP is important because it plays a central role in the Internet.
We encourage you to see the references [Griffin 2012; Stewart 1999 ; Labovitz 1997 ; Halabi 2000; Huitema 1998 ; Gao 2001; Feamster 2004 ; Caesar 2005b ; Li 2007] to learn more about BGP.
5.4.6 Putting the Pieces Together: Obtaining Internet Presence Although this subsection is not about BGP per se, it brings together many of the protocols and concepts we’ve seen thus far, including IP addressing, DNS, and BGP.
Suppose you have just created a small company that has a number of servers, including a public Web server that describes your company’s products and services, a mail server from which your employees obtain their e-mail messages, and a DNS server.
Naturally, you would like the entire world to be able tovisit your Web site in order to learn about your exciting products and services.
Moreover, you would likeyour employees to be able to send and receive e-mail to potential customers throughout the world.
To meet these goals, you first need to obtain Internet connectivity, which is done by contracting with, and connecting to, a local ISP.
Your company will have a gateway router, which will be connected to arouter in your local ISP.
This connection might be a DSL connection through the existing telephoneinfrastructure, a leased line to the ISP’s router, or one of the many other access solutions described in Chapter 1.
Your local ISP will also provide you with an IP address range, e.g., a /24 address range consisting of 256 addresses.
Once you have your physical connectivity and your IP address range, you will assign one of the IP addresses (in your address range) to your Web server, one to your mail server,one to your DNS server, one to your gateway router, and other IP addresses to other servers andnetworking devices in your company’s network.
In addition to contracting with an ISP, you will also need to contract with an Internet registrar to obtain a domain name for your company, as described in Chapter 2.
For example, if your company’s name is, say, Xanadu Inc., you will naturally try to obtain the domain name xanadu.com.
Your company must also obtain presence in the DNS system.
Specifically, because outsiders will want to contact your DNS server to obtain the IP addresses of your servers, you will also need to provide your registrar with the IPaddress of your DNS server.
Your registrar will then put an entry for your DNS server (domain name and corresponding IP address) in the .com top-level-domain servers, as described in Chapter 2.
After this step is completed, any user who knows your domain name (e.g., xanadu.com) will be able to obtain the IP address of your DNS server via the DNS system.
So that people can discover the IP addresses of your Web server, in your DNS server you will need to include entries that map the host name of your Web server (e.g., www.xanadu.com ) to its IP address.
You will want to have similar entries for other publicly available servers in your company, including your mail server.
In this manner, if Alice wants to browse your Web server, the DNS system will contact your DNS server, find the IP address of your Web server, and give it to Alice.
Alice can then establish a TCPconnection directly with your Web server.
However, there still remains one other necessary and crucial step to allow outsiders from around the
world to access your Web server.
Consider what happens when Alice, who knows the IP address of your Web server, sends an IP datagram (e.g., a TCP SYN segment) to that IP address.
This datagramwill be routed through the Internet, visiting a series of routers in many different ASs, and eventuallyreach your Web server.
When any one of the routers receives the datagram, it is going to look for anentry in its forwarding table to determine on which outgoing port it should forward the datagram.
Therefore, each of the routers needs to know about the existence of your company’s /24 prefix (or some aggregate entry).
How does a router become aware of your company’s prefix?
As we have just seen, itbecomes aware of it from BGP!
Specifically, when your company contracts with a local ISP and getsassigned a prefix (i.e., an address range), your local ISP will use BGP to advertise your prefix to theISPs to which it connects.
Those ISPs will then, in turn, use BGP to propagate the advertisement.
Eventually, all Internet routers will know about your prefix (or about some aggregate that includes yourprefix) and thus be able to appropriately forward datagrams destined to your Web and mail servers.
5.5 The SDN Control Plane In this section, we’ll dive into the SDN control plane—the network-wide logic that controls packet forwarding among a network’s SDN-enabled devices, as well as the configuration and management ofthese devices and their services.
Our study here builds on our earlier discussion of generalized SDN forwarding in Section 4.4, so you might want to first review that section, as well as Section 5.1 of this chapter, before continuing on.
As in Section 4.4, we’ll again adopt the terminology used in the SDN literature and refer to the network’s forwarding devices as “packet switches” (or just switches, with “packet” being understood), since forwarding decisions can be made on the basis of network-layersource/destination addresses, link-layer source/destination addresses, as well as many other values intransport-, network-, and link-layer packet-header fields.
Four key characteristics of an SDN architecture can be identified [Kreutz 2015] : Flow-based forwarding.
 Packet forwarding by SDN-controlled switches can be based on any number of header field values in the transport-layer, network-layer, or link-layer header.
We saw in Section 4.4 that the OpenFlow1.0 abstraction allows forwarding based on eleven different header field values.
This contrasts sharply with the traditional approach to router-based forwarding that we studied in Sections 5.2–5.4, where forwarding of IP datagrams was based solely on a datagram’s destination IP address.
Recall from Figure 5.2 that packet forwarding rules are specified in a switch’s flow table; it is the job of the SDN control plane to compute, manage and install flow table entries in all of the network’s switches.
Separation of data plane and control plane.
 This separation is shown clearly in Figures 5.2 and 5.14.
The data plane consists of the network’s switches— relatively simple (but fast) devices that execute the “match plus action” rules in their flow tables.
The control plane consists of servers and software that determine and manage the switches’ flow tables.
Network control functions: external to data-plane switches.
 Given that the “S” in SDN is for “software,” it’s perhaps not surprising that the SDN control plane is implemented in software.
Unliketraditional routers, however, this software executes on servers that are both distinct and remote from the network’s switches.
As shown in Figure 5.14, the control plane itself consists of two components —an SDN controller (or network operating system [Gude 2008]) and a set of network-control applications.
The controller maintains accurate network state information (e.g., the state of remote links, switches, and hosts); provides this information to the network-control applications running inthe control plane; and provides the means through which these applications can monitor, program, and control the underlying network devices.
Although the controller in Figure 5.14 is shown as a single central server, in practice the controller is only logically centralized; it is typically implemented on several servers that provide coordinated, scalable performance and high availability.
A programmable network.
The network is programmable through the network-control applications running in the control plane.
These applications represent the “brains” of the SDN control plane, using the APIs provided by the SDN controller to specify and control the data plane in the networkdevices.
For example, a routing network-control application might determine the end-end paths between sources and destinations (e.g., by executing Dijkstra’s algorithm using the node-state and link-state information maintained by the SDN controller).
Another network application might performaccess control, i.e., determine which packets are to be blocked at a switch, as in our third example in Section 4.4.3.
Yet another application might forward packets in a manner that performs server load balancing (the second example we considered in Section 4.4.3).
From this discussion, we can see that SDN represents a significant “unbundling” of network functionality—data plane switches, SDN controllers, and network-control applications are separate entities that mayeach be provided by different vendors and organizations.
This contrasts with the pre-SDN model in which a switch/router (together with its embedded control plane software and protocol implementations) was monolithic, vertically integrated, and sold by a single vendor.
This unbundling of network functionality in SDN has been likened to the earlier evolution from mainframe computers (wherehardware, system software, and applications were provided by a single vendor) to personal computers(with their separate hardware, operating systems, and applications).
The unbundling of computinghardware, system software, and applications has arguably led to a rich, open ecosystem driven byinnovation in all three of these areas; one hope for SDN is that it too will lead to a such rich innovation.
Given our understanding of the SDN architecture of Figure 5.14, many questions naturally arise.
How and where are the flow tables actually computed?
How are these tables updated in response to events at SDN-controlled devices (e.g., an attached link going up/down)?
And how are the flow table entries atmultiple switches coordinated in such a way as to result in orchestrated and consistent network-widefunctionality (e.g., end-to-end paths for forwarding packets from sources to destinations, or coordinateddistributed firewalls)?
It is the role of the SDN control plane to provide these, and many other,capabilities.
Figure 5.14 Components of the SDN architecture: SDN-controlled switches, the SDN controller, network-control applications 5.5.2 The SDN Control Plane: SDN Controller and SDN Network-control Applications Let’s begin our discussion of the SDN control plane in the abstract, by considering the generic capabilities that the control plane must provide.
As we’ll see, this abstract, “first principles” approach willlead us to an overall architecture that reflects how SDN control planes have been implemented inpractice.
As noted above, the SDN control plane divides broadly into two components—the SDN controller and the SDN network-control applications.
Let’s explore the controller first.
Many SDN controllers have been developed since the earliest SDN controller [Gude 2008]; see [Kreutz 2015]  for an extremely thorough and up-to-date survey.
Figure 5.15 provides a more detailed view of a generic SDN controller.
A controller’s functionality can be broadly organized into three layers.
Let’s consider these layers in an uncharacteristically bottom-up fashion: A communication layer: communicating between the SDN controller and controlled network devices.
Clearly, if an SDN controller is going to control the operation of a remote SDN-enabled
switch, host, or other device, a protocol is needed to transfer information between the controller and that device.
In addition, a device must be able to communicate locally-observed events to thecontroller (e.g., a message indicating that an attached link has gone up or down, that a device hasjust joined the network, or a heartbeat indicating that a device is up and operational).
These eventsprovide the SDN controller with an up-to-date view of the network’s state.
This protocol constitutes the lowest layer of the controller architecture, as shown in Figure 5.15.
The communication between the controller and the controlled devices cross what has come to be known as the controller’s “southbound” interface.
In Section 5.5.2, we’ll study OpenFlow—a specific protocol that provides this communication functionality.
OpenFlow is implemented in most, if not all, SDN controllers.
A network-wide state-management layer.
The ultimate control decisions made by the SDN control plane—e.g., configuring flow tables in all switches to achieve the desired end-end forwarding, to implement load balancing, or to implement a particular firewalling capability—will require that thecontroller have up-to-date information about state of the networks’ hosts, links, switches, and other SDN-controlled devices.
A switch’s flow table contains counters whose values might also be profitably used by network-control applications; these values should thus be available to theapplications.
Since the ultimate aim of the control plane is to determine flow tables for the variouscontrolled devices, a controller might also maintain a copy of these tables.
These pieces ofinformation all constitute examples of the network-wide “state” maintained by the SDN controller.
The interface to the network-control application layer.
 The controller interacts with network- control applications through its “northbound” interface.
This API
Figure 5.15 Components of an SDN controller allows network-control applications to read/write network state and flow tables within the state- management layer.
Applications can register to be notified when state-change events occur, so that they can take actions in response to network event notifications sent from SDN-controlled devices.
Different types of APIs may be provided; we’ll see that two popular SDN controllers communicate with their applications using a REST [Fielding 2000]  request-response interface.
We have noted several times that an SDN controller can be considered to be ­“logically centralized,” i.e., that the controller may be viewed externally (e.g., from the point of view of SDN-controlled devices and external network-control applications) as a single, monolithic service.
However, these services and the databases used to hold state information are implemented in practice by a distributed  set of servers for fault tolerance, high availability, or for performance reasons.
With controller functions being implemented by a set of servers, the semantics of the controller’s internal operations (e.g., maintaining logical time ordering of events, consistency, consensus, and more) must be considered [Panda 2013].
Such concerns are common across many different distributed systems; see [Lamport 1989, Lampson 1996]  for elegant solutions to these challenges.
Modern controllers such as OpenDaylight [OpenDaylight Lithium 2016]  and ONOS [ONOS 2016] (see sidebar) have placed considerable emphasis on architecting a logically centralized but physically distributed controller platform that provides scalable services and high availability to the controlled devices and network-control applications alike.
The architecture depicted in Figure 5.15 closely resembles the architecture of the originally proposed NOX controller in 2008 [Gude 2008], as well as that of today’s OpenDaylight [OpenDaylight Lithium 2016]  and ONOS [ONOS 2016] SDN controllers (see sidebar).
We’ll cover an example of controller operation in Section 5.5.3.
First, however, let’s examine the OpenFlow protocol, which lies in the controller’s communication layer.
5.5.2 OpenFlow Protocol The OpenFlow protocol [OpenFlow 2009 , ONF 2016] operates between an SDN controller and an SDN-controlled switch or other device implementing the OpenFlow API that we studied earlier in Section 4.4.
The OpenFlow protocol operates over TCP, with a default port number of 6653.
Among the important messages flowing from the controller to the controlled switch are the following: Configuration.
 This message allows the controller to query and set a switch’s configuration parameters.
Modify-State.
This message is used by a controller to add/delete or modify entries in the switch’s flow table, and to set switch port properties.
Read-State.
This message is used by a controller to collect statistics and counter values from theswitch’s flow table and ports.
Send-Packet.
This message is used by the controller to send a specific packet out of a specifiedport at the controlled switch.
The message itself contains the packet to be sent in its payload.
Among the messages flowing from the SDN-controlled switch to the controller are the following: Flow-Removed.
This message informs the controller that a flow table entry has been removed, for example by a timeout or as the result of a received modify-state message.
Port-status.
This message is used by a switch to inform the controller of a change in port status.
Packet-in.
Recall from Section 4.4 that a packet arriving at a switch port and not matching any flow table entry is sent to the controller for additional processing.
Matched packets may also be sent to the controller, as an action to be taken on a match.
The packet-in  message is used to send such packets to the controller.
Additional OpenFlow messages are defined in [OpenFlow 2009 , ONF 2016].
Principles in Practice Google’s Software-Defined Global Network Recall from the case study in Section 2.6 that Google deploys a dedicated wide-area network (WAN) that interconnects its data centers and server clusters (in IXPs and ISPs).
This network, called B4, has a Google-designed SDN control plane built on OpenFlow.
Google’s network isable to drive WAN links at near 70% utilization over the long run (a two to three fold increaseover typical link utilizations) and split application flows among multiple paths based on application priority and existing flow demands [Jain 2013].
The Google B4 network is particularly it well-suited for SDN: (i) Google controls all devices from the edge servers in IXPs and ISPs to routers in their network core; (ii) the most bandwidth- intensive applications are large-scale data copies between sites that can defer to higher-priority interactive applications during times of resource congestion; (iii) with only a few dozen data centers being connected, centralized control is feasible.
Google’s B4 network uses custom-built switches, each implementing a slightly extended version of OpenFlow, with a local Open Flow Agent (OFA) that is similar in spirit to the control agent we encountered in Figure 5.2.
Each OFA in turn connects to an Open Flow Controller (OFC) in the network control server (NCS), using a separate “out of band” network, distinct from the network that carries data-center traffic between data centers.
The OFC thus provides the services usedby the NCS to communicate with its controlled switches, similar in spirit to the lowest layer in the SDN architecture shown in Figure 5.15.
In B4, the OFC also performs state management functions, keeping node and link status in a Network Information Base (NIB).
Google’s implementation of the OFC is based on the ONIX SDN controller [Koponen 2010].
Two routing protocols, BGP (for routing between the data centers) and IS-IS (a close relative of OSPF, forrouting within a data center), are implemented.
Paxos [Chandra 2007]  is used to execute hot replicas of NCS components to protect against failure.
A traffic engineering network-control application, sitting logically above the set of network control servers, interacts with these servers to provide global, network-wide bandwidth provisioning for groups of application flows.
With B4, SDN made an important leap forward into the operational networks of a global network provider.
See [Jain 2013] for a detailed description of B4.
5.5.3 Data and Control Plane Interaction: An Example
In order to solidify our understanding of the interaction between SDN-controlled switches and the SDN controller, let’s consider the example shown in Figure 5.16, in which Dijkstra’s algorithm (which we studied in Section 5.2) is used to determine shortest path routes.
The SDN scenario in Figure 5.16 has two important differences from the earlier per-router-control scenario of Sections 5.2.1 and 5.3, where Dijkstra’s algorithm was implemented in each and every router and link-state updates were flooded among all network routers: Dijkstra’s algorithm is executed as a separate application, outside of the packet switches.
Packet switches send link updates to the SDN controller and not to each other.
In this example, let’s assume that the link between switch s1 and s2 goes down; that shortest path routing is implemented, and consequently and that incoming and outgoing flow forwarding rules at s1, s3, and s4 are affected, but that s2’s Figure 5.16 SDN controller scenario: Link-state change operation is unchanged.
Let’s also assume that OpenFlow is used as the communication layer protocol,and that the control plane performs no other function other than link-state routing.
1.
Switch s1, experiencing a link failure between itself and s2, notifies the SDN controller of the link-state change using the OpenFlow port-status message.
2.
The SDN controller receives the OpenFlow message indicating the link-state change, and notifies the link-state manager, which updates a link-state ­database.
3.
The network-control application that implements Dijkstra’s link-state routing has previouslyregistered to be notified when link state changes.
That application receives the notification of the link-state change.
4.
The link-state routing application interacts with the link-state manager to get updated link state; it might also consult other components in the state- ­management layer.
It then computes the new least-cost paths.
5.
The link-state routing application then interacts with the flow table manager, which determinesthe flow tables to be updated.
6.
The flow table manager then uses the OpenFlow protocol to update flow table entries at affectedswitches—s1 (which will now route packets destined to s2 via s4), s2 (which will now begin receiving packets from s1 via intermediate switch s4), and s4 (which must now forward packetsfrom s1 destined to s2).
This example is simple but illustrates how the SDN control plane provides control-plane services (in thiscase network-layer routing) that had been previously implemented with per-router control exercised ineach and every network router.
One can now easily appreciate how an SDN-enabled ISP could easilyswitch from least-cost path routing to a more hand-tailored approach to routing.
Indeed, since the controller can tailor the flow tables as it pleases, it can implement any form of forwarding that it pleases —simply by changing its application-control software.
This ease of change should be contrasted to the case of a traditional per-router control plane, where software in all routers (which might be provided tothe ISP by multiple independent vendors) must be changed.
5.5.4 SDN: Past and Future Although the intense interest in SDN is a relatively recent phenomenon, the technical roots of SDN, andthe separation of the data and control planes in particular, go back considerably further.
In 2004, [Feamster 2004 , Lakshman 2004 , RFC 3746]  all argued for the separation of the network’s data and control planes. [
van der Merwe 1998]  describes a control framework for ATM networks [Black 1995] with multiple controllers, each controlling a number of ATM switches.
The Ethane project [Casado 2007] pioneered the notion of a network of simple flow-based Ethernet switches with match-plus-action flow tables, a centralized controller that managed flow admission and routing, and the forwarding of unmatched packets from the switch to the controller.
A network of more than 300 Ethane switches wasoperational in 2007.
Ethane quickly evolved into the OpenFlow project, and the rest (as the saying goes)is history!
Numerous research efforts are aimed at developing future SDN architectures and capabilities.
As we have seen, the SDN revolution is leading to the disruptive replacement of dedicated monolithic switchesand routers (with both data and control planes) by simple commodity switching hardware and asophisticated software control plane.
A generalization of SDN known as network functions virtualization(NFV) similarly aims at disruptive replacement of sophisticated middleboxes (such as middleboxes with dedicated hardware and proprietary software for media caching/service) with simple commodity servers, switching, and storage [Gember-Jacobson 2014] .
A second area of important research seeks to extend SDN concepts from the intra-AS setting to the inter-AS setting [Gupta 2014].
PRINCIPLES IN PRACTICE SDN Controller Case Studies: The OpenDaylight and ONOS Controllers In the earliest days of SDN, there was a single SDN protocol (OpenFlow [McKeown 2008; OpenFlow 2009] ) and a single SDN controller (NOX [Gude 2008]).
Since then, the number of SDN controllers in particular has grown significantly [Kreutz 2015] .
Some SDN controllers are company-specific and proprietary, e.g., ONIX [Koponen 2010], Juniper Networks Contrail [Juniper Contrail 2016], and Google’s controller [Jain 2013] for its B4 wide-area network.
But many more controllers are open-source and implemented in a variety of programming languages [Erickson 2013] .
Most recently, the OpenDaylight controller [OpenDaylight Lithium 2016]  and the ONOS controller [ONOS 2016] have found considerable industry support.
They are both open-source and are being developed in partnership with the Linux Foundation.
The OpenDaylight Controller Figure 5.17 presents a simplified view of the OpenDaylight Lithium SDN controller platform [OpenDaylight Lithium 2016] .
ODL’s main set of controller components correspond closely to those we developed in Figure 5.15.
Network-Service Applications  are the applications that determine how data-plane forwarding and other services, such as firewalling and load balancing, are accomplished in the controlled switches.
Unlike the canonical controller in Figure 5.15, the ODL controller has two interfaces through which applications may communicate with native controller services and each other: external applications communicate with controller modules using a REST request-response APIrunning over HTTP.
Internal applications communicate with each other via the ServiceAbstraction Layer (SAL).
The choice as to whether a controller application is implementedexternally or internally is up to the application designer;
Figure 5.17 The OpenDaylight controller the particular configuration of applications shown in Figure 5.17 is only meant as an ­example.
ODL’s Basic Network-Service Functions  are at the heart of the controller, and they correspond closely to the network-wide state management capabilities that we encountered in Figure 5.15.
The SAL is the controller’s nerve center, allowing controller ­components and applications to invoke each other’s services and to subscribe to events they generate.
It also provides a uniform abstract interface to the specific underlying communications protocols  in the communication layer, including OpenFlow and SNMP (the Simple Network Management Protocol—a networkmanagement protocol that we will cover in Section 5.7).
OVSDB is a protocol used to manage data center switching, an important application area for SDN technology.
We’ll introduce datacenter networking in Chapter 6.
Figure 5.18 ONOS controller architecture The ONOS Controller Figure 5.18 presents a simplified view of the ONOS controller ONOS 2016].
Similar to the canonical controller in Figure 5.15, three layers can be identified in the ONOS ­controller: Northbound abstractions and protocols.
 A unique feature of ONOS is its intent framework, which allows an application to request a high-level service (e.g., to setup a connection between host A and Host B, or conversely to not allow Host A and host B tocommunicate) without having to know the details of how this service is performed.
State information is provided to network-control applications across the northbound API either synchronously (via query) or asynchronously (via listener callbacks, e.g., when network statechanges).
Distributed core.
The state of the network’s links, hosts, and devices is maintained in ONOS’s distributed core.
ONOS is deployed as a service on a set of interconnected servers, with each server running an identical copy of the ONOS software; an increased number ofservers offers an increased service capacity.
The ONOS core provides the mechanisms for service replication and coordination among instances, providing the applications above andthe network devices below with the abstraction of logically centralized core services.
Southbound abstractions and protocols.
 The southbound abstractions mask the heterogeneity of the underlying hosts, links, switches, and protocols, allowing the distributed core to be both device and protocol agnostic.
Because of this abstraction, the southboundinterface below the distributed core is logically higher than in our canonical controller in Figure 5.14 or the ODL controller in Figure 5.17.
5.6 ICMP: The Internet Control Message Protocol The Internet Control Message Protocol (ICMP), specified in [RFC 792], is used by hosts and routers to communicate network-layer information to each other.
The most typical use of ICMP is for error reporting.
For example, when running an HTTP session, you may have encountered an error messagesuch as “Destination network unreachable.”
This message had its origins in ICMP.
At some point, an IP router was unable to find a path to the host specified in your HTTP request.
That router created and sent an ICMP message to your host indicating the error.
ICMP is often considered part of IP, but architecturally it lies just above IP, as ICMP messages are carried inside IP datagrams.
That is, ICMP messages are carried as IP payload, just as TCP or UDPsegments are carried as IP payload.
Similarly, when a host receives an IP datagram with ICMPspecified as the upper-layer protocol (an upper-layer protocol number of 1), it demultiplexes thedatagram’s contents to ICMP, just as it would demultiplex a datagram’s content to TCP or UDP.
ICMP messages have a type and a code field, and contain the header and the first 8 bytes of the IP datagram that caused the ICMP message to be generated in the first place (so that the sender can determine the datagram that caused the error).
Selected ICMP message types are shown in Figure 5.19.
Note that ICMP messages are used not only for signaling error conditions.
The well-known ping program sends an ICMP type 8 code 0 message to the specified host.
The destination host, seeing the echo request, sends back a type 0 code 0 ICMP echo reply.
Most TCP/IPimplementations support the ping server directly in the operating system; that is, the server is not a process.
Chapter  11 of [Stevens 1990]  provides the source code for the ping client program.
Note that the client program needs to be able to instruct the operating system to generate an ICMP message of type 8 code 0.
Another interesting ICMP message is the source quench message.
This message is seldom used in practice.
Its original purpose was to perform congestion control—to allow a congested router to send anICMP source quench message to a host to force
Figure 5.19 ICMP message types that host to reduce its transmission rate.
We have seen in Chapter 3 that TCP has its own congestion- control mechanism that operates at the transport layer, without the use of network-layer feedback such as the ICMP source quench message.
In Chapter 1 we introduced the Traceroute program, which allows us to trace a route from a host to any other host in the world.
Interestingly, Traceroute is implemented with ICMP messages.
To determine the names and addresses of the routers between source and destination, Traceroute in the source sends aseries of ordinary IP datagrams to the destination.
Each of these datagrams carries a UDP segment withan unlikely UDP port number.
The first of these datagrams has a TTL of 1, the second of 2, the third of 3, and so on.
The source also starts timers for each of the datagrams.
When the nth datagram arrives at the nth router, the nth router observes that the TTL of the datagram has just expired.
According to the rules of the IP protocol, the router discards the datagram and sends an ICMP warning message to the source (type 11 code 0).
This warning message includes the name of the router and its IP address.
When this ICMP message arrives back at the source, the source obtains the round-trip time from the timer and the name and IP address of the nth router from the ICMP message.
How does a Traceroute source know when to stop sending UDP segments?
Recall that the source increments the TTL field for each datagram it sends.
Thus, one of the datagrams will eventually make it all the way to the destination host.
Because this datagram contains a UDP segment with an unlikely port
number, the destination host sends a port unreachable ICMP message (type 3 code 3) back to the source.
When the source host receives this particular ICMP message, it knows it does not need to send additional probe packets. (
The standard Traceroute program actually sends sets of three packets withthe same TTL; thus the Traceroute output provides three results for each TTL.)
In this manner, the source host learns the number and the identities of routers that lie between it and the destination host and the round-trip time between the two hosts.
Note that the Traceroute client program must be able to instruct the operating system to generate UDP datagrams with specific TTL values andmust also be able to be notified by its operating system when ICMP messages arrive.
Now that youunderstand how Traceroute works, you may want to go back and play with it some more.
A new version of ICMP has been defined for IPv6 in RFC 4443.
In addition to reorganizing the existing ICMP type and code definitions, ICMPv6 also added new types and codes required by the new IPv6functionality.
These include the “Packet Too Big” type and an “unrecognized IPv6 options” error code.
5.7 Network Management and SNMP Having now made our way to the end of our study of the network layer, with only the link-layer before us, we’re well aware that a network consists of many complex, interacting pieces of hardware and software—from the links, switches, routers, hosts, and other devices that comprise the physical components ofthe network to the many protocols that control and coordinate these devices.
When hundreds orthousands of such components are brought together by an organization to form a network, the job of the network administrator to keep the network “up and running” is surely a challenge.
We saw in Section 5.5 that the logically centralized controller can help with this process in an SDN context.
But the challenge of network management has been around long before SDN, with a rich set of network management tools and approaches that help the network administrator monitor, manage, and controlthe network.
We’ll study these tools and techniques in this section.
An often-asked question is “What is network management?”
A well-conceived, single-sentence (albeit a rather long run-on sentence) definition of network management from [Saydam 1996]  is: Network management includes the deployment, integration, and coordination of the hardware, software, and human elements to monitor, test, poll, configure, analyze, evaluate, and control thenetwork and element resources to meet the real-time, operational performance, and Quality ofService requirements at a reasonable cost.
Given this broad definition, we’ll cover only the rudiments of network management in this section—thearchitecture, protocols, and information base used by a network administrator in performing their task.
We’ll not cover the administrator’s decision-making processes, where topics such as fault identification [Labovitz 1997; Steinder 2002; Feamster 2005 ; Wu 2005; Teixeira 2006] , anomaly detection [Lakhina 2005; Barford 2009] , network design/engineering to meet contracted Service Level Agreements (SLA’s) [Huston 1999a], and more come into consideration.
Our focus is thus purposefully narrow; the interested reader should consult these references, the excellent network-management text by Subramanian [Subramanian 2000], and the more detailed treatment of network management available on the Web site for this text.
5.7.1 The Network Management Framework Figure 5.20 shows the key components of network management:
The managing server  is an application, typically with a human in the loop, running in a centralized network management station in the network operations center (NOC).
The managing server is the locus of activity for network management; it controls the collection, processing, analysis, and/ordisplay of network management information.
It is here that actions are initiated to control networkbehavior and here that the human network administrator interacts with the network’s devices.
A managed device  is a piece of network equipment (including its software) that resides on a managed network.
A managed device might be a host, router, switch, middlebox, modem,thermometer, or other network-connected device.
There may be several so-called managed objects within a managed device.
These managed objects are the actual pieces of hardware within the managed device (for example, a network interface card is but one component of a host or router), and configuration parameters for these hardware and software components (for example, an intra-AS routing protocol such as OSPF).
Each managed object within a managed device associated information that is collected into a Management Information Base (MIB); we’ll see that the values of these pieces of information are available to (and in many cases able to be set by) the managing server.
A MIB object might be acounter, such as the number of IP datagrams discarded at a router due to errors in an IP datagram header, or the number of UDP segments received at a host; descriptive information such as the version of the software running on a DNS server; status information such as whether a particulardevice is functioning correctly; or protocol-specific information such as a routing path to adestination.
MIB objects are specified in a data description language known as SMI (Structure of Management Information) [RFC 2578 ; RFC 2579 ; RFC 2580] .
A formal definition language is used to ensure that the syntax and semantics of the network management data are well defined and unambiguous.
Related MIB objects are gathered into MIB modules.
As of mid-2015, there werenearly 400 MIB modules defined by RFCs, and a much larger number of vendor-specific (private)MIB modules.
Also resident in each managed device is a network management agent , a process running in the managed device that communicates with the managing server,
Figure 5.20 Elements of network management: Managing server, ­managed devices, MIB data, remote agents, SNMP taking local actions at the managed device under the command and control of the managing server.
The network management agent is similar to the routing agent that we saw in Figure 5.2.
The final component of a network management framework is the network ­management protocol .
The protocol runs between the managing server and the managed devices, allowing the managing server to query the status of managed devices and indirectly take actions at these devices via itsagents.
Agents can use the network management protocol to inform the managing server of exceptional events (for example, component failures or violation of performance thresholds).
It’s important to note that the network management protocol does not itself manage the network.
Instead, it provides capabilities that a network administrator can use to manage (“monitor, test, poll,configure, analyze, evaluate, and control”) the network.
This is a subtle, but important, distinction.
Inthe following section, we’ll cover the Internet’s SNMP (Simple Network Management Protocol)protocol.
5.7.2 The Simple Network Management Protocol (SNMP)
The Simple Network Management Protocol  version 2 (SNMPv2) [RFC 3416]  is an application-layer protocol used to convey network-management control and information messages between a managing server and an agent executing on behalf of that managing server.
The most common usage of SNMP isin a request-response mode in which an SNMP managing server sends a request to an SNMP agent,who receives the request, performs some action, and sends a reply to the request.
Typically, a requestwill be used to query (retrieve) or modify (set) MIB object values associated with a managed device.
A second common usage of SNMP is for an agent to send an unsolicited message, known as a trap message, to a managing server.
Trap messages are used to notify a managing server of an exceptionalsituation (e.g., a link interface going up or down) that has resulted in changes to MIB object values.
SNMPv2 defines seven types of messages, known generically as protocol data units—PDUs—as shown in Table 5.2 and described below.
The format of the PDU is shown in Figure 5.21.
The GetRequest , GetNextRequest,  and GetBulkRequest  PDUs are all sent from a managing server to an agent to request the value of one or more MIB objects at the agent’s managed device.
The MIB objects whose values are being Table 5.2 SNMPv2 PDU types SNMPv2 PDU TypeSender-receiver Description GetRequest manager-to-agentget value of one or more MIB object instances GetNextRequest manager-to-agentget value of next MIB object instance in list or table GetBulkRequest manager-to- agentget values in large block of data, for example, valuesin a large table InformRequest manager-to- managerinform remote managing entity of MIB values remoteto its access SetRequest manager-to-agentset value of one or more MIB object instances Response agent-to-manager orgenerated in response to manager-to-manager GetRequest,
GetNextRequest,  GetBulkRequest,  SetRequest PDU,  or  InformRequest SNMPv2-Trap agent-to- managerinform manager of an exceptional event # Figure 5.21 SNMP PDU format requested are specified in the variable binding portion of the PDU.
­GetRequest , GetNextRequest , and GetBulkRequest differ in the granularity of their data requests.
GetRequest  can request an arbitrary set of MIB values; multiple GetNextRequest s can be used to sequence through a list or table of MIB objects; GetBulkRequest  allows a large block of data to be returned, avoiding the overhead incurred if multiple GetRequest  or ­GetNextRequest messages were to be sent.
In all three cases, the agent responds with a Response PDU  containing the object identifiers and their associated values.
The SetRequest  PDU is used by a managing server to set the value of one or more MIB objects in a managed device.
An agent replies with a Response  PDU with the “noError” error status to confirm that the value has indeed been set.
The InformRequest  PDU is used by a managing server to notify another managing server of MIB
information that is remote to the receiving server.
The Response PDU  is typically sent from a managed device to the managing server in response to a request message from that server, returning the requested information.
The final type of SNMPv2 PDU is the trap message.
Trap messages are generated asynchronously; that is, they are not generated in response to a received request but rather in response to an event for which the managing server requires notification.
RFC 3418 defines well-known trap types thatinclude a cold or warm start by a device, a link going up or down, the loss of a neighbor, or an authentication failure event.
A received trap request has no required response from a managing server.
Given the request-response nature of SNMP, it is worth noting here that although SNMP PDUs can becarried via many different transport protocols, the SNMP PDU is typically carried in the payload of aUDP datagram.
Indeed, RFC 3417 states that UDP is “the ­preferred transport mapping.”
However, since UDP is an unreliable transport protocol, there is no guarantee that a request, or its response, will be received at the intended destination.
The request ID field of the PDU (see Figure 5.21) is used by the managing server to number its requests to an agent; the agent’s response takes its request ID from that of the received request.
Thus, the request ID field can be used by the managing server to detect lost requests or replies.
It is up to the managing server to decide whether to retransmit a request if nocorresponding response is received after a given amount of time.
In particular, the SNMP standard doesnot mandate any particular procedure for retransmission, or even if retransmission is to be done in thefirst place.
It only requires that the managing server “needs to act responsibly in respect to thefrequency and duration of retransmissions.”
This, of course, leads one to wonder how a “responsible”protocol should act!
SNMP has evolved through three versions.
The designers of SNMPv3 have said that “SNMPv3 can be thought of as SNMPv2 with additional security and administration capabilities” [RFC 3410] .
Certainly, there are changes in SNMPv3 over SNMPv2, but nowhere are those changes more evident than in the area of administration and security.
The central role of security in SNMPv3 was particularly important,since the lack of adequate security resulted in SNMP being used primarily for monitoring rather than control (for example, SetRequest  is rarely used in SNMPv1).
Once again, we see that ­security—a topic we’ll cover in detail in Chapter 8 — is of critical concern, but once again a concern whose importance had been realized perhaps a bit late and only then “added on.”
5.7 Summary We have now completed our two-chapter journey into the network core—a journey that began with our study of the network layer’s data plane in Chapter 4 and finished here with our study of the network layer’s control plane.
We learned that the control plane is the network-wide logic that controls not only how a datagram is forwarded among routers along an end-to-end path from the source host to thedestination host, but also how network-layer components and services are configured and managed.
We learned that there are two broad approaches towards building a control plane: traditional per-router control (where a routing algorithm runs in each and every router and the routing component in the router communicates with the routing components in other routers) and software-defined networking  (SDN) control (where a logically centralized controller computes and distributes the forwarding tables to beused by each and every router).
We studied two fundamental routing algorithms for computing least cost paths in a graph—link-state routing and distance-vector routing—in Section 5.2; these algorithms find application in both per-router control and in SDN control.
These algorithms are the basis for two widely-deployed Internet routing protocols, OSPF and BGP, that we covered in Sections 5.3 and 5.4.
We covered the SDN approach to the network-layer control plane in Section 5.5, investigating SDN network-control applications, the SDN controller, and the OpenFlow protocol for communicatingbetween the controller and SDN-controlled devices.
In Sections 5.6 and 5.7, we covered some of the nuts and bolts of managing an IP network: ICMP (the Internet Control Message Protocol) and SNMP (the Simple Network Management Protocol).
Having completed our study of the network layer, our journey now takes us one step further down the protocol stack, namely, to the link layer.
Like the network layer, the link layer is part of each and every network-connected device.
But we will see in the next chapter that the link layer has the much more localized task of moving packets between nodes on the same link or LAN.
Although this task mayappear on the surface to be rather simple compared with that of the network layer’s tasks, we will seethat the link layer involves a number of important and fascinating issues that can keep us busy for a longtime.
Homework Problems and Questions Chapter 5 Review Questions SECTION 5.1 SECTION 5.2 SECTIONS 5.3–5.4R1.
What is meant by a control plane that is based on per-router control?
In such cases, when we say the network control and data planes are implemented “monolithically,” what do we mean?
R2.
What is meant by a control plane that is based on logically centralized control?
In such cases, are the data plane and the control plane implemented within the same device or in separate devices?
Explain.
R3.
Compare and contrast the properties of a centralized and a distributed routing algorithm.
Give an example of a routing protocol that takes a centralized and a decentralized approach.
R4.
Compare and contrast link-state and distance-vector routing algorithms.
R5.
What is the “count to infinity” problem in distance vector routing?R6.
Is it necessary that every autonomous system use the same intra-AS routing algorithm?
Why or why not?
R7.
Why are different inter-AS and intra-AS protocols used in the Internet?
R8.
True or false: When an OSPF route sends its link state information, it is sent only to those nodes directly attached neighbors.
Explain.
R9.
What is meant by an area in an OSPF autonomous system?
Why was the concept of an area introduced?
R10.
Define and contrast the following terms: subnet, prefix, and BGP route.
R11.
How does BGP use the NEXT-HOP attribute?
How does it use the AS-PATH attribute?
R12.
Describe how a network administrator of an upper-tier ISP can implement policy when configuring BGP.
R13.
True or false: When a BGP router receives an advertised path from its neighbor, it must add its own identity to the received path and then send that new path on to all of its neighbors.
SECTION 5.5 SECTIONS 5.6–5.7 ProblemsExplain.
R14.
Describe the main role of the communication layer, the network-wide state- ­management layer, and the network-control application layer in an SDN controller.
R15.
Suppose you wanted to implement a new routing protocol in the SDN control plane.
At which layer would you implement that protocol?
Explain.
R16.
What types of messages flow across an SDN controller’s northbound and southbound APIs?
Who is the recipient of these messages sent from the controller across the southbound interface, and who sends messages to the controller across the northbound interface?
R17.
Describe the purpose of two types of OpenFlow messages (of your choosing) that are sent from a controlled device to the controller.
Describe the purpose of two types of Openflow messages (of your choosing) that are send from the controller to a controlled device.
R18.
What is the purpose of the service abstraction layer in the OpenDaylight SDN controller?
R19.
Names four different types of ICMP messages R20.
What two types of ICMP messages are received at the sending host executing theTraceroute program?R21.
Define the following terms in the context of SNMP: managing server, ­managed device, network management agent and MIB.
R22.
What are the purposes of the SNMP GetRequest and SetRequest  messages?
R23.
What is the purpose of the SNMP trap message?
P1.
Looking at Figure 5.3 , enumerate the paths from y to u that do not contain any loops.
P2.
Repeat Problem P1 for paths from x to z, z to u, and z to w.P3.
Consider the following network.
With the indicated link costs, use Dijkstra’s shortest-pathalgorithm to compute the shortest path from x to all network nodes.
Show how the algorithm works by computing a table similar to Table 5.1 .
Dijkstra’s algorithm: discussion and example
P4.
Consider the network shown in Problem P3.
Using Dijkstra’s algorithm, and showing your work using a table similar to Table 5.1 , do the following: a. Compute the shortest path from t to all network nodes.
b. Compute the shortest path from u to all network nodes.
c. Compute the shortest path from v to all network nodes.
d. Compute the shortest path from w to all network nodes.
e. Compute the shortest path from y to all network nodes.
f. Compute the shortest path from z to all network nodes.
P5.
Consider the network shown below, and assume that each node initially knows the costs to each of its neighbors.
Consider the distance-vector algorithm and show the distance table entries at node z. P6.
Consider a general topology (that is, not the specific network shown above) and a
synchronous version of the distance-vector algorithm.
Suppose that at each iteration, a node exchanges its distance vectors with its neighbors and receives their distance vectors.
Assuming that the algorithm begins with each node knowing only the costs to its immediate neighbors,what is the maximum number of iterations required before the distributed algorithm converges?Justify your answer.
P7.
Consider the network fragment shown below.
x has only two attached neighbors, w and y. w has a minimum-cost path to destination u (not shown) of 5, and y has a minimum-cost path to u of 6.
The complete paths from w and y to u (and between w and y) are not shown.
All link costs in the network have strictly positive integer values.
a. Give x’s distance vector for destinations w, y, and u. b. Give a link-cost change for either c(x, w) or c(x, y) such that x will inform its neighbors of a new minimum-cost path to u as a result of executing the distance-vector algorithm.
c. Give a link-cost change for either c(x, w) or c(x, y) such that x will not inform its neighbors of a new minimum-cost path to u as a result of executing the distance-vector algorithm.
P8.
Consider the three-node topology shown in Figure 5.6 .
Rather than having the link costs shown in Figure 5.6 , the link costs are   Compute the distance tables after the initialization step and after each iteration of a synchronous version of the distance-vector algorithm (as we did in our earlier discussion of Figure 5.6 ).
P9.
Consider the count-to-infinity problem in the distance vector routing.
Will the count-to-infinity problem occur if we decrease the cost of a link?
Why?
How about if we connect two nodes which do not have a link?
P10.
Argue that for the distance-vector algorithm in Figure 5.6 , each value in the distance vector D(x) is non-increasing and will eventually stabilize in a finite number of steps.
P11.
Consider Figure 5.7.
Suppose there is another router w, connected to router y and z. The costs of all links are given as follows:   Suppose that poisoned reverse is used in the distance-vector routing algorithm.
a. When the distance vector routing is stabilized, router w, y, and z inform their distances to x to each other.
What distance values do they tell each other?
b. Now suppose that the link cost between x and y increases to 60.
Will there be a count-to-infinity problem even if poisoned reverse is used?
Why or why not?
If there is a count-to- infinity problem, then how many iterations are needed for the distance-vector routing toc(x,y)=3, c(y,z)=6, c(z,x)=4.
c(x,y)=4, c(x,z)=50, c(y,w)=1, c(z,w)=1, c(y,z)=3.
reach a stable state again?
Justify your answer.
c. How do you modify c(y, z) such that there is no count-to-infinity problem at all if c(y,x) changes from 4 to 60?
P12.
Describe how loops in paths can be detected in BGP.
P13.
Will a BGP router always choose the loop-free route with the shortest ASpath length?
Justify your answer.
P14.
Consider the network shown below.
Suppose AS3 and AS2 are running OSPF for their intra-AS routing protocol.
Suppose AS1 and AS4 are running RIP for their intra-AS routing protocol.
Suppose eBGP and iBGP are used for the inter-AS routing protocol.
Initially suppose there is no physical link between AS2 and AS4.
a. Router 3c learns about prefix x from which routing protocol: OSPF, RIP, eBGP, or iBGP?
b. Router 3a learns about x from which routing protocol?
c. Router 1c learns about x from which routing protocol?
d. Router 1d learns about x from which routing protocol?
P15.
Referring to the previous problem, once router 1d learns about x it will put an entry (x, I) in its forwarding table.
a. Will I be equal to I  or I for this entry?
Explain why in one sentence.
b. Now suppose that there is a physical link between AS2 and AS4, shown by the dotted line.
Suppose router 1d learns that x is accessible via AS2 as well as via AS3.
Will I be set to I  or I?
Explain why in one sentence.
c. Now suppose there is another AS, called AS5, which lies on the path between AS2 andAS4 (not shown in diagram).
Suppose router 1d learns that x is accessible via AS2 AS5 AS4 as well as via AS3 AS4.
Will I be set to I  or I?
Explain why in one sentence.1 2 1 2 1 2
P16.
Consider the following network.
ISP B provides national backbone service to regional ISP A. ISP C provides national backbone service to regional ISP D. Each ISP consists of one AS.
B and C peer with each other in two places using BGP.
Consider traffic going from A to D. B wouldprefer to hand that traffic over to C on the West Coast (so that C would have to absorb the cost of carrying the traffic cross-country), while C would prefer to get the traffic via its East Coast peering point with B (so that B would have carried the traffic across the country).
What BGPmechanism might C use, so that B would hand over A-to-D traffic at its East Coast peeringpoint?
To answer this question, you will need to dig into the BGP ­specification.
P17.
In Figure 5.13 , consider the path information that reaches stub networks W, X, and Y. Based on the information available at W and X, what are their respective views of the network topology?
Justify your answer.
The topology view at Y is shown below.
P18.
Consider Figure 5.13 .
B would never forward traffic destined to Y via X based on BGP routing.
But there are some very popular applications for which data packets go to X first andthen flow to Y. Identify one such application, and describe how data packets follow a path notgiven by BGP routing.
Socket Programming Assignment At the end of Chapter 2, there are four socket programming assignments.
Below, you will find a fifth assignment which employs ICMP, a protocol discussed in this chapter.
Assignment 5: ICMP Ping Ping is a popular networking application used to test from a remote location whether a particular host is up and reachable.
It is also often used to measure latency between the client host and the target host.
Itworks by sending ICMP “echo request” packets (i.e., ping packets) to the target host and listening forICMP “echo response” replies (i.e., pong packets).
Ping measures the RRT, records packet loss, andcalculates a statistical summary of multiple ping-pong exchanges (the minimum, mean, max, and standard deviation of the round-trip times).
In this lab, you will write your own Ping application in Python.
Your application will use ICMP.
But in order to keep your program simple, you will not exactly follow the official specification in RFC 1739.
Notethat you will only need to write the client side of the program, as the functionality needed on the serverside is built into almost all operating systems.
You can find full details of this assignment, as well as important snippets of the Python code, at the Web site http:/ /www.pearsonhighered.com/ cs- resources .
Programming AssignmentP19.
In Figure 5.13 , suppose that there is another stub network V that is a customer of ISP A. Suppose that B and C have a peering relationship, and A is a customer of both B and C. Suppose that A would like to have the traffic destined to W to come from B only, and the trafficdestined to V from either B or C. How should A advertise its routes to B and C?
What AS routesdoes C receive?
P20.
Suppose ASs X and Z are not directly connected but instead are connected by AS Y. Further suppose that X has a peering agreement with Y, and that Y has a peering agreement with Z. Finally, suppose that Z wants to transit all of Y’s traffic but does not want to transit X’straffic.
Does BGP allow Z to ­implement this policy?
P21.
Consider the two ways in which communication occurs between a managing entity and a managed device: request-response mode and trapping.
What are the pros and cons of these two approaches, in terms of (1) overhead, (2) notification time when exceptional events occur, and(3) robustness with respect to lost messages between the managing entity and the device?
P22.
In Section 5.7 we saw that it was preferable to transport SNMP messages in unreliable UDP datagrams.
Why do you think the designers of SNMP chose UDP rather than TCP as the transport protocol of choice for SNMP?
In this programming assignment, you will be writing a “distributed” set of procedures that implements a distributed asynchronous distance-vector routing for the network shown below.
You are to write the following routines that will “execute” asynchronously within the emulated environment provided for this assignment.
For node 0, you will write the routines: rtinit0().
This routine will be called once at the beginning of the emulation.
rtinit0() has no arguments.
It should initialize your distance table in node 0 to reflect the direct costs of 1, 3, and 7 to nodes 1, 2,and 3, respectively.
In the figure above, all links are bidirectional and the costs in both directions areidentical.
After initializing the distance table and any other data structures needed by your node 0 routines, it should then send its directly connected neighbors (in this case, 1, 2, and 3) the cost of its minimum-cost paths to all other network nodes.
This minimum-cost information is sent to neighboring nodes in a routing update packet by calling the routine tolayer2(), as described in the full assignment.
The format of the routing update packet is also described in the full assignment.
rtupdate0(struct rtpkt *rcvdpkt).
This routine will be called when node 0 receives a routing packet thatwas sent to it by one of its directly connected neighbors.
The parameter *rcvdpkt is a pointer to the packet that was received.
rtupdate0()  is the “heart” of the distance-vector algorithm.
The values it receives in a routing update packet from some other node i contain i’s current shortest-path costs to all other network nodes.
rtupdate0()  uses these received values to update its own distance table (as specified by the distance-vector algorithm).
If its own minimum cost to another node changes as aresult of the update, node 0 informs its directly connected neighbors of this change in minimum costby sending them a routing packet.
Recall that in the distance-vector algorithm, only directlyconnected nodes will exchange routing packets.
Thus, nodes 1 and 2 will communicate with eachother, but nodes 1 and 3 will not communicate with each other.
Similar routines are defined for nodes 1, 2, and 3.
Thus, you will write eight procedures in all: rtinit0(), rtinit1(), rtinit2(), rtinit3(), rtupdate0(), rtupdate1(), rtupdate2(),  and rtupdate3().
These routines will together implement a distributed, asynchronous computation of the distance tables for the topology andcosts shown in the figure on the preceding page.
You can find the full details of the programming assignment, as well as C code that you will need to create the simulated hardware/software environment, at http://www.pearsonhighered.com/cs-resource .
A Java version of the assignment is also available.
Wireshark Lab In the Web site for this textbook, www.pearsonhighered.com/ cs-resources , you’ll find a Wireshark lab assignment that examines the use of the ICMP protocol in the ping and traceroute commands.
An Interview With… Jennifer Rexford Jennifer Rexford is a Professor in the Computer Science department at Princeton University.
Herresearch has the broad goal of making computer networks easier to design and manage, withparticular emphasis on routing protocols.
From 1996–2004, she was a member of the Network Management and Performance department at AT&T Labs–Research.
While at AT&T, she designed techniques and tools for network measurement, traffic engineering, and routerconfiguration that were deployed in AT&T’s backbone network.
Jennifer is co-author of the book“Web Protocols and Practice: Networking Protocols, Caching, and Traffic Measurement,”published by Addison-Wesley in May 2001.
She served as the chair of ACM SIGCOMM from2003 to 2007.
She received her BSE degree in electrical engineering from Princeton Universityin 1991, and her PhD degree in electrical engineering and computer science from the Universityof Michigan in 1996.
In 2004, Jennifer was the winner of ACM’s Grace Murray Hopper Award for outstanding young computer professional and appeared on the MIT TR-100 list of top innovators under the age of 35.
Please describe one or two of the most exciting projects you have worked on during your career.
What were the biggest challenges?
When I was a researcher at AT&T, a group of us designed a new way to manage routing in Internet Service Provider backbone networks.
Traditionally, network operators configure eachrouter individually, and these routers run distributed protocols to compute paths through thenetwork.
We believed that network management would be simpler and more flexible if network
operators could exercise direct control over how routers forward traffic based on a network-wide view of the topology and traffic.
The Routing Control Platform (RCP) we designed and built could compute the routes for all of AT&T’s backbone on a single commodity computer, and couldcontrol legacy routers without modification.
To me, this project was exciting because we had aprovocative idea, a working system, and ultimately a real deployment in an operational network.
Fast forward a few years, and software-defined networking (SDN) has become a mainstream technology, and standard protocols (like OpenFlow) have made it much easier to tell the underlying switches what to do.
How do you think software-defined networking should evolve in the future?
In a major break from the past, control-plane software can be created by many different programmers, not just at companies selling network equipment.
Yet, unlike the applications running on a server or a smart phone, controller apps must work together to handle the same traffic.
Network operators do not want to perform load balancing on some traffic and routing on other traffic; instead, they want to perform load balancing and routing, together, on the sametraffic.
Future SDN controller platforms should offer good programming abstractions for composing  independently written multiple controller applications together.
More broadly, good programming abstractions can make it easier to create controller applications, without having to worry about low-level details like flow table entries, traffic counters, bit patterns in packetheaders, and so on.
Also, while an SDN controller is logically centralized, the network stillconsists of a distributed collection of devices.
Future controllers should offer good abstractionsfor updating the flow tables across the network, so apps can reason about what happens topackets in flight while the devices are updated.
Programming abstractions for control-planesoftware is an exciting area for interdisciplinary research between computer networking,distributed systems, and programming languages, with a real chance for practical impact in the years ahead.
Where do you see the future of networking and the Internet?
Networking is an exciting field because the applications and the underlying technologies change all the time.
We are always reinventing ourselves!
Who would have predicted even ten yearsago the dominance of smart phones, allowing mobile users to access existing applications aswell as new location-based services?
The emergence of cloud computing is fundamentallychanging the relationship between users and the applications they run, and networked sensorsand actuators (the “Internet of Things”) are enabling a wealth of new applications (and securityvulnerabilities!).
The pace of innovation is truly inspiring.
The underlying network is a crucial component in all of these innovations.
Yet, the network is notoriously “in the way”—limiting performance, compromising reliability, constraining applications, and complicating the deployment and management of services.
We should strive tomake the network of the future as invisible as the air we breathe, so it never stands in the way of
new ideas and valuable services.
To do this, we need to raise the level of abstraction above individual network devices and protocols (and their attendant acronyms!),
so we can reason about the network and the user’s high-level goals as a whole.
What people inspired you professionally?
I’ve long been inspired by Sally Floyd at the International Computer Science Institute.
Her research is always purposeful, focusing on the important challenges facing the Internet.
She digsdeeply into hard questions until she understands the problem and the space of solutions completely, and she devotes serious energy into “making things happen,” such as pushing her ideas into protocol standards and network equipment.
Also, she gives back to the community,through professional service in numerous standards and research organizations and by creatingtools (such as the widely used ns-2 and ns-3 simulators) that enable other researchers tosucceed.
She retired in 2009 but her influence on the field will be felt for years to come.
What are your recommendations for students who want careers in computer science and networking?
Networking is an inherently interdisciplinary field.
Applying techniques from other disciplines breakthroughs in networking come from such diverse areas as queuing theory, game theory,control theory, distributed systems, network optimization, programming languages, machinelearning, algorithms, data structures, and so on.
I think that becoming conversant in a relatedfield, or collaborating closely with experts in those fields, is a wonderful way to put networking on a stronger foundation, so we can learn how to build networks that are worthy of society’s trust.
Beyond the theoretical disciplines, networking is exciting because we create real artifacts thatreal people use.
Mastering how to design and build systems—by gaining experience in operatingsystems, computer architecture, and so on—is another fantastic way to amplify your knowledgeof networking to help make the world a better place.
Chapter 6 The Link Layer and LANs In the previous two chapters we learned that the network layer provides a communication service between any two network hosts.
Between the two hosts, datagrams travel over a series of communication links, some wired and some wireless, starting at the source host, passing through a series of packet switches (switches and routers) and ending at the destination host.
As we continuedown the protocol stack, from the network layer to the link layer, we naturally wonder how packets are sent across the individual links  that make up the end-to-end communication path.
How are the network- layer datagrams encapsulated in the link-layer frames for transmission over a single link?
Are different link-layer protocols used in the different links along the communication path?
How are transmissionconflicts in broadcast links resolved?
Is there addressing at the link layer and, if so, how does the link- layer addressing operate with the network-layer addressing we learned about in Chapter 4?
And what exactly is the difference between a switch and a router?
We’ll answer these and other important questions in this chapter.
In discussing the link layer, we’ll see that there are two fundamentally ­different types of link-layer channels.
The first type are broadcast channels, which connect multiple hosts in wireless LANs, satellite networks, and hybrid fiber-coaxial cable (HFC) access networks.
Since many hosts are connected to thesame broadcast communication channel, a so-called medium access protocol is needed to coordinateframe transmission.
In some cases, a central controller may be used to coordinate transmissions; in other cases, the hosts themselves coordinate transmissions.
The second type of link-layer channel isthe point-to-point communication link, such as that often found between two routers connected by along-distance link, or between a user’s office computer and the nearby Ethernet switch to which it isconnected.
Coordinating access to a point-to-point link is simpler; the reference material on this book’s Web site has a detailed discussion of the Point-to-Point Protocol (PPP), which is used in settings ranging from dial-up service over a telephone line to high-speed point-to-point frame transport overfiber-optic links.
We’ll explore several important link-layer concepts and technologies in this ­chapter.
We’ll dive deeper into error detection and correction, a topic we touched on briefly in Chapter 3.
We’ll consider multiple access networks and switched LANs, including Ethernet—by far the most prevalent wired LAN technology.
We’ll also look at virtual LANs, and data center networks.
Although WiFi, and more generally wireless LANs, are link-layer topics, we’ll postpone our study of these important topics until
Chapter 7.
6.1 Introduction to the Link Layer Let’s begin with some important terminology.
We’ll find it convenient in this chapter to refer to any device that runs a link-layer (i.e., layer 2) protocol as a node.
Nodes include hosts, routers, switches, and WiFi access points (discussed in Chapter 7).
We will also refer to the communication channels that connect adjacent nodes along the communication path as links.
In order for a datagram to be transferred from source host to destination host, it must be moved over each of the individual links  in the end-to-end path.
As an example, in the company network shown at the bottom of Figure 6.1, consider sending a datagram from one of the wireless hosts to one of the servers.
This datagram will actually pass throughsix links: a WiFi link between sending host and WiFi access point, an Ethernet link between the accesspoint and a link-layer switch; a link between the link-layer switch and the router, a link between the tworouters; an Ethernet link between the router and a link-layer switch; and finally an Ethernet link betweenthe switch and the server.
Over a given link, a transmitting node encapsulates the datagram in a link- layer frame  and transmits the frame into the link.
In order to gain further insight into the link layer and how it relates to the ­network layer, let’s consider a transportation analogy.
Consider a travel agent who is planning a trip for a tourist traveling fromPrinceton, New Jersey, to Lausanne, Switzerland.
The travel agent decides that it is most convenient forthe tourist to take a limousine from Princeton to JFK airport, then a plane from JFK airport to Geneva’sairport, and finally a train from Geneva’s airport to Lausanne’s train station.
Once the travel agent makesthe three reservations, it is the responsibility of the Princeton limousine company to get the tourist fromPrinceton to JFK; it is the responsibility of the airline company to get the tourist from JFK to Geneva; andit is the responsibility
Figure 6.1 Six link-layer hops between wireless host and server of the Swiss train service to get the tourist from Geneva to Lausanne.
Each of the three segments of the trip is “direct” between two “adjacent” locations.
Note that the three transportation segments are managed by different companies and use entirely different transportation modes (limousine, plane, andtrain).
Although the transportation modes are different, they each provide the basic service of movingpassengers from one location to an adjacent location.
In this transportation analogy, the tourist is adatagram, each transportation segment is a link, the transportation mode is a link-layer protocol, and the
travel agent is a routing protocol.
6.1.1 The Services Provided by the Link Layer Although the basic service of any link layer is to move a datagram from one node to an adjacent node over a single communication link, the details of the provided service can vary from one link-layerprotocol to the next.
Possible services that can be offered by a link-layer protocol include: Framing.
Almost all link-layer protocols encapsulate each network-layer datagram within a link-layer frame before transmission over the link.
A frame consists of a data field, in which the network-layer datagram is inserted, and a number of header fields.
The structure of the frame is specified by thelink-layer protocol.
We’ll see several different frame formats when we examine specific link-layer protocols in the second half of this chapter.
Link access.
A medium access control (MAC) protocol specifies the rules by which a frame is transmitted onto the link.
For point-to-point links that have a single sender at one end of the link and a single receiver at the other end of the link, the MAC protocol is simple (or nonexistent)—the sendercan send a frame whenever the link is idle.
The more interesting case is when multiple nodes share a single broadcast link—the so-called multiple access problem.
Here, the MAC protocol serves to coordinate the frame transmissions of the many nodes.
Reliable delivery.
When a link-layer protocol provides reliable delivery service, it guarantees to move each network-layer datagram across the link without error.
Recall that certain transport-layer protocols (such as TCP) also provide a reliable delivery service.
Similar to a transport-layer reliabledelivery service, a link-layer reliable delivery service can be achieved with acknowledgments and retransmissions (see Section 3.4).
A link-layer reliable delivery service is often used for links that are prone to high error rates, such as a wireless link, with the goal of correcting an error locally—on the link where the error occurs—rather than forcing an end-to-end retransmission of the data by atransport- or application-layer protocol.
However, link-layer reliable delivery can be considered anunnecessary overhead for low bit-error links, including fiber, coax, and many twisted-pair copperlinks.
For this reason, many wired link-layer protocols do not provide a reliable delivery service.
Error detection and correction.
 The link-layer hardware in a receiving node can incorrectly decide that a bit in a frame is zero when it was transmitted as a one, and vice versa.
Such bit errors areintroduced by signal attenuation and electromagnetic noise.
Because there is no need to forward adatagram that has an error, many link-layer protocols provide a mechanism to detect such bit errors.
This is done by having the transmitting node include error-detection bits in the frame, and having the receiving node perform an error check.
Recall from Chapters 3 and 4 that the Internet’s transport layer and network layer also provide a limited form of error detection—the Internet checksum.
Error detection in the link layer is usually more sophisticated and is implemented in hardware.
Errorcorrection is similar to error detection, except that a receiver not only detects when bit errors haveoccurred in the frame but also determines exactly where in the frame the errors have occurred (and
then corrects these errors).
6.1.2 Where Is the Link Layer Implemented?
Before diving into our detailed study of the link layer, let’s conclude this introduction by considering the question of where the link layer is implemented.
We’ll focus here on an end system, since we learned in Chapter 4 that the link layer is implemented in a router’s line card.
Is a host’s link layer implemented in hardware or software?
Is it implemented on a separate card or chip, and how does it interface with the rest of a host’s hardware and operating system components?
Figure 6.2 shows a typical host architecture.
For the most part, the link layer is implemented in a network adapter, also sometimes known as a network interface card (NIC).
At the heart of the network adapter is the link-layer controller, usually a single, special-purpose chip that implements many of the link-layer services (framing, link access, error detection, and so on).
Thus, much of a link-layer controller’s functionality is implemented in hardware.
For example, Intel’s 710 adapter [Intel 2016] implements the Ethernet protocols we’ll study in Section 6.5; the Atheros AR5006 [Atheros 2016] controller implements the 802.11 WiFi protocols we’ll study in Chapter 7.
Until the late 1990s, most network adapters were physically separate cards (such as a PCMCIA card or a plug-in card fitting into a PC’s PCI card slot) but increasingly, network adapters are being integrated onto the host’s motherboard—a so-called LAN-on-motherboard configuration.
On the sending side, the controller takes a datagram that has been created and stored in host memory by the higher layers of the protocol stack, encapsulates the datagram in a link-layer frame (filling in theframe’s various fields), and then transmits the frame into the communication link, following the link-access protocol.
On the receiving side, a controller receives the entire frame, and extracts the network-layer datagram.
If the link layer performs error detection, then it is the sending controller that sets the error-detection bits in the frame header and it is the receiving controller that performs error detection.
Figure 6.2 shows a network adapter attaching to a host’s bus (e.g., a PCI or PCI-X bus), where it looks much like any other I/O device to the other host
Figure 6.2 Network adapter: Its relationship to other host components and to protocol stack functionality components.
Figure 6.2 also shows that while most of the link layer is implemented in hardware, part of the link layer is implemented in software that runs on the host’s CPU.
The software components of the link layer implement higher-level link-layer functionality such as assembling link-layer addressinginformation and activating the controller hardware.
On the receiving side, link-layer software responds tocontroller interrupts (e.g., due to the receipt of one or more frames), handling error conditions andpassing a datagram up to the network layer.
Thus, the link layer is a combination of hardware and software—the place in the protocol stack where software meets hardware. [
Intel 2016] provides a readable overview (as well as a detailed description) of the XL710 controller from a software- programming point of view.
6.2 Error-Detection and -Correction Techniques In the previous section, we noted that bit-level error detection and correction —detecting and correcting the corruption of bits in a link-layer frame sent from one node to another physically connected neighboring node—are two services often ­provided by the link layer.
We saw in Chapter 3 that error- detection and -correction services are also often offered at the transport layer as well.
In this section, we’ll examine a few of the simplest techniques that can be used to detect and, in some cases, correct such bit errors.
A full treatment of the theory and implementation of this topic is itself the topic of many textbooks (for example, [Schwartz 1980]  or [Bertsekas 1991] ), and our treatment here is necessarily brief.
Our goal here is to develop an intuitive feel for the capabilities that error-detection and -correction techniques provide and to see how a few simple techniques work and are used in practice in the linklayer.
Figure 6.3 illustrates the setting for our study.
At the sending node, data, D, to be protected against bit errors is augmented with error-detection and -correction bits ( EDC).
Typically, the data to be protected includes not only the datagram passed down from the network layer for transmission across the link, but also link-level addressing information, sequence numbers, and other fields in the link frame header.
Both D and EDC are sent to the receiving node in a link-level frame.
At the receiving node, a sequence of bits, D′  and EDC′  is received.
Note that D′  and EDC′  may differ from the original D and EDC as a result of in-transit bit flips.
The receiver’s challenge is to determine whether or not D′ is the same as the original D, given that it has only received D′ and EDC′ .
The exact wording of the receiver’s decision in Figure 6.3 (we ask whether an error is detected, not whether an error has occurred!)
is important.
Error-detection and -correction techniques allow the receiver to sometimes, but not always , detect that bit errors have occurred.
Even with the use of error-detection bits there still may be undetected bit errors; that is, the receiver may be unaware that the received information contains bit errors.
As a
Figure 6.3 Error-detection and -correction scenario consequence, the receiver might deliver a corrupted datagram to the network layer, or be unaware that the contents of a field in the frame’s header has been corrupted.
We thus want to choose an error- detection scheme that keeps the probability of such occurrences small.
Generally, more sophisticatederror-detection and-correction techniques (that is, those that have a smaller probability of allowingundetected bit errors) incur a larger overhead—more computation is needed to compute and transmit alarger number of error-detection and -correction bits.
Let’s now examine three techniques for detecting errors in the transmitted data—parity checks (to illustrate the basic ideas behind error detection and correction), checksumming methods (which aremore typically used in the transport layer), and cyclic redundancy checks (which are more typically used in the link layer in an adapter).
6.2.1 Parity Checks Perhaps the simplest form of error detection is the use of a single parity bit.
Suppose that the information to be sent, D in Figure 6.4, has d bits.
In an even parity scheme, the sender simply includes one additional bit and chooses its value such that the total number of 1s in the  bits (the original information plus a parity bit) is even.
For odd parity schemes, the parity bit value is chosen such that there is an odd number of 1s.
Figure 6.4 illustrates an even parity scheme, with the single parity bit being stored in a separate field.d+1
Receiver operation is also simple with a single parity bit.
The receiver need only count the number of 1s in the received  bits.
If an odd number of 1-valued bits are found with an even parity scheme, the receiver knows that at least one bit error has occurred.
More precisely, it knows that some odd number of bit errors have occurred.
But what happens if an even number of bit errors occur?
You should convince yourself that this would result in an undetected error.
If the probability of bit errors is small and errors can be assumed to occurindependently from one bit to the next, the probability of multiple bit errors in a packet would beextremely small.
In this case, a single parity bit might suffice.
However, measurements have shown that,rather than occurring independently, errors are often clustered together in “bursts.”
Under burst errorconditions, the probability of undetected errors in a frame protected by single-bit parity can approach 50 percent [Spragins 1991].
Clearly, a more robust error-detection scheme is needed (and, fortunately, is used in practice!).
But before examining error-detection schemes that are used in practice, let’s consider a simple Figure 6.4 One-bit even parity generalization of one-bit parity that will provide us with insight into error-correction techniques.
Figure 6.5 shows a two-dimensional generalization of the single-bit parity scheme.
Here, the d bits in D are divided into i rows and j columns.
A parity value is computed for each row and for each column.
The resulting  parity bits comprise the link-layer frame’s error-detection bits.
Suppose now that a single bit error occurs in the original d bits of information.
With this two- dimensional parity  scheme, the parity of both the column and the row containing the flipped bit will be in error.
The receiver can thus not only detect the fact that a single bit error has occurred, but can use the column and row indices of the column and row with parity errors to actually identify the bit that was corrupted and correct that error!
Figure 6.5 shows an example in which the 1-valued bit in position (2,2) is corrupted and switched to a 0—an error that is both detectable and correctable at the receiver.
Although our discussion has focused on the original d bits of information, a single error in the parity bits themselves is also detectable and correctable.
Two-dimensional parity can also detect (but not correct!)
any combination of two errors in a packet.
Other properties of the two-dimensional parity scheme areexplored in the problems at the end of the chapter.d+1 i+j+1
Figure 6.5 Two-dimensional even parity The ability of the receiver to both detect and correct errors is known as forward error correction (FEC).
These techniques are commonly used in audio storage and playback devices such as audio CDs.
In a network setting, FEC techniques can be used by themselves, or in conjunction with link-layer ARQ techniques similar to those we examined in Chapter 3.
FEC techniques are valuable because they can decrease the number of sender retransmissions required.
Perhaps more important, they allow for immediate correction of errors at the receiver.
This avoids having to wait for the round-trip propagationdelay needed for the sender to receive a NAK packet and for the retransmitted packet to propagate back to the receiver—a potentially important advantage for real-time network applications [Rubenstein 1998] or links (such as deep-space links) with long propagation delays.
Research examining the use of FEC inerror-control protocols includes [Biersack 1992 ; Nonnenmacher 1998 ; Byers 1998 ; Shacham 1990] .
6.2.2 Checksumming Methods In checksumming techniques, the d bits of data in Figure 6.4 are treated as a sequence of k-bit integers.
One simple checksumming method is to simply sum these k-bit integers and use the resulting sum as the error-detection bits.
The Internet checksum is based on this approach—bytes of data are
treated as 16-bit integers and summed.
The 1s complement of this sum then forms the Internet checksum that is carried in the segment header.
As discussed in Section 3.3, the receiver checks the checksum by taking the 1s complement of the sum of the received data (including the checksum) and checking whether the result is all 1 bits.
If any of the bits are 0, an error is indicated.
RFC 1071discusses the Internet checksum algorithm and its implementation in detail.
In the TCP and UDP protocols, the Internet checksum is computed over all fields (header and data fields included).
In IP the checksum is computed over the IP header (since the UDP or TCP segment has its own checksum).
In other protocols, for example, XTP [Strayer 1992] , one checksum is computed over the header and another checksum is computed over the entire packet.
Checksumming methods require relatively little packet overhead.
For example, the checksums in TCP and UDP use only 16 bits.
However, they provide relatively weak protection against errors as comparedwith cyclic redundancy check, which is discussed below and which is often used in the link layer.
Anatural question at this point is, Why is checksumming used at the transport layer and cyclic redundancy check used at the link layer?
Recall that the transport layer is typically implemented in software in a host as part of the host’s operating system.
Because transport-layer error detection is implemented insoftware, it is important to have a simple and fast error-detection scheme such as checksumming.
Onthe other hand, error detection at the link layer is implemented in dedicated hardware in adapters, which can rapidly perform the more complex CRC operations.
Feldmeier [Feldmeier 1995]  presents fast software implementation techniques for not only weighted checksum codes, but CRC (see below) and other codes as well.
6.2.3 Cyclic Redundancy Check (CRC) An error-detection technique used widely in today’s computer networks is based on cyclic redundancy check (CRC) codes .
CRC codes are also known as polynomial codes , since it is possible to view the bit string to be sent as a polynomial whose coefficients are the 0 and 1 values in the bit string, withoperations on the bit string interpreted as polynomial arithmetic.
CRC codes operate as follows.
Consider the d-bit piece of data, D, that the sending node wants to send to the receiving node.
The sender and receiver must first agree on an  bit pattern, known as a generator , which we will denote as G. We will require that the most significant (leftmost) bit of G be a 1.
The key idea behind CRC codes is shown in Figure 6.6.
For a given piece of data, D, the sender will choose r additional bits, R, and append them to D such that the resulting  bit pattern (interpreted as a binary number) is exactly divisible by G (i.e., has no remainder) using modulo-2 arithmetic.
The process of error checking with CRCs is thus simple: The receiver divides the  received bits by G. If the remainder is nonzero, the receiver knows that an error has occurred; otherwise the data is accepted as being correct.r+1 d+r d+r
All CRC calculations are done in modulo-2 arithmetic without carries in addition or borrows in subtraction.
This means that addition and subtraction are identical, and both are equivalent to the bitwise exclusive-or (XOR) of the operands.
Thus, for example, 1011 XOR 0101 = 11101001 XOR 1101 = 0100 Also, we similarly have 1011 - 0101 = 11101001 - 1101 = 0100 Multiplication and division are the same as in base-2 arithmetic, except that any required addition orsubtraction is done without carries or borrows.
As in regular Figure 6.6 CRC binary arithmetic, multiplication by 2  left shifts a bit pattern by k places.
Thus, given D and R, the quantity  yields the  bit pattern shown in Figure 6.6.
We’ll use this algebraic characterization of the  bit pattern from Figure 6.6 in our discussion below.
Let us now turn to the crucial question of how the sender computes R. Recall that we want to find R such that there is an n such that That is, we want to choose R such that G divides into  without remainder.
If we XOR (that is, add modulo-2, without carry) R to both sides of the above equation, we getk D⋅2rXOR R d+r d+r D⋅2rXOR R=nG D⋅2rXOR R
This equation tells us that if we divide  by G, the value of the remainder is precisely R. In other words, we can calculate R as Figure 6.7 illustrates this calculation for the case of   and .
The 9 bits transmitted in this case are 101  110  011.
You should check these calculations for yourself and also check that indeed .
Figure 6.7 A sample CRC calculation International standards have been defined for 8-, 12-, 16-, and 32-bit generators, G. The CRC-32 32-bit standard, which has been adopted in a number of link-level IEEE protocols, uses a generator of Each of the CRC standards can detect burst errors of fewer than  bits. (
This means that all consecutive bit errors of r bits or fewer will be detected.)
Furthermore, under appropriate assumptions, a burst of length greater than  bits is detected with probability .
Also, each of the CRC standards can detect any odd number of bit errors.
See [Williams 1993] for a discussion of implementing CRC checks.
The theory behind CRC codes and even more powerful codes is beyond the scope of this text.
The text [Schwartz 1980]  provides an excellent introduction to this topic.
D⋅2r=nG XOR R D⋅2r R=remainder D⋅2rG D=101110 , d=6, G=1001 , r=3 D⋅2r=101011 ⋅G XOR R GCRC-32=100000100110000010001110110110111 r+1 r+1 1−0.5r
6.3 Multiple Access Links and Protocols In the introduction to this chapter, we noted that there are two types of network links: point-to-point links and broadcast links.
A point-to-point link  consists of a single sender at one end of the link and a single receiver at the other end of the link.
Many link-layer protocols have been designed for point-to-pointlinks; the point-to-point protocol (PPP) and high-level data link control (HDLC) are two such protocols.
The second type of link, a broadcast link, can have multiple sending and receiving nodes all connected to the same, single, shared broadcast channel.
The term broadcast  is used here because when any one node transmits a frame, the channel broadcasts the frame and each of the other nodes receives a copy.
Ethernet and wireless LANs are examples of broadcast link-layer technologies.
In this section we’ll takea step back from specific link-layer protocols and first examine a problem of central importance to thelink layer: how to coordinate the access of multiple sending and receiving nodes to a shared broadcastchannel—the multiple access problem.
Broadcast channels are often used in LANs, networks that are geographically concentrated in a single building (or on a corporate or university campus).
Thus, we’lllook at how multiple access channels are used in LANs at the end of this section.
We are all familiar with the notion of broadcasting—television has been using it since its invention.
But traditional television is a one-way broadcast (that is, one fixed node transmitting to many receivingnodes), while nodes on a computer network broadcast channel can both send and receive.
Perhaps amore apt human analogy for a broadcast channel is a cocktail party, where many people gather in alarge room (the air providing the broadcast medium) to talk and listen.
A second good analogy issomething many readers will be familiar with—a classroom—where teacher(s) and student(s) similarlyshare the same, single, broadcast medium.
A central problem in both scenarios is that of determining who gets to talk (that is, transmit into the channel) and when.
As humans, we’ve evolved an elaborate set of protocols for sharing the broadcast channel: “Give everyone a chance to speak.” “
Don’t speak until you are spoken to.
”“Don’t monopolize the conversation.
”“Raise your hand if you have a question.” “
Don’t interrupt when someone is speaking.” “
Don’t fall asleep when someone is talking.”
Computer networks similarly have protocols—so-called multiple access ­protocols—by which nodes
regulate their transmission into the shared broadcast channel.
As shown in Figure 6.8, multiple access protocols are needed in a wide variety of network settings, including both wired and wireless access networks, and satellite networks.
Although technically each node accesses the broadcast channel through its adapter, in this section we will refer to the node  as the sending and Figure 6.8 Various multiple access channels receiving device.
In practice, hundreds or even thousands of nodes can directly communicate over a broadcast channel.
Because all nodes are capable of transmitting frames, more than two nodes can transmit frames at the same time.
When this happens, all of the nodes receive multiple frames at the same time; that is, thetransmitted frames collide  at all of the receivers.
Typically, when there is a collision, none of the receiving nodes can make any sense of any of the frames that were transmitted; in a sense, the signalsof the colliding frames become inextricably tangled together.
Thus, all the frames involved in thecollision are lost, and the broadcast channel is wasted during the collision interval.
Clearly, if manynodes want to transmit frames frequently, many transmissions will result in collisions, and much of the bandwidth of the broadcast channel will be wasted.
In order to ensure that the broadcast channel performs useful work when multiple nodes are active, it is
necessary to somehow coordinate the transmissions of the active nodes.
This coordination job is the responsibility of the multiple access protocol.
Over the past 40 years, thousands of papers and hundreds of PhD dissertations have been written on multiple access protocols; a comprehensive survey of the first 20 years of this body of work is [Rom 1990] .
Furthermore, active research in multiple access protocols continues due to the continued emergence of new types of links, particularly new wireless links.
Over the years, dozens of multiple access protocols have been implemented in a variety of link-layer technologies.
Nevertheless, we can classify just about any multiple access protocol as belonging to oneof three categories: channel partitioning protocols , random access protocols, and taking-turns protocols.
We’ll cover these categories of multiple access protocols in the following three subsections.
Let’s conclude this overview by noting that, ideally, a multiple access protocol for a broadcast channel of rate R bits per second should have the following desirable characteristics: 1.
When only one node has data to send, that node has a throughput of R bps.
2.
When M nodes have data to send, each of these nodes has a throughput of R/M bps.
This need not necessarily imply that each of the M nodes always has an instantaneous rate of R/M, but rather that each node should have an average transmission rate of R/M over some suitably defined interval of time.
3.
The protocol is decentralized; that is, there is no master node that represents a single point of failure for the network.
4.
The protocol is simple, so that it is inexpensive to implement.
6.3.1 Channel Partitioning Protocols Recall from our early discussion back in Section 1.3 that time-division ­multiplexing (TDM) and frequency-division multiplexing (FDM) are two techniques that can
Figure 6.9 A four-node TDM and FDM example be used to partition a broadcast channel’s bandwidth among all nodes sharing that channel.
As an example, suppose the channel supports N nodes and that the transmission rate of the channel is R bps.
TDM divides time into time frames  and further divides each time frame into N time slots . (
The TDM time frame should not be confused with the link-layer unit of data exchanged between sending and receiving adapters, which is also called a frame.
In order to reduce confusion, in this subsection we’llrefer to the link-layer unit of data exchanged as a packet.)
Each time slot is then assigned to one of the N nodes.
Whenever a node has a packet to send, it transmits the packet’s bits during its assigned time slot in the revolving TDM frame.
Typically, slot sizes are chosen so that a single packet can be transmitted during a slot time.
Figure 6.9 shows a simple four-node TDM example.
Returning to our cocktail party analogy, a TDM-regulated cocktail party would allow one partygoer to speak for a fixed period of time, then allow another partygoer to speak for the same amount of time, and so on.
Once everyone had had a chance to talk, the ­pattern would repeat.
TDM is appealing because it eliminates collisions and is perfectly fair: Each node gets a dedicated transmission rate of R/N bps during each frame time.
However, it has two major drawbacks.
First, a node is limited to an average rate of R/N bps even when it is the only node with packets to send.
A second drawback is that a node must always wait for its turn in the transmission sequence—again, evenwhen it is the only node with a frame to send.
Imagine the partygoer who is the only one with anything to say (and imagine that this is the even rarer circumstance where everyone wants to hear what that one person has to say).
Clearly, TDM would be a poor choice for a multiple access protocol for this particular party.
While TDM shares the broadcast channel in time, FDM divides the R bps channel into different frequencies (each with a bandwidth of R/N) and assigns each frequency to one of the N nodes.
FDM thus creates N smaller channels of R/N bps out of the single, larger R bps channel.
FDM shares both the advantages and drawbacks of TDM.
It avoids collisions and divides the bandwidth fairly among the N nodes.
However, FDM also shares a principal disadvantage with TDM—a node is limited to a bandwidth of R/N, even when it is the only node with packets to send.
A third channel partitioning protocol is code division multiple access (CDMA).
While TDM and FDM assign time slots and frequencies, respectively, to the nodes, CDMA assigns a different code to each node.
Each node then uses its unique code to encode the data bits it sends.
If the codes are chosen carefully, CDMA networks have the wonderful property that different nodes can transmit simultaneously and yet have their respective receivers correctly receive a sender’s encoded data bits (assuming the receiver knows the sender’s code) in spite of interfering transmissions by other nodes.
CDMA has beenused in military systems for some time (due to its anti-jamming properties) and now has widespread civilian use, particularly in cellular telephony.
Because CDMA’s use is so tightly tied to wireless channels, we’ll save our discussion of the technical details of CDMA until Chapter 7.
For now, it will suffice to know that CDMA codes, like time slots in TDM and frequencies in FDM, can be allocated to the multiple access channel users.
6.3.2 Random Access Protocols The second broad class of multiple access protocols are random access protocols.
In a random access protocol, a transmitting node always transmits at the full rate of the channel, namely, R bps.
When there is a collision, each node involved in the collision repeatedly retransmits its frame (that is, packet) until its frame gets through without a collision.
But when a node experiences a collision, it doesn’t necessarily retransmit the frame right away.
Instead it waits a random delay before retransmitting the frame .
Each node involved in a collision chooses independent random delays.
Because the random delays are independently chosen, it is possible that one of the nodes will pick a delay that is sufficiently less thanthe delays of the other colliding nodes and will therefore be able to sneak its frame into the channelwithout a collision.
There are dozens if not hundreds of random access protocols described in the literature [Rom 1990 ; Bertsekas 1991] .
In this section we’ll describe a few of the most commonly used random access protocols—the ALOHA protocols [Abramson 1970 ; Abramson 1985 ; Abramson 2009]  and the carrier sense multiple access (CSMA) protocols [Kleinrock 1975b].
Ethernet [Metcalfe 1976]  is a popular and widely deployed CSMA protocol.
Slotted ALOHA
Let’s begin our study of random access protocols with one of the simplest random access protocols, the slotted ALOHA protocol.
In our description of slotted ALOHA, we assume the following: All frames consist of exactly L bits.
Time is divided into slots of size L/R seconds (that is, a slot equals the time to transmit one frame).
Nodes start to transmit frames only at the beginnings of slots.
The nodes are synchronized so that each node knows when the slots begin.
If two or more frames collide in a slot, then all the nodes detect the collision event before the slot ends.
Let p be a probability, that is, a number between 0 and 1.
The operation of slotted ALOHA in each node is simple: When the node has a fresh frame to send, it waits until the beginning of the next slot and transmits the entire frame in the slot.
If there isn’t a collision, the node has successfully transmitted its frame and thus need not considerretransmitting the frame. (
The node can prepare a new frame for transmission, if it has one.)
If there is a collision, the node detects the collision before the end of the slot.
The node retransmits its frame in each subsequent slot with probability p until the frame is transmitted without a collision.
By retransmitting with probability p, we mean that the node effectively tosses a biased coin; the event heads corresponds to “retransmit,” which occurs with probability p. The event tails corresponds to “skip the slot and toss the coin again in the next slot”; this occurs with probability .
All nodes involved in the collision toss their coins independently.
Slotted ALOHA would appear to have many advantages.
Unlike channel partitioning, slotted ALOHA allows a node to transmit continuously at the full rate, R, when that node is the only active node. (
A node is said to be active if it has frames to send.)
Slotted ALOHA is also highly decentralized, because each node detects collisions and independently decides when to retransmit. (
Slotted ALOHA does, however,require the slots to be synchronized in the nodes; shortly we’ll discuss an unslotted version of the ALOHA protocol, as well as CSMA protocols, none of which require such synchronization.)
Slotted ALOHA is also an extremely simple protocol.
Slotted ALOHA works well when there is only one active node, but how ­efficient is it when there are multiple active nodes?
There are two possible efficiency(1−p)
Figure 6.10 Nodes 1, 2, and 3 collide in the first slot.
Node 2 finally succeeds in the fourth slot, node 1 in the eighth slot, and node 3 in the ninth slot concerns here.
First, as shown in Figure 6.10, when there are multiple active nodes, a certain fraction of the slots will have collisions and will therefore be “wasted.”
The second concern is that another fraction of the slots will be empty because all active nodes refrain from transmitting as a result of the probabilistic transmission policy.
The only “unwasted” slots will be those in which exactly one node transmits.
A slot in which exactly one node transmits is said to be a successful slot.
The efficiency of a slotted multiple access protocol is defined to be the long-run fraction of successful slots in the casewhen there are a large number of active nodes, each always having a large number of frames to send.
Note that if no form of access control were used, and each node were to immediately retransmit aftereach collision, the efficiency would be zero.
Slotted ALOHA clearly increases the efficiency beyond zero, but by how much?
We now proceed to outline the derivation of the maximum efficiency of slotted ALOHA.
To keep this derivation simple, let’s modify the protocol a little and assume that each node attempts to transmit a frame in each slot with probability p. (That is, we assume that each node always has a frame to send and that the node transmits with probability p for a fresh frame as well as for a frame that has already suffered a collision.)
Suppose there are N nodes.
Then the probability that a given slot is a successful slot is the probability that one of the nodes transmits and that the remaining  nodes do not transmit.
The probability that a given node transmits is p; the probability that the remaining nodes do not transmit is .
Therefore the probability a given node has a success is .
Because there are N nodes, the probability that any one of the N nodes has a success is .
Thus, when there are N active nodes, the efficiency of slotted ALOHA is .
To obtain the maximum efficiency for N active nodes, we have to find the p* that maximizes this expression. (
See theN−1 (1−p)N−1 p(1−p)N−1 Np(1−p)N−1 Np(1−p)N−1
homework problems for a general outline of this derivation.)
And to obtain the maximum efficiency for a large number of active nodes, we take the limit of  as N approaches infinity. (
Again, see the homework problems.)
After performing these calculations, we’ll find that the maximum efficiency of the protocol is given by .
That is, when a large number of nodes have many frames to transmit, then (at best) only 37 percent of the slots do useful work.
Thus the effective transmission rate of the channel is not R bps but only 0.37 R bps!
A similar analysis also shows that 37 percent of the slots go empty and 26 percent of slots have collisions.
Imagine the poor network administrator who has purchased a 100-Mbps slotted ALOHA system, expecting to be able to use the network to transmit dataamong a large number of users at an aggregate rate of, say, 80 Mbps!
Although the channel is capableof transmitting a given frame at the full channel rate of 100 Mbps, in the long run, the successfulthroughput of this channel will be less than 37 Mbps.
ALOHA The slotted ALOHA protocol required that all nodes synchronize their transmissions to start at the beginning of a slot.
The first ALOHA protocol [Abramson 1970]  was actually an unslotted, fully decentralized protocol.
In pure ALOHA, when a frame first arrives (that is, a network-layer datagram is passed down from the network layer at the sending node), the node immediately transmits the frame inits entirety into the broadcast channel.
If a transmitted frame experiences a collision with one or moreother transmissions, the node will then immediately (after completely transmitting its collided frame) retransmit the frame with probability p. Otherwise, the node waits for a frame transmission time.
After this wait, it then transmits the frame with probability p, or waits (remaining idle) for another frame time with probability 1 – p. To determine the maximum efficiency of pure ALOHA, we focus on an individual node.
We’ll make the same assumptions as in our slotted ALOHA analysis and take the frame transmission time to be the unit of time.
At any given time, the probability that a node is transmitting a frame is p. Suppose this frame begins transmission at time t. As shown in Figure 6.11, in order for this frame to be successfully transmitted, no other nodes can begin their transmission in the interval of time .
Such a transmission would overlap with the beginning of the transmission of node i’s frame.
The probability that all other nodes do not begin a transmission in this interval is .
Similarly, no other node can begin a transmission while node i is transmitting, as such a transmission would overlap with the latter part of node i’s transmission.
The probability that all other nodes do not begin a transmission in this interval is also .
Thus, the probability that a given node has a successful transmission is .
By taking limits as in the slotted ALOHA case, we find that the maximum efficiency of the pure ALOHA protocol is only 1/(2 e)—exactly half that of slotted ALOHA.
This then is the price to be paid for a fully decentralized ALOHA protocol.
Np*(1−p*)N−1 1/e=0.37 0 [ t0−1,t0] (1−p)N−1 (1−p)N−1 p(1−p)2(N−1)
Figure 6.11 Interfering transmissions in pure ALOHA Carrier Sense Multiple Access (CSMA) In both slotted and pure ALOHA, a node’s decision to transmit is made independently of the activity of the other nodes attached to the broadcast channel.
In particular, a node neither pays attention towhether another node happens to be transmitting when it begins to transmit, nor stops transmitting ifanother node begins to interfere with its transmission.
In our cocktail party analogy, ALOHA protocolsare quite like a boorish partygoer who continues to chatter away regardless of whether other people aretalking.
As humans, we have human protocols that allow us not only to behave with more civility, butalso to decrease the amount of time spent “colliding” with each other in conversation and, consequently,to increase the amount of data we exchange in our conversations.
Specifically, there are two important rules for polite human conversation: Listen before speaking.
 If someone else is speaking, wait until they are finished.
In the networking world, this is called carrier sensing—a node listens to the channel before transmitting.
If a frame from another node is currently being transmitted into the channel, a node then waits until it detects no transmissions for a short amount of time and then begins transmission.
If someone else begins talking at the same time, stop talking.
 In the networking world, this is called collision detection —a transmitting node listens to the channel while it is transmitting.
If it detects that another node is transmitting an interfering frame, it stops transmitting and waits arandom amount of time before repeating the sense-and-transmit-when-idle cycle.
These two rules are embodied in the family of carrier sense multiple access (CSMA)  and CSMA with collision detection (CSMA/CD)  protocols [Kleinrock 1975b; Metcalfe 1976 ; Lam 1980 ; Rom 1990] .
Many variations on CSMA and CASE HISTORY
NORM ABRAMSON AND ALOHANET Norm Abramson, a PhD engineer, had a passion for surfing and an interest in packet switching.
This combination of interests brought him to the University of Hawaii in 1969.
Hawaii consists ofmany mountainous islands, making it difficult to install and operate land-based networks.
Whennot surfing, Abramson thought about how to design a network that does packet switching overradio.
The network he designed had one central host and several secondary nodes scattered over the Hawaiian Islands.
The network had two channels, each using a different frequency band.
The downlink channel broadcasted packets from the central host to the secondary hosts;and the upstream channel sent packets from the secondary hosts to the central host.
In additionto sending informational packets, the central host also sent on the downstream channel anacknowledgment for each packet successfully received from the secondary hosts.
Because the secondary hosts transmitted packets in a decentralized fashion, collisions on the upstream channel inevitably occurred.
This observation led Abramson to devise the pureALOHA protocol, as described in this chapter.
In 1970, with continued funding from ARPA, Abramson connected his ALOHAnet to the ARPAnet.
Abramson’s work is important not only because it was the first example of a radio packet network, but also because it inspired BobMetcalfe.
A few years later, Metcalfe modified the ALOHA protocol to create the CSMA/CDprotocol and the Ethernet LAN.
CSMA/CD have been proposed.
Here, we’ll consider a few of the most important, and fundamental,characteristics of CSMA and CSMA/CD.
The first question that you might ask about CSMA is why, if all nodes perform carrier sensing, do collisions occur in the first place?
After all, a node will refrain from transmitting whenever it senses thatanother node is transmitting.
The answer to the question can best be illustrated using space-time diagrams [Molle 1987].
­Figure 6.12 shows a space-time diagram of four nodes (A, B, C, D) attached to a linear broadcast bus.
The horizontal axis shows the position of each node in space; the vertical axis represents time.
At time t , node B senses the channel is idle, as no other nodes are currently transmitting.
Node B thus begins transmitting, with its bits propagating in both directions along the broadcast medium.
The downward propagation of B’s bits in Figure 6.12 with increasing time indicates that a nonzero amount of time is needed for B’s bits actually to propagate (albeit at near the speed of light) along the broadcast medium.
At time , node D has a frame to send.
Although node B is currently transmitting at time t, the bits being transmitted by B have yet to reach D, and thus D senses 0 t1(t1>t0) 1
Figure 6.12 Space-time diagram of two CSMA nodes with colliding transmissions the channel idle at t. In accordance with the CSMA protocol, D thus begins transmitting its frame.
A short time later, B’s transmission begins to interfere with D’s transmission at D. From Figure 6.12, it is evident that the end-to-end channel propagation delay  of a broadcast channel—the time it takes for a signal to propagate from one of the nodes to another—will play a crucial role in determining its performance.
The longer this propagation delay, the larger the chance that a carrier-sensing node is notyet able to sense a transmission that has already begun at another node in the network.
Carrier Sense Multiple Access with Collision Dection (CSMA/CD) In Figure 6.12, nodes do not perform collision detection; both B and D continue to transmit their frames in their entirety even though a collision has occurred.
When a node performs collision detection, it ceases transmission as soon as it detects a collision.
Figure 6.13 shows the same scenario as in Figure 6.12, except that the two1
Figure 6.13 CSMA with collision detection nodes each abort their transmission a short time after detecting a collision.
Clearly, adding collision detection to a multiple access protocol will help protocol performance by not transmitting a useless, damaged (by interference with a frame from another node) frame in its entirety.
Before analyzing the CSMA/CD protocol, let us now summarize its operation from the perspective of an adapter (in a node) attached to a broadcast channel: 1.
The adapter obtains a datagram from the network layer, prepares a link-layer frame, and puts the frame adapter buffer.
2.
If the adapter senses that the channel is idle (that is, there is no signal energy entering theadapter from the channel), it starts to transmit the frame.
If, on the other hand, the adapter senses that the channel is busy, it waits until it senses no signal energy and then starts totransmit the frame.
3.
While transmitting, the adapter monitors for the presence of signal energy coming from other adapters using the broadcast channel.
4.
If the adapter transmits the entire frame without detecting signal energy from other adapters, the
adapter is finished with the frame.
If, on the other hand, the adapter detects signal energy from other adapters while transmitting, it aborts the transmission (that is, it stops transmitting its frame).
5.
After aborting, the adapter waits a random amount of time and then returns to step 2.
The need to wait a random (rather than fixed) amount of time is hopefully clear—if two nodestransmitted frames at the same time and then both waited the same fixed amount of time, they’d continue colliding forever.
But what is a good interval of time from which to choose the random backoff time?
If the interval is large and the number of colliding nodes is small, nodes are likely to wait a largeamount of time (with the channel remaining idle) before repeating the sense-and-transmit-when-idlestep.
On the other hand, if the interval is small and the number of colliding nodes is large, it’s likely thatthe chosen random values will be nearly the same, and transmitting nodes will again collide.
What we’dlike is an interval that is short when the number of colliding nodes is small, and long when the number ofcolliding nodes is large.
The binary exponential backoff  algorithm, used in Ethernet as well as in DOCSIS cable network multiple access protocols [DOCSIS 2011], elegantly solves this problem.
Specifically, when transmitting a frame that has already experienced n collisions, a node chooses the value of K at random from  .
Thus, the more collisions experienced by a frame, the larger the interval from which K is chosen.
For Ethernet, the actual amount of time a node waits is  bit times (i.e., K times the amount of time needed to send 512 bits into the Ethernet) and the maximum value that n can take is capped at 10.Let’s look at an example.
Suppose that a node attempts to transmit a frame for the first time and while transmitting it detects a collision.
The node then chooses  with probability 0.5 or chooses  with probability 0.5.
If the node chooses , then it immediately begins sensing the channel.
If the node chooses , it waits 512 bit times (e.g., 5.12 microseconds for a 100 Mbps Ethernet) before beginning the sense-and-transmit-when-idle cycle.
After a second collision, K is chosen with equal probability from {0,1,2,3}.
After three collisions, K is chosen with equal probability from {0,1,2,3,4,5,6,7}.
After 10 or more collisions, K is chosen with equal probability from {0,1,2,…, 1023}.
Thus, the size of the sets from which K is chosen grows exponentially with the number of collisions; for this reason this algorithm is referred to as binary exponential backoff.
We also note here that each time a node prepares a new frame for transmission, it runs the CSMA/CD algorithm, not taking into account any collisions that may have occurred in the recent past.
So it ispossible that a node with a new frame will immediately be able to sneak in a successful transmissionwhile several other nodes are in the exponential backoff state.
CSMA/CD Efficiency{ 0,1,2,…2n−1} K⋅512 K=0 K=1 K=0 K=1
When only one node has a frame to send, the node can transmit at the full channel rate (e.g., for Ethernet typical rates are 10 Mbps, 100 Mbps, or 1 Gbps).
However, if many nodes have frames to transmit, the effective transmission rate of the channel can be much less.
We define the efficiency of CSMA/CD to be the long-run fraction of time during which frames are being transmitted on the channelwithout collisions when there is a large number of active nodes, with each node having a large number of frames to send.
In order to present a closed-form approximation of the efficiency of Ethernet, let d denote the maximum time it takes signal energy to propagate between any two adapters.
Let d be the time to transmit a maximum-size frame (approximately 1.2 msecs for a 10 Mbps Ethernet).
A derivationof the efficiency of CSMA/CD is beyond the scope of this book (see [Lam 1980]  and [Bertsekas 1991] ).
Here we simply state the following approximation: We see from this formula that as d  approaches 0, the efficiency approaches 1.
This matches our intuition that if the propagation delay is zero, colliding nodes will abort immediately without wasting the channel.
Also, as d becomes very large, efficiency approaches 1.
This is also intuitive because when a frame grabs the channel, it will hold on to the channel for a very long time; thus, the channel will be doing productive work most of the time.
6.3.3 Taking-Turns Protocols Recall that two desirable properties of a multiple access protocol are (1) when only one node is active, the active node has a throughput of R bps, and (2) when M nodes are active, then each active node has a throughput of nearly R/M bps.
The ALOHA and CSMA protocols have this first property but not the second.
This has motivated researchers to create another class of protocols—the taking-turns protocols.
As with random access protocols, there are dozens of taking-turns protocols, and each one of these protocols has many variations.
We’ll discuss two of the more important protocols here.
The first one is the polling protocol .
The polling protocol requires one of the nodes to be designated as a master node.
The master node polls  each of the nodes in a round-robin fashion.
In particular, the master node first sends a message to node 1, saying that it (node 1) can transmit up to some maximumnumber of frames.
After node 1 transmits some frames, the master node tells node 2 it (node 2) cantransmit up to the maximum number of frames. (
The master node can determine when a node hasfinished sending its frames by observing the lack of a signal on the channel.)
The procedure continuesin this manner, with the master node polling each of the nodes in a cyclic manner.
The polling protocol eliminates the collisions and empty slots that plague random access protocols.
This allows polling to achieve a much higher efficiency.
But it also has a few drawbacks.
The first drawback is that the protocol introduces a polling delay—the amount of time required to notify a node that it canprop trans Efficiency=11+5dprop/dtrans prop trans
transmit.
If, for example, only one node is active, then the node will transmit at a rate less than R bps, as the master node must poll each of the inactive nodes in turn each time the active node has sent its maximum number of frames.
The second drawback, which is potentially more serious, is that if themaster node fails, the entire channel becomes inoperative.
The 802.15 protocol and the Bluetooth protocol we will study in Section 6.3 are examples of polling protocols.
The second taking-turns protocol is the token-passing protocol .
In this protocol there is no master node.
A small, special-purpose frame known as a token is exchanged among the nodes in some fixed order.
For example, node 1 might always send the token to node 2, node 2 might always send the tokento node 3, and node N might always send the token to node 1.
When a node receives a token, it holds onto the token only if it has some frames to transmit; otherwise, it immediately forwards the token to the next node.
If a node does have frames to transmit when it receives the token, it sends up to a maximumnumber of frames and then forwards the token to the next node.
Token passing is decentralized andhighly efficient.
But it has its problems as well.
For example, the failure of one node can crash the entire channel.
Or if a node accidentally neglects to release the token, then some recovery procedure must be invoked to get the token back in circulation.
Over the years many token-passing protocols have been developed, including the fiber distributed data interface (FDDI) protocol [Jain 1994] and the IEEE 802.5 token ring protocol [IEEE 802.5 2012], and each one had to address these as well as other sticky issues.
6.3.4 DOCSIS: The Link-Layer Protocol for Cable Internet Access In the previous three subsections, we’ve learned about three broad classes of multiple access protocols: channel partitioning protocols, random access protocols, and taking turns protocols.
A cable access network will make for an excellent case study here, as we’ll find aspects of each of these three classes of multiple access protocols with the cable access network!
Recall from Section 1.2.1 that a cable access network typically connects several thousand residential cable modems to a cable modem termination system (CMTS) at the cable network headend.
The Data- Over-Cable Service Interface Specifications (DOCSIS) [DOCSIS 2011] specifies the cable data network architecture and its protocols.
DOCSIS uses FDM to divide the downstream (CMTS to modem) and upstream (modem to CMTS) network segments into multiple frequency channels.
Each downstreamchannel is 6 MHz wide, with a maximum throughput of approximately 40 Mbps per channel (althoughthis data rate is seldom seen at a cable modem in practice); each upstream channel has a maximumchannel width of 6.4 MHz, and a maximum upstream throughput of approximately 30 Mbps.
Each upstream and
Figure 6.14 Upstream and downstream channels between CMTS and cable modems downstream channel is a broadcast channel.
Frames transmitted on the downstream channel by the CMTS are received by all cable modems receiving that channel; since there is just a single CMTS transmitting into the downstream channel, however, there is no multiple access problem.
The upstreamdirection, however, is more interesting and technically challenging, since multiple cable modems sharethe same upstream channel (frequency) to the CMTS, and thus collisions can potentially occur.
As illustrated in Figure 6.14, each upstream channel is divided into intervals of time (TDM-like), each containing a sequence of mini-slots during which cable modems can transmit to the CMTS.
The CMTS explicitly grants permission to individual cable modems to transmit during specific mini-slots.
The CMTSaccomplishes this by sending a control message known as a MAP message on a downstream channel to specify which cable modem (with data to send) can transmit during which mini-slot for the interval of time specified in the control message.
Since mini-slots are explicitly allocated to cable modems, theCMTS can ensure there are no colliding transmissions during a mini-slot.
But how does the CMTS know which cable modems have data to send in the first place?
This is accomplished by having cable modems send mini-slot-request frames to the CMTS during a special set of interval mini-slots that are dedicated for this purpose, as shown in Figure 6.14.
These mini-slot- request frames are transmitted in a random access manner and so may collide with each other.
A cable modem can neither sense whether the upstream channel is busy nor detect collisions.
Instead, the cable modem infers that its mini-slot-request frame experienced a collision if it does not receive a response tothe requested allocation in the next downstream control message.
When a collision is inferred, a cablemodem uses binary exponential backoff to defer the retransmission of its mini-slot-request frame to a future time slot.
When there is little traffic on the upstream channel, a cable modem may actuallytransmit data frames during slots nominally assigned for mini-slot-request frames (and thus avoid having
to wait for a mini-slot assignment).
A cable access network thus serves as a terrific example of multiple access protocols in action—FDM, TDM, random access, and centrally allocated time slots all within one network!
6.4 Switched Local Area Networks Having covered broadcast networks and multiple access protocols in the previous section, let’s turn our attention next to switched local networks.
Figure 6.15 shows a switched local network connecting three departments, two servers and a router with four switches.
Because these switches operate at the link layer, they switch link-layer frames (rather than network-layer datagrams), don’t recognize network-layeraddresses, and don’t use routing algorithms like RIP or OSPF to determine Figure 6.15 An institutional network connected together by four switches paths through the network of layer-2 switches.
Instead of using IP addresses, we will soon see that they use link-layer addresses to forward link-layer frames through the network of switches.
We’ll begin our study of switched LANs by first covering link-layer addressing ( Section 6.4.1).
We then examine the celebrated Ethernet protocol ( Section 6.5.2).
After examining link-layer addressing and Ethernet, we’ll look at how link-layer switches operate ( Section 6.4.3), and then see (Section 6.4.4) how these switches are often used to build large-scale LANs.
6.4.1 Link-Layer Addressing and ARP Hosts and routers have link-layer addresses.
Now you might find this surprising, recalling from Chapter 4 that hosts and routers have network-layer addresses as well.
You might be asking, why in the world do we need to have addresses at both the network and link layers?
In addition to describing the syntax and function of the link-layer addresses, in this section we hope to shed some light on why the two layers of addresses are useful and, in fact, indispensable.
We’ll also cover the Address Resolution Protocol (ARP), which provides a mechanism to translate IP addresses to link-layer addresses.
MAC Addresses In truth, it is not hosts and routers that have link-layer addresses but rather their adapters (that is, network interfaces) that have link-layer addresses.
A host or router with multiple network interfaces willthus have multiple link-layer addresses associated with it, just as it would also have multiple IPaddresses associated with it.
It's important to note, however, that link-layer switches do not have link- layer addresses associated with their interfaces that connect to hosts and routers.
This is because the job of the link-layer switch is to carry datagrams between hosts and routers; a switch does this jobtransparently, that is, without the host or router having to explicitly address the frame to the intervening switch.
This is illustrated in Figure 6.16.
A link-layer address is variously called a LAN address , a physical address, or a MAC address .
Because MAC address seems to be the most popular term, we’ll henceforth refer to link-layer addresses as MAC addresses.
For most LANs (including Ethernet and802.11 wireless LANs), the MAC address is 6 bytes long, giving 2  possible MAC addresses.
As shown in Figure 6.16, these 6-byte addresses are typically expressed in hexadecimal notation, with each byte of the address expressed as a pair of hexadecimal numbers.
Although MAC addresses were designed to be permanent, it is now possible to change an adapter’s MAC address via software.
For the rest ofthis section, however, we’ll assume that an adapter’s MAC address is fixed.
One interesting property of MAC addresses is that no two adapters have the same address.
This might seem surprising given that adapters are manufactured in many countries by many companies.
Howdoes a company manufacturing adapters in Taiwan make sure that it is using different addresses from a company manufacturing48
Figure 6.16 Each interface connected to a LAN has a unique MAC address adapters in Belgium?
The answer is that the IEEE manages the MAC address space.
In particular, when a company wants to manufacture adapters, it purchases a chunk of the address space consisting of 2 addresses for a nominal fee.
IEEE allocates the chunk of 2  addresses by fixing the first 24 bits of a MAC address and letting the company create unique combinations of the last 24 bits for each adapter.
An adapter’s MAC address has a flat structure (as opposed to a hierarchical structure) and doesn’t change no matter where the adapter goes.
A laptop with an Ethernet interface always has the sameMAC address, no matter where the computer goes.
A smartphone with an 802.11 interface always hasthe same MAC address, no matter where the smartphone goes.
Recall that, in contrast, IP addresseshave a hierarchical structure (that is, a network part and a host part), and a host’s IP addresses needsto be changed when the host moves, i.e., changes the network to which it is attached.
An adapter’sMAC address is analogous to a person’s social security number, which also has a flat addressingstructure and which doesn’t change no matter where the person goes.
An IP address is analogous to a person’s postal address, which is hierarchical and which must be changed whenever a person moves.
Just as a person may find it useful to have both a postal address and a social security number, it isuseful for a host and router interfaces to have both a network-layer address and a MAC address.
When an adapter wants to send a frame to some destination adapter, the sending adapter inserts the destination adapter’s MAC address into the frame and then sends the frame into the LAN.
As we willsoon see, a switch occasionally broadcasts an incoming frame onto all of its interfaces.
We’ll see in Chapter 7 that 802.11 also broadcasts frames.
Thus, an adapter may receive a frame that isn’t addressed to it.
Thus, when an adapter receives a frame, it will check to see whether the destination MAC address in the frame matches its own MAC address.
If there is a match, the adapter extracts the enclosed datagram and passes the datagram up the protocol stack.
If there isn’t a match, the adapterdiscards the frame, without passing the network-layer datagram up.
Thus, the destination only will be24 24
interrupted when the frame is received.
However, sometimes a sending adapter does want all the other adapters on the LAN to receive and process the frame it is about to send.
In this case, the sending adapter inserts a special MAC broadcast address into the destination address field of the frame.
For LANs that use 6-byte addresses (such as Ethernet and 802.11), the broadcast address is a string of 48 consecutive 1s (that is, FF-FF-FF-FF-FF- FF in hexadecimal notation).
Address Resolution Protocol (ARP) Because there are both network-layer addresses (for example, Internet IP addresses) and link-layer addresses (that is, MAC addresses), there is a need to translate between them.
For the Internet, this is the job of the Address Resolution Protocol (ARP)  [RFC 826].
To understand the need for a protocol such as ARP, consider the network shown in Figure 6.17.
In this simple example, each host and router has a single IP address and single MAC address.
As usual, IP addresses are shown in dotted-decimal PRINCIPLES IN PRACTICE KEEPING THE LAYERS INDEPENDENT There are several reasons why hosts and router interfaces have MAC addresses in ­addition to network-layer addresses.
First, LANs are designed for arbitrary network-layer protocols, not just for IP and the Internet.
If adapters were assigned IP addresses rather than “neutral” MAC addresses, then adapters would not easily be able to support other network-layer protocols (for example, IPX or DECnet).
Second, if adapters were to use network-layer addresses instead ofMAC addresses, the network-layer address would have to be stored in the adapter RAM andreconfigured every time the adapter was moved (or powered up).
Another option is to not useany addresses in the adapters and have each adapter pass the data (typically, an IP datagram)of each frame it receives up the protocol stack.
The network layer could then check for amatching network-layer address.
One problem with this option is that the host would beinterrupted by every frame sent on the LAN, including by frames that were destined for other hosts on the same broadcast LAN.
In summary, in order for the layers to be largely independent building blocks in a network architecture, different layers need to have their own addressingscheme.
We have now seen three types of addresses: host names for the application layer, IPaddresses for the network layer, and MAC addresses for the link layer.
Figure 6.17 Each interface on a LAN has an IP address and a MAC address notation and MAC addresses are shown in hexadecimal notation.
For the purposes of this discussion, we will assume in this section that the switch broadcasts all frames; that is, whenever a switch receives a frame on one interface, it forwards the frame on all of its other interfaces.
In the next section, we willprovide a more accurate explanation of how switches operate.
Now suppose that the host with IP address 222.222.222.220 wants to send an IP datagram to host 222.222.222.222.
In this example, both the source and destination are in the same subnet, in the addressing sense of Section 4.3.3.
To send a datagram, the source must give its adapter not only the IP datagram but also the MAC address for destination 222.222.222.222.
The sending adapter will then construct a link-layer frame containing the destination’s MAC address and send the frame into the LAN.
The important question addressed in this section is, How does the sending host determine the MAC address for the destination host with IP address 222.222.222.222?
As you might have guessed, it usesARP.
An ARP module in the sending host takes any IP address on the same LAN as input, and returnsthe corresponding MAC address.
In the example at hand, sending host 222.222.222.220 provides itsARP module the IP address 222.222.222.222, and the ARP module returns the corresponding MACaddress 49-BD-D2-C7-56-2A. So we see that ARP resolves an IP address to a MAC address.
In many ways it is analogous to DNS (studied in Section 2.5), which resolves host names to IP addresses.
However, one important difference between the two resolvers is that DNS resolves host names for hosts anywhere in the Internet, whereas ARP resolves IP addresses only for hosts and router interfaces on the same subnet.
If a node inCalifornia were to try to use ARP to resolve the IP address for a node in Mississippi, ARP would returnwith an error.
Figure 6.18 A possible ARP table in 222.222.222.220 Now that we have explained what ARP does, let’s look at how it works.
Each host and router has an ARP table  in its memory, which contains mappings of IP addresses to MAC addresses.
Figure 6.18 shows what an ARP table in host 222.222.222.220 might look like.
The ARP table also contains a time- to-live (TTL) value, which indicates when each mapping will be deleted from the table.
Note that a tabledoes not necessarily contain an entry for every host and router on the subnet; some may have neverbeen entered into the table, and others may have expired.
A typical expiration time for an entry is 20minutes from when an entry is placed in an ARP table.
Now suppose that host 222.222.222.220 wants to send a datagram that is IP-addressed to another host or router on that subnet.
The sending host needs to obtain the MAC address of the destination given the IP address.
This task is easy if the sender’s ARP table has an entry for the destination node.
But what if the ARP table doesn’t currently have an entry for the destination?
In particular, suppose222.222.222.220 wants to send a datagram to 222.222.222.222.
In this case, the sender uses the ARPprotocol to resolve the address.
First, the sender constructs a special packet called an ARP packet .
An ARP packet has several fields, including the sending and receiving IP and MAC addresses.
Both ARPquery and response packets have the same format.
The purpose of the ARP query packet is to query allthe other hosts and routers on the subnet to determine the MAC address corresponding to the IPaddress that is being resolved.
Returning to our example, 222.222.222.220 passes an ARP query packet to the adapter along with an indication that the adapter should send the packet to the MAC broadcast address, namely, FF-FF-FF-FF-FF-FF.
The adapter encapsulates the ARP packet in a link-layer frame, uses the broadcast addressfor the frame’s destination address, and transmits the frame into the subnet.
Recalling our socialsecurity ­number/postal address analogy, an ARP query is equivalent to a person shouting out in acrowded room of cubicles in some company (say, AnyCorp): “What is the social security number of theperson whose postal address is Cubicle 13, Room 112, AnyCorp, Palo Alto, California?”
The framecontaining the ARP query is received by all the other adapters on the subnet, and (because of the broadcast address) each adapter passes the ARP packet within the frame up to its ARP module.
Each of these ARP modules checks to see if its IP address matches the destination IP address in the ARPpacket.
The one with a match sends back to the querying host a response ARP packet with the desiredmapping.
The querying host 222.222.222.220 can then update its ARP table and send its IP datagram,encapsulated in a link-layer frame whose destination MAC is that of the host or router responding to the earlier ARP query.
There are a couple of interesting things to note about the ARP protocol.
First, the query ARP message is sent within a broadcast frame, whereas the response ARP message is sent within a standard frame.
Before reading on you should think about why this is so.
Second, ARP is plug-and-play; that is, an ARPtable gets built ­automatically—it doesn’t have to be configured by a system administrator.
And if a host becomes disconnected from the subnet, its entry is eventually deleted from the other ARP tables in the subnet.
Students often wonder if ARP is a link-layer protocol or a network-layer protocol.
As we’ve seen, an ARP packet is encapsulated within a link-layer frame and thus lies architecturally above the link layer.
However, an ARP packet has fields containing link-layer addresses and thus is arguably a link-layerprotocol, but it also contains network-layer addresses and thus is also arguably a network-layer protocol.
In the end, ARP is probably best considered a protocol that straddles the boundary between the link and network layers—not fitting neatly into the simple layered protocol stack we studied in Chapter 1.
Such are the complexities of real-world protocols!
Sending a Datagram off the SubnetIt should now be clear how ARP operates when a host wants to send a datagram to another host on the same subnet.
But now let’s look at the more complicated situation when a host on a subnet wants to send a network-layer datagram to a host off the subnet (that is, across a router onto another subnet).
Let’s discuss this issue in the context of Figure 6.19, which shows a simple network consisting of two subnets interconnected by a router.
There are several interesting things to note about Figure 6.19.
Each host has exactly one IP address and one adapter.
But, as discussed in Chapter 4, a router has an IP address for each of its interfaces.
For each router interface there is also an ARP module (in the router) and an adapter.
Because the router in Figure 6.19 has two interfaces, it has two IP addresses, two ARP modules, and two adapters.
Of course, each adapter in the network has its own MAC address.
Figure 6.19 Two subnets interconnected by a router
Also note that Subnet 1 has the network address 111.111.111/24 and that Subnet 2 has the network address 222.222.222/24.
Thus all of the interfaces connected to Subnet 1 have addresses of the form111.111.111.xxx and all of the interfaces connected to Subnet 2 have addresses of the form222.222.222.xxx.
Now let’s examine how a host on Subnet 1 would send a datagram to a host on Subnet 2.
Specifically, suppose that host 111.111.111.111 wants to send an IP datagram to a host 222.222.222.222.
Thesending host passes the datagram to its adapter, as usual.
But the sending host must also indicate to itsadapter an appropriate destination MAC address.
What MAC address should the adapter use?
Onemight be tempted to guess that the appropriate MAC address is that of the adapter for host222.222.222.222, namely, 49-BD-D2-C7-56-2A. This guess, however, would be wrong!
If the sendingadapter were to use that MAC address, then none of the ­adapters on Subnet 1 would bother to pass the IP datagram up to its network layer, since the frame’s destination address would not match the MAC address of any adapter on Subnet 1.
The datagram would just die and go to datagram heaven.
If we look carefully at Figure 6.19, we see that in order for a datagram to go from 111.111.111.111 to a host on Subnet 2, the datagram must first be sent to the router interface 111.111.111.110, which is the IP address of the first-hop router on the path to the final destination.
Thus, the appropriate MAC addressfor the frame is the address of the adapter for router interface 111.111.111.110, namely, E6-E9-00-17-BB-4B. How does the sending host acquire the MAC address for 111.111.111.110?
By using ARP, ofcourse!
Once the sending adapter has this MAC address, it creates a frame (containing the datagramaddressed to 222.222.222.222) and sends the frame into Subnet 1.
The router adapter on Subnet 1 sees that the link-layer frame is addressed to it, and therefore passes the frame to the network layer of the router.
Hooray—the IP datagram has successfully been moved from source host to the router!
Butwe are not finished.
We still have to move the datagram from the router to the destination.
The routernow has to determine the correct interface on which the datagram is to be forwarded.
As discussed in Chapter 4, this is done by consulting a forwarding table in the router.
The forwarding table tells the router that the datagram is to be forwarded via router interface 222.222.222.220.
This interface then passes the datagram to its adapter, which encapsulates the datagram in a new frame and sends theframe into Subnet 2.
This time, the destination MAC address of the frame is indeed the MAC address of the ultimate destination.
And how does the router obtain this destination MAC address?
From ARP, of course!
ARP for Ethernet is defined in RFC 826.
A nice introduction to ARP is given in the TCP/IP tutorial, RFC 1180.
We’ll explore ARP in more detail in the homework problems.
6.4.2 Ethernet
Ethernet has pretty much taken over the wired LAN market.
In the 1980s and the early 1990s, Ethernet faced many challenges from other LAN technologies, ­including token ring, FDDI, and ATM.
Some of these other technologies succeeded in capturing a part of the LAN market for a few years.
But since its invention in the mid-1970s, Ethernet has continued to evolve and grow and has held on to its dominantposition.
Today, Ethernet is by far the most prevalent wired LAN technology, and it is likely to remain sofor the foreseeable future.
One might say that Ethernet has been to local area networking what the Internet has been to global networking.
There are many reasons for Ethernet’s success.
First, Ethernet was the first widely deployed high-speed LAN.
Because it was deployed early, network administrators became intimately familiar with Ethernet—its wonders and its quirks—and were reluctant to switch over to other LAN technologies when theycame on the scene.
Second, token ring, FDDI, and ATM were more complex and expensive thanEthernet, which further discouraged network administrators from switching over.
Third, the mostcompelling reason to switch to another LAN technology (such as FDDI or ATM) was usually the higher data rate of the new technology; however, Ethernet always fought back, producing versions that operated at equal data rates or higher.
Switched Ethernet was also introduced in the early 1990s, whichfurther increased its effective data rates.
Finally, because Ethernet has been so popular, Ethernethardware (in particular, adapters and switches) has become a commodity and is remarkably cheap.
The original Ethernet LAN was invented in the mid-1970s by Bob Metcalfe and David Boggs.
The original Ethernet LAN used a coaxial bus to interconnect the nodes.
Bus topologies for Ethernet actuallypersisted throughout the 1980s and into the mid-1990s.
Ethernet with a bus topology is a broadcast LAN —all transmitted frames travel to and are processed by all adapters connected to the bus.
Recall that we covered Ethernet’s CSMA/CD multiple access protocol with binary exponential backoff in Section 6.3.2.
By the late 1990s, most companies and universities had replaced their LANs with Ethernet installations using a hub-based star topology.
In such an installation the hosts (and routers) are directly connected toa hub with twisted-pair copper wire.
A hub is a physical-layer device that acts on individual bits rather than frames.
When a bit, representing a zero or a one, arrives from one interface, the hub simply re-creates the bit, boosts its energy strength, and transmits the bit onto all the other interfaces.
Thus,Ethernet with a hub-based star topology is also a broadcast LAN—whenever a hub receives a bit from one of its interfaces, it sends a copy out on all of its other interfaces.
In particular, if a hub receives frames from two different interfaces at the same time, a collision occurs and the nodes that created theframes must retransmit.
In the early 2000s Ethernet experienced yet another major evolutionary change.
Ethernet installations continued to use a star topology, but the hub at the center was replaced with a switch.
We’ll be examining switched Ethernet in depth later in this chapter.
For now, we only mention that a switch is notonly “collision-less” but is also a bona-fide store-and-forward packet switch; but unlike routers, whichoperate up through layer 3, a switch operates only up through layer 2.
Figure 6.20 Ethernet frame structure Ethernet Frame Structure We can learn a lot about Ethernet by examining the Ethernet frame, which is shown in Figure 6.20.
To give this discussion about Ethernet frames a tangible context, let’s consider sending an IP datagram from one host to another host, with both hosts on the same Ethernet LAN (for example, the Ethernet LAN in Figure 6.17.) (
Although the payload of our Ethernet frame is an IP datagram, we note that an Ethernet frame can carry other network-layer packets as well.)
Let the sending adapter, adapter A, have the MAC address AA-AA-AA-AA-AA-AA and the receiving adapter, adapter B, have the MAC addressBB-BB-BB-BB-BB-BB.
The sending adapter encapsulates the IP datagram within an Ethernet frame andpasses the frame to the physical layer.
The receiving adapter receives the frame from the physical layer, extracts the IP datagram, and passes the IP datagram to the network layer.
In this context, let’s now examine the six fields of the Ethernet frame, as shown in Figure 6.20.
Data field (46 to 1,500 bytes).
This field carries the IP datagram.
The maximum transmission unit (MTU) of Ethernet is 1,500 bytes.
This means that if the IP datagram exceeds 1,500 bytes, then the host has to fragment the datagram, as discussed in Section 4.3.2.
The minimum size of the data field is 46 bytes.
This means that if the IP datagram is less than 46 bytes, the data field has to be “stuffed” to fill it out to 46 bytes.
When stuffing is used, the data passed to the network layer contains the stuffing as well as an IP datagram.
The network layer uses the length field in the IP datagramheader to remove the stuffing.
Destination address (6 bytes).
This field contains the MAC address of the destination adapter, BB- BB-BB-BB-BB-BB.
When adapter B receives an Ethernet frame whose destination address is either BB-BB-BB-BB-BB-BB or the MAC broadcast address, it passes the contents of the frame’s data fieldto the network layer; if it receives a frame with any other MAC address, it discards the frame.
Source address (6 bytes).
This field contains the MAC address of the adapter that transmits the frame onto the LAN, in this example, AA-AA-AA-AA-AA-AA.
Type field (2 bytes).
 The type field permits Ethernet to multiplex network-layer protocols.
To understand this, we need to keep in mind that hosts can use other network-layer protocols besides IP.
In fact, a given host may support multiple network-layer protocols using different protocols fordifferent applications.
For this reason, when the Ethernet frame arrives at adapter B, adapter B needs to know to which network-layer protocol it should pass (that is, demultiplex) the contents ofthe data field.
IP and other network-layer protocols (for example, Novell IPX or AppleTalk) each havetheir own, standardized type number.
Furthermore, the ARP protocol (discussed in the previous
section) has its own type number, and if the arriving frame contains an ARP packet (i.e., has a type field of 0806 hexadecimal), the ARP packet will be demultiplexed up to the ARP protocol.
Note that the type field is analogous to the protocol field in the network-layer datagram and the port-numberfields in the transport-layer segment; all of these fields serve to glue a protocol at one layer to aprotocol at the layer above.
Cyclic redundancy check (CRC) (4 bytes).
 As discussed in Section 6.2.3, the purpose of the CRC field is to allow the receiving adapter, adapter B, to detect bit errors in the frame.
Preamble (8 bytes).
The Ethernet frame begins with an 8-byte preamble field.
Each of the first 7 bytes of the preamble has a value of 10101010; the last byte is 10101011.
The first 7 bytes of the preamble serve to “wake up” the receiving adapters and to synchronize their clocks to that of thesender’s clock.
Why should the clocks be out of synchronization?
Keep in mind that adapter A aims to transmit the frame at 10 Mbps, 100 Mbps, or 1 Gbps, depending on the type of Ethernet LAN.
However, because nothing is absolutely perfect, adapter A will not transmit the frame at exactly the target rate; there will always be some drift from the target rate, a drift which is not known a priori  by the other adapters on the LAN.
A receiving adapter can lock onto adapter A’s clock simply by locking onto the bits in the first 7 bytes of the preamble.
The last 2 bits of the eighth byte of the preamble(the first two consecutive 1s) alert adapter B that the “important stuff” is about to come.
All of the Ethernet technologies provide connectionless service to the network layer.
That is, whenadapter A wants to send a datagram to adapter B, adapter A encapsulates the datagram in an Ethernet frame and sends the frame into the LAN, without first handshaking with adapter B. This layer-2 connectionless service is analogous to IP’s layer-3 datagram service and UDP’s layer-4 connectionlessservice.
Ethernet technologies provide an unreliable service to the network layer.
Specifically, when adapter B receives a frame from adapter A, it runs the frame through a CRC check, but neither sends anacknowledgment when a frame passes the CRC check nor sends a negative acknowledgment when aframe fails the CRC check.
When a frame fails the CRC check, adapter B simply discards the frame.
Thus, adapter A has no idea whether its transmitted frame reached adapter B and passed the CRC check.
This lack of reliable transport (at the link layer) helps to make Ethernet simple and cheap.
But italso means that the stream of datagrams passed to the network layer can have gaps.
CASE HISTORY BOB METCALFE AND ETHERNET As a PhD student at Harvard University in the early 1970s, Bob Metcalfe worked on the ARPAnet at MIT.
During his studies, he also became exposed to Abramson’s work on ALOHAand random access protocols.
After completing his PhD and just before beginning a job at Xerox Palo Alto Research Center (Xerox PARC), he visited Abramson and his University of Hawaii colleagues for three months, getting a firsthand look at ALOHAnet.
At Xerox PARC, Metcalfe
became exposed to Alto computers, which in many ways were the forerunners of the personal computers of the 1980s.
Metcalfe saw the need to network these computers in an inexpensive manner.
So armed with his knowledge about ARPAnet, ALOHAnet, and random accessprotocols, Metcalfe—along with colleague David Boggs—invented Ethernet.
Metcalfe and Boggs’s original Ethernet ran at 2.94 Mbps and linked up to 256 hosts separated by up to one mile.
Metcalfe and Boggs succeeded at getting most of the researchers at Xerox PARC to communicate through their Alto computers.
Metcalfe then forged an alliance betweenXerox, Digital, and Intel to establish Ethernet as a 10 Mbps Ethernet standard, ratified by theIEEE.
Xerox did not show much interest in commercializing Ethernet.
In 1979, Metcalfe formedhis own company, 3Com, which developed and commercialized networking technology,including Ethernet technology.
In particular, 3Com developed and marketed Ethernet cards inthe early 1980s for the immensely popular IBM PCs.
If there are gaps due to discarded Ethernet frames, does the application at Host B see gaps as well?
As we learned in Chapter 3, this depends on whether the application is using UDP or TCP.
If the application is using UDP, then the application in Host B will indeed see gaps in the data.
On the other hand, if the application is using TCP, then TCP in Host B will not acknowledge the data contained indiscarded frames, causing TCP in Host A to retransmit.
Note that when TCP retransmits data, the datawill eventually return to the Ethernet adapter at which it was discarded.
Thus, in this sense, Ethernetdoes retransmit data, although Ethernet is unaware of whether it is transmitting a brand-new datagramwith brand-new data, or a datagram that contains data that has already been transmitted at least once.
Ethernet Technologies In our discussion above, we’ve referred to Ethernet as if it were a single protocol standard.
But in fact, Ethernet comes in many different flavors, with somewhat bewildering acronyms such as 10BASE-T, 10BASE-2, 100BASE-T, 1000BASE-LX, 10GBASE-T and 40GBASE-T. These and many other Ethernet technologies have been standardized over the years by the IEEE 802.3 CSMA/CD (Ethernet) working group [IEEE 802.3 2012] .
While these acronyms may appear bewildering, there is actually considerable order here.
The first part of the acronym refers to the speed of the standard: 10, 100, 1000, or 10G, for 10 Megabit (per second), 100 Megabit, Gigabit, 10 Gigabit and 40 Gigibit Ethernet, respectively. “
BASE” refers to baseband Ethernet, meaning that the physical media only carries Ethernet traffic; almost all of the 802.3 standards are for baseband Ethernet.
The final part of the acronym refers to the physical media itself; Ethernet is both a link-layer and a physical-layer specification and is carried over a variety of physical media including coaxial cable, copper wire, and fiber.
Generally, a “T” refers to twisted-pair copper wires.
Historically, an Ethernet was initially conceived of as a segment of coaxial cable.
The early 10BASE-2 and 10BASE-5 standards specify 10 Mbps Ethernet over two types of coaxial cable, each limited in
length to 500 meters.
Longer runs could be obtained by using a repeater —a physical-layer device that receives a signal on the input side, and regenerates the signal on the output side.
A coaxial cable corresponds nicely to our view of Ethernet as a broadcast medium—all frames transmitted by oneinterface are received at other interfaces, and Ethernet’s CDMA/CD protocol nicely solves the multiple access problem.
Nodes simply attach to the cable, and voila, we have a local area network!
Ethernet has passed through a series of evolutionary steps over the years, and today’s Ethernet is very different from the original bus-topology designs using coaxial cable.
In most installations today, nodesare connected to a switch via point-to-point segments made of twisted-pair copper wires or fiber-optic cables, as shown in Figures 6.15–6.17.
In the mid-1990s, Ethernet was standardized at 100 Mbps, 10 times faster than 10 Mbps Ethernet.
The original Ethernet MAC protocol and frame format were preserved, but higher-speed physical layers were defined for copper wire (100BASE-T) and fiber (100BASE-FX, 100BASE-SX, 100BASE-BX).
Figure 6.21 shows these different standards and the common Ethernet MAC protocol and frame format.
100 Mbps Ethernet is limited to a 100-meter distance over twisted pair, and to Figure 6.21 100 Mbps Ethernet standards: A common link layer, ­different physical layers several kilometers over fiber, allowing Ethernet switches in different buildings to be connected.
Gigabit Ethernet is an extension to the highly successful 10 Mbps and 100 Mbps Ethernet standards.
Offering a raw data rate of 40,000 Mbps, 40 Gigabit Ethernet maintains full compatibility with the hugeinstalled base of Ethernet equipment.
The standard for Gigabit Ethernet, referred to as IEEE 802.3z,does the following: Uses the standard Ethernet frame format (Figure 6.20) and is backward compatible with 10BASE-T and 100BASE-T technologies.
This allows for easy integration of Gigabit Ethernet with the existing installed base of Ethernet equipment.
Allows for point-to-point links as well as shared broadcast channels.
Point-to-point links use switches while broadcast channels use hubs, as described earlier.
In Gigabit Ethernet jargon, hubs are called buffered distributors .
Uses CSMA/CD for shared broadcast channels.
In order to have acceptable efficiency, the
maximum distance between nodes must be severely restricted.
Allows for full-duplex operation at 40 Gbps in both directions for point-to-point channels.
Initially operating over optical fiber, Gigabit Ethernet is now able to run over category 5 UTP cabling.
Let’s conclude our discussion of Ethernet technology by posing a question that may have begun troubling you.
In the days of bus topologies and hub-based star topologies, Ethernet was clearly a broadcast link (as defined in Section 6.3) in which frame collisions occurred when nodes transmitted at the same time.
To deal with these collisions, the Ethernet standard included the CSMA/CD protocol, which is particularly effective for a wired broadcast LAN spanning a small geographical region.
But if theprevalent use of Ethernet today is a switch-based star topology, using store-and-forward packetswitching, is there really a need anymore for an Ethernet MAC protocol?
As we’ll see shortly, a switchcoordinates its transmissions and never forwards more than one frame onto the same interface at anytime.
Furthermore, modern switches are full-duplex, so that a switch and a node can each send frames to each other at the same time without interference.
In other words, in a switch-based Ethernet LAN there are no collisions and, therefore, there is no need for a MAC protocol!
As we’ve seen, today’s Ethernets are very different from the original Ethernet conceived by Metcalfe and Boggs more than 30 years ago—speeds have increased by three orders of magnitude, Ethernet frames are carried over a variety of media, switched-Ethernets have become dominant, and now even the MAC protocol is often unnecessary!
Is all of this really  still Ethernet?
The answer, of course, is “yes, by definition.”
It is interesting to note, however, that through all of these changes, there has indeed been one enduring constant that has remained unchanged over 30 years—Ethernet’s frame format.
Perhaps this then is the one true and timeless centerpiece of the Ethernet standard.
6.4.3 Link-Layer Switches Up until this point, we have been purposefully vague about what a switch actually does and how itworks.
The role of the switch is to receive incoming link-layer frames and forward them onto outgoinglinks; we’ll study this forwarding function in detail in this subsection.
We’ll see that the switch itself istransparent to the hosts and routers in the subnet; that is, a host/router addresses a frame to anotherhost/router (rather than addressing the frame to the switch) and happily sends the frame into the LAN, unaware that a switch will be receiving the frame and forwarding it.
The rate at which frames arrive to any one of the switch’s output interfaces may temporarily exceed the link capacity of that interface.
Toaccommodate this problem, switch output interfaces have buffers, in much the same way that routeroutput interfaces have buffers for datagrams.
Let’s now take a closer look at how switches operate.
Forwarding and Filtering
Filtering  is the switch function that determines whether a frame should be forwarded to some interface or should just be dropped.
Forwarding  is the switch function that determines the interfaces to which a frame should be directed, and then moves the frame to those interfaces.
Switch filtering and forwarding are done with a switch table.
The switch table contains entries for some, but not necessarily all, of the hosts and routers on a LAN.
An entry in the switch table contains (1) a MAC address, (2) the switchinterface that leads toward that MAC address, and (3) the time at which the entry was placed in the table.
An example switch table for the uppermost switch in Figure 6.15 is shown in Figure 6.22.
This description of frame forwarding may sound similar to our discussion of datagram forwarding Figure 6.22 Portion of a switch table for the uppermost switch in Figure 6.15 in Chapter 4.
Indeed, in our discussion of generalized forwarding in Section 4.4, we learned that many modern packet switches can be configured to forward on the basis of layer-2 destination MAC addresses (i.e., function as a layer-2 switch) or layer-3 IP destination addresses (i.e., function as alayer-3 router).
Nonetheless, we’ll make the important distinction that switches forward packets basedon MAC addresses rather than on IP addresses.
We will also see that a traditional (i.e., in a non-SDNcontext) switch table is constructed in a very different manner from a router’s forwarding table.
To understand how switch filtering and forwarding work, suppose a frame with destination address DD- DD-DD-DD-DD-DD arrives at the switch on interface x. The switch indexes its table with the MAC address DD-DD-DD-DD-DD-DD.
There are three possible cases: There is no entry in the table for DD-DD-DD-DD-DD-DD.
In this case, the switch forwards copies of the frame to the output buffers preceding all interfaces except for interface x. In other words, if there is no entry for the destination address, the switch broadcasts the frame.
There is an entry in the table, associating DD-DD-DD-DD-DD-DD with interface x. In this case, the frame is coming from a LAN segment that contains adapter DD-DD-DD-DD-DD-DD.
There being noneed to forward the frame to any of the other interfaces, the switch performs the filtering function bydiscarding the frame.
There is an entry in the table, associating DD-DD-DD-DD-DD-DD with interface  In this case, the frame needs to be forwarded to the LAN segment attached to interface y. The switch performs its forwarding function by putting the frame in an output buffer that precedes interface y.y≠x.
Let’s walk through these rules for the uppermost switch in Figure 6.15 and its switch table in Figure 6.22.
Suppose that a frame with destination address 62-FE-F7-11-89-A3 arrives at the switch from interface 1.
The switch examines its table and sees that the destination is on the LAN segment connected to interface 1 (that is, Electrical Engineering).
This means that the frame has already beenbroadcast on the LAN segment that contains the destination.
The switch therefore filters (that is, discards) the frame.
Now suppose a frame with the same destination address arrives from interface 2.
The switch again examines its table and sees that the destination is in the direction of interface 1; ittherefore forwards the frame to the output buffer preceding interface 1.
It should be clear from thisexample that as long as the switch table is complete and accurate, the switch forwards frames towarddestinations without any broadcasting.
In this sense, a switch is “smarter” than a hub.
But how does this switch table get configured in the first place?
Are there link-layer equivalents to network-layer routing protocols?
Or must an overworkedmanager manually configure the switch table?
Self-Learning A switch has the wonderful property (particularly for the already-overworked network administrator) that its table is built automatically, dynamically, and autonomously—without any intervention from a networkadministrator or from a configuration protocol.
In other words, switches are self-learning.
This capability is accomplished as follows: 1.
The switch table is initially empty.
2.
For each incoming frame received on an interface, the switch stores in its table (1) the MACaddress in the frame’s source address field , (2) the interface from which the frame arrived, and (3) the current time.
In this manner the switch records in its table the LAN segment on which thesender resides.
If every host in the LAN eventually sends a frame, then every host will eventually get recorded in the table.
3.
The switch deletes an address in the table if no frames are received with that address as the source address after some period of time (the aging time ).
In this manner, if a PC is replaced by another PC (with a different adapter), the MAC address of the original PC will eventually be purged from the switch table.
Let’s walk through the self-learning property for the uppermost switch in Figure 6.15 and its corresponding switch table in Figure 6.22.
Suppose at time 9:39 a frame with source address 01-12-23- 34-45-56 arrives from interface 2.
Suppose that this address is not in the switch table.
Then the switch adds a new entry to the table, as shown in Figure 6.23.
Continuing with this same example, suppose that the aging time for this switch is 60 minutes, and no frames with source address 62-FE-F7-11-89-A3 arrive to the switch between 9:32 and 10:32.
Then at
time 10:32, the switch removes this address from its table.
Figure 6.23 Switch learns about the location of an adapter with address 01-12-23-34-45-56 Switches are plug-and-play devices  because they require no intervention from a network administrator or user.
A network administrator wanting to install a switch need do nothing more than connect the LAN segments to the switch interfaces.
The administrator need not configure the switch tables at the time ofinstallation or when a host is removed from one of the LAN segments.
Switches are also full-duplex,meaning any switch interface can send and receive at the same time.
Properties of Link-Layer Switching Having described the basic operation of a link-layer switch, let’s now consider their features and properties.
We can identify several advantages of using switches, rather than broadcast links such as buses or hub-based star topologies: Elimination of collisions.
 In a LAN built from switches (and without hubs), there is no wasted bandwidth due to collisions!
The switches buffer frames and never transmit more than one frame on a segment at any one time.
As with a router, the maximum aggregate throughput of a switch is thesum of all the switch interface rates.
Thus, switches provide a significant performance improvement over LANs with broadcast links.
Heterogeneous links.
Because a switch isolates one link from another, the different links in the LAN can operate at different speeds and can run over different media.
For example, the uppermost switch in Figure 6.15 might have three1 Gbps 1000BASE-T copper links, two 100 Mbps 100BASE- FX fiber links, and one 100BASE-T copper link.
Thus, a switch is ideal for mixing legacy equipment with new equipment.
Management.
In addition to providing enhanced security (see sidebar on Focus on Security), a switch also eases network management.
For example, if an adapter malfunctions and continually sends Ethernet frames (called a jabbering adapter), a switch can detect the problem and internallydisconnect the malfunctioning adapter.
With this feature, the network administrator need not get out of bed and drive back to work in order to correct the problem.
Similarly, a cable cut disconnects only that host that was using the cut cable to connect to the switch.
In the days of coaxial cable, many a
network manager spent hours “walking the line” (or more accurately, “crawling the floor”) to find the cable break that brought down the entire network.
Switches also gather statistics on bandwidth usage, collision rates, and traffic types, and make this information available to the network manager.
This information can be used to debug and correct problems, and to plan how the LAN shouldevolve in the future.
Researchers are exploring adding yet more management functionality into Ethernet LANs in prototype deployments [Casado 2007 ; Koponen 2011].
FOCUS ON SECURITY SNIFFING A SWITCHED LAN: SWITCH POISONING When a host is connected to a switch, it typically only receives frames that are intended for it.
For example, consider a switched LAN in Figure 6.17.
When host A sends a frame to host B, and there is an entry for host B in the switch table, then the switch will forward the frame only to host B. If host C happens to be running a sniffer, host C will not be able to sniff this A-to-B frame.
Thus, in a switched-LAN environment (in contrast to a broadcast link environment suchas 802.11 LANs or hub–based Ethernet LANs), it is more difficult for an attacker to sniff frames.
However , because the switch broadcasts frames that have destination addresses that are not in the switch table, the sniffer at C can still sniff some frames that are not intended for C. Furthermore, a sniffer will be able sniff all Ethernet broadcast frames with broadcast destinationaddress FF–FF–FF–FF–FF–FF.
A well-known attack against a switch, called switch poisoning , is to send tons of packets to the switch with many different bogus source MAC addresses,thereby filling the switch table with bogus entries and leaving no room for the MAC addresses ofthe legitimate hosts.
This causes the switch to broadcast most frames, which can then be picked up by the sniffer [Skoudis 2006].
As this attack is rather involved even for a sophisticated attacker, switches are significantly less vulnerable to sniffing than are hubs and wireless LANs.
Switches Versus Routers As we learned in Chapter 4, routers are store-and-forward packet switches that forward packets using network-layer addresses.
Although a switch is also a store-and-forward packet switch, it is fundamentally different from a router in that it forwards packets using MAC addresses.
Whereas a routeris a layer-3 packet switch, a switch is a layer-2 packet switch.
Recall, however, that we learned in Section 4.4 that modern switches using the “match plus action” operation can be used to forward a layer-2 frame based on the frame's destination MAC address, as well as a layer-3 datagram using the datagram's destination IP address.
Indeed, we saw that switches using the OpenFlow standard canperform generalized packet forwarding based on any of eleven different frame, datagram, and transport-layer header fields.
Even though switches and routers are fundamentally different, network administrators must often choose between them when installing an interconnection device.
For example, for the network in Figure 6.15, the network administrator could just as easily have used a router instead of a switch to connect the department LANs, servers, and internet gateway router.
Indeed, a router would permit interdepartmental communication without creating collisions.
Given that both switches and routers are candidates for interconnection devices, what are the pros and cons of the two approaches?
Figure 6.24 Packet processing in switches, routers, and hosts First consider the pros and cons of switches.
As mentioned above, switches are plug-and-play, aproperty that is cherished by all the overworked network administrators of the world.
Switches can also have relatively high filtering and forwarding rates—as shown in Figure 6.24, switches have to process frames only up through layer 2, whereas routers have to process datagrams up through layer 3.
On the other hand, to prevent the cycling of broadcast frames, the active topology of a switched network isrestricted to a spanning tree.
Also, a large switched network would require large ARP tables in the hostsand routers and would generate substantial ARP traffic and processing.
Furthermore, switches aresusceptible to broadcast storms—if one host goes haywire and transmits an endless stream of Ethernetbroadcast frames, the switches will forward all of these frames, causing the entire network to collapse.
Now consider the pros and cons of routers.
Because network addressing is often hierarchical (and not flat, as is MAC addressing), packets do not normally cycle through routers even when the network hasredundant paths. (
However, packets can cycle when router tables are misconfigured; but as we learned in Chapter 4, IP uses a special datagram header field to limit the cycling.)
Thus, packets are not restricted to a spanning tree and can use the best path between source and destination.
Because routers do not have the spanning tree restriction, they have allowed the Internet to be built with a richtopology that includes, for example, multiple active links between Europe and North America.
Anotherfeature of routers is that they provide firewall protection against layer-2 broadcast storms.
Perhaps the most significant drawback of routers, though, is that they are not plug-and-play—they and the hosts that connect to them need their IP addresses to be configured.
Also, routers often have a larger per-packetprocessing time than switches, because they have to process up through the layer-3 fields.
Finally, there
are two different ways to pronounce the word router, either as “rootor” or as “rowter,” and people waste a lot of time arguing over the proper pronunciation [Perlman 1999] .
Given that both switches and routers have their pros and cons (as summarized in Table 6.1), when should an institutional network (for example, a university campus Table 6.1 Comparison of the typical features of popular interconnection devices Hubs Routers Switches Traffic isolation No Yes Yes Plug and play Yes No Yes Optimal routing No Yes No network or a corporate campus network) use switches, and when should it use routers?
Typically, small networks consisting of a few hundred hosts have a few LAN segments.
Switches suffice for these smallnetworks, as they localize traffic and increase aggregate throughput without requiring any configuration of IP addresses.
But larger networks consisting of thousands of hosts typically include routers within the network (in addition to switches).
The routers provide a more robust isolation of traffic, control broadcaststorms, and use more “intelligent” routes among the hosts in the network.
For more discussion of the pros and cons of switched versus routed networks, as well as a discussion of how switched LAN technology can be extended to accommodate two orders of magnitude more hosts than today’s Ethernets, see [Meyers 2004 ; Kim 2008] .
6.4.4 Virtual Local Area Networks (VLANs) In our earlier discussion of Figure 6.15, we noted that modern institutional LANs are often configured hierarchically, with each workgroup (department) having its own switched LAN connected to the switched LANs of other groups via a switch hierarchy.
While such a configuration works well in an idealworld, the real world is often far from ideal.
Three drawbacks can be identified in the configuration in Figure 6.15: Lack of traffic isolation.
 Although the hierarchy localizes group traffic to within a single switch, broadcast traffic (e.g., frames carrying ARP and DHCP messages or frames whose destination has not yet been learned by a self-learning switch) must still traverse the entire institutional network.
Limiting the scope of such broadcast traffic would improve LAN performance.
Perhaps more importantly, it also may be desirable to limit LAN broadcast traffic for security/privacy reasons.
For example, if one group contains the company’s executive management team and another groupcontains disgruntled employees running Wireshark packet sniffers, the network manager may wellprefer that the executives’ traffic never even reaches employee hosts.
This type of isolation could be provided by replacing the center switch in Figure 6.15 with a router.
We’ll see shortly that this isolation also can be achieved via a switched (layer 2) solution.
Inefficient use of switches.
 If instead of three groups, the institution had 10 groups, then 10 first- level switches would be required.
If each group were small, say less than 10 people, then a single 96-port switch would likely be large enough to accommodate everyone, but this single switch wouldnot provide traffic isolation.
Managing users.
If an employee moves between groups, the physical cabling must be changed toconnect the employee to a different switch in Figure 6.15.
Employees belonging to two groups make the problem even harder.
Fortunately, each of these difficulties can be handled by a switch that supports virtual local area networks (VLANs) .
As the name suggests, a switch that supports VLANs allows multiple virtual local area networks to be defined over a single physical  local area network infrastructure.
Hosts within a VLAN communicate with each other as if they (and no other hosts) were connected to the switch.
In aport-based VLAN, the switch’s ports (interfaces) are divided into groups by the network manager.
Eachgroup constitutes a VLAN, with the ports in each VLAN forming a broadcast domain (i.e., broadcast traffic from one port can only reach other ports in the group).
Figure 6.25 shows a single switch with 16 ports.
Ports 2 to 8 belong to the EE VLAN, while ports 9 to 15 belong to the CS VLAN (ports 1 and 16 are unassigned).
This VLAN solves all of the difficulties noted above—EE and CS VLAN frames are isolated from each other, the two switches in Figure 6.15 have been replaced by a single switch, and if the user at switch port 8 joins the CS Department, the network operator simply reconfigures the VLAN software so that port 8 is now associated with the CS VLAN.
One can easily imagine how the VLANswitch is configured and operates—the network manager declares a port to belong Figure 6.25 A single switch with two configured VLANs
to a given VLAN (with undeclared ports belonging to a default VLAN) using switch management software, a table of port-to-VLAN mappings is maintained within the switch; and switch hardware onlydelivers frames between ports belonging to the same VLAN.
But by completely isolating the two VLANs, we have introduced a new difficulty!
How can traffic from the EE Department be sent to the CS Department?
One way to handle this would be to connect a VLAN switch port (e.g., port 1 in Figure 6.25) to an external router and configure that port to belong both the EE and CS VLANs.
In this case, even though the EE and CS departments share the same physical switch, the logical configuration would look as if the EE and CS departments had separate switchesconnected via a router.
An IP datagram going from the EE to the CS department would first cross theEE VLAN to reach the router and then be forwarded by the router back over the CS VLAN to the CShost.
Fortunately, switch vendors make such configurations easy for the network manager by building a single device that contains both a VLAN switch and a router, so a separate external router is not needed.
A homework problem at the end of the chapter explores this scenario in more detail.
Returning again to Figure 6.15, let’s now suppose that rather than having a separate Computer Engineering department, some EE and CS faculty are housed in a separate building, where (of course!)
they need network access, and (of course!)
they’d like to be part of their department’s VLAN.
Figure 6.26 shows a second 8-port switch, where the switch ports have been defined as belonging to the EE or the CS VLAN, as needed.
But how should these two switches be interconnected?
One easy solution would be to define a port belonging to the CS VLAN on each switch (similarly for the EE VLAN) and to connect these ports to each other, as shown in Figure 6.26(a).
This solution doesn’t scale, however, since N VLANS would require N ports on each switch simply to interconnect the two switches.
A more scalable approach to interconnecting VLAN switches is known as VLAN trunking .
In the VLAN trunking approach shown in Figure 6.26(b), a special port on each switch (port 16 on the left switch and port 1 on the right switch) is configured as a trunk port to interconnect the two VLAN switches.
The trunk port belongs to all VLANs, and frames sent to any VLAN are forwarded over the trunk link to the otherswitch.
But this raises yet another question: How does a switch know that a frame arriving on a trunkport belongs to a particular VLAN?
The IEEE has defined an extended Ethernet frame format, 802.1Q, for frames crossing a VLAN trunk.
As shown in Figure 6.27, the 802.1Q frame consists of the standard Ethernet frame with a four-byte VLAN tag added into the header that carries the identity of the VLAN to which the frame belongs.
The VLAN tag is added into a frame by the switch at the sending side of aVLAN trunk, parsed, and removed by the switch at the receiving side of the trunk.
The VLAN tag itselfconsists of a 2-byte Tag Protocol Identifier (TPID) field (with a fixed hexadecimal value of 81-00), a 2-byte Tag Control Information field that contains a 12-bit VLAN identifier field, and a 3-bit priority field thatis similar in intent to the IP datagram TOS field.
Figure 6.26 Connecting two VLAN switches with two VLANs: (a) two cables (b) trunked Figure 6.27 Original Ethernet frame (top), 802.1Q-tagged Ethernet VLAN frame (below) In this discussion, we’ve only briefly touched on VLANs and have focused on port-based VLANs.
We should also mention that VLANs can be defined in several other ways.
In MAC-based VLANs, the network manager specifies the set of MAC addresses that belong to each VLAN; whenever a deviceattaches to a port, the port is connected into the appropriate VLAN based on the MAC address of thedevice.
VLANs can also be defined based on network-layer protocols (e.g., IPv4, IPv6, or Appletalk) andother criteria.
It is also possible for VLANs to be extended across IP routers, allowing islands of LANs to be connected together to form a single VLAN that could span the globe [Yu 2011].
See the 802.1Q standard [IEEE 802.1q 2005] for more details.
6.5 Link Virtualization: A Network as a Link Layer Because this chapter concerns link-layer protocols, and given that we’re now nearing the chapter’s end, let’s reflect on how our understanding of the term link has evolved.
We began this chapter by viewing the link as a physical wire connecting two communicating hosts.
In studying multiple access protocols, we saw that multiple hosts could be connected by a shared wire and that the “wire” connecting the hostscould be radio spectra or other media.
This led us to consider the link a bit more abstractly as a channel, rather than as a wire.
In our study of Ethernet LANs (Figure 6.15) we saw that the interconnecting media could actually be a rather complex switched infrastructure.
Throughout this evolution, however, the hosts themselves maintained the view that the interconnecting medium was simply a link-layerchannel connecting two or more hosts.
We saw, for example, that an Ethernet host can be blissfully unaware of whether it is connected to other LAN hosts by a single short LAN segment ( Figure 6.17) or by a geographically dispersed switched LAN ( Figure 6.15) or by a VLAN (Figure 6.26).
In the case of a dialup modem connection between two hosts, the link connecting the two hosts is actually the telephone network—a logically separate, global telecommunications network with its own switches, links, and protocol stacks for data transfer and signaling.
From the Internet link-layer point ofview, however, the dial-up connection through the telephone network is viewed as a simple “wire.”
Inthis sense, the Internet virtualizes the telephone network, viewing the telephone network as a link-layertechnology providing link-layer connectivity between two Internet hosts.
You may recall from our discussion of overlay networks in Chapter 2 that an overlay network similarly views the Internet as a means for providing connectivity between overlay nodes, seeking to overlay the Internet in the same way that the Internet overlays the telephone network.
In this section, we’ll consider Multiprotocol Label Switching (MPLS) networks.
Unlike the circuit-switched telephone network, MPLS is a packet-switched, virtual-circuit network in its own right.
It has its own packet formats and forwarding behaviors.
Thus, from a pedagogical viewpoint, a discussion of MPLS fitswell into a study of either the network layer or the link layer.
From an Internet viewpoint, however, wecan consider MPLS, like the telephone network and switched- ­Ethernets, as a link-layer technology that serves to interconnect IP devices.
Thus, we’ll consider MPLS in our discussion of the link layer.
Frame-relay and ATM networks can also be used to interconnect IP devices, though they represent a slightly older (but still deployed) technology and will not be covered here; see the very readable book [Goralski 1999]  for details.
Our treatment of MPLS will be necessarily brief, as entire books could be (and have been) written on these networks.
We recommend [Davie 2000]  for details on MPLS.
We’ll focus here primarily on how MPLS ­servers interconnect to IP devices, although we’ll dive a bit deeper into the underlying technologies as well.
6.5.1 Multiprotocol Label Switching (MPLS) Multiprotocol Label Switching (MPLS) evolved from a number of industry efforts in the mid-to-late 1990s to improve the forwarding speed of IP routers by adopting a key concept from the world of virtual-circuitnetworks: a fixed-length label.
The goal was not to abandon the destination-based IP datagram-forwarding infrastructure for one based on fixed-length labels and virtual circuits, but to augment it byselectively labeling datagrams and allowing routers to forward datagrams based on fixed-length labels (rather than destination IP addresses) when possible.
Importantly, these techniques work hand-in-hand with IP, using IP addressing and routing.
The IETF unified these efforts in the MPLS protocol [RFC 3031 , RFC 3032] , effectively blending VC techniques into a routed datagram network.
Let’s begin our study of MPLS by considering the format of a link-layer frame that is handled by anMPLS-capable router.
Figure 6.28 shows that a link-layer frame transmitted between MPLS-capable devices has a small MPLS header added between the layer-2 (e.g., Ethernet) header and layer-3 (i.e., IP) header.
RFC 3032 defines the format of the MPLS header for such links; headers are defined for ATM and frame-relayed networks as well in other RFCs.
Among the fields in the MPLS Figure 6.28 MPLS header: Located between link- and network-layer headers header are the label, 3 bits reserved for experimental use, a single S bit, which is used to indicate theend of a series of “stacked” MPLS headers (an advanced topic that we’ll not cover here), and a time-to- live field.
It’s immediately evident from Figure 6.28 that an MPLS-enhanced frame can only be sent between routers that are both MPLS capable (since a non-MPLS-capable router would be quite confused when it found an MPLS header where it had expected to find the IP header!).
An MPLS-capable router is often referred to as a label-switched router, since it forwards an MPLS frame by looking up the MPLS label in its forwarding table and then immediately passing the datagram to the appropriate output interface.
Thus, the MPLS-capable router need not extract the destination IP address and perform a lookup of the longest prefix match in the forwarding table.
But how does a router know if its neighbor is indeed MPLS capable, and how does a router know what label to associate with the given IP destination?
To answerthese questions, we’ll need to take a look at the interaction among a group of MPLS-capable routers.
In the example in Figure 6.29, routers R1 through R4 are MPLS capable.
R5 and R6 are standard IP routers.
R1 has advertised to R2 and R3 that it (R1) can route to destination A, and that a received frame with MPLS label 6 will be forwarded to destination A. Router R3 has advertised to router R4 that itcan route to destinations A and D, and that incoming frames with MPLS labels 10 and 12, respectively, will be switched toward those destinations.
Router R2 has also advertised to router R4 that it (R2) can reach destination A, and that a received frame with MPLS label 8 will be switched toward A. Note thatrouter R4 is now in the interesting position of having Figure 6.29 MPLS-enhanced forwarding two MPLS paths to reach A: via interface 0 with outbound MPLS label 10, and via interface 1 with an MPLS label of 8.
The broad picture painted in Figure 6.29 is that IP devices R5, R6, A, and D are connected together via an MPLS infrastructure (MPLS-capable routers R1, R2, R3, and R4) in much the same way that a switched LAN or an ATM network can connect together IP devices.
And like a switched LAN or ATM network, the MPLS-capable routers R1 through R4 do so without ever touching the IP header of a packet .
In our discussion above, we’ve not specified the specific protocol used to distribute labels among the MPLS-capable routers, as the details of this signaling are well beyond the scope of this book.
We note, however, that the IETF working group on MPLS has specified in [RFC 3468]  that an extension of the RSVP protocol, known as RSVP-TE [RFC 3209] , will be the focus of its efforts for MPLS signaling.
We’ve also not discussed how MPLS actually computes the paths for packets among MPLS capable routers, nor how it gathers link-state information (e.g., amount of link bandwidth unreserved by MPLS) to
use in these path computations.
Existing link-state routing algorithms (e.g., OSPF) have been extended to flood this information to MPLS-capable routers.
Interestingly, the actual path computation algorithms are not standardized, and are currently vendor-specific.
Thus far, the emphasis of our discussion of MPLS has been on the fact that MPLS performs switching based on labels, without needing to consider the IP address of a packet.
The true advantages of MPLS and the reason for current interest in MPLS, however, lie not in the potential increases in switching speeds, but rather in the new traffic management capabilities that MPLS enables.
As noted above, R4 has two MPLS paths to A. If forwarding were performed up at the IP layer on the basis of IP address, the IP routing protocols we studied in Chapter 5 would specify only a single, least-cost path to A. Thus, MPLS provides the ability to forward packets along routes that would not be possible using standard IP routing protocols.
This is one simple form of traffic engineering  using MPLS [RFC 3346 ; RFC 3272 ; RFC 2702 ; Xiao 2000], in which a network operator can override normal IP routing and force some of the traffic headed toward a given destination along one path, and other traffic destined toward the same destination along another path (whether for policy, performance, or some other reason).
It is also possible to use MPLS for many other purposes as well.
It can be used to perform fast restoration of MPLS forwarding paths, e.g., to reroute traffic over a precomputed failover path in response to link failure [Kar 2000 ; Huang 2002; RFC 3469] .
Finally, we note that MPLS can, and has, been used to implement so-called ­virtual private networks (VPNs).
In implementing a VPN for a customer, an ISP uses its MPLS-enabled network to connect together the customer’s various networks.
MPLS can be used to isolate both the resources and addressing used by the customer’s VPN from that of other users crossing the ISP’s network; see [DeClercq 2002]  for details.
Our discussion of MPLS has been brief, and we encourage you to consult the references we’ve mentioned.
We note that with so many possible uses for MPLS, it appears that it is rapidly becoming theSwiss Army knife of Internet traffic engineering!
6.6 Data Center Networking In recent years, Internet companies such as Google, Microsoft, Facebook, and ­Amazon (as well as their counterparts in Asia and Europe) have built massive data centers, each housing tens to hundreds of thousands of hosts, and concurrently supporting many distinct cloud applications (e.g., search, e-mail,social networking, and e-commerce).
Each data center has its own data center network that interconnects its hosts with each other and interconnects the data center with the Internet.
In this section, we provide a brief introduction to data center networking for cloud applications.
The cost of a large data center is huge, exceeding $12 million per month for a 100,000 host data center [Greenberg 2009a] .
Of these costs, about 45 percent can be attributed to the hosts themselves (which need to be replaced every 3–4 years); 25 percent to infrastructure, including transformers, uninterruptable power supplies (UPS) systems, generators for long-term outages, and cooling systems;15 percent for electric utility costs for the power draw; and 15 percent for networking, including networkgear (switches, routers and load balancers), external links, and transit traffic costs. (
In these percentages, costs for equipment are amortized so that a common cost metric is applied for one-time purchases and ongoing expenses such as power.)
While networking is not the largest cost, networking innovation is the key to reducing overall cost and maximizing performance [Greenberg 2009a] .
The worker bees in a data center are the hosts: They serve content (e.g., Web pages and videos), store e-mails and documents, and collectively perform massively distributed computations (e.g., distributedindex computations for search engines).
The hosts in data centers, called blades and resembling pizza boxes, are generally commodity hosts that include CPU, memory, and disk storage.
The hosts arestacked in racks, with each rack typically having 20 to 40 blades.
At the top of each rack there is a switch, aptly named the Top of Rack (TOR) switch , that interconnects the hosts in the rack with each other and with other switches in the data center.
Specifically, each host in the rack has a network interface card that connects to its TOR switch, and each TOR switch has additional ports that can beconnected to other switches.
Today hosts typically have 40 Gbps Ethernet connections to their TOR switches [Greenberg 2015] .
Each host is also assigned its own data-center-internal IP address.
The data center network supports two types of traffic: traffic flowing between external clients and internal hosts and traffic flowing between internal hosts.
To handle flows between external clients and internal hosts, the data center network includes one or more border routers, connecting the data center network to the public Internet.
The data center network therefore interconnects the racks with each other and connects the racks to the border routers.
Figure 6.30 shows an example of a data center network.
Data center network design, the art of designing the interconnection network and protocols that connect the racks with each other and with the border routers, has become an important branch of
computer networking research in recent years [Al-Fares 2008 ; Greenberg 2009a ; Greenberg 2009b; Mysore 2009 ; Guo 2009; Wang 2010].
Figure 6.30 A data center network with a hierarchical topology Load Balancing A cloud data center, such as a Google or Microsoft data center, provides many applications concurrently, such as search, e-mail, and video applications.
To support requests from external clients,each application is associated with a publicly visible IP address to which clients send their requests andfrom which they receive responses.
Inside the data center, the external requests are first directed to aload balancer whose job it is to distribute requests to the hosts, balancing the load across the hosts asa function of their current load.
A large data center will often have several load balancers, each onedevoted to a set of specific cloud applications.
Such a load balancer is sometimes referred to as a“layer-4 switch” since it makes decisions based on the destination port number (layer 4) as well as destination IP address in the packet.
Upon receiving a request for a particular application, the load balancer forwards it to one of the hosts that handles the application. (
A host may then invoke theservices of other hosts to help process the request.)
When the host finishes processing the request, itsends its response back to the load balancer, which in turn relays the response back to the externalclient.
The load balancer not only balances the work load across hosts, but also provides a NAT-like function, translating the public external IP address to the internal IP address of the appropriate host, and
then translating back for packets traveling in the reverse direction back to the clients.
This prevents clients from contacting hosts directly, which has the security benefit of hiding the internal network structure and preventing clients from directly interacting with the hosts.
Hierarchical Architecture For a small data center housing only a few thousand hosts, a simple network consisting of a border router, a load balancer, and a few tens of racks all interconnected by a single Ethernet switch couldpossibly suffice.
But to scale to tens to hundreds of thousands of hosts, a data center often employs ahierarchy of routers and switches , such as the topology shown in Figure 6.30.
At the top of the hierarchy, the border router connects to access routers (only two are shown in Figure 6.30, but there can be many more).
Below each access router there are three tiers of switches.
Each access routerconnects to a top-tier switch, and each top-tier switch connects to multiple second-tier switches and aload balancer.
Each second-tier switch in turn connects to multiple racks via the racks’ TOR switches(third-tier switches).
All links typically use Ethernet for their link-layer and physical-layer protocols, with a mix of copper and fiber cabling.
With such a hierarchical design, it is possible to scale a data center to hundreds of thousands of hosts.
Because it is critical for a cloud application provider to continually provide applications with high availability, data centers also include redundant network equipment and redundant links in their designs (not shown in Figure 6.30).
For example, each TOR switch can connect to two tier-2 switches, and each access router, tier-1 switch, and tier-2 switch can be duplicated and integrated into the design [Cisco 2012 ; Greenberg 2009b].
In the hierarchical design in Figure 6.30, observe that the hosts below each access router form a single subnet.
In order to localize ARP broadcast traffic, each of these subnets isfurther partitioned into smaller VLAN subnets, each comprising a few hundred hosts [Greenberg 2009a] .
Although the conventional hierarchical architecture just described solves the problem of scale, it suffersfrom limited host-to-host capacity  [Greenberg 2009b].
To understand this limitation, consider again Figure 6.30, and suppose each host connects to its TOR switch with a 1 Gbps link, whereas the links between switches are 10 Gbps Ethernet links.
Two hosts in the same rack can always communicate at a full 1 Gbps, limited only by the rate of the hosts’ network interface cards.
However, if there are many simultaneous flows in the data center network, the maximum rate between two hosts in different racks can be much less.
To gain insight into this issue, consider a traffic pattern consisting of 40 simultaneous flows between 40 pairs of hosts in different racks.
Specifically, suppose each of 10 hosts in rack 1 in Figure 6.30 sends a flow to a corresponding host in rack 5.
Similarly, there are ten simultaneous flows between pairs of hosts in racks 2 and 6, ten simultaneous flows between racks 3 and 7, and ten simultaneous flows between racks 4 and 8.
If each flow evenly shares a link’s capacity with other flows traversing that link, then the 40 flows crossing the 10 Gbps A-to-B link (as well as the 10 Gbps B-to-Clink) will each only receive  which is significantly less than the 1 Gbps network 10 Gbps/40=250 Mbps,
interface card rate.
The problem becomes even more acute for flows between hosts that need to travel higher up the hierarchy.
One possible solution to this limitation is to deploy higher-rate switches and routers.
But this would significantly increase the cost of the data center, because switches and routerswith high port speeds are very expensive.
Supporting high-bandwidth host-to-host communication is important because a key requirement in data centers is flexibility in placement of computation and services [Greenberg 2009b; Farrington 2010].
For example, a large-scale Internet search engine may run on thousands of hosts spread across multiple racks with significant bandwidth requirements between all pairs of hosts.
Similarly, a cloudcomputing service such as EC2 may wish to place the multiple virtual machines comprising acustomer’s service on the physical hosts with the most capacity irrespective of their location in the datacenter.
If these physical hosts are spread across multiple racks, network bottlenecks as describedabove may result in poor performance.
Trends in Data Center Networking In order to reduce the cost of data centers, and at the same time improve their delay and throughput performance, Internet cloud giants such as Google, Facebook, ­Amazon, and Microsoft are continually deploying new data center network designs.
Although these designs are proprietary, many importanttrends can nevertheless be identified.
One such trend is to deploy new interconnection architectures and network protocols that overcome the drawbacks of the traditional hierarchical designs.
One such approach is to replace the hierarchy of switches and routers with a fully connected topology  [Facebook 2014; Al-Fares 2008 ; Greenberg 2009b; Guo 2009], such as the topology shown in Figure 6.31.
In this design, each tier-1 switch connects to all of the tier-2 switches so that (1) host-to-host traffic never has to rise above the switchtiers, and (2) with n tier-1 switches, between any two tier-2 switches there are n disjoint paths.
Such a design can significantly improve the host-to-host capacity.
To see this, consider again our example of 40flows.
The topology in Figure 6.31 can handle such a flow pattern since there are four distinct paths between the first tier-2 switch and the second tier-2 switch, together providing an aggregate capacity of 40 Gbps between the first two tier-2 switches.
Such a design not only alleviates the host-to-hostcapacity limitation, but also creates a more flexible computation and service environment in which communication between any two racks not connected to the same switch is logically equivalent, irrespective of their locations in the data center.
Another major trend is to employ shipping container–based modular data centers (MDCs) [YouTube 2009 ; Waldrop 2007].
In an MDC, a factory builds, within a
Figure 6.31 Highly interconnected data network topology standard 12-meter shipping container, a “mini data center” and ships the container to the data center location.
Each container has up to a few thousand hosts, stacked in tens of racks, which are packed closely together.
At the data center location, multiple containers are interconnected with each other andalso with the Internet.
Once a prefabricated container is deployed at a data center, it is often difficult toservice.
Thus, each container is designed for graceful performance degradation: as components(servers and switches) fail over time, the container continues to operate but with degraded performance.
When many components have failed and performance has dropped below a threshold, the entirecontainer is removed and replaced with a fresh one.
Building a data center out of containers creates new networking challenges.
With an MDC, there are two types of networks: the container-internal networks within each of the containers and the core network connecting each container [Guo 2009; Farrington 2010].
Within each container, at the scale of up to a few thousand hosts, it is possible to build a fully connected network (as described above) using inexpensive commodity Gigabit Ethernet switches.
However, the design of the core network,interconnecting hundreds to thousands of containers while providing high host-to-host bandwidth acrosscontainers for typical workloads, remains a challenging problem.
A hybrid electrical/optical switch architecture for interconnecting the containers is proposed in [Farrington 2010].
When using highly interconnected topologies, one of the major issues is designing routing algorithmsamong the switches.
One possibility [Greenberg 2009b] is to use a form of random routing.
Another possibility [Guo 2009] is to deploy multiple network interface cards in each host, connect each host to multiple low-cost commodity switches, and allow the hosts themselves to intelligently route traffic among the switches.
Variations and extensions of these approaches are currently being deployed incontemporary data centers.
Another important trend is that large cloud providers are increasingly building or customizing just about everything that is in their data centers, including network adapters, switches routers, TORs, software,
and networking protocols [Greenberg 2015 , Singh 2015].
Another trend, pioneered by Amazon, is to improve reliability with “availability zones,” which essentially replicate distinct data centers in different nearby buildings.
By having the buildings nearby (a few kilometers apart), transactional data can besynchronized across the data centers in the same availability zone while providing fault tolerance [Amazon 2014] .
Many more innovations in data center design are likely to continue to come; interested readers are encouraged to see the recent papers and videos on data center network design.
6.7 Retrospective: A Day in the Life of a Web Page Request Now that we’ve covered the link layer in this chapter, and the network, transport and application layers in earlier chapters, our journey down the protocol stack is complete!
In the very beginning of this book (Section 1.1), we wrote “much of this book is concerned with computer network protocols,” and in the first five chapters, we’ve certainly seen that this is indeed the case!
Before heading into the topical chapters in second part of this book, we’d like to wrap up our journey down the protocol stack by taking an integrated, holistic view of the protocols we’ve learned about so far.
One way then to take this “big picture” view is to identify the many (many!)
protocols that are involved in satisfying even the simplest request: downloading a Web page.
Figure 6.32 illustrates our setting: a student, Bob, connects a laptop to his school’s Ethernet switch and downloads a Web page (say the home page of www.google.com ).
As we now know, there’s a lot going on “under the hood” to satisfy this seemingly simple request.
A Wireshark lab at the end of this chapter examines trace files containing a number of the packets involved in similar scenarios in more detail.
6.7.1 Getting Started: DHCP, UDP, IP, and Ethernet Let’s suppose that Bob boots up his laptop and then connects it to an Ethernet cable connected to the school’s Ethernet switch, which in turn is connected to the school’s router, as shown in Figure 6.32.
The school’s router is connected to an ISP, in this example, comcast.net.
In this example, comcast.net is providing the DNS service for the school; thus, the DNS server resides in the Comcast network ratherthan the school network.
We’ll assume that the DHCP server is running within the router, as is often the case.
When Bob first connects his laptop to the network, he can’t do anything (e.g., download a Web page) without an IP address.
Thus, the first network-related
Figure 6.32 A day in the life of a Web page request: Network setting and actions action taken by Bob’s laptop is to run the DHCP protocol to obtain an IP address, as well as other information, from the local DHCP server: 1.
The operating system on Bob’s laptop creates a DHCP request message  ­(Section 4.3.3) and puts this message within a UDP segment  (Section 3.3) with destination port 67 (DHCP server) and source port 68 (DHCP client).
The UDP segment is then placed within an IP datagram (Section 4.3.1) with a broadcast IP destination address (255.255.255.255) and a source IP address of 0.0.0.0, since Bob’s laptop doesn’t yet have an IP address.
2.
The IP datagram containing the DHCP request message is then placed within an Ethernet frame  (Section 6.4.2).
The Ethernet frame has a destination MAC addresses of FF:FF:FF:FF:FF:FF so that the frame will be broadcast to all devices connected to the switch (hopefully including a DHCP server); the frame’s source MAC address is that of Bob’s laptop, 00:16:D3:23:68:8A. 3.
The broadcast Ethernet frame containing the DHCP request is the first frame sent by Bob’s laptop to the Ethernet switch.
The switch broadcasts the incoming frame on all outgoing ports, including the port connected to the router.
4.
The router receives the broadcast Ethernet frame containing the DHCP request on its interface with MAC address 00:22:6B:45:1F:1B and the IP datagram is extracted from the Ethernet frame.
The datagram’s broadcast IP destination address indicates that this IP datagram should beprocessed by upper layer protocols at this node, so the datagram’s payload (a UDP segment) is
thus demultiplexed  (Section 3.2) up to UDP, and the DHCP request message is extracted from the UDP segment.
The DHCP server now has the DHCP request message.
5.
Let’s suppose that the DHCP server running within the router can allocate IP addresses in the CIDR  (Section 4.3.3) block 68.85.2.0/24.
In this example, all IP addresses used within the school are thus within Comcast’s address block.
Let’s suppose the DHCP server allocates address 68.85.2.101 to Bob’s laptop.
The DHCP server creates a DHCP ACK message (Section 4.3.3) containing this IP address, as well as the IP address of the DNS server (68.87.71.226), the IP address for the default gateway router (68.85.2.1), and the subnet block (68.85.2.0/24) (equivalently, the “network mask”).
The DHCP message is put inside a UDPsegment, which is put inside an IP datagram, which is put inside an Ethernet frame.
TheEthernet frame has a source MAC address of the router’s interface to the home network(00:22:6B:45:1F:1B) and a destination MAC address of Bob’s laptop (00:16:D3:23:68:8A).
6.
The Ethernet frame containing the DHCP ACK is sent (unicast) by the router to the switch.
Because the switch is self-learning  (Section 6.4.3) and previously received an Ethernet frame (containing the DHCP request) from Bob’s laptop, the switch knows to forward a frameaddressed to 00:16:D3:23:68:8A only to the output port leading to Bob’s laptop.
7.
Bob’s laptop receives the Ethernet frame containing the DHCP ACK, extracts the IP datagram from the Ethernet frame, extracts the UDP segment from the IP datagram, and extracts the DHCP ACK message from the UDP segment.
Bob’s DHCP client then records its IP addressand the IP address of its DNS server.
It also installs the address of the default gateway into its IP forwarding table  (Section 4.1).
Bob’s laptop will send all datagrams with destination address outside of its subnet 68.85.2.0/24 to the default gateway.
At this point, Bob’s laptop hasinitialized its networking components and is ready to begin processing the Web page fetch. (
Note that only the last two DHCP steps of the four presented in Chapter 4 are actually necessary.)
6.7.2 Still Getting Started: DNS and ARP When Bob types the URL for www.google.com  into his Web browser, he begins the long chain of events that will eventually result in Google’s home page being displayed by his Web browser.
Bob’sWeb browser begins the process by creating a TCP socket  (Section 2.7) that will be used to send the HTTP request  (Section 2.2) to www.google.com .
In order to create the socket, Bob’s laptop will need to know the IP address of www.google.com .
We learned in Section 2.5, that the DNS ­protocol  is used to provide this name-to-IP-address translation service.
8.
The operating system on Bob’s laptop thus creates a DNS query message  (Section 2.5.3), putting the string “www.google.com ” in the question section of the DNS message.
This DNS message is then placed within a UDP segment with a destination port of 53 (DNS server).
The UDP segment is then placed within an IP datagram with an IP destination address of
68.87.71.226 (the address of the DNS server returned in the DHCP ACK in step 5) and a source IP address of 68.85.2.101.
9.
Bob’s laptop then places the datagram containing the DNS query message in an Ethernet frame.
This frame will be sent (addressed, at the link layer) to the gateway router in Bob’s school’s network.
However, even though Bob’s laptop knows the IP address of the school’s gatewayrouter (68.85.2.1) via the DHCP ACK message in step 5 above, it doesn’t know the gateway router’s MAC address.
In order to obtain the MAC address of the gateway router, Bob’s ­laptop will need to use the ARP protocol  (Section 6.4.1).
10.
Bob’s laptop creates an ARP query  message with a target IP address of 68.85.2.1 (the default gateway), places the ARP message within an Ethernet frame with a broadcast destination address (FF:FF:FF:FF:FF:FF) and sends the Ethernet frame to the switch, which delivers theframe to all connected devices, including the gateway router.
11.
The gateway router receives the frame containing the ARP query message on the interface to the school network, and finds that the target IP address of 68.85.2.1 in the ARP message matches the IP address of its interface.
The gateway router thus prepares an ARP reply, indicating that its MAC address of 00:22:6B:45:1F:1B corresponds to IP address 68.85.2.1.
It places the ARP reply message in an Ethernet frame, with a destination address of 00:16:D3:23:68:8A (Bob’s laptop) and sends the frame to the switch, which delivers the frame toBob’s laptop.
12.
Bob’s laptop receives the frame containing the ARP reply message and extracts the MAC address of the gateway router (00:22:6B:45:1F:1B) from the ARP reply message.
13.
Bob’s laptop can now ( finally!)
address the Ethernet frame containing the DNS query to the gateway router’s MAC address.
Note that the IP datagram in this frame has an IP destination address of 68.87.71.226 (the DNS server), while the frame has a destination address of00:22:6B:45:1F:1B (the gateway router).
Bob’s laptop sends this frame to the switch, which delivers the frame to the gateway router.
6.7.3 Still Getting Started: Intra-Domain Routing to the DNS Server 14.
The gateway router receives the frame and extracts the IP datagram containing the DNS query.
The router looks up the destination address of this datagram (68.87.71.226) and determines from its forwarding table that the datagram should be sent to the leftmost router in the Comcast network in Figure 6.32.
The IP datagram is placed inside a link-layer frame appropriate for the link connecting the school’s router to the leftmost Comcast router and the frame is sent over this link.
15.
The leftmost router in the Comcast network receives the frame, extracts the IP datagram, examines the datagram’s destination address (68.87.71.226) and determines the outgoing interface on which to forward the datagram toward the DNS server from its forwarding table,which has been filled in by ­Comcast’s intra-domain protocol (such as RIP, OSPF or IS-IS,
Section 5.3) as well as the Internet’s inter-domain protocol , BGP  (Section 5.4).
16.
Eventually the IP datagram containing the DNS query arrives at the DNS server.
The DNS server extracts the DNS query message, looks up the name www.google.com  in its DNS database ( Section 2.5), and finds the DNS resource record  that contains the IP address (64.233.169.105) for www.google.com . (
assuming that it is currently cached in the DNS server).
Recall that this cached data originated in the authoritative DNS server  (Section 2.5.2) for googlecom.
The DNS server forms a DNS reply message  containing this hostname-to-IP- address mapping, and places the DNS reply message in a UDP segment, and the segmentwithin an IP datagram addressed to Bob’s laptop (68.85.2.101).
This datagram will be forwardedback through the Comcast network to the school’s router and from there, via the Ethernet switchto Bob’s laptop.
17.
Bob’s laptop extracts the IP address of the server www.google.com  from the DNS message.
Finally , after a lot of work, Bob’s laptop is now ready to contact the www.google.com  server!
6.7.4 Web Client-Server Interaction: TCP and HTTP 18.
Now that Bob’s laptop has the IP address of www.google.com , it can create the TCP socket (Section 2.7) that will be used to send the HTTP GET  message (Section 2.2.3) to www.google.com .
When Bob creates the TCP socket, the TCP in Bob’s laptop must first perform a three-way handshake  (Section 3.5.6) with the TCP in www.google.com .
Bob’s laptop thus first creates a TCP SYN segment with destination port 80 (for HTTP), places theTCP segment inside an IP datagram with a destination IP address of 64.233.169.105 (www.google.com ), places the datagram inside a frame with a destination MAC address of 00:22:6B:45:1F:1B (the gateway router) and sends the frame to the switch.
19.
The routers in the school network, Comcast’s network, and Google’s network forward the datagram containing the TCP SYN toward www.google.com , using the forwarding table in each router, as in steps 14–16 above.
Recall that the router forwarding table entries governingforwarding of packets over the inter-domain link between the Comcast and Google networks are determined by the BGP  protocol (Chapter 5).
20.
Eventually, the datagram containing the TCP SYN arrives at www.google.com .
The TCP SYN message is extracted from the datagram and demultiplexed to the welcome socket associatedwith port 80.
A connection socket ( Section 2.7) is created for the TCP connection between the Google HTTP server and Bob’s laptop.
A TCP SYNACK ( Section 3.5.6) segment is generated, placed inside a datagram addressed to Bob’s laptop, and finally placed inside a link-layer frameappropriate for the link connecting www.google.com  to its first-hop router.
21.
The datagram containing the TCP SYNACK segment is forwarded through the Google, Comcast, and school networks, eventually arriving at the Ethernet card in Bob’s laptop.
The datagram is demultiplexed within the operating system to the TCP socket created in step 18,which enters the connected state.
22.
With the socket on Bob’s laptop now ( finally!)
ready to send bytes to www.google.com , Bob’s browser creates the HTTP GET message (Section 2.2.3) containing the URL to be fetched.
The HTTP GET message is then written into the socket, with the GET message becoming the payload of a TCP segment.
The TCP segment is placed in a datagram and sent and delivered to www.google.com  as in steps 18–20 above.
23.
The HTTP server at www.google.com  reads the HTTP GET message from the TCP socket, creates an HTTP response  message (Section 2.2), places the requested Web page content in the body of the HTTP response message, and sends the message into the TCP socket.
24.
The datagram containing the HTTP reply message is forwarded through the Google, Comcast, and school networks, and arrives at Bob’s laptop.
Bob’s Web browser program reads the HTTP response from the socket, extracts the html for the Web page from the body of the HTTP response, and finally ( finally!)
displays the Web page!
Our scenario above has covered a lot of networking ground!
If you’ve understood most or all of theabove example, then you’ve also covered a lot of ground since you first read Section 1.1, where we wrote “much of this book is concerned with computer network protocols” and you may have wondered what a protocol actually was!
As detailed as the above example might seem, we’ve omitted a number ofpossible additional protocols (e.g., NAT running in the school’s gateway router, wireless access to theschool’s network, security protocols for accessing the school network or encrypting segments ordatagrams, network management protocols), and considerations (Web caching, the DNS hierarchy) thatone would encounter in the public ­Internet.
We’ll cover a number of these topics and more in the second part of this book.
Lastly, we note that our example above was an integrated and holistic, but also very “nuts and bolts,” view of many of the protocols that we’ve studied in the first part of this book.
The example focused moreon the “how” than the “why.”
For a broader, more reflective view on the design of network protocols in general, see [Clark 1988 , RFC 5218] .
6.8 Summary In this chapter, we’ve examined the link layer—its services, the principles underlying its operation, and a number of important specific protocols that use these principles in implementing link-layer services.
We saw that the basic service of the link layer is to move a network-layer datagram from one node (host, switch, router, WiFi access point) to an adjacent node.
We saw that all link-layer protocols operate by encapsulating a network-layer datagram within a link-layer frame before transmitting the frame over the link to the adjacent node.
Beyond this common framing function, however, we learned that differentlink-layer protocols provide very different link access, delivery, and transmission services.
Thesedifferences are due in part to the wide variety of link types over which link-layer protocols must operate.
A simple point-to-point link has a single sender and receiver communicating over a single “wire.”
Amultiple access link is shared among many senders and receivers; consequently, the link-layer protocolfor a multiple access channel has a protocol (its multiple access protocol) for coordinating link access.
Inthe case of MPLS, the “link” connecting two adjacent nodes (for example, two IP routers that are adjacent in an IP sense—that they are next-hop IP routers toward some destination) may actually be a network in and of itself.
In one sense, the idea of a network being considered as a link should not seem odd.
A telephone link connecting a home modem/computer to a remote modem/router, for example, is actually a path through a sophisticated and complex telephone network.
Among the principles underlying link-layer communication, we examined error-detection and -correction techniques, multiple access protocols, link-layer addressing, virtualization (VLANs), and the constructionof extended switched LANs and data center networks.
Much of the focus today at the link layer is onthese switched networks.
In the case of error detection/correction, we examined how it is possible to add additional bits to a frame’s header in order to detect, and in some cases correct, bit-flip errors that might occur when the frame is transmitted over the link.
We covered simple parity and checksummingschemes, as well as the more robust cyclic redundancy check.
We then moved on to the topic ofmultiple access protocols.
We identified and studied three broad approaches for coordinating access toa broadcast channel: channel partitioning approaches (TDM, FDM), random access approaches (theALOHA protocols and CSMA protocols), and taking-turns approaches (polling and token passing).
Westudied the cable access network and found that it uses many of these multiple access methods.
Wesaw that a consequence of having multiple nodes share a single broadcast channel was the need to provide node addresses at the link layer.
We learned that link-layer addresses were quite different from network-layer addresses and that, in the case of the Internet, a special protocol (ARP—the AddressResolution Protocol) is used to translate between these two forms of addressing and studied the hugelysuccessful Ethernet protocol in detail.
We then examined how nodes sharing a broadcast channel form
a LAN and how multiple LANs can be connected together to form larger LANs—all without the intervention of network-layer routing to interconnect these local nodes.
We also learned how ­multiple virtual LANs can be created on a single physical LAN infrastructure.
We ended our study of the link layer by focusing on how MPLS networks provide link-layer services when they interconnect IP routers and an overview of the network designs for today’s massive data centers.
We wrapped up this chapter (and indeed the first five chapters) by identifying the many protocols that are needed to fetch a simple Web page.
Having covered the link layer, our journey down the protocol stack is now ove r!
Certainly, the physical layer lies below the link layer, but the details of the physical layer are probably best left for another course (for example, in communication theory, rather than computer networking).
We have, however, touched upon several aspects of the physical layer in this chapter and in Chapter 1 (our discussion of physical media in Section 1.2).
We’ll consider the physical layer again when we study wireless link characteristics in the next chapter.
Although our journey down the protocol stack is over, our study of computer networking is not yet at an end.
In the following three chapters we cover wireless networking, network security, and multimedianetworking.
These four topics do not fit conveniently into any one layer; indeed, each topic crosscutsmany layers.
Understanding these topics (billed as advanced topics in some networking texts) thusrequires a firm foundation in all layers of the protocol stack—a foundation that our study of the link layerhas now completed!
Homework Problems and Questions Chapter 6 Review Questions SECTIONS 6.1–6.2 SECTION 6.3 SECTION 6.4R1.
Consider the transportation analogy in Section 6.1.1 .
If the passenger is analagous to a datagram, what is analogous to the link layer frame?
R2.
If all the links in the Internet were to provide reliable delivery service, would the TCP reliable delivery service be redundant?
Why or why not?
R3.
What are some of the possible services that a link-layer protocol can offer to the network layer?
Which of these link-layer services have corresponding services in IP?
In TCP?
R4.
Suppose two nodes start to transmit at the same time a packet of length L over a broadcast channel of rate R. Denote the propagation delay between the two nodes as d. Will there be a collision if ?
Why or why not?
R5.
In Section 6.3 , we listed four desirable characteristics of a broadcast channel.
Which of these characteristics does slotted ALOHA have?
Which of these characteristics does token passing have?
R6.
In CSMA/CD, after the fifth collision, what is the probability that a node chooses ?
The result  corresponds to a delay of how many ­seconds on a 10 Mbps Ethernet?
R7.
Describe polling and token-passing protocols using the analogy of cocktail party interactions.
R8.
Why would the token-ring protocol be inefficient if a LAN had a very large perimeter?prop dprop<L/R K=4 K=4 R9.
How big is the MAC address space?
The IPv4 address space?
The IPv6 address space?R10.
Suppose nodes A, B, and C each attach to the same broadcast LAN (through their adapters).
If A sends thousands of IP datagrams to B with each encapsulating frame addressed to the MAC address of B, will C’s adapter process these frames?
If so, will C’s adapter pass the IP datagrams in these frames to the network layer C?
How would your answers change if Asends frames with the MAC broadcast address?
R11.
Why is an ARP query sent within a broadcast frame?
Why is an ARP response sent within
Problemsa frame with a specific destination MAC address?
R12.
For the network in Figure 6.19 , the router has two ARP modules, each with its own ARP table.
Is it possible that the same MAC address appears in both tables?
R13.
Compare the frame structures for 10BASE-T, 100BASE-T, and Gigabit ­Ethernet.
How do they differ?
R14.
Consider Figure 6.15 .
How many subnetworks are there, in the addressing sense of Section 4.3 ?
R15.
What is the maximum number of VLANs that can be configured on a switch supporting the 802.1Q protocol?
Why?
R16.
Suppose that N switches supporting K VLAN groups are to be connected via a trunking protocol.
How many ports are needed to connect the switches?
Justify your answer.
P1.
Suppose the information content of a packet is the bit pattern 1110 0110 1001 1101 and an even parity scheme is being used.
What would the value of the field containing the parity bits be for the case of a two-dimensional parity scheme?
Your answer should be such that a minimum-length checksum field is used.
P2.
Show (give an example other than the one in Figure 6.5 ) that two-dimensional parity checks can correct and detect a single bit error.
Show (give an example of) a double-bit error that can be detected but not corrected.
P3.
Suppose the information portion of a packet ( D in Figure 6.3 ) contains 10 bytes consisting of the 8-bit unsigned binary ASCII representation of string “Networking.”
Compute the Internet checksum for this data.
P4.
Consider the previous problem, but instead suppose these 10 bytes contain a. the binary representation of the numbers 1 through 10.
b. the ASCII representation of the letters B through K (uppercase).
c. the ASCII representation of the letters b through k (lowercase).
Compute the Internet checksum for this data.
P5.
Consider the 5-bit generator, , and suppose that D has the value 1010101010.
What is the value of R?
P6.
Consider the previous problem, but suppose that D has the value a. 1001010101.
b. 0101101010.
c. 1010100000.
P7.
In this problem, we explore some of the properties of the CRC.
For the ­generator  given in Section 6.2.3 , answer the following questions.
G=10011 G(=1001 )
a. Why can it detect any single bit error in data D?
b. Can the above G detect any odd number of bit errors?
Why?
P8.
In Section 6.3 , we provided an outline of the derivation of the efficiency of slotted ALOHA.
In this problem we’ll complete the derivation.
a. Recall that when there are N active nodes, the efficiency of slotted ALOHA is .
Find the value of p that maximizes this expression.
b. Using the value of p found in (a), find the efficiency of slotted ALOHA by letting N approach infinity.
Hint:  approaches 1/ e as N approaches infinity.
P9.
Show that the maximum efficiency of pure ALOHA is 1/(2 e).
Note: This problem is easy if you have completed the problem above!
P 10.
Consider two nodes, A and B, that use the slotted ALOHA protocol to contend for a channel.
Suppose node A has more data to transmit than node B, and node A’s retransmission probability p is greater than node B’s retransmission probability, p. a. Provide a formula for node A’s average throughput.
What is the total efficiency of the protocol with these two nodes?
b. If  is node A’s average throughput twice as large as that of node B?
Why or why not?
If not, how can you choose p  and p  to make that happen?
c. In general, suppose there are N nodes, among which node A has retransmission probability 2p and all other nodes have retransmission probability p. Provide expressions to compute the average throughputs of node A and of any other node.
P11.
Suppose four active nodes—nodes A, B, C and D—are competing for access to a channel using slotted ALOHA.
Assume each node has an infinite number of packets to send.
Each node attempts to transmit in each slot with probability p. The first slot is numbered slot 1, the second slot is numbered slot 2, and so on.
a. What is the probability that node A succeeds for the first time in slot 5?
b. What is the probability that some node (either A, B, C or D) succeeds in slot 4?
c. What is the probability that the first success occurs in slot 3?
d. What is the efficiency of this four-node system?
P12.
Graph the efficiency of slotted ALOHA and pure ALOHA as a function of p for the following values of N: a. .
b. .
c. .
P13.
Consider a broadcast channel with N nodes and a transmission rate of R bps.
Suppose the broadcast channel uses polling (with an additional polling node) for multiple access.
Suppose theNp(1−p)N−1 (1−1/N)N A B pA=2pB, A B N=15 N=25N=35
amount of time from when a node completes transmission until the subsequent node is permitted to transmit (that is, the polling delay) is d. Suppose that within a polling round, a given node is allowed to transmit at most Q bits.
What is the maximum throughput of the broadcast channel?
P14.
Consider three LANs interconnected by two routers, as shown in Figure 6.33 .
a. Assign IP addresses to all of the interfaces.
For Subnet 1 use addresses of the form 192.168.1.xxx; for Subnet 2 uses addresses of the form 192.168.2.xxx; and for Subnet 3 use addresses of the form 192.168.3.xxx.
b. Assign MAC addresses to all of the adapters.
c. Consider sending an IP datagram from Host E to Host B. Suppose all of the ARP tablesare up to date.
Enumerate all the steps, as done for the single-router example in Section 6.4.1 .
d. Repeat (c), now assuming that the ARP table in the sending host is empty (and the other tables are up to date).
P15.
Consider Figure 6.33 .
Now we replace the router between subnets 1 and 2 with a switch S1, and label the router between subnets 2 and 3 as R1.
Figure 6.33 Three subnets, interconnected by routers a. Consider sending an IP datagram from Host E to Host F. Will Host E ask router R1 tohelp forward the datagram?
Why?
In the Ethernet frame containing the IP datagram, what are the source and destination IP and MAC addresses?
b. Suppose E would like to send an IP datagram to B, and assume that E’s ARP cache does not contain B’s MAC address.
Will E perform an ARP query to find B’s MACpoll
address?
Why?
In the Ethernet frame (containing the IP datagram destined to B) that is delivered to router R1, what are the source and destination IP and MAC addresses?
c. Suppose Host A would like to send an IP datagram to Host B, and neither A’s ARP cache contains B’s MAC address nor does B’s ARP cache contain A’s MAC address.
Further suppose that the switch S1’s forwarding table contains entries for Host B and router R1only.
Thus, A will broadcast an ARP request message.
What actions will switch S1 perform once it receives the ARP request message?
Will router R1 also receive this ARP request message?
If so, will R1 forward the message to Subnet 3?
Once Host B receivesthis ARP request message, it will send back to Host A an ARP response message.
Butwill it send an ARP query message to ask for A’s MAC address?
Why?
What will switchS1 do once it receives an ARP response message from Host B?
P16.
Consider the previous problem, but suppose now that the router between subnets 2 and 3 is replaced by a switch.
Answer questions (a)–(c) in the previous problem in this new context.
P17.
Recall that with the CSMA/CD protocol, the adapter waits  bit times after a collision, where K is drawn randomly.
For , how long does the adapter wait until returning to Step 2 for a 10 Mbps broadcast channel?
For a 100 Mbps broadcast channel?P18.
Suppose nodes A and B are on the same 10 Mbps broadcast channel, and the propagation delay between the two nodes is 325 bit times.
Suppose CSMA/CD and Ethernet packets are used for this broadcast channel.
Suppose node A begins transmitting a frame and, before itfinishes, node B begins transmitting a frame.
Can A finish transmitting before it detects that Bhas transmitted?
Why or why not?
If the answer is yes, then A incorrectly believes that its frame was successfully transmitted without a collision.
Hint: Suppose at time  bits, A begins transmitting a frame.
In the worst case, A transmits a minimum-sized frame of  bit times.
So A would finish transmitting the frame at  bit times.
Thus, the answer is no, if B’s signal reaches A before bit time  bits.
In the worst case, when does B’s signal reach A?
P19.
Suppose nodes A and B are on the same 10 Mbps broadcast channel, and the propagation delay between the two nodes is 245 bit times.
Suppose A and B send Ethernet frames at the same time, the frames collide, and then A and B choose different values of K in the CSMA/CD algorithm.
Assuming no other nodes are active, can the retransmissions from A and B collide?
For our purposes, it suffices to work out the following example.
Suppose A and B begintransmission at  bit times.
They both detect collisions at  t bit times.
Suppose  and .
At what time does B schedule its retransmission?
At what time does A begin transmission? (
Note: The nodes must wait for an idle channel after returning to Step 2—see protocol.)
At what time does A’s signal reach B?
Does B refrain from transmitting at itsscheduled time?
P20.
In this problem, you will derive the efficiency of a CSMA/CD-like multiple access protocol.
In this protocol, time is slotted and all adapters are synchronized to the slots.
Unlike slotted ALOHA, however, the length of a slot (in seconds) is much less than a frame time (the time to transmit a frame).
Let S be the length of a slot.
Suppose all frames are of constant lengthK⋅512 K=100 t=0 512+64 t=512+64 t=512+64 t=0 t=245 KA=0 KB=1
, where R is the transmission rate of the channel and k is a large integer.
Suppose there are N nodes, each with an infinite number of frames to send.
We also assume that , so that all nodes can detect a collision before the end of a slot time.
The protocol is as follows: If, for a given slot, no node has possession of the channel, all nodes contend for the channel; in particular, each node transmits in the slot with probability p. If exactly one node transmits in the slot, that node takes possession of the channel for the subsequent  slots and transmits its entire frame.
If some node has possession of the channel, all other nodes refrain from transmitting until the node that possesses the channel has finished transmitting its frame.
Once this node has transmitted its frame, all nodes contend for the channel.
Note that the channel alternates between two states: the productive state, which lasts exactly k slots, and the nonproductive state, which lasts for a random number of slots.
Clearly, the channel efficiency is the ratio of , where x is the expected number of consecutive unproductive slots.
a. For fixed N and p, determine the efficiency of this protocol.
b. For fixed N, determine the p that maximizes the efficiency.
c. Using the p (which is a function of N) found in (b), determine the efficiency as N approaches infinity.
d. Show that this efficiency approaches 1 as the frame length becomes large.
P21.
Consider Figure 6.33 in problem P14.
Provide MAC addresses and IP addresses for the interfaces at Host A, both routers, and Host F. Suppose Host A sends a datagram to Host F. Give the source and destination MAC addresses in the frame encapsulating this IP datagram as the frame is transmitted (i) from A to the left router, (ii) from the left router to the right router, (iii) from the right router to F. Also give the source and destination IP addresses in the IP datagram encapsulated within the frame at each of these points in time.
P22.
Suppose now that the leftmost router in Figure 6.33 is replaced by a switch.
Hosts A, B, C, and D and the right router are all star-connected into this switch.
Give the source and destinationMAC addresses in the frame encapsulating this IP datagram as the frame is transmitted (i) from A to the switch, (ii) from the switch to the right router, (iii)  from the right router to F. Also give the source and destination IP addresses in the IP datagram encapsulated within the frame at each of these points in time.
P23.
Consider Figure 6.15 .
Suppose that all links are 100 Mbps.
What is the maximum total aggregate throughput that can be achieved among the 9 hosts and 2 servers in this network?
You can assume that any host or server can send to any other host or server.
Why?
P24.
Suppose the three departmental switches in Figure 6.15 are replaced by hubs.
All links are 100 Mbps.
Now answer the questions posed in problem P23.P25.
Suppose that all the switches in Figure 6.15 are replaced by hubs.
All links are 100 Mbps.
Now answer the questions posed in problem P23.L=kRS dprop<S k−1 k/(k+x)
P26.
Let’s consider the operation of a learning switch in the context of a network in which 6 nodes labeled A through F are star connected into an Ethernet switch.
Suppose that (i) B sends a frame to E, (ii) E replies with a frame to B, (iii)  A sends a frame to B, (iv) B replies with a frame to A. The switch table is initially empty.
Show the state of the switch table before and after each of these events.
For each of these events, identify the link(s) on which the transmitted frame will be forwarded, and briefly justify your answers.
P27.
In this problem, we explore the use of small packets for Voice-over-IP applications.
One of the drawbacks of a small packet size is that a large fraction of link bandwidth is consumed by overhead bytes.
To this end, suppose that the packet consists of P bytes and 5 bytes of header.
a. Consider sending a digitally encoded voice source directly.
Suppose the source is encoded at a constant rate of 128 kbps.
Assume each packet is entirely filled before the source sends the packet into the network.
The time required to fill a packet is thepacketization delay .
In terms of L, determine the packetization delay in milliseconds.
b. Packetization delays greater than 20 msec can cause a noticeable and unpleasant echo.
Determine the packetization delay for  bytes (roughly corresponding to a maximum-sized Ethernet packet) and for  (corresponding to an ATM packet).
c. Calculate the store-and-forward delay at a single switch for a link rate of  Mbps for  bytes, and for  bytes.
d. Comment on the advantages of using a small packet size.
P28.
Consider the single switch VLAN in Figure 6.25 , and assume an external router is connected to switch port 1.
Assign IP addresses to the EE and CS hosts and router interface.
Trace the steps taken at both the network layer and the link layer to transfer an IP datagram from an EE host to a CS host ( Hint: Reread the discussion of Figure 6.19 in the text).
P29.
Consider the MPLS network shown in Figure 6.29 , and suppose that routers R5 and R6 are now MPLS enabled.
Suppose that we want to perform traffic engineering so that packets from R6 destined for A are switched to A via R6-R4-R3-R1, and packets from R5 destined for Aare switched via R5-R4-R2-R1.
Show the MPLS tables in R5 and R6, as well as the modifiedtable in R4, that would make this possible.
P30.
Consider again the same scenario as in the previous problem, but suppose that packets from R6 destined for D are switched via R6-R4-R3, while packets from R5 destined to D are switched via R4-R2-R1-R3.
Show the MPLS tables in all routers that would make this possible.
P31.
In this problem, you will put together much of what you have learned about Internet protocols.
Suppose you walk into a room, connect to Ethernet, and want to download a Web page.
What are all the protocol steps that take place, starting from powering on your PC togetting the Web page?
Assume there is nothing in our DNS or browser caches when you power on your PC. (
Hint: The steps include the use of Ethernet, DHCP, ARP, DNS, TCP, and HTTP protocols.)
Explicitly indicate in your steps how you obtain the IP and MAC addresses of a gateway router.
P32.
Consider the data center network with hierarchical topology in Figure 6.30 .
Suppose nowL=1,500 L=50 R=622 L=1,500 L=50
Wireshark Labs At the Companion website for this textbook, http:/ /www.pearsonhighered.com/ cs-resources/ , you’ll find a Wireshark lab that examines the operation of the IEEE 802.3 protocol and the Wireshark frame format.
A second Wireshark lab examines packet traces taken in a home network scenario.
AN INTERVIEW WITH… Simon S. Lam Simon S. Lam is Professor and Regents Chair in Computer Sciences at the University of Texasat Austin.
From 1971 to 1974, he was with the ARPA Network Measurement Center at UCLA, where he worked on satellite and radio packet switching.
He led a research group that invented secure sockets and prototyped, in 1993, the first secure sockets layer named Secure NetworkProgramming, which won the 2004 ACM Software System Award.
His research interests are indesign and analysis of network protocols and security services.
He received his BSEE fromthere are 80 pairs of flows, with ten flows between the first and ninth rack, ten flows between the second and tenth rack, and so on.
Further suppose that all links in the network are 10 Gbps,except for the links between hosts and TOR switches, which are 1 Gbps.
a. Each flow has the same data rate; determine the maximum rate of a flow.
b. For the same traffic pattern, determine the maximum rate of a flow for the highlyinterconnected topology in Figure 6.31 .
c. Now suppose there is a similar traffic pattern, but involving 20 hosts on each rack and 160 pairs of flows.
Determine the maximum flow rates for the two topologies.
P33.
Consider the hierarchical network in Figure 6.30 and suppose that the data center needs to support e-mail and video distribution among other applications.
Suppose four racks of servers are reserved for e-mail and four racks are reserved for video.
For each of the applications, allfour racks must lie below a single tier-2 switch since the tier-2 to tier-1 links do not havesufficient bandwidth to support the intra-application traffic.
For the e-mail application, supposethat for 99.9 percent of the time only three racks are used, and that the video application hasidentical usage patterns.
a. For what fraction of time does the e-mail application need to use a fourth rack?
How about for the video application?
b. Assuming e-mail usage and video usage are independent, for what fraction of time do(equivalently, what is the probability that) both applications need their fourth rack?
c. Suppose that it is acceptable for an application to have a shortage of servers for 0.001percent of time or less (causing rare periods of performance degradation for users).
Discuss how the topology in Figure 6.31 can be used so that only seven racks are collectively assigned to the two applications (assuming that the topology can support all the traffic).
Washington State University and his MS and PhD from UCLA.
He was elected to the National Academy of Engineering in 2007.
Why did you decide to specialize in networking?
When I arrived at UCLA as a new graduate student in Fall 1969, my intention was to study control theory.
Then I took the queuing theory classes of Leonard Kleinrock and was veryimpressed by him.
For a while, I was working on adaptive control of queuing systems as apossible thesis topic.
In early 1972, Larry Roberts initiated the ARPAnet Satellite System project (later called Packet Satellite).
Professor Kleinrock asked me to join the project.
The first thing we did was to introduce a simple, yet realistic, backoff algorithm to the slotted ALOHA protocol.
Shortly thereafter, I found many interesting research problems, such as ALOHA’s instabilityproblem and need for adaptive backoff, which would form the core of my thesis.
You were active in the early days of the Internet in the 1970s, beginning with your student days at UCLA.
What was it like then?
Did people have any inkling of what the Internet would become?
The atmosphere was really no different from other system-building projects I have seen in industry and academia.
The initially stated goal of the ARPAnet was fairly modest, that is, toprovide access to expensive computers from remote locations so that many more scientistscould use them.
However, with the startup of the Packet Satellite project in 1972 and the PacketRadio project in 1973, ARPA’s goal had expanded substantially.
By 1973, ARPA was buildingthree different packet networks at the same time, and it became necessary for Vint Cerf and Bob Kahn to develop an interconnection strategy.
Back then, all of these progressive developments in networking were viewed (I believe) as logical rather than magical.
No one could have envisioned the scale of the Internet and power ofpersonal computers today.
It was a decade before appearance of the first PCs.
To put things inperspective, most students submitted their computer programs as decks of punched cards for batch processing.
Only some students had direct access to computers, which were typicallyhoused in a restricted area.
Modems were slow and still a rarity.
As a graduate student, I hadonly a phone on my desk, and I used pencil and paper to do most of my work.
Where do you see the field of networking and the Internet heading in the future?
In the past, the simplicity of the Internet’s IP protocol was its greatest strength in vanquishing competition and becoming the de facto standard for internetworking.
Unlike competitors, such as X.25 in the 1980s and ATM in the 1990s, IP can run on top of any link-layer networking technology, because it offers only a best-effort datagram service.
Thus, any packet network can connect to the Internet.
Today, IP’s greatest strength is actually a shortcoming.
IP is like a straitjacket that confines the Internet’s development to specific directions.
In recent years, many researchers have redirectedtheir efforts to the application layer only.
There is also a great deal of research on wireless adhoc networks, sensor networks, and satellite networks.
These networks can be viewed either asstand-alone systems or link-layer systems, which can flourish because they are outside of the IPstraitjacket.
Many people are excited about the possibility of P2P systems as a platform for novel Internet applications.
However, P2P systems are highly inefficient in their use of Internet resources.
A concern of mine is whether the transmission and switching capacity of the Internet core willcontinue to increase faster than the traffic demand on the Internet as it grows to interconnect allkinds of devices and support future P2P-enabled applications.
Without substantialoverprovisioning of capacity, ensuring network stability in the presence of malicious attacks andcongestion will continue to be a significant challenge.
The Internet’s phenomenal growth also requires the allocation of new IP addresses at a rapid rate to network operators and enterprises worldwide.
At the current rate, the pool of unallocated IPv4 addresses would be depleted in a few years.
When that happens, large contiguous blocks of address space can only be allocated from the IPv6 address space.
Since adoption of IPv6 isoff to a slow start, due to lack of incentives for early adopters, IPv4 and IPv6 will most likely co-exist on the Internet for many years to come.
Successful migration from an IPv4-dominantInternet to an IPv6-dominant Internet will require a substantial global effort.
What is the most challenging part of your job?The most challenging part of my job as a professor is teaching and motivating every student in my class, and every doctoral student under my supervision, rather than just the high achievers.
The very bright and motivated may require a little guidance but not much else.
I often learn more from these students than they learn from me.
Educating and motivating the underachievers present a major challenge.
What impacts do you foresee technology having on learning in the future?
Eventually, almost all human knowledge will be accessible through the Internet, which will be the most powerful tool for learning.
This vast knowledge base will have the potential of leveling the
playing field for students all over the world.
For example, motivated students in any country will be able to access the best-class Web sites, multimedia lectures, and teaching materials.
Already, it was said that the IEEE and ACM digital libraries have accelerated the development ofcomputer science researchers in China.
In time, the Internet will transcend all geographicbarriers to learning.
Chapter 7 Wireless and Mobile Networks In the telephony world, the past 20 years have arguably been the golden years of cellular telephony.
The number of worldwide mobile cellular subscribers increased from 34 million in 1993 to nearly 7.0billion subscribers by 2014, with the number of cellular subscribers now surpassing the number of wiredtelephone lines.
There are now a larger number of mobile phone subscriptions than there are people onour planet.
The many advantages of cell phones are evident to all—anywhere, anytime, untethered access to the global telephone network via a highly portable lightweight device.
More recently, laptops, smartphones, and tablets are wirelessly connected to the Internet via a cellular or WiFi network.
Andincreasingly, devices such as gaming consoles, thermostats, home security systems, home appliances,watches, eye glasses, cars, traffic control systems and more are being wirelessly connected to theInternet.
From a networking standpoint, the challenges posed by networking these wireless and mobile devices, particularly at the link layer and the network layer, are so different from traditional wired computer networks that an individual chapter devoted to the study of wireless and mobile networks (i.e., this chapter) is appropriate.
We’ll begin this chapter with a discussion of mobile users, wireless links, and networks, and their relationship to the larger (typically wired) networks to which they connect.
We’ll draw a distinction between the challenges posed by the ­wireless  nature of the communication links in such networks, and by the mobility  that these wireless links enable.
Making this important distinction—between wireless and mobility—will allow us to better isolate, identify, and master the key concepts in each area.
Note that there are indeed many networked environments in which the network nodes are wireless but not mobile (e.g., wireless home or office networks with stationary workstations and large displays), and that there are limited forms of mobility that do not require wireless links (e.g., a worker who uses a wired laptop at home, shuts down the laptop, drives to work, and attaches the laptop to the company’s wirednetwork).
Of course, many of the most exciting networked environments are those in which users are both wireless and mobile—for example, a scenario in which a mobile user (say in the back seat of car) maintains a Voice-over-IP call and multiple ongoing TCP connections while racing down the autobahn at 160 kilometers per hour, soon in an autonomous vehicle.
It is here, at the intersection of wireless andmobility, that we’ll find the most interesting technical challenges!
We’ll begin by illustrating the setting in which we’ll consider wireless communication and mobility—a network in which wireless (and possibly mobile) users are connected into the larger network infrastructure by a wireless link at the network’s edge.
We’ll then consider the characteristics of this wireless link in Section 7.2.
We include a brief introduction to code division multiple access (CDMA), a shared-medium access protocol that is often used in wireless networks, in Section 7.2.
In Section 7.3, we’ll examine the link-level aspects of the IEEE 802.11 (WiFi) wireless LAN standard in some depth; we’ll also say a few words about Bluetooth and other wireless personal area networks.
In Section 7.4, we’ll provide an overview of cellular Internet access, including 3G and emerging 4G cellular technologies that provide both voice and high-speed Internet access.
In Section 7.5, we’ll turn our attention to mobility, focusing on the problems of locating a mobile user, routing to the mobile user, and “handing off” the mobile user who dynamically moves from one point of attachment to the network toanother.
We’ll examine how these mobility services are implemented in the mobile IP standard in enterprise 802.11 networks, and in LTE cellular networks in Sections 7.6 and 7.7, respectively.
Finally, we’ll consider the impact of wireless links and mobility on transport-layer protocols and networkedapplications in Section 7.8.
7.1 Introduction Figure 7.1 shows the setting in which we’ll consider the topics of wireless data communication and mobility.
We’ll begin by keeping our discussion general enough to cover a wide range of networks, including both wireless LANs such as IEEE 802.11 and cellular networks such as a 4G network; we’lldrill down into a more detailed discussion of specific wireless architectures in later sections.
We can identify the following elements in a wireless network: Wireless hosts.
As in the case of wired networks, hosts are the end-system devices that run applications.
A wireless host might be a laptop, tablet, smartphone, or desktop computer.
The hosts themselves may or may not be mobile.
Figure 7.1 Elements of a wireless network Wireless links.
A host connects to a base station (defined below) or to another wireless hostthrough a wireless communication link .
Different wireless link technologies have different
transmission rates and can transmit over different distances.
Figure 7.2 shows two key characteristics (coverage area and link rate) of the more popular wireless network standards. (
The figure is only meant to provide a rough idea of these characteristics.
For example, some of thesetypes of networks are only now being deployed, and some link rates can increase or decreasebeyond the values shown depending on distance, channel conditions, and the number of users inthe wireless network.)
We’ll cover these standards later in the first half of this chapter; we’ll also consider other wireless link characteristics (such as their bit error rates and the causes of bit errors) in Section 7.2.
In Figure 7.1, wireless links connect wireless hosts located at the edge of the network into the larger network infrastructure.
We hasten to add that wireless links are also sometimes used within  a network to connect routers, switches, and Figure 7.2 Link characteristics of selected wireless network standards other network equipment.
However, our focus in this chapter will be on the use of wireless communication at the network edge, as it is here that many of the most exciting technical challenges, and most of the growth, are occurring.
Base station.
The base station  is a key part of the wireless network infrastructure.
Unlike the wireless host and wireless link, a base station has no obvious counterpart in a wired network.
A basestation is responsible for sending and receiving data (e.g., packets) to and from a wireless host thatis associated with that base station.
A base station will often be responsible for coordinating the transmission of multiple wireless hosts with which it is associated.
When we say a wireless host is
“associated” with a base station, we mean that (1) the host is within the wireless communication distance of the base station, and (2) the host uses that base station to relay data between it (thehost) and the larger network.
Cell towers in cellular networks and access points in 802.11 wireless LANs are examples of base stations.
In Figure 7.1, the base station is connected to the larger network (e.g., the ­Internet, corporate or home network, or telephone network), thus functioning as a link-layer relay between the wireless host and the rest of the world with which the host communicates.
Hosts associated with a base station are often referred to as operating in ­infrastructure mode, since all traditional network services (e.g., address assignment and routing) are provided by the network to which a host is connected via CASE HISTORY PUBLIC WIFI ACCESS: COMING SOON TO A LAMP POST NEAR YOU?
WiFi hotspots—public locations where users can find 802.11 wireless access—are becoming increasingly common in hotels, airports, and cafés around the world.
Most college campuses offer ubiquitous wireless access, and it’s hard to find a hotel that doesn’t offer wirelessInternet access.
Over the past decade a number of cities have designed, deployed, and operated municipal WiFi networks.
The vision of providing ubiquitous WiFi access to the community as a publicservice (much like streetlights)—helping to bridge the digital divide by providing Internetaccess to all citizens and to promote economic development—is compelling.
Many citiesaround the world, including Philadelphia, Toronto, Hong Kong, Minneapolis, London, and Auckland, have plans to provide ubiquitous wireless within the city, or have already done so to varying degrees.
The goal in Philadelphia was to “turn Philadelphia into the nation’slargest WiFi hotspot and help to improve education, bridge the digital divide, enhanceneighborhood development, and reduce the costs of government.”
The ambitious program—an agreement between the city, Wireless Philadelphia (a nonprofit entity), and the InternetService Provider Earthlink—built an operational network of 802.11b hotspots on streetlamppole arms and traffic control devices that covered 80 percent of the city.
But financial andoperational concerns caused the network to be sold to a group of private investors in 2008, who later sold the network back to the city in 2010.
Other cities, such as Minneapolis, Toronto, Hong Kong, and Auckland, have had success with smaller-scale efforts.
The fact that 802.11 networks operate in the unlicensed spectrum (and hence can be deployed without purchasing expensive spectrum use rights) would seem to make them financially attractive.
However, 802.11 access points (see Section 7.3) have much shorter ranges than 4G cellular base stations (see Section 7.4), requiring a larger number of deployed endpoints to cover the same geographic region.
Cellular data networks providing Internet access, on the other hand, operate in the licensed spectrum.
Cellular providers pay
billions of dollars for spectrum access rights for their networks, making cellular data networks a business rather than municipal undertaking.
the base station.
In ad hoc networks, wireless hosts have no such infrastructure with which to connect.
In the absence of such infrastructure, the hosts themselves must provide for services suchas routing, address assignment, DNS-like name translation, and more.
When a mobile host moves beyond the range of one base station and into the range of another, it will change its point of attachment into the larger network (i.e., change the base station with which it is associated)—a process referred to as handoff .
Such mobility raises many challenging questions.
If a host can move, how does one find the mobile host’s current location in the network so that datacan be forwarded to that mobile host?
How is addressing performed, given that a host can be in one of many possible locations?
If the host moves during  a TCP connection or phone call, how is data routed so that the connection continues uninterrupted?
These and many (many!)
other questions make wireless and mobile networking an area of exciting networking research.
Network infrastructure.
This is the larger network with which a wireless host may wish to communicate.
Having discussed the “pieces” of a wireless network, we note that these pieces can be combined in many different ways to form different types of wireless networks.
You may find a taxonomy of these types of wireless networks useful as you read on in this chapter, or read/learn more about wirelessnetworks beyond this book.
At the highest level we can classify wireless networks according to two criteria: (i) whether a packet in the wireless network crosses exactly one wireless hop or multiple wireless hops , and (ii) whether there is infrastructure such as a base station in the network: Single-hop, infrastructure-based.
 These networks have a base station that is connected to a larger wired network (e.g., the Internet).
Furthermore, all communication is between this base station and a wireless host over a single wireless hop.
The 802.11 networks you use in the classroom, café,or library; and the 4G LTE data networks that we will learn about shortly all fall in this category.
The vast majority of our daily interactions are with single-hop, infrastructure-based ­wireless networks.
Single-hop, infrastructure-less.
 In these networks, there is no base station that is connected to a wireless network.
However, as we will see, one of the nodes in this single-hop network may coordinate the transmissions of the other nodes.
­Bluetooth networks (that connect small wireless devices such as keyboards, speakers, and headsets, and which we will study in Section 7.3.6) and 802.11 networks in ad hoc mode are single-hop, infrastructure-less networks.
Multi-hop, infrastructure-based.
 In these networks, a base station is present that is wired to the larger network.
However, some wireless nodes may have to relay their communication through otherwireless nodes in order to communicate via the base station.
Some wireless sensor networks andso-called wireless mesh networks fall in this category.
Multi-hop, infrastructure-less.
 There is no base station in these networks, and nodes may have to relay messages among several other nodes in order to reach a destination.
Nodes may also be
mobile, with connectivity changing among nodes—a class of networks known as mobile ad hoc networks (MANETs).
If the mobile nodes are vehicles, the network is a vehicular ad hoc network (VANET).
As you might imagine, the development of protocols for such networks is challenging and is the subject of much ongoing research.
In this chapter, we’ll mostly confine ourselves to single-hop networks, and then mostly to infrastructure- based networks.
Let’s now dig deeper into the technical challenges that arise in wireless and mobile networks.
We’ll begin by first considering the individual wireless link, deferring our discussion of mobility until later in thischapter.
7.2 Wireless Links and Network Characteristics Let’s begin by considering a simple wired network, say a home network, with a wired Ethernet switch (see Section 6.4) interconnecting the hosts.
If we replace the wired Ethernet with a wireless 802.11 network, a wireless network interface would replace the host’s wired Ethernet interface, and an access point would replace the Ethernet switch, but virtually no changes would be needed at the network layeror above.
This suggests that we focus our attention on the link layer when looking for important differences between wired and wireless networks.
Indeed, we can find a number of important differences between a wired link and a wireless link: Decreasing signal strength.
Electromagnetic radiation attenuates as it passes through matter (e.g., a radio signal passing through a wall).
Even in free space, the signal will disperse, resulting in decreased signal strength (sometimes referred to as path loss) as the distance between sender and receiver increases.
Interference from other sources.
Radio sources transmitting in the same frequency band will interfere with each other.
For example, 2.4 GHz wireless phones and 802.11b wireless LANs transmit in the same frequency band.
Thus, the 802.11b wireless LAN user talking on a 2.4 GHzwireless phone can expect that neither the network nor the phone will perform particularly well.
In addition to interference from transmitting sources, electromagnetic noise within the environment (e.g., a nearby motor, a microwave) can result in interference.
Multipath propagation.
 Multipath propagation  occurs when portions of the electromagnetic wave reflect off objects and the ground, taking paths of different lengths between a sender and receiver.
This results in the blurring of the received signal at the receiver.
Moving objects between the senderand receiver can cause multipath propagation to change over time.
For a detailed discussion of wireless channel characteristics, models, and measurements, see [Anderson 1995].
The discussion above suggests that bit errors will be more common in wireless links than in wired links.
For this reason, it is perhaps not surprising that wireless link protocols (such as the 802.11 protocol we’llexamine in the following section) employ not only powerful CRC error detection codes, but also link-levelreliable-data-transfer protocols that retransmit corrupted frames.
Having considered the impairments that can occur on a wireless channel, let’s next turn our attention to the host receiving the wireless signal.
This host receives an electromagnetic signal that is a combinationof a degraded form of the original signal transmitted by the sender (degraded due to the attenuation and multipath propagation effects that we discussed above, among others) and background noise in the
environment.
The signal-to-noise ratio (SNR)  is a relative measure of the strength of the received signal (i.e., the information being transmitted) and this noise.
The SNR is typically measured in units of decibels (dB), a unit of measure that some think is used by electrical engineers primarily to confusecomputer scientists.
The SNR, measured in dB, is twenty times the ratio of the base-10 logarithm of theamplitude of the received signal to the amplitude of the noise.
For our purposes here, we need only know that a larger SNR makes it easier for the receiver to extract the transmitted signal from the background noise.
Figure 7.3 (adapted from [Holland 2001]) shows the bit error rate (BER)—roughly speaking, the probability that a transmitted bit is received in error at the receiver—versus the SNR for three different modulation techniques for encoding information for transmission on an idealized wireless channel.
Thetheory of modulation and coding, as well as signal extraction and BER, is well beyond the scope of Figure 7.3 Bit error rate, transmission rate, and SNR Figure 7.4 Hidden terminal problem caused by obstacle (a) and fading (b)
this text (see [Schwartz 1980]  for a discussion of these topics).
Nonetheless, Figure 7.3 illustrates several physical-layer characteristics that are important in understanding higher-layer wireless communication protocols: For a given modulation scheme, the higher the SNR, the lower the BER.
 Since a sender can increase the SNR by increasing its transmission power, a sender can decrease the probability that aframe is received in error by increasing its transmission power.
Note, however, that there is arguablylittle practical gain in increasing the power beyond a certain threshold, say to decrease the BER from  to .
There are also disadvantages  associated with increasing the transmission power: More energy must be expended by the sender (an important concern for battery-powered mobileusers), and the sender’s transmissions are more likely to interfere with the transmissions of another sender (see Figure 7.4(b)).
For a given SNR, a modulation technique with a higher bit transmission rate (whether in error or not) will have a higher BER.
 For example, in Figure 7.3, with an SNR of 10 dB, BPSK modulation with a transmission rate of 1 Mbps has a BER of less than , while with QAM16 modulation with a transmission rate of 4 Mbps, the BER is , far too high to be practically useful.
However, with an SNR of 20 dB, QAM16 modulation has a transmission rate of 4 Mbps and a BERof , while BPSK modulation has a transmission rate of only 1 Mbps and a BER that is so low as to be (literally) “off the charts.”
If one can tolerate a BER of , the higher transmission rate offered by QAM16 would make it the preferred modulation technique in this situation.
Theseconsiderations give rise to the final characteristic, described next.
Dynamic selection of the physical-layer modulation technique can be used to adapt themodulation technique to channel conditions.
 The SNR (and hence the BER) may change as a result of mobility or due to changes in the environment.
Adaptive modulation and coding are used incellular data systems and in the 802.11 WiFi and 4G cellular data networks that we’ll study in Sections 7.3 and 7.4.
This allows, for example, the selection of a modulation technique that provides the highest transmission rate possible subject to a constraint on the BER, for given channel characteristics.10−12 10−13 10−7 10−1 10−7 10−7
A higher and time-varying bit error rate is not the only difference between a wired and wireless link.
Recall that in the case of wired broadcast links, all nodes receive the transmissions from all other nodes.
In the case of wireless links, the situation is not as simple, as shown in Figure 7.4.
Suppose that Station A is transmitting to Station B. Suppose also that Station C is transmitting to Station B. With the so-called hidden terminal problem , physical obstructions in the environment (for example, a mountain or a building) may prevent A and C from hearing each other’s transmissions, even though A’s and C’s transmissions are indeed interfering at the destination, B. This is shown in Figure 7.4(a).
A second scenario that results in undetectable collisions at the receiver results from the fading  of a signal’s strength as it propagates through the wireless medium.
Figure 7.4(b) illustrates the case where A and C are placed such that their signals are not strong enough to detect each other’s transmissions, yet theirsignals are strong enough to interfere with each other at station B. As we’ll see in Section 7.3, the hidden terminal problem and fading make multiple access in a wireless network considerably more complex than in a wired network.
7.2.1 CDMA Recall from Chapter 6 that when hosts communicate over a shared medium, a protocol is needed sothat the signals sent by multiple senders do not interfere at the receivers.
In Chapter 6 we described three classes of medium access protocols: channel partitioning, random access, and taking turns.
Codedivision multiple access (CDMA) belongs to the family of channel partitioning protocols.
It is prevalent in wireless LAN and cellular technologies.
Because CDMA is so important in the wireless world, we’ll take a quick look at CDMA now, before getting into specific wireless access technologies in the subsequentsections.
In a CDMA protocol, each bit being sent is encoded by multiplying the bit by a signal (the code) that changes at a much faster rate (known as the chipping rate ) than the original sequence of data bits.
Figure 7.5 shows a simple, idealized CDMA encoding/decoding scenario.
Suppose that the rate at which original data bits reach the CDMA encoder defines the unit of time; that is, each original data bit to be transmitted requires a one-bit slot time.
Let d be the value of the data bit for the ith bit slot.
For mathematical convenience, we represent a data bit with a 0 value as .
Each bit slot is further subdivided into M mini-slots; in Figure 7.5, , i −1 M=8
Figure 7.5 A simple CDMA example: Sender encoding, receiver decoding although in practice M is much larger.
The CDMA code used by the sender consists of a sequence of M values, , each taking  or  value.
In the example in Figure 7.5, the M-bit CDMA code being used by the sender is .
To illustrate how CDMA works, let us focus on the ith data bit, d .
For the mth mini-slot of the bit- transmission time of d , the output of the CDMA encoder, Z , is the value of d  multiplied by the mth bit in the assigned CDMA code, c: In a simple world, with no interfering senders, the receiver would receive the encoded bits, Z, and recover the original data bit, d, by computing:cm, m=1,…, M a+1−1 (1,1,1,−1,1,−1,−1,−1) i i i,m i m Zi,m=di⋅cm (7.1) i,m i
The reader might want to work through the details of the example in Figure 7.5 to see that the original data bits are indeed correctly recovered at the receiver using Equation  7.2.
The world is far from ideal, however, and as noted above, CDMA must work in the presence of interfering senders that are encoding and transmitting their data using a different assigned code.
But how can a CDMA receiver recover a sender’s original data bits when those data bits are being tangledwith bits being transmitted by other senders?
CDMA works under the assumption that the interferingtransmitted bit signals are additive.
This means, for example, that if three senders send a 1 value, and afourth sender sends a  value during the same mini-slot, then the received signal at all receivers during that mini-slot is a 2 (since ).
In the presence of multiple senders, sender s computes its encoded transmissions, , in exactly the same manner as in Equation  7.1.
The value received at a receiver during the mth mini-slot of the ith bit slot, however, is now the sum of the transmitted bits from all N senders during that mini-slot: Amazingly, if the senders’ codes are chosen carefully, each receiver can recover the data sent by a given sender out of the aggregate signal simply by using the sender’s code in exactly the same manner as in Equation  7.2: as shown in Figure 7.6, for a two-sender CDMA example.
The M-bit CDMA code being used by the upper sender is , while the CDMA code being used by the lower sender is .
Figure 7.6 illustrates a receiver recovering the original data bits from the upper sender.
Note that the receiver is able to extract the data from sender 1 in spite of the interfering transmission from sender 2.
Recall our cocktail analogy from Chapter 6.
A CDMA protocol is similar to having partygoers speaking in multiple languages; in such circumstances humans are actually quite good at locking into the conversation in the language they understand, while filtering out the remaining conversations.
We see here that CDMA is a partitioning protocol in that it partitions the codespace (as opposed to time orfrequency) and assigns each node a dedicated piece of the codespace.
Our discussion here of CDMA is necessarily brief; in practice a number of difficult issues must be addressed.
First, in order for the CDMA receivers to be abledi=1M∑m=1MZi,m⋅cm (7.2) −1 1+1+1−1=2 Zi,ms Zi,m*=∑s=1NZi,ms di=1M∑m=1MZi,m*⋅cm (7.3) (1,1,1,−1,1,−1,−1,−1) (1,−1,1,1,1,−1,1,1)
Figure 7.6 A two-sender CDMA example to extract a particular sender’s signal, the CDMA codes must be carefully chosen.
­Second, our discussion has assumed that the received signal strengths from various senders are the same; in reality this can be difficult to achieve.
There is a considerable body of literature addressing these and other issues related to CDMA; see ­[Pickholtz 1982; Viterbi 1995] for details.
7.3 WiFi: 802.11 Wireless LANs Pervasive in the workplace, the home, educational institutions, cafés, airports, and street corners, wireless LANs are now one of the most important access network technologies in the Internet today.
Although many technologies and standards for wireless LANs were developed in the 1990s, oneparticular class of standards has clearly emerged as the winner: the IEEE 802.11 wireless LAN, also known as WiFi .
In this section, we’ll take a close look at 802.11 wireless LANs, examining its frame structure, its medium access protocol, and its internetworking of 802.11 LANs with wired Ethernet LANs.
There are several 802.11 standards for wireless LAN technology in the IEEE 802.11 (“WiFi”) family, as summarized in Table 7.1.
The different 802.11 standards all share some common characteristics.
They all use the same medium access protocol, CSMA/CA, which we’ll discuss shortly.
All three use the same frame structure for their link-layer frames as well.
All three standards have the ability to reduce theirtransmission rate in order to reach out over greater distances.
And, importantly, 802.11 products arealso all backwards compatible, meaning, for example, that a mobile capable only of 802.11g may still interact with a newer 802.11ac base station.
However, as shown in Table 7.1, the standards have some major differences at the physical layer.
802.11 devices operate in two difference frequency ranges: 2.4–2.485 GHz (referred to as the 2.4 GHz range) and 5.1 – 5.8 GHz (referred to as the 5 GHz range).
The 2.4 GHz range is an unlicensedfrequency band, where 802.11 devices may compete for frequency spectrum with 2.4 GHz phones andmicrowave ovens.
At 5 GHz, 802.11 LANs have a shorter transmission distance for a given power level and suffer more from multipath propagation.
The two most recent standards, 802.11n [IEEE 802.11n 2012]  and 802.11ac [IEEE 802.11ac 2013 ; Cisco 802.11ac 2015] uses multiple input multiple-output (MIMO) antennas; i.e., two or more antennas on the sending side and two or more antennas on thereceiving side that are transmitting/receiving different signals [Diggavi 2004].
802.11ac base Table 7.1 Summary of IEEE 802.11 standards Standard Frequency Range Data Rate 802.11b 2.4 GHz up to 11 Mbps 802.11a 5 GHz up to 54 Mbps 802.11g 2.4 GHz up to 54 Mbps
802.11n 2.5 GHz and 5 GHz up to 450 Mbps 802.11ac 5 GHz up to 1300 Mbps stations may transmit to multiple stations simultaneously, and use “smart” antennas to adaptively beamform to target transmissions in the direction of a receiver.
This decreases interference and increases the distance reached at a given data rate.
The data rates shown in Table 7.1 are for an idealized environment, e.g., a receiver placed 1 meter away from the base station, with no interference —a scenario that we’re unlikely to experience in practice!
So as the saying goes, YMMV: Your Mileage (or in this case your wireless data rate) May Vary.
7.3.1 The 802.11 Architecture Figure 7.7 illustrates the principal components of the 802.11 wireless LAN architecture.
The fundamental building block of the 802.11 architecture is the basic service set (BSS) .
A BSS contains one or more wireless stations and a central base station, known as an access point (AP) in 802.11 parlance.
Figure 7.7 shows the AP in each of two BSSs connecting to an interconnection device (such as a switch or router), which in turn leads to the Internet.
In a typical home network, there is one AP and one router (typically integrated together as one unit) that connects the BSS to the Internet.
As with Ethernet devices, each 802.11 wireless station has a 6-byte MAC address that is stored in the firmware of the station’s adapter (that is, 802.11 network interface card).
Each AP also has a MACaddress for its wireless interface.
As with Ethernet, these MAC addresses are administered by IEEE andare (in theory) ­globally unique.
Figure 7.7 IEEE 802.11 LAN architecture Figure 7.8 An IEEE 802.11 ad hoc network As noted in Section 7.1, wireless LANs that deploy APs are often referred to as infrastructure wireless LANs, with the “infrastructure” being the APs along with the wired Ethernet infrastructure that interconnects the APs and a router.
Figure 7.8 shows that IEEE 802.11 stations can also group themselves together to form an ad hoc network—a network with no central control and with no connections to the ­“outside world.”
Here, the network is formed “on the fly,” by mobile devices thathave found themselves in proximity to each other, that have a need to communicate, and that find nopreexisting network infrastructure in their location.
An ad hoc network might be formed when people with
laptops get together (for example, in a conference room, a train, or a car) and want to exchange data in the absence of a centralized AP.
There has been tremendous interest in ad hoc networking, as communicating portable devices continue to proliferate.
In this section, though, we’ll focus our attentionon infrastructure wireless LANs.
Channels and Association In 802.11, each wireless station needs to associate with an AP before it can send or receive network- layer data.
Although all of the 802.11 standards use association, we’ll discuss this topic specifically inthe context of IEEE 802.11b/g. When a network administrator installs an AP, the administrator assigns a one- or two-word Service Set Identifier (SSID)  to the access point. (
When you choose Wi-Fi under Setting on your iPhone, for example, a list is displayed showing the SSID of each AP in range.)
The administrator must also assign a channel number to the AP.
To understand channel numbers, recall that 802.11 operates in the frequency range of 2.4 GHz to 2.4835 GHz.
Within this 85 MHz band, 802.11 defines 11 partially overlapping channels.
Any two channels are non-overlapping if and only if they are separated by four ormore channels.
In particular, the set of channels 1, 6, and 11 is the only set of three non-overlapping channels.
This means that an administrator could create a wireless LAN with an aggregate maximumtransmission rate of 33 Mbps by installing three 802.11b APs at the same physical location, assigningchannels 1, 6, and 11 to the APs, and interconnecting each of the APs with a switch.
Now that we have a basic understanding of 802.11 channels, let’s describe an interesting (and not completely uncommon) situation—that of a WiFi jungle.
A WiFi jungle  is any physical location where a wireless station receives a sufficiently strong signal from two or more APs.
For example, in many cafés in New York City, a wireless station can pick up a signal from numerous nearby APs.
One of the APsmight be managed by the café, while the other APs might be in residential apartments near the café.
Each of these APs would likely be located in a different IP subnet and would have been independentlyassigned a channel.
Now suppose you enter such a WiFi jungle with your phone, tablet, or ­laptop, seeking wireless Internet access and a blueberry muffin.
Suppose there are five APs in the WiFi jungle.
To gain Internet access, your wireless device needs to join exactly one of the subnets and hence needs to associate  with exactly one of the APs.
­Associating means the wireless device creates a virtual wire between itself and the AP.
Specifically, only the associated AP will send data frames (that is, frames containing data, suchas a datagram) to your wireless device, and your wireless device will send data frames into the Internetonly through the associated AP.
But how does your wireless device associate with a particular AP?
Andmore fundamentally, how does your wireless device know which APs, if any, are out there in the jungle?
The 802.11 standard requires that an AP periodically send beacon frames , each of which includes the
AP’s SSID and MAC address.
Your wireless device, knowing that APs are sending out beacon frames, scans the 11 channels, seeking beacon frames from any APs that may be out there (some of which may be transmitting on the same channel—it’s a jungle out there!).
Having learned about available APs fromthe beacon frames, you (or your wireless device) select one of the APs for association.
The 802.11 standard does not specify an algorithm for selecting which of the available APs to associate with; that algorithm is left up to the designers of the 802.11 firmware and software in your wireless device.
Typically, the device chooses the AP whose beacon frame is received with the highest signal strength.
While a high signal strength is good (see, e.g., Figure 7.3), signal strength is not the only AP characteristic that will determine the performance a device receives.
In particular, it’s possible that the selected AP may have a strong signal, but may be overloaded with other affiliated devices (that willneed to share the wireless bandwidth at that AP), while an unloaded AP is not selected due to a slightlyweaker signal.
A number of alternative ways of choosing APs have thus recently been proposed [Vasudevan 2005; Nicholson 2006; Sundaresan 2006] .
For an interesting and down-to-earth discussion of how signal strength is measured, see [Bardwell 2004].
Figure 7.9 Active and passive scanning for access points The process of scanning channels and listening for beacon frames is known as passive scanning (see Figure 7.9a).
A wireless device can also perform active scanning , by broadcasting a probe frame that will be received by all APs within the wireless device’s range, as shown in Figure 7.9b.
APs respond to the probe request frame with a probe response frame.
The wireless device can then choose the AP with which to associate from among the responding APs.
After selecting the AP with which to associate, the wireless device sends an association request frame to the AP, and the AP responds with an association response frame.
Note that this second request/response handshake is needed with active scanning, since an AP responding to the initial proberequest frame doesn’t know which of the (possibly many) responding APs the device will choose toassociate with, in much the same way that a DHCP client can choose from among multiple DHCP servers (see Figure 4.21).
Once associated with an AP, the device will want to join the subnet (in the IP addressing sense of Section 4.3.3) to which the AP belongs.
Thus, the device will typically send a DHCP discovery message (see Figure 4.21) into the subnet via the AP in order to obtain an IP address on the subnet.
Once the address is obtained, the rest of the world then views that device simply as another host with an IP address in that subnet.
In order to create an association with a particular AP, the wireless device may be required to authenticate itself to the AP.
802.11 wireless LANs provide a number of alternatives for authenticationand access.
One approach, used by many companies, is to permit access to a wireless network based on a device’s MAC address.
A second approach, used by many Internet cafés, employs usernames and passwords.
In both cases, the AP typically communicates with an authentication server, relayinginformation between the wireless device and the authentication server using a protocol such as RADIUS [RFC 2865]  or DIAMETER [RFC 3588] .
Separating the authentication server from the AP allows one authentication server to serve many APs, centralizing the (often sensitive) decisions of authenticationand access within the single server, and keeping AP costs and complexity low.
We’ll see in Chapter 8 that the new IEEE 802.11i protocol defining security aspects of the 802.11 protocol family takes precisely this approach.
7.3.2 The 802.11 MAC Protocol Once a wireless device is associated with an AP, it can start sending and receiving data frames to andfrom the access point.
But because multiple wireless devices, or the AP itself may want to transmit dataframes at the same time over the same channel, a multiple access protocol is needed to coordinate thetransmissions.
In the following, we'll refer to the devices or the AP as wireless “stations” that share the multiple access channel.
As discussed in Chapter 6 and Section 7.2.1, broadly speaking there are three classes of multiple access protocols: channel partitioning (including CDMA), random access, and taking turns.
Inspired by the huge success of Ethernet and its random access protocol, the designers of802.11 chose a random access protocol for 802.11 wireless LANs.
This random access protocol isreferred to as CSMA with collision avoidance , or more succinctly as CSMA/CA.
As with Ethernet’s CSMA/CD, the “CSMA” in CSMA/CA stands for “carrier sense multiple access,” meaning that eachstation senses the channel before transmitting, and refrains from transmitting when the channel issensed busy.
Although both ­Ethernet and 802.11 use carrier-sensing random access, the two MAC protocols have important differences.
First, instead of using collision detection, 802.11 uses collision- avoidance techniques.
Second, because of the relatively high bit error rates of wireless channels,
802.11 (unlike Ethernet) uses a link-layer acknowledgment/retransmission (ARQ) scheme.
We’ll describe 802.11’s collision-avoidance and link-layer acknowledgment schemes below.
Recall from Sections 6.3.2 and 6.4.2 that with Ethernet’s collision-detection algorithm, an Ethernet station listens to the channel as it transmits.
If, while transmitting, it detects that another station is also transmitting, it aborts its transmission and tries to transmit again after waiting a small, random amount of time.
Unlike the 802.3 Ethernet protocol, the 802.11 MAC protocol does not implement collision detection.
There are two important reasons for this: The ability to detect collisions requires the ability to send (the station’s own ­signal) and receive (to determine whether another station is also transmitting) at the same time.
Because the strength of the received signal is typically very small compared to the strength of the transmitted signal at the802.11 adapter, it is costly to build hardware that can detect a collision.
More importantly, even if the adapter could transmit and listen at the same time (and presumably abort transmission when it senses a busy channel), the adapter would still not be able to detect all collisions, due to the hidden terminal problem and fading, as discussed in Section 7.2.
Because 802.11wireless LANs do not use collision detection, once a station begins to transmit a frame,it transmits the frame in its entirety; that is, once a station gets started, there is no turning back.
As one might expect, transmitting entire frames (particularly long frames) when collisions are prevalent can significantly degrade a multiple access protocol’s performance.
In order to reduce the likelihood ofcollisions, 802.11 employs several collision-avoidance techniques, which we’ll shortly discuss.
Before considering collision avoidance, however, we’ll first need to examine 802.11’s link-layer acknowledgment  scheme.
Recall from Section 7.2 that when a station in a wireless LAN sends a frame, the frame may not reach the destination station intact for a variety of reasons.
To deal with this non-negligible chance of failure, the 802.11 MAC protocol uses link-layer acknowledgments.
As shown in Figure 7.10, when the destination station receives a frame that passes the CRC, it waits a short period of time known as the Short Inter-frame Spacing (SIFS)  and then sends back
Figure 7.10 802.11 uses link-layer acknowledgments an acknowledgment frame.
If the transmitting station does not receive an acknowledgment within a given amount of time, it assumes that an error has occurred and retransmits the frame, using the CSMA/CA protocol to access the channel.
If an acknowledgment is not received after some fixednumber of retransmissions, the transmitting station gives up and discards the frame.
Having discussed how 802.11 uses link-layer acknowledgments, we’re now in a position to describe the 802.11 CSMA/CA protocol.
Suppose that a station (wireless device or an AP) has a frame to transmit.
1.
If initially the station senses the channel idle, it transmits its frame after a short period of timeknown as the Distributed Inter-frame Space (DIFS) ; see ­Figure 7.10.
2.
Otherwise, the station chooses a random backoff value using binary exponential backoff (as weencountered in Section 6.3.2) and counts down this value after DIFS when the channel is sensed idle.
While the channel is sensed busy, the counter value remains frozen.
3.
When the counter reaches zero (note that this can only occur while the channel is sensed idle), the station transmits the entire frame and then waits for an acknowledgment.
4.
If an acknowledgment is received, the transmitting station knows that its frame has beencorrectly received at the destination station.
If the station has another frame to send, it begins
the CSMA/CA protocol at step 2.
If the acknowledgment isn’t received, the transmitting station reenters the backoff phase in step 2, with the random value chosen from a larger interval.
Recall that under Ethernet’s CSMA/CD, multiple access protocol ( Section 6.3.2), a station begins transmitting as soon as the channel is sensed idle.
With CSMA/CA, however, the station refrains from transmitting while counting down, even when it senses the channel to be idle.
Why do CSMA/CD and CDMA/CA take such different approaches here?
To answer this question, let’s consider a scenario in which two stations each have a data frame to transmit, but neither station transmits immediately because each senses that a third station is alreadytransmitting.
With Ethernet’s CSMA/CD, the two stations would each transmit as soon as they detectthat the third station has finished transmitting.
This would cause a collision, which isn’t a serious issue inCSMA/CD, since both stations would abort their transmissions and thus avoid the useless transmissionsof the remainders of their frames.
In 802.11, however, the situation is quite different.
Because 802.11 does not detect a collision and abort transmission, a frame suffering a collision will be transmitted in its entirety.
The goal in 802.11 is thus to avoid collisions whenever possible.
In 802.11, if the two stationssense the channel busy, they both immediately enter random backoff, hopefully choosing differentbackoff values.
If these values are indeed different, once the channel becomes idle, one of the twostations will begin transmitting before the other, and (if the two stations are not hidden from each other)the “losing station” will hear the “winning station’s” signal, freeze its counter, and refrain from transmitting until the winning station has completed its transmission.
In this manner, a costly collision isavoided.
Of course, collisions can still occur with 802.11 in this scenario: The two stations could behidden from each other, or the two stations could choose random backoff values that are close enough that the transmission from the station starting first have yet to reach the second station.
Recall that we encountered this problem earlier in our discussion of random access algorithms in the context of Figure 6.12.
Dealing with Hidden Terminals: RTS and CTSThe 802.11 MAC protocol also includes a nifty (but optional) reservation scheme that helps avoid collisions even in the presence of hidden terminals.
Let’s investigate this scheme in the context of Figure 7.11, which shows two wireless ­stations and one access point.
Both of the wireless stations are within range of the AP (whose ­coverage is shown as a shaded circle) and both have associated with the AP.
­However, due to fading, the signal ranges of wireless stations are limited to the interiors of theshaded circles shown in Figure 7.11.
Thus, each of the wireless stations is hidden from the other, although neither is hidden from the AP.
Let’s now consider why hidden terminals can be problematic.
Suppose Station H1 is transmitting a frame and halfway through H1’s transmission, Station H2 wants to send a frame to the AP.
H2, nothearing the transmission from H1, will first wait a DIFS interval and then transmit the frame, resulting in
a collision.
The channel will therefore be wasted during the entire period of H1’s transmission as well as during H2’s transmission.
In order to avoid this problem, the IEEE 802.11 protocol allows a station to use a short Request to Send (RTS)  control frame and a short Clear to Send (CTS)  control frame to reserve access to the channel.
When a sender wants to send a DATA Figure 7.11 Hidden terminal example: H1 is hidden from H2, and vice versa frame, it can first send an RTS frame to the AP, indicating the total time required to transmit the DATA frame and the acknowledgment (ACK) frame.
When the AP receives the RTS frame, it responds by broadcasting a CTS frame.
This CTS frame serves two purposes: It gives the sender explicit permissionto send and also instructs the other stations not to send for the reserved duration.
Thus, in Figure 7.12, before transmitting a DATA frame, H1 first broadcasts an RTS frame, which is heard by all stations in its circle, including the AP.
The AP then responds
Figure 7.12 Collision avoidance using the RTS and CTS frames with a CTS frame, which is heard by all stations within its range, including H1 and H2.
Station H2, having heard the CTS, refrains from transmitting for the time specified in the CTS frame.
The RTS, CTS, DATA, and ACK frames are shown in Figure 7.12.
The use of the RTS and CTS frames can improve performance in two important ways: The hidden station problem is mitigated, since a long DATA frame is transmitted only after the channel has been reserved.
Because the RTS and CTS frames are short, a collision involving an RTS or CTS frame will last only
for the duration of the short RTS or CTS frame.
Once the RTS and CTS frames are correctly transmitted, the following DATA and ACK frames should be transmitted without collisions.
You are encouraged to check out the 802.11 applet in the textbook’s Web site.
This interactive applet illustrates the CSMA/CA protocol, including the RTS/CTS exchange sequence.
Although the RTS/CTS exchange can help reduce collisions, it also introduces delay and consumes channel resources.
For this reason, the RTS/CTS exchange is only used (if at all) to reserve the channelfor the transmission of a long DATA frame.
In practice, each wireless station can set an RTS thresholdsuch that the RTS/CTS sequence is used only when the frame is longer than the threshold.
For manywireless stations, the default RTS threshold value is larger than the maximum frame length, so theRTS/CTS sequence is skipped for all DATA frames sent.
Using 802.11 as a Point-to-Point Link Our discussion so far has focused on the use of 802.11 in a multiple access setting.
We should mention that if two nodes each have a directional antenna, they can point their directional antennas at each otherand run the 802.11 protocol over what is essentially a point-to-point link.
Given the low cost ofcommodity 802.11 hardware, the use of directional antennas and an increased transmission powerallow 802.11 to be used as an inexpensive means of providing wireless point-to-point connections over tens of kilometers distance. [
Raman 2007]  describes one of the first such multi-hop wireless networks, operating in the rural Ganges plains in India using point-to-point 802.11 links.
7.3.3 The IEEE 802.11 Frame Although the 802.11 frame shares many similarities with an Ethernet frame, it also contains a number offields that are specific to its use for wireless links.
The 802.11 frame is shown in Figure 7.13.
The numbers above each of the fields in the frame represent the lengths of the fields in bytes; the numbers above each of the subfields in the frame control field represent the lengths of the subfields in bits.
Let’s now examine the fields in the frame as well as some of the more important subfields in the frame’s control field.
Figure 7.13 The 802.11 frame Payload and CRC Fields At the heart of the frame is the payload, which typically consists of an IP datagram or an ARP packet.
Although the field is permitted to be as long as 2,312 bytes, it is typically fewer than 1,500 bytes, holdingan IP datagram or an ARP packet.
As with an Ethernet frame, an 802.11 frame includes a 32-bit cyclicredundancy check (CRC) so that the receiver can detect bit errors in the received frame.
As we’ve seen,bit errors are much more common in wireless LANs than in wired LANs, so the CRC is even more usefulhere.
Address Fields Perhaps the most striking difference in the 802.11 frame is that it has four address fields, each of which can hold a 6-byte MAC address.
But why four address fields?
Doesn’t a source MAC field and destination MAC field suffice, as they do for ­Ethernet?
It turns out that three address fields are needed for internetworking ­purposes—specifically, for moving the network-layer datagram from a wireless station through an AP to a router interface.
The fourth address field is used when APs ­forward frames to each other in ad hoc mode.
Since we are only considering infrastructure networks here, let’s focusour attention on the first three address fields.
The 802.11 standard defines these fields as follows: Address 2 is the MAC address of the station that transmits the frame.
Thus, if a wireless station transmits the frame, that station’s MAC address is inserted in the address 2 field.
Similarly, if an AP transmits the frame, the AP’s MAC address is inserted in the address 2 field.
Address 1 is the MAC address of the wireless station that is to receive the frame.
Thus if a mobile wireless station transmits the frame, address 1 contains the MAC address of the destination AP.
Similarly, if an AP transmits the frame, address 1 contains the MAC address of the destinationwireless station.
Figure 7.14 The use of address fields in 802.11 frames: Sending frames between H1 and R1 To understand address 3, recall that the BSS (consisting of the AP and wireless stations) is part of a subnet, and that this subnet connects to other subnets via some router interface.
Address 3 contains the MAC address of this router ­interface.
To gain further insight into the purpose of address 3, let’s walk through an internetworking example in the context of Figure 7.14.
In this figure, there are two APs, each of which is responsible for a number of wireless stations.
Each of the APs has a direct connection to a router, which in turn connects to the global Internet.
We should keep in mind that an AP is a link-layer device, and thus neither “speaks” IPnor understands IP addresses.
Consider now moving a datagram from the router interface R1 to thewireless Station H1.
The router is not aware that there is an AP between it and H1; from the router’sperspective, H1 is just a host in one of the subnets to which it (the router) is connected.
The router, which knows the IP address of H1 (from the destination address of the datagram), uses ARP to determine the MAC address of H1, just as in an ordinary Ethernet LAN.
After obtaining H1’s MAC address, router interface R1 encapsulates the datagram within an Ethernet frame.
The sourceaddress field of this frame contains R1’s MAC address, and the destination address field contains H1’s MAC address.
When the Ethernet frame arrives at the AP, the AP converts the 802.3 Ethernet frame to an 802.11 frame before transmitting the frame into the wireless channel.
The AP fills in address 1 and address 2 with H1’s MAC address and its own MAC address, respectively, as described above.
For address3, the AP inserts the MAC address of R1.
In this manner, H1 can determine (from address 3) the MAC address of the router interface that sent the datagram into the subnet.
Now consider what happens when the wireless station H1 responds by moving a datagram from H1 to R1.
H1 creates an 802.11 frame, filling the fields for address 1 and address 2 with the AP’s MAC address and H1’s MAC address, respectively, as described above.
For address 3, H1 inserts R1’s MAC address.
When the AP receives the 802.11 frame, it converts the frame to an Ethernet frame.
The source address field for this frame is H1’s MAC address, and the destination address field is R1’s MAC address.
Thus, address 3 allows the AP to determine the appropriate destination MAC addresswhen constructing the Ethernet frame.
In summary, address 3 plays a crucial role for internetworking the BSS with a wired LAN.
Sequence Number, Duration, and Frame Control Fields Recall that in 802.11, whenever a station correctly receives a frame from another station, it sends back an acknowledgment.
Because acknowledgments can get lost, the sending station may send multiple copies of a given frame.
As we saw in our discussion of the rdt2.1 protocol ( Section 3.4.1), the use of sequence numbers allows the receiver to distinguish between a newly transmitted frame and the retransmission of a previous frame.
The sequence number field in the 802.11 frame thus serves exactly the same purpose here at the link layer as it did in the transport layer in Chapter 3.
Recall that the 802.11 protocol allows a transmitting station to reserve the channel for a period of time that includes the time to transmit its data frame and the time to transmit an acknowledgment.
Thisduration value is included in the frame’s duration field (both for data frames and for the RTS and CTSframes).
As shown in Figure 7.13, the frame control field includes many subfields.
We’ll say just a few words about some of the more important subfields; for a more complete discussion, you are encouraged to consult the 802.11 specification [Held 2001; Crow 1997; IEEE 802.11 1999] .
The type and subtype fields are used to distinguish the association, RTS, CTS, ACK, and data frames.
The to and from fields are used to define the meanings of the different address fields. (
These meanings change depending on whether ad hoc or infrastructure modes are used and, in the case of infrastructure mode, whether awireless station or an AP is sending the frame.)
Finally the WEP field indicates whether encryption is being used or not (WEP is discussed in Chapter 8).
7.3.4 Mobility in the Same IP Subnet
In order to increase the physical range of a wireless LAN, companies and universities will often deploy multiple BSSs within the same IP subnet.
This naturally raises the issue of mobility among the BSSs— how do wireless stations seamlessly move from one BSS to another while maintaining ongoing TCPsessions?
As we’ll see in this subsection, mobility can be handled in a relatively straightforward mannerwhen the BSSs are part of the subnet.
When stations move between subnets, more sophisticated mobility management protocols will be needed, such as those we’ll study in Sections 7.5 and 7.6.
Let’s now look at a specific example of mobility between BSSs in the same subnet.
Figure 7.15 shows two interconnected BSSs with a host, H1, moving from BSS1 to BSS2.
Because in this example theinterconnection device that connects the two BSSs is not a router, all of the stations in the two BSSs, including the APs, belong to the same IP subnet.
Thus, when H1 moves from BSS1 to BSS2, it may keep its IP address and all of its ongoing TCP connections.
If the interconnection device were a router,then H1 would have to obtain a new IP address in the subnet in which it was moving.
This address change would disrupt (and eventually terminate) any on-going TCP connections at H1.
In Section 7.6, we’ll see how a network-layer mobility protocol, such as mobile IP, can be used to avoid this problem.
But what specifically happens when H1 moves from BSS1 to BSS2?
As H1 wanders away from AP1, H1 detects a weakening signal from AP1 and starts to scan for a stronger signal.
H1 receives beaconframes from AP2 (which in many corporate and university settings will have the same SSID as AP1).
H1then disassociates with AP1 and associates with AP2, while keeping its IP address and maintaining itsongoing TCP sessions.
This addresses the handoff problem from the host and AP viewpoint.
But what about the switch in Figure 7.15?
How does it know that the host has moved from one AP to another?
As you may recall from Chapter 6, switches are “self-learning” and automatically build their forwarding tables.
This self- learning feature nicely handles Figure 7.15 Mobility in the same subnet
occasional moves (for example, when an employee gets transferred from one department to another); however, switches were not designed to support highly mobile users who want to maintain TCP connections while moving between BSSs.
To appreciate the problem here, recall that before the move,the switch has an entry in its forwarding table that pairs H1’s MAC address with the outgoing switchinterface through which H1 can be reached.
If H1 is initially in BSS1, then a datagram destined to H1 willbe directed to H1 via AP1.
Once H1 associates with BSS2, however, its frames should be directed to AP2.
One solution (a bit of a hack, really) is for AP2 to send a broadcast Ethernet frame with H1’s source address to the switch just after the new association.
When the switch receives the frame, itupdates its forwarding table, allowing H1 to be reached via AP2.
The 802.11f standards group isdeveloping an inter-AP protocol to handle these and related issues.
Our discussion above has focused on mobility with the same LAN subnet.
Recall that VLANs, which we studied in Section 6.4.4, can be used to connect together islands of LANs into a large virtual LAN that can span a large geographical region.
Mobility among base stations within such a VLAN can be handledin exactly the same manner as above [Yu 2011].
7.3.5 Advanced Features in 802.11 We’ll wrap up our coverage of 802.11 with a short discussion of two advanced capabilities found in802.11 networks.
As we’ll see, these capabilities are not completely specified in the 802.11 standard, but rather are made possible by mechanisms specified in the standard.
This allows different vendors to implement these capabilities using their own (proprietary) approaches, presumably giving them an edgeover the competition.
802.11 Rate Adaptation We saw earlier in Figure 7.3 that different modulation techniques (with the different transmission rates that they provide) are appropriate for different SNR scenarios.
Consider for example a mobile 802.11 user who is initially 20 meters away from the base station, with a high signal-to-noise ratio.
Given thehigh SNR, the user can communicate with the base station using a physical-layer modulation techniquethat provides high transmission rates while maintaining a low BER.
This is one happy user!
Supposenow that the user becomes mobile, walking away from the base station, with the SNR falling as the distance from the base station increases.
In this case, if the modulation technique used in the 802.11 protocol operating between the base station and the user does not change, the BER will becomeunacceptably high as the SNR decreases, and eventually no transmitted frames will be receivedcorrectly.
For this reason, some 802.11 implementations have a rate adaptation capability that adaptively selects the underlying physical-layer modulation technique to use based on current or recent channel
characteristics.
If a node sends two frames in a row without receiving an acknowledgment (an implicit indication of bit errors on the channel), the transmission rate falls back to the next lower rate.
If 10 frames in a row are acknowledged, or if a timer that tracks the time since the last fallback expires, the transmission rate increases to the next higher rate.
This rate adaptation mechanism shares the same“probing” philosophy as TCP’s congestion-control mechanism—when conditions are good (reflected byACK receipts), the transmission rate is increased until something “bad” happens (the lack of ACK receipts); when something “bad” happens, the transmission rate is reduced.
802.11 rate adaptation and TCP congestion control are thus similar to the young child who is constantly pushing his/her parents formore and more (say candy for a young child, later curfew hours for the teenager) until the parents finallysay “Enough!”
and the child backs off (only to try again later after conditions have hopefully improved!).A number of other schemes have also been proposed to improve on this basic automatic rate- adjustment scheme [Kamerman 1997 ; Holland 2001; Lacage 2004] .
Power Management Power is a precious resource in mobile devices, and thus the 802.11 standard provides power- management capabilities that allow 802.11 nodes to minimize the amount of time that their sense,transmit, and receive functions and other circuitry need to be “on.”
802.11 power management operatesas follows.
A node is able to explicitly alternate between sleep and wake states (not unlike a sleepystudent in a classroom!).
A node indicates to the access point that it will be going to sleep by setting thepower-management bit in the header of an 802.11 frame to 1.
A timer in the node is then set to wake upthe node just before the AP is scheduled to send its beacon frame (recall that an AP typically sends abeacon frame every 100 msec).
Since the AP knows from the set power-transmission bit that the node is going to sleep, it (the AP) knows that it should not send any frames to that node, and will buffer any frames destined for the sleeping host for later transmission.
A node will wake up just before the AP sends a beacon frame, and quickly enter the fully active state (unlike the sleepy student, this wakeup requires only 250 microseconds [Kamerman 1997] !).
The beacon frames sent out by the AP contain a list of nodes whose frames have been buffered at the AP.
If there are no buffered frames for the node, it can go back to sleep.
Otherwise, the node can explicitlyrequest that the buffered frames be sent by sending a polling message to the AP.
With an inter-beacontime of 100 msec, a wakeup time of 250 microseconds, and a similarly small time to receive a beacon frame and check to ensure that there are no buffered frames, a node that has no frames to send or receive can be asleep 99% of the time, resulting in a significant energy savings.
7.3.6 Personal Area Networks: Bluetooth and Zigbee As illustrated in Figure 7.2, the IEEE 802.11 WiFi standard is aimed at communication among devices separated by up to 100 meters (except when 802.11 is used in a point-to-point configuration with a
directional antenna).
Two other wireless protocols in the IEEE 802 family are Bluetooth and Zigbee (defined in the IEEE 802.15.1 and IEEE 802.15.4 standards [IEEE 802.15 2012] ).
Bluetooth An IEEE 802.15.1 network operates over a short range, at low power, and at low cost.
It is essentially a low-power, short-range, low-rate “cable replacement” technology for interconnecting a computer with itswireless keyboard, mouse or other peripheral device; cellular phones, speakers, headphones, and manyother devices, whereas 802.11 is a higher-power, medium-range, higher-rate “access” technology.
Forthis reason, 802.15.1 networks are sometimes referred to as wireless personal area networks (WPANs).The link and physical layers of 802.15.1 are based on the earlier Bluetooth  specification for personal area networks [Held 2001, Bisdikian 2001].
802.15.1 networks operate in the 2.4 GHz unlicensed radio band in a TDM manner, with time slots of 625 microseconds.
During each time slot, a sender transmitson one of 79 channels, with the channel changing in a known but pseudo-random manner from slot to slot.
This form of channel hopping, known as frequency-hopping spread spectrum (FHSS) , spreads transmissions in time over the frequency spectrum.
802.15.1 can provide data rates up to 4 Mbps.
802.15.1 networks are ad hoc networks: No network infrastructure (e.g., an access point) is needed to interconnect 802.15.1 devices.
Thus, 802.15.1 devices must organize themselves.
802.15.1 devices are first organized into a piconet  of up to eight active devices, as shown in Figure 7.16.
One of these devices is designated as the master, with the remaining devices acting as slaves.
The master node truly rules the piconet—its clock determines time in the piconet, it can transmit in each odd-numbered slot, and a Figure 7.16 A Bluetooth piconet
slave can transmit only after the master has communicated with it in the previous slot and even then the slave can only transmit to the master.
In addition to the slave devices, there can also be up to 255 parked devices in the network.
These devices cannot communicate until their status has been changedfrom parked to active by the master node.
For more information about WPANs, the interested reader should consult the Bluetooth references [Held 2001, Bisdikian 2001] or the official IEEE 802.15 Web site [IEEE 802.15 2012] .
Zigbee A second personal area network standardized by the IEEE is the 802.15.4 standard [IEEE 802.15 2012] known as Zigbee.
While Bluetooth networks provide a “cable replacement” data rate of over a Megabit per second, Zigbee is targeted at lower-powered, lower-data-rate, lower-duty-cycle applications thanBluetooth.
While we may tend to think that “bigger and faster is better,” not all network applications needhigh bandwidth and the consequent higher costs (both economic and power costs).
For example, home temperature and light sensors, security devices, and wall-mounted switches are all very simple, low- power, low-duty-cycle, low-cost devices.
Zigbee is thus well-suited for these devices.
Zigbee defineschannel rates of 20, 40, 100, and 250 Kbps, depending on the channel frequency.
Nodes in a Zigbee network come in two flavors.
So-called “reduced-function devices” operate as slave devices under the control of a single “full-function device,” much as Bluetooth slave devices.
A full-function device can operate as a master device as in Bluetooth by controlling multiple slave devices,and multiple full-function devices can additionally be configured into a mesh network in which full- function devices route frames amongst themselves.
Zigbee shares many protocol mechanisms that we’ve already encountered in other link-layer protocols: beacon frames and link-layer acknowledgments(similar to 802.11), carrier-sense random access protocols with binary exponential backoff (similar to802.11 and Ethernet), and fixed, guaranteed allocation of time slots (similar to DOCSIS).
Zigbee networks can be configured in many different ways.
Let’s consider the simple case of a single full-function device controlling multiple reduced-function devices in a time-slotted manner using beacon frames.
Figure 7.17 shows the case
Figure 7.17 Zigbee 802.15.4 super-frame structure where the Zigbee network divides time into recurring super frames, each of which begins with a beacon frame.
Each beacon frame divides the super frame into an active period (during which devices may transmit) and an inactive period (during which all devices, including the controller, can sleep and thusconserve power).
The active period consists of 16 time slots, some of which are used by devices in aCSMA/CA random access manner, and some of which are allocated by the controller to specificdevices, thus providing guaranteed channel access for those devices.
More details about Zigbee networks can be found at [Baronti 2007, IEEE 802.15.4 2012] .
7.4 Cellular Internet Access In the previous section we examined how an Internet host can access the Internet when inside a WiFi hotspot—that is, when it is within the vicinity of an 802.11 access point.
But most WiFi hotspots have asmall coverage area of between 10 and 100 meters in diameter.
What do we do then when we have adesperate need for wireless Internet access and we cannot access a WiFi hotspot?
Given that cellular telephony is now ubiquitous in many areas throughout the world, a natural strategy is to extend cellular networks so that they support not only voice telephony but wireless Internet access aswell.
Ideally, this Internet access would be at a reasonably high speed and would provide for seamlessmobility, allowing users to maintain their TCP sessions while traveling, for example, on a bus or a train.
With sufficiently high upstream and downstream bit rates, the user could even maintain video-conferencing sessions while roaming about.
This scenario is not that far-fetched.
Data rates of severalmegabits per second are becoming available as broadband data services such as those we will coverhere become more widely deployed.
In this section, we provide a brief overview of current and emerging cellular Internet access technologies.
Our focus here will be on both the wireless first hop as well as the network that connects the wireless first hop into the larger telephone network and/or the Internet; in Section 7.7 we’ll consider how calls are routed to a user moving between base stations.
Our brief discussion will necessarily provide only a simplified and high-level description of cellular technologies.
Modern cellularcommunications, of course, has great breadth and depth, with many universities offering several courses on the topic.
Readers seeking a deeper understanding are encouraged to see [Goodman 1997 ; Kaaranen 2001 ; Lin 2001; Korhonen 2003; Schiller 2003; Palat 2009 ; Scourias 2012; Turner 2012 ; Akyildiz 2010], as well as the particularly excellent and exhaustive references [Mouly 1992; Sauter 2014] .
7.4.1 An Overview of Cellular Network Architecture In our description of cellular network architecture in this section, we’ll adopt the terminology of theGlobal System for Mobile Communications ( GSM ) standards. (
For history buffs, the GSM acronym was originally derived from Groupe Spécial Mobile , until the more anglicized name was adopted, preserving the original acronym letters.)
In the 1980s, Europeans recognized the need for a pan-European digital cellular telephony system that would replace the numerous incompatible analog cellular telephony systems, leading to the GSM standard [Mouly 1992].
Europeans deployed GSM technology with great
success in the early 1990s, and since then GSM has grown to be the 800-pound gorilla of the cellular telephone world, with more than 80% of all cellular subscribers worldwide using GSM.
CASE HISTORY 4G Cellular Mobile Versus Wireless LANs Many cellular mobile phone operators are deploying 4G cellular mobile systems.
In some countries (e.g., Korea and Japan), 4G LTE coverage is higher than 90%—nearly ubiquitous.
In 2015, average download rates over deployed LTE systems range from 10Mbps in the US andIndia to close to 40 Mbps in New Zealand.
These 4G systems are being deployed in licensedradio-frequency bands, with some operators paying considerable sums to governments forspectrum-use licenses.
4G systems allow users to access the Internet from remote outdoorlocations while on the move, in a manner similar to today’s cellular phone-only access.
In manycases, a user may have simultaneous access to both wireless LANs and 4G. With the capacityof 4G systems being both more constrained and more expensive, many mobile devices defaultto the use of WiFi rather than 4G, when both are avilable.
The question of whether wireless edge network access will be primarily over wireless LANs or cellular systems remains an open question: The emerging wireless LAN infrastructure may become nearly ubiquitous.
IEEE 802.11 wireless LANs, operating at 54 Mbps and higher, are enjoying widespread deployment.
Essentially all laptops, tablets and smartphones are factory-equipped with 802.11 LANcapabilities.
Furthermore, emerging Internet appliances—such as wireless cameras and picture frames—also have low-powered wireless LAN capabilities.
Wireless LAN base stations can also handle mobile phone appliances.
Many phones are already capable of connecting to the cellular phone network or to an IP network either natively or using a Skype-like Voice-over-IP service, thus bypassing the operator’s cellularvoice and 4G data services.
Of course, many other experts believe that 4G not only will be a major ­success, but will also dramatically revolutionize the way we work and live.
Most likely, both WiFi and 4G will bothbecome prevalent wireless technologies, with roaming ­wireless devices automatically selecting the access technology that provides the best service at their current physical location.
When people talk about cellular technology, they often classify the technology as belonging to one ofseveral “generations.”
The earliest generations were designed primarily for voice traffic.
First generation (1G) systems were analog FDMA systems designed exclusively for voice-only communication.
These 1G systems are almost extinct now, having been replaced by digital 2G systems.
The original 2Gsystems were also designed for voice, but later extended (2.5G) to support data (i.e., Internet) as well asvoice service.
3G systems also support voice and data, but with an emphasis on data capabilities and
higher-speed radio access links.
The 4G systems being deployed today are based on LTE technology, feature an all-IP core network, and provide integrated voice and data at multi-Megabit speeds.
Cellular Network Architecture, 2G: Voice Connections to the ­Telephone Network The term cellular  refers to the fact that the region covered by a cellular network is partitioned into a number of geographic coverage areas, known as cells , shown as hexagons on the left side of Figure 7.18.
As with the 802.11WiFi standard we ­studied in Section 7.3.1, GSM has its own particular nomenclature.
Each cell Figure 7.18 Components of the GSM 2G cellular network architecture contains a base transceiver station (BTS)  that transmits signals to and receives signals from the mobile stations in its cell.
The coverage area of a cell depends on many factors, including the transmitting power of the BTS, the transmitting power of the user devices, obstructing buildings in the cell, and the height of base station antennas.
Although Figure 7.18 shows each cell containing one base transceiver station residing in the middle of the cell, many systems today place the BTS at corners where three cells intersect, so that a single BTS with directional antennas can service three cells.
The GSM standard for 2G cellular systems uses combined FDM/TDM (radio) for the air interface.
Recall from Chapter 1 that, with pure FDM, the channel is partitioned into a number of frequency bands with each band devoted to a call.
Also recall from Chapter 1 that, with pure TDM, time is partitioned into
frames with each frame further partitioned into slots and each call being assigned the use of a particular slot in the revolving frame.
In combined FDM/TDM systems, the channel is partitioned into a number of frequency sub-bands; within each sub-band, time is partitioned into frames and slots.
Thus, for a combined FDM/TDM system, if the channel is partitioned into F sub-bands and time is partitioned into T slots, then the channel will be able to support F.T simultaneous calls.
Recall that we saw in Section 6.3.4 that cable access networks also use a combined FDM/TDM approach.
GSM systems consist of 200-kHz frequency bands with each band supporting eight TDM calls.
GSM encodes speech at 13 kbps and 12.2 kbps.
A GSM network’s base station controller (BSC) will typically service several tens of base transceiver stations.
The role of the BSC is to allocate BTS radio channels to mobile subscribers, perform paging (finding the cell in which a mobile user is resident), and perform handoff of mobile users—a topic we’ll cover shortly in Section 7.7.2.
The base station controller and its controlled base transceiver stations collectively constitute a GSM base station subsystem (BSS).
As we’ll see in Section 7.7, the mobile switching center (MSC)  plays the central role in user authorization and accounting (e.g., determining whether a mobile device is allowed to connect to the cellular network), call establishment and teardown, and handoff.
A single MSC will typically contain up tofive BSCs, resulting in approximately 200K subscribers per MSC.
A cellular provider’s network will havea number of MSCs, with special MSCs known as gateway MSCs connecting the provider’s cellularnetwork to the larger public telephone network.
7.4.2 3G Cellular Data Networks: Extending the Internet to Cellular Subscribers Our discussion in Section 7.4.1 focused on connecting cellular voice users to the public telephone network.
But, of course, when we’re on the go, we’d also like to read e-mail, access the Web, getlocation-dependent services (e.g., maps and restaurant recommendations) and perhaps even watchstreaming video.
To do this, our smartphone will need to run a full TCP/IP protocol stack (including the physical link, network, transport, and application layers) and connect into the Internet via the cellular data network.
The topic of cellular data networks is a rather bewildering collection of competing andever-evolving standards as one generation (and half-generation) succeeds the former and introducesnew technologies and services with new acronyms.
To make matters worse, there’s no single officialbody that sets requirements for 2.5G, 3G, 3.5G, or 4G technologies, making it hard to sort out thedifferences among competing standards.
In our discussion below, we’ll focus on the UMTS (UniversalMobile Telecommunications Service) 3G and 4G standards developed by the 3rd Generation Partnership project (3GPP) [3GPP 2016].
Let’s first take a top-down look at 3G cellular data network architecture shown in Figure 7.19.
Figure 7.19 3G system architecture 3G Core Network The 3G core cellular data network connects radio access networks to the public Internet.
The core network interoperates with components of the existing cellular voice network (in particular, the MSC) that we previously encountered in Figure 7.18.
Given the considerable amount of existing infrastructure (and profitable services!)
in the existing cellular voice network, the approach taken by the designers of3G data services is clear: leave the existing core GSM cellular voice network untouched, adding additional cellular data functionality in parallel to the existing cellular voice network .
The alternative— integrating new data services directly into the core of the existing cellular voice network—would have raised the same challenges encountered in Section 4.3, where we discussed integrating new (IPv6) and legacy (IPv4) technologies in the Internet.
There are two types of nodes in the 3G core network: Serving GPRS Support Nodes (SGSNs)  and Gateway GPRS Support Nodes (GGSNs) . (
GPRS stands for Generalized Packet Radio Service, an early cellular data service in 2G networks; here we discuss the evolved version of GPRS in 3G networks).
An SGSN is responsible for delivering datagrams to/from the mobile nodes in the radioaccess network to which the SGSN is attached.
The SGSN interacts with the cellular voice network’sMSC for that area, providing user authorization and handoff, maintaining location (cell) information about active mobile nodes, and performing datagram forwarding between mobile nodes in the radio access network and a GGSN.
The GGSN acts as a gateway, connecting multiple SGSNs into the largerInternet.
A GGSN is thus the last piece of 3G infrastructure that a datagram originating at a mobile nodeencounters before entering the larger Internet.
To the outside world, the GGSN looks like any othergateway router; the mobility of the 3G nodes within the GGSN’s network is hidden from the outsideworld behind the GGSN.
3G Radio Access Network: The Wireless Edge The 3G radio access network is the wireless first-hop network that we see as a 3G user.
The Radio Network Controller (RNC) typically controls several cell base transceiver stations similar to the base stations that we encountered in 2G systems (but officially known in 3G UMTS parlance as a “NodeBs”—a rather non-descriptive name!).
Each cell’s wireless link operates between the mobile nodes anda base transceiver station, just as in 2G networks.
The RNC connects to both the circuit-switchedcellular voice network via an MSC, and to the packet-switched Internet via an SGSN.
Thus, while 3Gcellular voice and cellular data services use different core networks, they share a common first/last-hopradio access network.
A significant change in 3G UMTS over 2G networks is that rather than using GSM’s FDMA/TDMA scheme, UMTS uses a CDMA technique known as Direct Sequence Wideband CDMA (DS-WCDMA) [Dahlman 1998]  within TDMA slots; TDMA slots, in turn, are available on multiple frequencies—an interesting use of all three dedicated channel-sharing approaches that we earlier identified in Chapter 6 and similar to the approach taken in wired cable access networks (see Section 6.3.4).
This change requires a new 3G cellular wireless-access network operating in parallel with the 2G BSS radio networkshown in Figure 7.19.
The data service associated with the WCDMA specification is known as HSPA (High Speed Packet Access) and promises downlink data rates of up to 14 Mbps.
Details regarding 3G networks can be found at the 3rd Generation Partnership Project (3GPP) Web site [3GPP 2016].
7.4.3 On to 4G: LTE Fourth generation (4G) cellular systems are becoming widely deployed.
In 2015, more than 50 countries had 4G coverage exceeding 50%.
The 4G Long-Term ­Evolution (LTE) standard [Sauter 2014]  put forward by the 3GPP has two important innovations over 3G systems an all-IP core network and an
enhanced radio access network, as discussed below.
4G System Architecture: An All-IP Core Network Figure 7.20 shows the overall 4G network architecture, which (unfortunately) introduces yet another (rather impenetrable) new vocabulary and set of acronyms for Figure 7.20 4G network architecture ­network ­components.
But let’s not get lost in these acronyms!
There are two important high-level observations about the 4G architecture: A unified, all-IP network architecture.
 Unlike the 3G network shown in Figure 7.19, which has separate network components and paths for voice and data traffic, the 4G architecture shown in Figure 7.20 is “all-IP”—both voice and data are carried in IP datagrams to/from the wireless device (the User Equipment, UE in 4G parlance) to the gateway to the packet gateway (P-GW) that connects the 4G edge network to the rest of the network.
With 4G, the last vestiges of cellular networks’ roots in the telephony have disappeared, giving way to universal IP service!
A clear separation of the 4G data plane and 4G control plane.
 Mirroring our distinction between the data and control planes for IP’s network layer in Chapters 4 and 5 respectively, the 4G network architecture also clearly separates the data and control planes.
We’ll discuss their functionalitybelow.
A clear separation between the radio access network, and the all-IP-core ­network.
IP datagrams carrying user data are forwarded between the user (UE) and the gateway (P-GW in
Figure 7.20) over a 4G-internal IP network to the external Internet.
Control packets are exchanged over this same internal network among the 4G’s control services components, whose roles are described below.
The principal components of the 4G architecture are as follows.
The eNodeB  is the logical descendant of the 2G base station and the 3G Radio Network Controller (a.k.a Node B) and again plays a central role here.
Its data-plane role is to forward datagramsbetween UE (over the LTE radio access ­network) and the P-GW.
UE datagrams are encapsulated at the eNodeB and tunneled to the P-GW through the 4G network’s all-IP enhanced packet core (EPC).
This tunneling between the eNodeB and P-GW is similar the tunneling we saw in Section 4.3 of IPv6 datagrams between two IPv6 endpoints through a network of IPv4 routers.
These tunnels may have associated quality of service (QoS) guarantees.
For example, a 4G network may guarantee that voice traffic experiences no more than a 100 msec delaybetween UE and P-GW, and has a packet loss rate of less than 1%; TCP traffic might have a guarantee of 300 msec and a packet loss rate of less than .0001% [Palat 2009] .
We’ll cover QoS in Chapter 9.
In the control plane, the eNodeB handles registration and mobility signaling traffic on behalf of the UE.
The Packet Data Network Gateway (P-GW)  allocates IP addresses to the UEs and performs QoS enforcement.
As a tunnel endpoint it also performs datagram encapsulation/decapsulation when forwarding a datagram to/from a UE.
The Serving Gateway (S-GW) is the data-plane mobility anchor point—all UE traffic will pass through the S-GW.
The S-GW also performs charging/billing functions and lawful traffic interception.
The Mobility Management Entity (MME)  performs connection and mobility management on behalf of the UEs resident in the cell it controls.
It receives UE subscription information from the HHS.
We cover mobility in cellular networks in detail in Section 7.7.
The Home Subscriber Server (HSS)  contains UE information including roaming access capabilities, quality of service profiles, and authentication information.
As we’ll see in Section 7.7, the HSS obtains this information from the UE’s home cellular provider.
Very readable introductions to 4G network architecture and its EPC are [Motorola 2007; Palat 2009 ; Sauter 2014] .
LTE Radio Access Network LTE uses a combination of frequency division multiplexing and time division multiplexing on the downstream channel, known as orthogonal frequency division multiplexing (OFDM) [Rohde 2008; Ericsson 2011]. (
The term “orthogonal” comes from the fact the signals being sent on different frequency
channels are created so that they interfere very little with each other, even when channel frequencies are tightly spaced).
In LTE, each active mobile node is allocated one or more 0.5 ms time slots in one or more of the channel frequencies.
Figure 7.21 shows an allocation of eight time slots over four frequencies.
By being allocated increasingly more time slots (whether on the same frequency or on different frequencies), a mobile node is able to achieve increasingly higher transmission rates.
Slot(re)allocation among mobile Figure 7.21 Twenty 0.5 ms slots organized into 10 ms frames at each frequency.
An eight-slot allocation is shown shaded.
nodes can be performed as often as once every millisecond.
Different modulation schemes can also be used to change the transmission rate; see our earlier discussion of Figure 7.3 and dynamic selection of modulation schemes in WiFi networks.
The particular allocation of time slots to mobile nodes is not mandated by the LTE standard.
Instead, the decision of which mobile nodes will be allowed to transmit in a given time slot on a given frequency isdetermined by the scheduling algorithms provided by the LTE equipment vendor and/or the network operator.
With opportunistic scheduling [Bender 2000 ; Kolding 2003; Kulkarni 2005], matching the physical-layer protocol to the channel conditions between the sender and receiver and choosing the receivers to which packets will be sent based on channel conditions allow the radio network controller to make best use of the wireless medium.
In addition, user priorities and contracted levels of service (e.g.,silver, gold, or platinum) can be used in scheduling downstream packet transmissions.
In addition to theLTE capabilities described above, LTE-Advanced allows for downstream bandwidths of hundreds of Mbps by allocating aggregated channels to a mobile node [Akyildiz 2010].
An additional 4G wireless technology—WiMAX (World Interoperability for Microwave Access)—is a family of IEEE 802.16 standards that differ significantly from LTE.
WiMAX has not yet been able to enjoy the widespread deployment of LTE.
A detailed discussion of WiMAX can be found on this book’s Website.
7.5 Mobility Management: Principles Having covered the wireless  nature of the communication links in a wireless network, it’s now time to turn our attention to the mobility  that these wireless links enable.
In the broadest sense, a mobile node is one that changes its point of attachment into the network over time.
Because the term mobility  has taken on many meanings in both the computer and telephony worlds, it will serve us well first to consider several dimensions of mobility in some detail.
From the network layer’s standpoint, how mobile is a user?
 A physically mobile user will present a very different set of challenges to the network layer, depending on how he or she moves between points of attachment to the network.
At one end of the spectrum in Figure 7.22, a user may carry a laptop with a wireless network interface card around in a building.
As we saw in Section 7.3.4, this user is not mobile from a network-layer perspective.
Moreover, if the user associates with the same access point regardless of location, the user is not even mobile from the perspective of the link layer.
At the other end of the spectrum, consider the user zooming along the autobahn in a BMW or Tesla at 150 kilometers per hour, passing through multiple wireless access networks and wanting tomaintain an uninterrupted TCP connection to a remote application throughout the trip.
This user is definitely  mobile!
In between Figure 7.22 Various degrees of mobility, from the network layer’s point of view these extremes is a user who takes a laptop from one location (e.g., office or dormitory) into another (e.g., coffeeshop, classroom) and wants to connect into the-network in the new location.
This user is also mobile (although less so than the BMW driver!)
but does not need to maintain an ongoing connection while moving between points of attachment to the network.
Figure 7.22 illustrates this spectrum of user mobility from the network layer’s perspective.
How important is it for the mobile node’s address to always remain the same?
 With mobile telephony, your phone number—essentially the network-layer address of your phone—remains the same as you travel from one provider’s mobile phone network to another.
Must a laptop similarly
maintain the same IP address while moving between IP networks?
The answer to this question will depend strongly on the applications being run.
For the BMW or Tesla driver who wants to maintain an uninterrupted TCP connection to a remote application whilezipping along the autobahn, it would be convenient to maintain the same IP address.
Recall from Chapter 3 that an Internet application needs to know the IP address and port number of the remote entity with which it is communicating.
If a mobile entity is able to maintain its IP address as it moves, mobility becomes invisible from the application standpoint.
There is great value to this transparency—an application need not be concerned with a potentially changing IP address, and the sameapplication code serves mobile and nonmobile connections alike.
We’ll see in the following sectionthat mobile IP provides this transparency, allowing a mobile node to maintain its permanent IPaddress while moving among networks.
On the other hand, a less glamorous mobile user might simply want to turn off an office laptop, bring that laptop home, power up, and work from home.
If the laptop functions primarily as a client in client-server applications (e.g., send/read e-mail, browse the Web, Telnet to a remote host) fromhome, the particular IP address used by the laptop is not that important.
In particular, one could getby fine with an address that is temporarily allocated to the laptop by the ISP serving the home.
We saw in Section 4.3 that DHCP already provides this functionality.
What supporting wired infrastructure is available?
 In all of our scenarios above, we’ve implicitly assumed that there is a fixed infrastructure to which the mobile user can connect—for example, the home’s ISP network, the wireless access network in the office, or the wireless access networkslining the autobahn.
What if no such infrastructure exists?
If two users are within communication proximity of each other, can they establish a network connection in the absence of any other network-layer infrastructure?
Ad hoc networking provides precisely these capabilities.
This rapidlydeveloping area is at the cutting edge of mobile networking research and is beyond the scope of this book. [
Perkins 2000]  and the IETF Mobile Ad Hoc Network (manet) working group Web pages [manet 2016] provide thorough treatments of the subject.
In order to illustrate the issues involved in allowing a mobile user to maintain ongoing connections while moving between networks, let’s consider a human analogy.
A twenty-something adult moving out of thefamily home becomes mobile, living in a series of dormitories and/or apartments, and often changing addresses.
If an old friend wants to get in touch, how can that friend find the address of her mobile friend?
One common way is to contact the family, since a mobile adult will often register his or hercurrent address with the family (if for no other reason than so that the parents can send money to helppay the rent!).
The family home, with its permanent address, becomes that one place that others can goas a first step in communicating with the mobile adult.
Later communication from the friend may beeither indirect (for example, with mail being sent first to the parents’ home and then forwarded to themobile adult) or direct (for example, with the friend using the address obtained from the parents to sendmail directly to her mobile friend).
In a network setting, the permanent home of a mobile node (such as a laptop or smartphone) is known as the home network, and the entity within the home network that performs the mobility management functions discussed below on behalf of the mobile node is known as the home agent.
The network in which the mobile node is currently residing is known as the foreign (or visited) network, and the entity within the foreign network that helps the mobile node with the mobility management functions discussedbelow is known as a foreign agent .
For mobile professionals, their home network might likely be their company network, while the visited network might be the network of a colleague they are visiting.
Acorrespondent  is the entity wishing to communicate with the mobile node.
Figure 7.23 illustrates these concepts, as well as addressing concepts considered below.
In Figure 7.23, note that agents are shown as being collocated with routers (e.g., as processes running on routers), but alternatively they could beexecuting on other hosts or servers in the network.
7.5.1 Addressing We noted above that in order for user mobility to be transparent to network applications, it is desirablefor a mobile node to keep its address as it moves from one network Figure 7.23 Initial elements of a mobile network architecture
to another.
When a mobile node is resident in a foreign network, all traffic addressed to the node’s permanent address now needs to be routed to the foreign network.
How can this be done?
One option isfor the foreign network to advertise to all other networks that the mobile node is resident in its network.
This could be via the usual exchange of intradomain and interdomain routing information and wouldrequire few changes to the existing routing infrastructure.
The foreign network could simply advertise to its neighbors that it has a highly specific route to the mobile node’s permanent address (that is, essentially inform other networks that it has the correct path for routing datagrams to the mobile node’s permanent address; see Section 4.3).
These neighbors would then propagate this routing information throughout the network as part of the normal procedure of updating routing information and forwarding tables.
When the mobile node leaves one foreign network and joins another, the new foreign networkwould advertise a new, highly specific route to the mobile node, and the old foreign network wouldwithdraw its routing information regarding the mobile node.
This solves two problems at once, and it does so without making significant changes to the network- layer infrastructure.
Other networks know the location of the mobile node, and it is easy to route datagrams to the mobile node, since the forwarding tables will direct datagrams to the foreign network.
A significant drawback, however, is that of scalability.
If mobility management were to be theresponsibility of network routers, the routers would have to maintain forwarding table entries forpotentially millions of mobile nodes, and update these entries as nodes move.
Some additionaldrawbacks are explored in the problems at the end of this chapter.
An alternative approach (and one that has been adopted in practice) is to push mobility functionality from the network core to the network edge—a recurring theme in our study of Internet architecture.
A natural way to do this is via the mobile node’s home network.
In much the same way that parents of themobile twenty-something track their child’s location, the home agent in the mobile node’s home networkcan track the foreign network in which the mobile node resides.
A protocol between the mobile node (ora foreign agent representing the mobile node) and the home agent will certainly be needed to updatethe mobile node’s location.
Let’s now consider the foreign agent in more detail.
The conceptually simplest approach, shown in Figure 7.23, is to locate foreign agents at the edge routers in the foreign network.
One role of the foreign agent is to create a so-called care-of address (COA)  for the mobile node, with the network portion of the COA matching that of the foreign network.
There are thus two addresses associated with a mobile node, its permanent address  (analogous to our mobile youth’s family’s home address) and its COA, sometimes known as a foreign address (analogous to the address of the house in which our mobile youth is currently residing).
In the example in Figure 7.23, the permanent address of the mobile node is 128.119.40.186.
When visiting network 79.129.13/24, the mobile node has a COA of 79.129.13.2.
A second role of the foreign agent is to inform the home agent that the mobile node isresident in its (the foreign agent’s) network and has the given COA.
We’ll see shortly that the COA will
be used to “reroute” datagrams to the mobile node via its foreign agent.
Although we have separated the functionality of the mobile node and the foreign agent, it is worth noting that the mobile node can also assume the responsibilities of the foreign agent.
For example, the mobilenode could obtain a COA in the foreign network (for example, using a protocol such as DHCP) and itselfinform the home agent of its COA.
7.5.2 Routing to a Mobile Node We have now seen how a mobile node obtains a COA and how the home agent can be informed of thataddress.
But having the home agent know the COA solves only part of the problem.
How shoulddatagrams be addressed and forwarded to the mobile node?
Since only the home agent (and notnetwork-wide routers) knows the location of the mobile node, it will no longer suffice to simply address adatagram to the mobile node’s permanent address and send it into the network-layer infrastructure.
Something more must be done.
Two approaches can be identified, which we will refer to as indirect and direct routing.
Indirect Routing to a Mobile Node Let’s first consider a correspondent that wants to send a datagram to a mobile node.
In the indirect routing  approach, the correspondent simply addresses the datagram to the mobile node’s permanent address and sends the datagram into the network, blissfully unaware of whether the mobile node is resident in its home network or is visiting a foreign network; mobility is thus completely transparent to the correspondent.
Such datagrams are first routed, as usual, to the mobile node’s home network.
This is illustrated in step 1 in Figure 7.24.
Let’s now turn our attention to the home agent.
In addition to being responsible for interacting with a foreign agent to track the mobile node’s COA, the home agent has another very important function.
Itssecond job is to be on the lookout for arriving datagrams addressed to nodes whose home network isthat of the home agent but that are currently resident in a foreign network.
The home agent interceptsthese datagrams and then forwards them to a mobile node in a two-step process.
The datagram is first forwarded to the foreign agent, using the mobile node’s COA (step 2 in Figure 7.24), and then forwarded from the foreign agent to the mobile node (step 3 in Figure 7.24).
Figure 7.24 Indirect routing to a mobile node It is instructive to consider this rerouting in more detail.
The home agent will need to address the datagram using the mobile node’s COA, so that the network layer will route the datagram to the foreign network.
On the other hand, it is desirable to leave the correspondent’s datagram intact, since theapplication receiving the datagram should be unaware that the datagram was forwarded via the homeagent.
Both goals can be satisfied by having the home agent encapsulate the correspondent’s original complete datagram within a new (larger) datagram.
This larger datagram is addressed and delivered tothe mobile node’s COA.
The foreign agent, who “owns” the COA, will receive and decapsulate thedatagram—that is, remove the correspondent’s original datagram from within the larger encapsulating datagram and forward (step 3 in Figure 7.24) the original datagram to the mobile node.
Figure 7.25 shows a correspondent’s original datagram being sent to the home network, an encapsulated datagram being sent to the foreign agent, and the original datagram being delivered to the mobile node.
The sharp reader will note that the encapsulation/decapsulation described here is identical to the notion of tunneling, discussed in Section 4.3 in the context of IP multicast and IPv6.
Let’s next consider how a mobile node sends datagrams to a correspondent.
This is quite simple, as themobile node can address its datagram directly to the correspondent (using its own permanent address as the source address, and the
Figure 7.25 Encapsulation and decapsulation correspondent’s address as the destination address).
Since the mobile node knows the correspondent’s address, there is no need to route the datagram back through the home agent.
This is shown as step 4 in Figure 7.24.
Let’s summarize our discussion of indirect routing by listing the new network-layer functionality required to support mobility.
A mobile-node–to–foreign-agent protocol.
 The mobile node will register with the foreign agent when attaching to the foreign network.
Similarly, a mobile node will deregister with the foreign agentwhen it leaves the foreign network.
A foreign-agent–to–home-agent registration protocol.
 The foreign agent will register the mobile node’s COA with the home agent.
A foreign agent need not explicitly deregister a COA when amobile node leaves its network, because the subsequent registration of a new COA, when themobile node moves to a new network, will take care of this.
A home-agent datagram encapsulation protocol.
 Encapsulation and forwarding of the correspondent’s original datagram within a datagram addressed to the COA.
A foreign-agent decapsulation protocol.
 Extraction of the correspondent’s original datagram from the encapsulating datagram, and the forwarding of the original datagram to the mobile node.
The previous discussion provides all the pieces—foreign agents, the home agent, and indirect
forwarding—needed for a mobile node to maintain an ongoing connection while moving among networks.
As an example of how these pieces fit together, assume the mobile node is attached to foreign network A, has registered a COA in network A with its home agent, and is receiving datagramsthat are being indirectly routed through its home agent.
The mobile node now moves to foreign networkB and registers with the foreign agent in network B, which informs the home agent of the mobile node’snew COA.
From this point on, the home agent will reroute datagrams to foreign network B. As far as a correspondent is concerned, mobility is transparent—datagrams are routed via the same home agent both before and after the move.
As far as the home agent is concerned, there is no disruption in the flowof datagrams—arriving datagrams are first forwarded to foreign network A; after the change in COA,datagrams are forwarded to foreign network B. But will the mobile node see an interrupted flow ofdatagrams as it moves between networks?
As long as the time between the mobile node’sdisconnection from network A (at which point it can no longer receive datagrams via A) and itsattachment to network B (at which point it will register a new COA with its home agent) is small, few datagrams will be lost.
Recall from Chapter 3 that end-to-end connections can suffer datagram loss due to network congestion.
Hence occasional datagram loss within a connection when a node moves between networks is by no means a catastrophic problem.
If loss-free communication is required, upper-layer mechanisms will recover from datagram loss, whether such loss results from network congestion or from user mobility.
An indirect routing approach is used in the mobile IP standard [RFC 5944] , as discussed in Section 7.6.
Direct Routing to a Mobile NodeThe indirect routing approach illustrated in Figure 7.24 suffers from an inefficiency known as the triangle routing problem —datagrams addressed to the mobile node must be routed first to the home agent and then to the foreign network, even when a much more efficient route exists between the correspondent and the mobile node.
In the worst case, imagine a mobile user who is visiting the foreignnetwork of a colleague.
The two are sitting side by side and exchanging data over the network.
Datagrams from the correspondent (in this case the colleague of the visitor) are routed to the mobileuser’s home agent and then back again to the foreign network!
Direct routing overcomes the inefficiency of triangle routing, but does so at the cost of additional complexity.
In the direct routing approach, a correspondent agent in the correspondent’s network first learns the COA of the mobile node.
This can be done by having the correspondent agent query the home agent, assuming that (as in the case of indirect routing) the mobile node has an up-to-date valuefor its COA registered with its home agent.
It is also possible for the correspondent itself to perform thefunction of the correspondent agent, just as a mobile node could perform the function of the foreign agent.
This is shown as steps 1 and 2 in Figure 7.26.
The correspondent agent then tunnels datagrams directly to the mobile node’s COA, in a manner analogous to the tunneling performed by the homeagent, steps 3 and 4 in Figure 7.26.
While direct routing overcomes the triangle routing problem, it introduces two important additional challenges: A mobile-user location protocol  is needed for the correspondent agent to query the home agent to obtain the mobile node’s COA (steps 1 and 2 in Figure 7.26).
When the mobile node moves from one foreign network to another, how will data now be forwarded to the new foreign network?
In the case of indirect routing, this problem was easily solved by updating the COA maintained by the home agent.
However, with direct routing, the home agent isqueried for the COA by the correspondent agent only once, at the beginning of the session.
Thus, updating the COA at the home agent, while necessary, will not be enough to solve the problem of routing data to the mobile node’s new foreign network.
One solution would be to create a new protocol to notify the correspondent of the changing COA.
Analternate solution, and one that we’ll see adopted in practice Figure 7.26 Direct routing to a mobile user
in GSM networks, works as follows.
Suppose data is currently being forwarded to the mobile node in the foreign network where the mobile node was located when the session first started (step 1 in Figure 7.27).
We’ll identify the foreign agent in that foreign network where the mobile node was first found as the anchor ­foreign agent .
When the mobile node moves to a new foreign network (step 2 in Figure 7.27), the mobile node registers with the new foreign agent (step 3), and the new foreign agent provides the anchor foreign agent with the mobile node’s new COA (step 4).
When the anchor foreign agent receives an encapsulated datagram for a departed mobile node, it can then re-encapsulate thedatagram and forward it to the mobile node (step 5) using the new COA.
If the mobile node later movesyet again to a new foreign network, the foreign agent in that new visited network would then contact theanchor foreign agent in order to set up forwarding to this new foreign network.
Figure 7.27 Mobile transfer between networks with direct routing
7.6 Mobile IP The Internet architecture and protocols for supporting mobility, collectively known as mobile IP, are defined primarily in RFC 5944 for IPv4.
Mobile IP is a flexible standard, supporting many differentmodes of operation (for example, operation with or without a foreign agent), multiple ways for agentsand mobile nodes to discover each other, use of single or multiple COAs, and multiple forms ofencapsulation.
As such, mobile IP is a complex standard, and would require an entire book to describe in detail; indeed one such book is [Perkins 1998b] .
Our modest goal here is to provide an overview of the most important aspects of mobile IP and to illustrate its use in a few common-case scenarios.
The mobile IP architecture contains many of the elements we have considered above, including the concepts of home agents, foreign agents, care-of addresses, and encapsulation/decapsulation.
The current standard [RFC 5944]  specifies the use of indirect routing to the mobile node.
The mobile IP standard consists of three main pieces: Agent discovery.
Mobile IP defines the protocols used by a home or foreign agent to advertise its services to mobile nodes, and protocols for mobile nodes to solicit the services of a foreign or home agent.
Registration with the home agent.
 Mobile IP defines the protocols used by the mobile node and/or foreign agent to register and deregister COAs with a mobile node’s home agent.
Indirect routing of datagrams.
 The standard also defines the manner in which datagrams are forwarded to mobile nodes by a home agent, including rules for forwarding datagrams, rules for handling error conditions, and several forms of encapsulation [RFC 2003 , RFC 2004] .
Security considerations are prominent throughout the mobile IP standard.
For example, authentication of a mobile node is clearly needed to ensure that a ­malicious user does not register a bogus care-of address with a home agent, which could cause all datagrams addressed to an IP address to beredirected to the malicious user.
Mobile IP achieves security using many of the mechanisms that we will examine in Chapter 8, so we will not address security considerations in our discussion below.
Agent Discovery A mobile IP node arriving to a new network, whether attaching to a foreign network or returning to its home network, must learn the identity of the corresponding foreign or home agent.
Indeed it is the discovery of a new foreign agent, with a new network address, that allows the network layer in a mobile
node to learn that it has moved into a new foreign network.
This process is known as agent discovery.
Agent discovery can be accomplished in one of two ways: via agent advertisement or via agent solicitation.
With agent advertisement , a foreign or home agent advertises its services using an extension to the existing router discovery protocol [RFC 1256] .
The agent periodically broadcasts an ICMP message with a type field of 9 (router discovery) on all links to which it is connected.
The router discovery message contains the IP address of the router (that is, the agent), thus allowing a mobile node to learnthe agent’s IP address.
The router discovery message also contains a mobility agent advertisementextension that contains additional information needed by the mobile node.
Among the more importantfields in the extension are the following: Home agent bit (H).
 Indicates that the agent is a home agent for the network in which it resides.
Foreign agent bit (F).
 Indicates that the agent is a foreign agent for the network in which it resides.
Registration required bit (R).
 Indicates that a mobile user in this network must register with a foreign agent.
In particular, a mobile user cannot obtain a care-of address in the foreign network (forexample, using DHCP) and assume the functionality of the foreign agent for itself, without registeringwith the foreign agent.
Figure 7.28 ICMP router discovery message with mobility agent ­advertisement extension M, G encapsulation bits.
 Indicate whether a form of encapsulation other than IP-in-IP encapsulation will be used.
Care-of address (COA) fields.
A list of one or more care-of addresses provided by the foreign
agent.
In our example below, the COA will be associated with the foreign agent, who will receive datagrams sent to the COA and then forward them to the appropriate mobile node.
The mobile user will select one of these addresses as its COA when registering with its home agent.
Figure 7.28 illustrates some of the key fields in the agent advertisement message.
With agent solicitation , a mobile node wanting to learn about agents without waiting to receive an agent advertisement can broadcast an agent solicitation message, which is simply an ICMP message with type value 10.
An agent receiving the solicitation will unicast an agent advertisement directly to themobile node, which can then proceed as if it had received an unsolicited advertisement.
Registration with the Home Agent Once a mobile IP node has received a COA, that address must be registered with the home agent.
This can be done either via the foreign agent (who then registers the COA with the home agent) or directly by the mobile IP node itself.
We consider the former case below.
Four steps are involved.
1.
Following the receipt of a foreign agent advertisement, a mobile node sends a mobile IP registration message to the foreign agent.
The registration message is carried within a UDP datagram and sent to port 434.
The registration message carries a COA advertised by theforeign agent, the address of the home agent (HA), the permanent address of the mobile node (MA), the requested lifetime of the registration, and a 64-bit registration identification.
The requested registration lifetime is the number of seconds that the registration is to be valid.
If theregistration is not renewed at the home agent within the specified lifetime, the registration willbecome invalid.
The registration identifier acts like a sequence number and serves to match areceived registration reply with a registration request, as discussed below.
2.
The foreign agent receives the registration message and records the mobile node’s permanent IP address.
The foreign agent now knows that it should be looking for datagrams containing an encapsulated datagram whose destination address matches the permanent address of themobile node.
The foreign agent then sends a mobile IP registration message (again, within a UDP datagram) to port 434 of the home agent.
The message contains the COA, HA, MA, encapsulation format requested, requested registration lifetime, and registration identification.
3.
The home agent receives the registration request and checks for authenticity and correctness.
The home agent binds the mobile node’s permanent IP address with the COA; in the future, datagrams arriving at the home agent and addressed to the mobile node will now beencapsulated and tunneled to the COA.
The home agent sends a mobile IP registration reply containing the HA, MA, actual registration lifetime, and the registration identification of the request that is being satisfied with this reply.
4.
The foreign agent receives the registration reply and then forwards it to the mobile node.
At this point, registration is complete, and the mobile node can receive datagrams sent to its permanent address.
Figure 7.29 illustrates these steps.
Note that the home agent specifies a lifetime that is smaller than the lifetime requested by the mobile node.
A foreign agent need not explicitly deregister a COA when a mobile node leaves its network.
This will occur automatically, when the mobile node moves to a new network (whether another foreign network or its home network) and registers a new COA.
The mobile IP standard allows many additional scenarios and capabilities in addition to those described previously.
The interested reader should consult [Perkins 1998b ; RFC 5944] .
Figure 7.29 Agent advertisement and mobile IP registration
7.7 Managing Mobility in Cellular Networks Having examined how mobility is managed in IP networks, let’s now turn our attention to networks with an even longer history of supporting mobility—cellular telephony networks.
Whereas we focused on the first-hop wireless link in cellular networks in Section 7.4, we’ll focus here on mobility, using the GSM cellular network [Goodman 1997; Mouly 1992; Scourias 2012 ; Kaaranen 2001 ; Korhonen 2003; Turner 2012] as our case study, since it is a mature and widely deployed technology.
Mobility in 3G and 4G networks is similar in principle to that used in GSM.
As in the case of mobile IP, we’ll see that a number of the fundamental principles we identified in Section 7.5 are embodied in GSM’s network architecture.
Like mobile IP, GSM adopts an indirect routing approach (see Section 7.5.2), first routing the correspondent’s call to the mobile user’s home network and from there to the visited network.
In GSM terminology, the mobile users’s home network is referred to as the mobile user’s home public land mobile network (home PLMN) .
Since the PLMN acronym is a bit of a mouthful, and mindful of our quest to avoid an alphabet soup of acronyms, we’ll refer to the GSM home PLMN simply as the home network.
The home network is the cellular provider with which the mobile user has a subscription (i.e.,the provider that bills the user for monthly cellular service).
The visited PLMN, which we’ll refer to simplyas the visited network, is the network in which the mobile user is currently residing.
As in the case of mobile IP, the responsibilities of the home and visited networks are quite different.
The home network maintains a database known as the home location register (HLR) , which contains the permanent cell phone number and subscriber profile information for each of its subscribers.
Importantly, the HLR also contains information about the current locations of thesesubscribers.
That is, if a mobile user is currently roaming in another provider’s cellular network, the HLR contains enough information to obtain (via a process we’ll describe shortly) an address in the visited network to which a call to the mobile user should be routed.
As we’ll see, a special switch inthe home network, known as the Gateway Mobile services Switching Center (GMSC)  is contacted by a correspondent when a call is placed to a mobile user.
Again, in our quest to avoid an alphabetsoup of acronyms, we’ll refer to the GMSC here by a more descriptive term, home MSC.
The visited network maintains a database known as the visitor location register (VLR) .
The VLR contains an entry for each mobile user that is currently in the portion of the network served by the VLR.
VLR entries thus come and go as mobile users enter and leave the network.
A VLR is usuallyco-located with the mobile switching center (MSC) that coordinates the setup of a call to and from the visited network.
In practice, a provider’s cellular network will serve as a home network for its subscribers and as a visited network for mobile users whose subscription is with a different cellular provider.
Figure 7.30 Placing a call to a mobile user: Indirect routing 7.7.1 Routing Calls to a Mobile User We’re now in a position to describe how a call is placed to a mobile GSM user in a visited network.
We’ll consider a simple example below; more complex scenarios are described in [Mouly 1992].
The steps, as illustrated in Figure 7.30, are as follows: 1.
The correspondent dials the mobile user’s phone number.
This number itself does not refer to a particular telephone line or location (after all, the phone number is fixed and the user is mobile!).
The leading digits in the number are sufficient to globally identify the mobile’s home network.
The call is routed from the correspondent through the PSTN to the home MSC in the mobile’s home network.
This is the first leg of the call.
2.
The home MSC receives the call and interrogates the HLR to determine the location of the mobile user.
In the simplest case, the HLR returns the mobile station roaming number (MSRN), which we will refer to as the roaming number.
Note that this number is different from the mobile’s permanent phone number, which is associated with the mobile’s home network.
The
roaming number is ephemeral: It is temporarily assigned to a mobile when it enters a visited network.
The roaming number serves a role similar to that of the care-of address in mobile IP and, like the COA, is invisible to the correspondent and the mobile.
If HLR does not have the roaming number, it returns the address of the VLR in the visited network.
In this case (not shown in Figure 7.30), the home MSC will need to query the VLR to obtain the roaming number of the mobile node.
But how does the HLR get the roaming number or the VLR address in the first place?
What happens to these values when the mobile user moves to another visited network?
We’ll consider these important questions shortly.
3.
Given the roaming number, the home MSC sets up the second leg of the call through the network to the MSC in the visited network.
The call is completed, being routed from the correspondent to the home MSC, and from there to the visited MSC, and from there to the basestation serving the mobile user.
An unresolved question in step 2 is how the HLR obtains information about the location of the mobileuser.
When a mobile telephone is switched on or enters a part of a visited network that is covered by anew VLR, the mobile must register with the visited network.
This is done through the exchange ofsignaling messages between the mobile and the VLR.
The visited VLR, in turn, sends a location updaterequest message to the mobile’s HLR.
This message informs the HLR of either the roaming number atwhich the mobile can be contacted, or the address of the VLR (which can then later be queried to obtainthe mobile number).
As part of this exchange, the VLR also obtains subscriber information from the HLRabout the mobile and determines what services (if any) should be accorded the mobile user by the visited network.
7.7.2 Handoffs in GSM A handoff occurs when a mobile station changes its association from one base station to another during a call.
As shown in Figure 7.31, a mobile’s call is initially (before handoff) routed to the mobile through one base station (which we’ll refer to as the old base station), and after handoff is routed to the mobile through another base
Figure 7.31 Handoff scenario between base stations with a common MSC station (which we’ll refer to as the new base station).
Note that a handoff between base stations results not only in the mobile transmitting/receiving to/from a new base station, but also in the rerouting of the ongoing call from a switching point within the network to the new base station.
Let’s initially assume thatthe old and new base stations share the same MSC, and that the rerouting occurs at this MSC.
There may be several reasons for handoff to occur, including (1) the signal between the current base station and the mobile may have deteriorated to such an extent that the call is in danger of beingdropped, and (2) a cell may have become overloaded, handling a large number of calls.
This congestionmay be alleviated by handing off mobiles to less congested nearby cells.
While it is associated with a base station, a mobile periodically measures the strength of a beacon signal from its current base station as well as beacon signals from nearby base stations that it can “hear.
”These measurements are reported once or twice a second to the mobile’s current base station.
Handoffin GSM is initiated by the old base station based on these measurements, the current loads of mobiles in nearby cells, and other factors [Mouly 1992].
The GSM standard does not specify the specific algorithm to be used by a base station to determine whether or not to perform handoff.
Figure 7.32 illustrates the steps involved when a base station does decide to hand off a mobile user: 1.
The old base station (BS) informs the visited MSC that a handoff is to be performed and the BS (or possible set of BSs) to which the mobile is to be handed off.
2.
The visited MSC initiates path setup to the new BS, allocating the resources needed to carry thererouted call, and signaling the new BS that a handoff is about to occur.
3.
The new BS allocates and activates a radio channel for use by the mobile.
4.
The new BS signals back to the visited MSC and the old BS that the visited-MSC-to-new-BSpath has been established and that the mobile should be
Figure 7.32 Steps in accomplishing a handoff between base stations with a common MSC informed of the impending handoff.
The new BS provides all of the information that the mobile will need to associate with the new BS.
5.
The mobile is informed that it should perform a handoff.
Note that up until this point, the mobile has been blissfully unaware that the network has been laying the groundwork (e.g., allocating a channel in the new BS and allocating a path from the visited MSC to the new BS) for a handoff.
6.
The mobile and the new BS exchange one or more messages to fully activate the new channel in the new BS.
7.
The mobile sends a handoff complete message to the new BS, which is forwarded up to thevisited MSC.
The visited MSC then reroutes the ongoing call to the mobile via the new BS.
8.
The resources allocated along the path to the old BS are then released.
Let’s conclude our discussion of handoff by considering what happens when the mobile moves to a BS that is associated with a different MSC than the old BS, and what happens when this inter-MSC handoff occurs more than once.
As shown in Figure 7.33, GSM defines the notion of an anchor MSC .
The anchor MSC is the MSC visited by the mobile when a call first begins; the anchor MSC thus remainsunchanged during the call.
Throughout the call’s duration and regardless of the number of inter-MSC Figure 7.33 Rerouting via the anchor MSC
Table 7.2 Commonalities between mobile IP and GSM mobility GSM element Comment on GSM element Mobile IPelement Home system Network to which the mobile user's permanent phone numberbelongs.
Homenetwork Gateway mobile switching center or simply home MSC,Home location register(HLR)Home MSC: point of contact to obtain routable address of mobile user.
HLR: database in home system containing permanent phone number, profile information, current locationof mobile user, subscription information.
Home agent Visited system Network other than home system where mobile user is currently residing.
Visitednetwork Visited mobile servicesswitching center,Visitor location register(VLR)Visited MSC: responsible for setting up calls to/from mobilenodes in cells associated with MSC.
VLR: temporary databaseentry in visited system, containing subscription information foreach visiting mobile user.
Foreignagent
Mobile station roaming number (MSRN) or simply roaming numberRoutable address for telephone call segment between home MSC and visited MSC, visible to neither the mobile nor thecorrespondent.
Care-of address transfers performed by the mobile, the call is routed from the home MSC to the anchor MSC, and thenfrom the anchor MSC to the visited MSC where the mobile is currently located.
When a mobile movesfrom the coverage area of one MSC to another, the ongoing call is rerouted from the anchor MSC to the new visited MSC containing the new base station.
Thus, at all times there are at most three MSCs (the home MSC, the anchor MSC, and the visited MSC) between the correspondent and the mobile.
Figure 7.33 illustrates the routing of a call among the MSCs visited by a mobile user.
Rather than maintaining a single MSC hop from the anchor MSC to the current MSC, an alternative approach would have been to simply chain the MSCs visited by the mobile, having an old MSC forwardthe ongoing call to the new MSC each time the mobile moves to a new MSC.
Such MSC chaining can infact occur in IS-41 cellular networks, with an optional path minimization step to remove MSCs between the anchor MSC and the current visited MSC [Lin 2001].
Let’s wrap up our discussion of GSM mobility management with a comparison of mobility managementin GSM and Mobile IP.
The comparison in Table 7.2 indicates that although IP and cellular networks are fundamentally different in many ways, they share a surprising number of common functional elements and overall approaches in handling mobility.
7.8 Wireless and Mobility: Impact on ­Higher-Layer Protocols In this chapter, we’ve seen that wireless networks differ significantly from their wired counterparts at both the link layer (as a result of wireless channel characteristics such as fading, multipath, and hiddenterminals) and at the network layer (as a result of mobile users who change their points of attachment to the network).
But are there important differences at the transport and application layers?
It’s tempting tothink that these differences will be minor, since the network layer provides the same best-effort delivery service model to upper layers in both wired and wireless networks.
Similarly, if protocols such as TCP or UDP are used to provide transport-layer services to applications in both wired and wireless networks,then the application layer should remain unchanged as well.
In one sense our intuition is right—TCP andUDP can (and do) operate in networks with wireless links.
On the other hand, transport protocols ingeneral, and TCP in particular, can sometimes have very different performance in wired and wirelessnetworks, and it is here, in terms of performance, that differences are manifested.
Let’s see why.
Recall that TCP retransmits a segment that is either lost or corrupted on the path between sender and receiver.
In the case of mobile users, loss can result from either network congestion (router buffer overflow) or from handoff (e.g., from delays in rerouting segments to a mobile’s new point of attachmentto the network).
In all cases, TCP’s receiver-to-sender ACK indicates only that a segment was notreceived intact; the sender is unaware of whether the segment was lost due to congestion, duringhandoff, or due to detected bit errors.
In all cases, the sender’s response is the same—to retransmit the segment.
TCP’s congestion-control response is also the same in all cases—TCP decreases its congestion window, as discussed in Section 3.7.
By unconditionally decreasing its congestion window, TCP implicitly assumes that segment loss results from congestion rather than corruption or handoff.
Wesaw in Section 7.2 that bit errors are much more common in wireless networks than in wired networks.
When such bit errors occur or when handoff loss occurs, there’s really no reason for the TCP sender to decrease its congestion window (and thus decrease its sending rate).
Indeed, it may well be the casethat router buffers are empty and packets are flowing along the end-to-end path unimpeded bycongestion.
Researchers realized in the early to mid 1990s that given high bit error rates on wireless links and the possibility of handoff loss, TCP’s congestion-control response could be problematic in a wireless setting.
Three broad classes of approaches are possible for dealing with this problem: Local recovery.
Local recovery protocols recover from bit errors when and where (e.g., at thewireless link) they occur, e.g., the 802.11 ARQ protocol we studied in Section 7.3, or more sophisticated approaches that use both ARQ and FEC [Ayanoglu 1995].
TCP sender awareness of wireless links.
In the local recovery approaches, the TCP sender is blissfully unaware that its segments are traversing a wireless link.
An alternative approach is for the TCP sender and receiver to be aware of the existence of a wireless link, to distinguish betweencongestive losses occurring in the wired network and corruption/loss occurring at the wireless link,and to invoke congestion control only in response to congestive wired-network losses. [
Balakrishnan 1997]  investigates various types of TCP, assuming that end ­systems can make this distinction. [
Liu 2003] investigates techniques for distinguishing between losses on the wired and wireless segments of an end-to-end path.
Split-connection approaches.
 In a split-connection approach [Bakre 1995] , the end-to-end connection between the mobile user and the other end point is broken into two transport-layer connections: one from the mobile host to the wireless access point, and one from the wirelessaccess point to the other communication end point (which we’ll assume here is a wired host).
The end-to-end connection is thus formed by the concatenation of a wireless part and a wired part.
The transport layer over the wireless segment can be a standard TCP connection [Bakre 1995] , or a specially tailored error recovery protocol on top of UDP. [
Yavatkar 1994]  investigates the use of a transport-layer selective repeat protocol over the wireless connection.
Measurements reported in[Wei 2006] indicate that split TCP connections are widely used in cellular data networks, and that significant improvements can indeed be made through the use of split TCP connections.
Our treatment of TCP over wireless links has been necessarily brief here.
­In-depth surveys of TCP challenges and solutions in wireless networks can be found in [Hanabali 2005; Leung 2006].
We encourage you to consult the references for details of this ongoing area of research.
Having considered transport-layer protocols, let us next consider the effect of wireless and mobility on application-layer protocols.
Here, an important consideration is that wireless links often have relatively low bandwidths, as we saw in Figure 7.2.
As a result, applications that operate over wireless links, particularly over cellular wireless links, must treat bandwidth as a scarce commodity.
For example, a Web server serving content to a Web browser executing on a 4G phone will likely not be able to providethe same image-rich content that it gives to a browser operating over a wired connection.
Although wireless links do provide challenges at the application layer, the mobility they enable also makes possible a rich set of location-aware and context-aware applications [Chen 2000; Baldauf 2007].
More generally, wireless and mobile networks will play a key role in realizing the ubiquitous computingenvironments of the future [Weiser 1991] .
It’s fair to say that we’ve only seen the tip of the iceberg when it comes to the impact of wireless and mobile networks on networked applications and their protocols!
7.9 Summary Wireless and mobile networks have revolutionized telephony and are having an increasingly profound impact in the world of computer networks as well.
With their anytime, anywhere, untethered access intothe global network infrastructure, they are not only making network access more ubiquitous, they arealso enabling an exciting new set of location-dependent services.
Given the growing importance ofwireless and mobile networks, this chapter has focused on the principles, common link technologies, and network architectures for supporting wireless and mobile communication.
We began this chapter with an introduction to wireless and mobile networks, drawing an important distinction between the challenges posed by the wireless  nature of the communication links in such networks, and by the mobility  that these wireless links enable.
This allowed us to better isolate, identify, and master the key concepts in each area.
We focused first on wireless communication, considering thecharacteristics of a wireless link in Section 7.2.
In Sections 7.3 and 7.4, we examined the link-level aspects of the IEEE 802.11 (WiFi) wireless LAN standard, two IEEE 802.15 personal area networks (Bluetooth and Zigbee), and 3G and 4G cellular Internet access.
We then turned our attention to the issue of mobility.
In Section 7.5, we identified several forms of mobility, with points along this spectrum posing different challenges and admitting different solutions.
We considered the problems of locating and routing to a mobile user, as well as approaches for handing off the mobile user who dynamicallymoves from one point of attachment to the network to another.
We examined how these issues were addressed in the mobile IP standard and in GSM, in Sections 7.6 and 7.7, respectively.
Finally, we considered the impact of wireless links and mobility on transport-layer protocols and networkedapplications in ­Section 7.8.
Although we have devoted an entire chapter to the study of wireless and mobile networks, an entire book (or more) would be required to fully explore this exciting and rapidly expanding field.
Weencourage you to delve more deeply into this field by consulting the many references provided in thischapter.
Homework Problems and Questions Chapter 7 Review Questions Section 7.1 Section 7.2 Sections 7.3 and 7.4R1.
What does it mean for a wireless network to be operating in “infrastructure mode”?
If the network is not in infrastructure mode, what mode of operation is it in, and what is the difference between that mode of operation and infrastructure mode?
R2.
What are the four types of wireless networks identified in our taxonomy in Section 7.1 ?
Which of these types of wireless networks have you used?
R3.
What are the differences between the following types of wireless channel impairments: path loss, multipath propagation, interference from other sources?
R4.
As a mobile node gets farther and farther away from a base station, what are two actions that a base station could take to ensure that the loss probability of a transmitted frame does not increase?
R5.
Describe the role of the beacon frames in 802.11.
R6.
True or false: Before an 802.11 station transmits a data frame, it must first send an RTS frame and receive a corresponding CTS frame.
R7.
Why are acknowledgments used in 802.11 but not in wired Ethernet?
R8.
True or false: Ethernet and 802.11 use the same frame structure.
R9.
Describe how the RTS threshold works.
R10.
Suppose the IEEE 802.11 RTS and CTS frames were as long as the standard DATA and ACK frames.
Would there be any advantage to using the CTS and RTS frames?
Why or why not?
R11.
Section 7.3.4 discusses 802.11 mobility, in which a wireless station moves from one BSS to another within the same subnet.
When the APs are interconnected with a switch, an AP may need to send a frame with a spoofed MAC address to get the switch to forward the frame properly.
Why?
Sections 7.5 and 7.6 Section 7.7 Section 7.8 ProblemsR12.
What are the differences between a master device in a Bluetooth network and a base station in an 802.11 network?
R13.
What is meant by a super frame in the 802.15.4 Zigbee standard?
R14.
What is the role of the “core network” in the 3G cellular data architecture?R15.
What is the role of the RNC in the 3G cellular data network architecture?
What role does the RNC play in the cellular voice network?
R16.
What is the role of the eNodeB, MME, P-GW, and S-GW in 4G architecture?
R17.
What are three important differences between the 3G and 4G cellular ­architectures?
R18.
If a node has a wireless connection to the Internet, does that node have to be mobile?
Explain.
Suppose that a user with a laptop walks around her house with her laptop, and always accesses the Internet through the same access point.
Is this user mobile from a networkstandpoint?
Explain.
R19.
What is the difference between a permanent address and a care-of address?
Who assigns a care-of address?
R20.
Consider a TCP connection going over Mobile IP.
True or false: The TCP connection phase between the correspondent and the mobile host goes through the mobile’s home network, but the data transfer phase is directly between the correspondent and the mobile host,bypassing the home network.
R21.
What are the purposes of the HLR and VLR in GSM networks?
What elements of mobile IP are similar to the HLR and VLR?
R22.
What is the role of the anchor MSC in GSM networks?
R23.
What are three approaches that can be taken to avoid having a single ­wireless link degrade the performance of an end-to-end transport-layer TCP ­connection?
P1.
Consider the single-sender CDMA example in Figure 7.5 .
What would be the sender’s output (for the 2 data bits shown) if the sender’s CDMA code were ?
P2.
Consider sender 2 in Figure 7.6 .
What is the sender’s output to the channel (before it is added to the signal from sender 1), ?(
1,−1,1,−1,1,−1,1,−1) Zi,m2
P3.
Suppose that the receiver in Figure 7.6 wanted to receive the data being sent by sender 2.
Show (by calculation) that the receiver is indeed able to recover sender 2’s data from the aggregate channel signal by using sender 2’s code.
P4.
For the two-sender, two-receiver example, give an example of two CDMA codes containing 1 and 21 values that do not allow the two receivers to extract the original transmitted bits from the two CDMA senders.
P5.
Suppose there are two ISPs providing WiFi access in a particular café, with each ISP operating its own AP and having its own IP address block.
a. Further suppose that by accident, each ISP has configured its AP to operate overchannel 11.
Will the 802.11 protocol completely break down in this situation?
Discuss what happens when two stations, each associated with a different ISP, attempt totransmit at the same time.
b. Now suppose that one AP operates over channel 1 and the other over channel 11.
How do your answers change?
P6.
In step 4 of the CSMA/CA protocol, a station that successfully transmits a frame begins the CSMA/CA protocol for a second frame at step 2, rather than at step 1.
What rationale might the designers of CSMA/CA have had in mind by having such a station not transmit the second frameimmediately (if the channel is sensed idle)?
P7.
Suppose an 802.11b station is configured to always reserve the channel with the RTS/CTS sequence.
Suppose this station suddenly wants to ­transmit 1,000 bytes of data, and all other stations are idle at this time.
As a ­function of SIFS and DIFS, and ignoring propagation delay and assuming no bit errors, calculate the time required to transmit the frame and receive theacknowledgment.
P8.
Consider the scenario shown in Figure 7.34 , in which there are four wireless nodes, A, B, C, and D. The radio coverage of the four nodes is shown via the shaded ovals; all nodes share the same frequency.
When A transmits, it Figure 7.34 Scenario for problem P8 can only be heard/received by B; when B transmits, both A and C can hear/receive from B; when C transmits, both B and D can hear/receive from C; when D transmits, only C can hear/receive
from D. Suppose now that each node has an infinite supply of messages that it wants to send to each of the other nodes.
If a message’s destination is not an immediate neighbor, then the messagemust be relayed.
For example, if A wants to send to D, a message from A must first be sent to B,which then sends the message to C, which then sends the message to D. Time is slotted, with amessage transmission time taking exactly one time slot, e.g., as in slotted Aloha.
During a slot, a node can do one of the following: ( i) send a message, (ii) receive a message (if exactly one message is being sent to it), (iii) remain silent.
As always, if a node hears two or more simultaneous transmissions, a collision occurs and none of the transmitted messages are received successfully.
You can assume here that there are no bit-level errors, and thus if exactlyone message is sent, it will be received correctly by those within the transmission radius of thesender.
a. Suppose now that an omniscient controller (i.e., a controller that knows the state of every node in the network) can command each node to do whatever it (the omniscient controller) wishes, i.e., to send a message, to receive a message, or to remain silent.
Given this omniscient controller, what is the maximum rate at which a data message can be transferred from C to A, given that there are no other messages between any other source/destination pairs?
b. Suppose now that A sends messages to B, and D sends messages to C. What is the combined maximum rate at which data messages can flow from A to B and from D to C?
c. Suppose now that A sends messages to B, and C sends messages to D. What is thecombined maximum rate at which data messages can flow from A to B and from C to D?
d. Suppose now that the wireless links are replaced by wired links.
Repeat questions (a)through (c) again in this wired scenario.
e. Now suppose we are again in the wireless scenario, and that for every data messagesent from source to destination, the destination will send an ACK message back to the source (e.g., as in TCP).
Also suppose that each ACK message takes up one slot.
Repeat questions (a)–(c) above for this scenario.
P9.
Describe the format of the 802.15.1 Bluetooth frame.
You will have to do some reading outside of the text to find this information.
Is there anything in the frame format that inherently limits the number of active nodes in an 802.15.1 network to eight active nodes?
Explain.
P10.
Consider the following idealized LTE scenario.
The downstream channel (see Figure 7.21 ) is slotted in time, across F frequencies.
There are four nodes, A, B, C, and D, reachable from the base station at rates of 10 Mbps, 5 Mbps, 2.5 Mbps, and 1 Mbps, respectively, on thedownstream channel.
These rates assume that the base station utilizes all time slots availableon all F frequencies to send to just one station.
The base station has an infinite amount of data to send to each of the nodes, and can send to any one of these four nodes using any of the F frequencies during any time slot in the ­downstream sub-frame.
a. What is the maximum rate at which the base station can send to the nodes, assuming it
can send to any node it chooses during each time slot?
Is your solution fair?
Explain and define what you mean by “fair.”
b. If there is a fairness requirement that each node must receive an equal amount of data during each one second interval, what is the average transmission rate by the base station (to all nodes) during the downstream sub-frame?
Explain how you arrived at youranswer.
c. Suppose that the fairness criterion is that any node can receive at most twice as much data as any other node during the sub-frame.
What is the average transmission rate by the base station (to all nodes) during the sub-frame?
Explain how you arrived at youranswer.
P11.
In Section 7.5 , one proposed solution that allowed mobile users to maintain their IP addresses as they moved among foreign networks was to have a foreign network advertise a highly specific route to the mobile user and use the existing routing infrastructure to propagatethis information throughout the network.
We identified scalability as one concern.
Suppose thatwhen a mobile user moves from one network to another, the new foreign network advertises aspecific route to the mobile user, and the old foreign network withdraws its route.
Consider howrouting information propagates in a distance-vector algorithm (particularly for the case ofinterdomain routing among networks that span the globe).
a. Will other routers be able to route datagrams immediately to the new foreign network as soon as the foreign network begins advertising its route?
b. Is it possible for different routers to believe that different foreign networks contain themobile user?
c. Discuss the timescale over which other routers in the network will eventually learn thepath to the mobile users.
P12.
Suppose the correspondent in Figure 7.23 were mobile.
Sketch the additional network- layer infrastructure that would be needed to route the datagram from the original mobile user to the (now mobile) correspondent.
Show the structure of the datagram(s) between the original mobile user and the (now mobile) correspondent, as in Figure 7.24 .
P13.
In mobile IP, what effect will mobility have on end-to-end delays of datagrams between the source and destination?
P14.
Consider the chaining example discussed at the end of Section 7.7.2 .
Suppose a mobile user visits foreign networks A, B, and C, and that a correspondent begins a connection to the mobile user when it is resident in foreign ­network A. List the sequence of messages between foreign agents, and between foreign agents and the home agent as the mobile user moves from network A to network B to network C. Next, suppose chaining is not performed, and the correspondent (as well as the home agent) must be explicitly notified of the changes in themobile user’s care-of address.
List the sequence of messages that would need to be exchangedin this second scenario.
Wireshark Lab At the Web site for this textbook, www.pearsonhighered.com/ cs-resources , you’ll find a Wireshark lab for this chapter that captures and studies the 802.11 frames exchanged between a wireless laptop and an access point.
AN INTERVIEW WITH… Deborah Estrin Deborah Estrin is a Professor of Computer Science at Cornell Tech in New York City and a Professor of Public Health at Weill Cornell Medical College.
She is founder of the Health Tech Hub at Cornell Tech and co-founder of the non-profit startup Open mHealth .
She received her Ph.D. (1985) in Computer Science from M.I.T. and her B.S. (1980) from UC Berkeley.
Estrin’s early research focused on the design of network protocols, including multicast and inter-domainrouting.
In 2002 Estrin founded the NSF-funded Science and Technology Center at UCLA, Center for Embedded Networked Sensing (CENS http:/ /cens.ucla.edu.).
CENS launched new areas of multi-disciplinary computer systems research from sensor networks for environmental monitoring, to participatory sensing for citizen science.
Her current focus is on mobile health andsmall data, leveraging the pervasiveness of mobile devices and digital interactions for health andlife management, as described in her 2013 TEDMED talk.
Professor Estrin is an elected memberof the American Academy of Arts and Sciences (2007) and the National Academy ofEngineering (2009).
She is a fellow of the IEEE, ACM, and AAAS.
She was selected as the firstACM-W Athena Lecturer (2006), awarded the Anita Borg Institute’s Women of Vision Award for Innovation (2007), inducted into the WITI hall of fame (2008) and awarded Doctor Honoris Causa  from EPFL (2008) and Uppsala University (2011).
P15.
Consider two mobile nodes in a foreign network having a foreign agent.
Is it possible for the two mobile nodes to use the same care-of address in mobile IP?
Explain your answer.
P16.
In our discussion of how the VLR updated the HLR with information about the mobile’s current location, what are the advantages and disadvantages of providing the MSRN as opposed to the address of the VLR to the HLR?
Please describe a few of the most exciting projects you have worked on during your career.
What were the biggest challenges?
In the mid-90s at USC and ISI, I had the great fortune to work with the likes of Steve Deering, Mark Handley, and Van Jacobson on the design of multicast routing protocols (in particular,PIM).
I tried to carry many of the architectural design lessons from multicast into the design ofecological monitoring arrays, where for the first time I really began to take applications andmultidisciplinary research seriously.
That interest in jointly innovating in the social and technological space is what interests me so much about my latest area of research, mobile health.
The challenges in these projects were as diverse as the problem domains, but what they all had in common was the need to keep our eyes open to whether we had the problemdefinition right as we iterated between design and deployment, prototype and pilot.
None of themwere problems that could be solved analytically, with simulation or even in constructedlaboratory experiments.
They all challenged our ability to retain clean architectures in thepresence of messy problems and contexts, and they all called for extensive collaboration.
What changes and innovations do you see happening in wireless networks and mobility in the future?
In a prior edition of this interview I said that I have never put much faith into predicting the future, but I did go on to speculate that we might see the end of feature phones (i.e., those that are notprogrammable and are used only for voice and text messaging) as smart phones become more and more powerful and the primary point of Internet access for many—and now not so many years later that is clearly the case.
I also predicted that we would see the continued proliferationof embedded SIMs by which all sorts of devices have the ability to communicate via the cellularnetwork at low data rates.
While that has occurred, we see many devices and “Internet ofThings” that use embedded WiFi and other lower power, shorter range, forms of connectivity tolocal hubs.
I did not anticipate at that time the emergence of a large consumer wearablesmarket.
By the time the next edition is published I expect broad proliferation of personalapplications that leverage data from IoT and other digital traces.
Where do you see the future of networking and the Internet?
Again I think its useful to look both back and forward.
Previously I observed that the efforts in named data and software-defined networking would emerge to create a more manageable, evolvable, and richer infrastructure and more generally represent moving the role of architecturehigher up in the stack.
In the beginnings of the Internet, architecture was layer 4 and below, with
applications being more siloed/monolithic, sitting on top.
Now data and analytics dominate transport.
The adoption of SDN (which I’m really happy to see is featured in this 7th edition of this book) has been well beyond what I ever anticipated.
However, looking up the stack, ourdominant applications increasingly live in walled gardens, whether mobile apps or largeconsumer platforms such as Facebook.
As Data Science and Big Data techniques develop, theymight help to lure these applications out of their silos because of the value in connecting with other apps and platforms.
What people inspired you professionally?
There are three people who come to mind.
First, Dave Clark, the secret sauce and under-sung hero of the Internet community.
I was lucky to be around in the early days to see him act as the“organizing principle” of the IAB and Internet governance; the priest of rough consensus andrunning code.
Second, Scott Shenker, for his intellectual brilliance, integrity, and persistence.
Istrive for, but rarely attain, his clarity in defining problems and solutions.
He is always the firstperson I e-mail for advice on matters large and small.
Third, my sister Judy Estrin, who had thecreativity and courage to spend her career bringing ideas and concepts to market.
Without theJudys of the world the Internet technologies would never have transformed our lives.
What are your recommendations for students who want careers in computer science and networking?
First, build a strong foundation in your academic work, balanced with any and every real-world work experience you can get.
As you look for a working environment, seek opportunities in problem areas you really care about and with smart teams that you can learn from.
Chapter 8 Security in Computer Networks Way back in Section 1.6 we described some of the more prevalent and damaging classes of Internet attacks, including malware attacks, denial of service, sniffing, source masquerading, and message modification and deletion.
Although we have since learned a tremendous amount about computernetworks, we still haven’t examined how to secure networks from those attacks.
Equipped with our newly acquired expertise in computer networking and Internet protocols, we’ll now study in-depth secure communication and, in particular, how computer networks can be defended from those nasty bad guys.
Let us introduce Alice and Bob, two people who want to communicate and wish to do so “securely.”
This being a networking text, we should remark that Alice and Bob could be two routers that want toexchange routing tables securely, a client and server that want to establish a secure transportconnection, or two e-mail applications that want to exchange secure e-mail—all case studies that we willconsider later in this chapter.
Alice and Bob are well-known fixtures in the security community, perhapsbecause their names are more fun than a generic entity named “A” that wants to communicate securely with a generic entity named “B.” Love affairs, wartime communication, and business transactions are the commonly cited human needs for secure communications; preferring the first to the latter two, we’rehappy to use Alice and Bob as our sender and receiver, and imagine them in this first scenario.
We said that Alice and Bob want to communicate and wish to do so “securely,” but what precisely does this mean?
As we will see, security (like love) is a many-splendored thing; that is, there are many facetsto security.
Certainly, Alice and Bob would like for the contents of their communication to remain secretfrom an eavesdropper.
They probably would also like to make sure that when they are communicating, they are indeed communicating with each other, and that if their communication is tampered with by aneavesdropper, that this tampering is detected.
In the first part of this chapter, we’ll cover thefundamental cryptography techniques that allow for encrypting communication, authenticating the partywith whom one is communicating, and ensuring message integrity.
In the second part of this chapter, we’ll examine how the fundamental ­cryptography principles can be used to create secure networking protocols.
Once again taking a top-down approach, we’ll examine secure protocols in each of the (top four) layers, beginning with the application layer.
We’ll examine howto secure e-mail, how to secure a TCP connection, how to provide blanket security at the network layer, and how to secure a wireless LAN.
In the third part of this chapter we’ll consider operational security,
which is about protecting organizational networks from attacks.
In particular, we’ll take a careful look at how firewalls and intrusion detection systems can enhance the security of an organizational network.
8.1 What Is Network Security?
Let’s begin our study of network security by returning to our lovers, Alice and Bob, who want to communicate “securely.”
What precisely does this mean?
Certainly, Alice wants only Bob to be able to understand a message that she has sent, even though they are communicating over an insecure medium where an intruder (Trudy, the intruder) may intercept whatever is transmitted from Alice to Bob.
Bob also wants to be sure that the message he receives from Alice was indeed sent by Alice, and Alice wants to make sure that the person with whom she is communicating is indeed Bob.
Alice and Bob also want to make sure that the contents of their messages have not been altered in transit.
They also wantto be assured that they can communicate in the first place (i.e., that no one denies them access to theresources needed to communicate).
Given these considerations, we can identify the following desirableproperties of secure communication.
Confidentiality.
 Only the sender and intended receiver should be able to understand the contents of the transmitted message.
Because eavesdroppers may intercept the message, this necessarilyrequires that the message be somehow encrypted so that an intercepted message cannot be understood by an interceptor.
This aspect of confidentiality is probably the most commonly perceived meaning of the term secure communication .
We’ll study cryptographic techniques for encrypting and decrypting data in Section 8.2.
Message integrity.
Alice and Bob want to ensure that the content of their ­communication is not altered, either maliciously or by accident, in transit.
Extensions to the checksumming techniques that we encountered in reliable transport and data link protocols can be used to provide such message integrity.
We will study message integrity in Section 8.3.
End-point authentication.
 Both the sender and receiver should be able to confirm the identity of the other party involved in the communication—to confirm that the other party is indeed who or whatthey claim to be.
Face-to-face human communication solves this problem easily by visualrecognition.
When communicating entities exchange messages over a medium where they cannot see the other party, authentication is not so simple.
When a user wants to access an inbox, how does the mail server verify that the user is the person he or she claims to be?
We study end-point authentication in Section 8.4.
Operational security.
Almost all organizations (companies, universities, and so on) today have networks that are attached to the public Internet.
These networks therefore can potentially be compromised.
Attackers can attempt to deposit worms into the hosts in the network, obtaincorporate secrets, map the internal network configurations, and launch DoS attacks.
We’ll see in Section 8.9 that operational devices such as firewalls and intrusion detection systems are used to counter attacks against an organization’s network.
A firewall sits between the organization’s network and the public network, controlling packet access to and from the network.
An intrusion detection
system performs “deep packet ­inspection,” ­alerting the network administrators about suspicious activity.
Having established what we mean by network security, let’s next consider exactly what information an intruder may have access to, and what actions can be taken by the intruder.
Figure 8.1 illustrates the scenario.
Alice, the sender, wants to send data to Bob, the receiver.
In order to exchange data securely, while meeting the requirements of confidentiality, end-point authentication, and message integrity, Aliceand Bob will exchange control messages and data messages (in much the same way that TCP sendersand receivers exchange control segments and data segments).
Figure 8.1 Sender, receiver, and intruder (Alice, Bob, and Trudy) All or some of these messages will typically be encrypted.
As discussed in Section 1.6, an intruder can potentially perform eavesdropping —sniffing and recording control and data messages on the ­channel.
modification, insertion , or deletion  of messages or message content.
As we’ll see, unless appropriate countermeasures are taken, these capabilities allow an intruder tomount a wide variety of security attacks: snooping on communication (possibly stealing passwords and data), impersonating another entity, hijacking an ongoing session, denying service to legitimate network users by overloading system resources, and so on.
A summary of reported attacks is maintained at the CERT Coordination Center [CERT 2016] .
Having established that there are indeed real threats loose in the Internet, what are the Internet equivalents of Alice and Bob, our friends who need to communicate securely?
Certainly, Bob and Alicemight be human users at two end systems, for example, a real Alice and a real Bob who really do wantto exchange secure e-mail.
They might also be participants in an electronic commerce transaction.
Forexample, a real Bob might want to transfer his credit card number securely to a Web server to purchase
an item online.
Similarly, a real Alice might want to interact with her bank online.
The parties needing secure communication might themselves also be part of the network infrastructure.
Recall that the domain name system (DNS, see Section 2.4) or routing daemons that exchange routing information (see Chapter 5) require secure communication between two parties.
The same is true for network management applications, a topic we examined in Chapter 5).
An intruder that could actively interfere with DNS lookups (as discussed in Section 2.4), routing computations [RFC 4272] , or network management functions [RFC 3414]  could wreak havoc in the Internet.
Having now established the framework, a few of the most important definitions, and the need fornetwork security, let us next delve into cryptography.
While the use of cryptography in providingconfidentiality is self-evident, we’ll see shortly that it is also central to providing end-point authenticationand message integrity—making cryptography a cornerstone of network security.
8.2 Principles of Cryptography Although cryptography has a long history dating back at least as far as Julius Caesar, modern cryptographic techniques, including many of those used in the Internet, are based on advances made in the past 30 years.
Kahn’s book, The Codebreakers  [Kahn 1967], and Singh’s book, The Code Book: The Science of Secrecy from Ancient Egypt to Quantum Cryptography  [Singh 1999], provide a fascinating look at the Figure 8.2 Cryptographic components long history of cryptography.
A complete discussion of cryptography itself requires a complete book[Kaufman 1995 ; Schneier 1995] and so we only touch on the essential aspects of cryptography, particularly as they are practiced on the Internet.
We also note that while our focus in this section will be on the use of cryptography for confidentiality, we’ll see shortly that cryptographic techniques areinextricably woven into authentication, message integrity, nonrepudiation, and more.
Cryptographic techniques allow a sender to disguise data so that an intruder can gain no information from the intercepted data.
The receiver, of course, must be able to recover the original data from the disguised data.
Figure 8.2 illustrates some of the important terminology.
Suppose now that Alice wants to send a message to Bob.
Alice’s message in its original form (forexample, “ Bob, I love you.
Alice ”) is known as ­ plaintext , or cleartext .
Alice encrypts her plaintext message using an encryption algorithm  so that the encrypted message, known as ciphertext, looks unintelligible to any intruder.
Interestingly, in many modern cryptographic systems,
including those used in the Internet, the encryption technique itself is known —published, standardized, and available to everyone (for example, [RFC 1321 ; RFC 3447 ; RFC 2420 ; NIST 2001]), even a potential intruder!
Clearly, if everyone knows the method for encoding data, then there must be some secret information that prevents an intruder from decrypting the transmitted data.
This is where keys come in.
In Figure 8.2, Alice provides a key, K, a string of numbers or characters, as input to the encryption algorithm.
The encryption algorithm takes the key and the plaintext message, m, as input and produces ciphertext as output.
The notation K(m) refers to the ciphertext form (encrypted using the key K) of the plaintext message, m. The actual encryption algorithm that uses key K will be evident from the context.
Similarly, Bob will provide a key, K, to the decryption algorithm  that takes the ciphertext and Bob’s key as input and produces the original plaintext as output.
That is, if Bob receives an encrypted message K (m), he decrypts it by computing  In symmetric key systems , Alice’s and Bob’s keys are identical and are secret.
In public key systems, a pair of keys is used.
One of the keys is known to both Bob and Alice (indeed, it is known to the whole world).
The other key is known only by either Bob or Alice (but not both).
In the following two subsections, we consider symmetric key andpublic key systems in more detail.
8.2.1 Symmetric Key Cryptography All cryptographic algorithms involve substituting one thing for another, for example, taking a piece ofplaintext and then computing and substituting the appropriate ciphertext to create the encryptedmessage.
Before studying a modern key-based cryptographic system, let us first get our feet wet bystudying a very old, very simple symmetric key algorithm attributed to Julius Caesar, known as theCaesar cipher  (a cipher is a method for encrypting data).
For English text, the Caesar cipher would work by taking each letter in the plaintext message and substituting the letter that is k letters later (allowing wraparound; that is, having the letter z followed by the letter a) in the alphabet.
For example if , then the letter a in plaintext becomes d in ciphertext; b in plaintext becomes e in ciphertext, and so on.
Here, the value of k serves as the key.
As an example, the plaintext message “ bob, i love you.
Alice ” becomes “ ere, l oryh brx.
dolfh ” in ciphertext.
While the ciphertext does indeed look like gibberish, it wouldn’t take long to break the code if you knew that the Caesar cipher was being used, as there are only 25 possible key values.
An improvement on the Caesar cipher is the monoalphabetic cipher , which also substitutes one letter of the alphabet with another letter of the alphabet.
­However, rather than substituting according to a regular pattern (for example, substitution with an offset of k for all letters), any letter can be substituted for any other letter, as long as each letter has a unique substitute letter, and vice versa.
The substitutionA A A A B A KB(KA(m))=m.
k=3
rule in Figure 8.3 shows one possible rule for encoding plaintext.
The plaintext message “ bob, i love you.
Alice ” becomes “nkn, s gktc wky.
Mgsbc.”
Thus, as in the case of the Caesar cipher, this looks like gibberish.
A monoalphabetic cipher would also appear to be better than the Caesar cipher in that there are 26! (
on the order of 10 ) possible pairings of letters rather than 25 possible pairings.
A brute-force approach of trying all 10  possible pairings Figure 8.3 A monoalphabetic cipher would require far too much work to be a feasible way of breaking the encryption algorithm and decoding the message.
However, by statistical analysis of the plaintext language, for example, knowing that the letters e and t are the most frequently occurring letters in typical English text (accounting for 13 percent and 9 percent of letter occurrences), and knowing that particular two-and three-letter occurrences of letters appear quite often together (for example, “in,” “it,” “the,” “ion,” “ing,” and so forth) make itrelatively easy to break this code.
If the intruder has some knowledge about the possible contents of themessage, then it is even easier to break the code.
For example, if Trudy the intruder is Bob’s wife andsuspects Bob of having an affair with Alice, then she might suspect that the names “bob” and “alice”appear in the text.
If Trudy knew for certain that those two names appeared in the ciphertext and had acopy of the example ciphertext message above, then she could immediately determine seven of the 26 letter pairings, requiring 10  fewer possibilities to be checked by a brute-force method.
Indeed, if Trudy suspected Bob of having an affair, she might well expect to find some other choice words in the message as well.
When considering how easy it might be for Trudy to break Bob and Alice’s encryption scheme, one can distinguish three different scenarios, depending on what information the intruder has.
Ciphertext-only attack.
In some cases, the intruder may have access only to the intercepted ciphertext, with no certain information about the contents of the plaintext message.
We have seen how statistical analysis can help in a ciphertext-only attack on an encryption scheme.
Known-plaintext attack.
We saw above that if Trudy somehow knew for sure that “bob” and “alice” appeared in the ciphertext message, then she could have determined the (plaintext, ciphertext) pairings for the letters a, l, i, c, e, b , and o. Trudy might also have been fortunate enough to have recorded all of the ciphertext transmissions and then found Bob’s own decrypted version of one of the transmissions scribbled on a piece of paper.
When an intruder knows some of the (plaintext, ciphertext) pairings, we refer to this as a known-plaintext attack on the encryption scheme.
Chosen-plaintext attack.
In a chosen-plaintext attack , the intruder is able to choose the plaintext26 26 9
message and obtain its corresponding ciphertext form.
For the simple encryption algorithms we’ve seen so far, if Trudy could get Alice to send the message, “ The quick brown fox jumps over the lazy dog, ” she could completely break the encryption scheme.
We’ll see shortly that for more sophisticated encryption techniques, a chosen-plaintext attack does not necessarily mean that the encryption technique can be broken.
Five hundred years ago, techniques improving on monoalphabetic encryption, known aspolyalphabetic encryption , were invented.
The idea behind polyalphabetic encryption is to use multiple monoalphabetic ciphers, with a specific Figure 8.4 A polyalphabetic cipher using two Caesar ciphers monoalphabetic cipher to encode a letter in a specific position in the plaintext message.
Thus, the same letter, appearing in different positions in the plaintext message, might be encoded differently.
An example of a polyalphabetic encryption scheme is shown in Figure 8.4.
It has two Caesar ciphers (with  and ), shown as rows.
We might choose to use these two Caesar ciphers, C  and C , in the repeating pattern C , C, C, C, C. That is, the first letter of plaintext is to be encoded using C , the second and third using C , the fourth using C , and the fifth using C .
The pattern then repeats, with the sixth letter being encoded using C , the seventh with C , and so on.
The plaintext message “ bob, i love you. ”
is thus encrypted “ ghu, n etox dhz. ”
Note that the first b in the plaintext message is encrypted using C , while the second b is encrypted using C .
In this example, the encryption and decryption “key” is the knowledge of the two Caesar keys  and the pattern C , C, C, C, C. Block Ciphers Let us now move forward to modern times and examine how symmetric key encryption is done today.
There are two broad classes of symmetric encryption techniques: stream ciphers  and block ciphers.
We’ll briefly examine stream ciphers in ­Section 8.7 when we investigate security for wireless LANs.
In this section, we focus on block ciphers, which are used in many secure Internet protocols, includingPGP (for secure e-mail), SSL (for securing TCP connections), and IPsec (for securing the network-layer transport).
In a block cipher, the message to be encrypted is processed in blocks of k bits.
For example, if , then the message is broken into 64-bit blocks, and each block is encrypted independently.
To encode a block, the cipher uses a one-to-one mapping to map the k-bit block of cleartext to a k-bit block ofk=5 k=191 2 1 2 2 1 2 1 2 1 2 1 2 1 2 (k=5, k=19)1 2 2 1 2 k=64
ciphertext.
Let’s look at an example.
Suppose that , so that the block cipher maps 3-bit inputs (cleartext) to 3-bit outputs (ciphertext).
One possible mapping is given in Table 8.1.
Notice that this is a one-to-one mapping; that is, there is a different output for each input.
This block cipher breaks the message up into 3-bit blocks and encrypts each block according to the above mapping.
You shouldverify that the message 010110001111 gets encrypted into 101000111001.
Continuing with this 3-bit block example, note that the mapping in Table 8.1 is just one mapping of many possible mappings.
How many possible mappings are Table 8.1 A specific 3-bit block cipher input output input output 000 110 100 011 001 111 101 010 010 101 110 000 011 100 111 001 there?
To answer this question, observe that a mapping is nothing more than a permutation of all the possible inputs.
There are  possible inputs (listed under the input columns).
These eight inputs can be permuted in  different ways.
Since each of these permutations specifies a mapping, there are 40,320 possible mappings.
We can view each of these mappings as a key—if Alice and Bobboth know the mapping (the key), they can encrypt and decrypt the messages sent between them.
The brute-force attack for this cipher is to try to decrypt ciphtertext by using all mappings.
With only 40,320 mappings (when ), this can quickly be accomplished on a desktop PC.
To thwart brute-force attacks, block ciphers typically use much larger blocks, consisting of  bits or even larger.
Note that the number of possible mappings for a general k-block cipher is 2 !,
which is astronomical for even moderate values of k (such as ).
Although full-table block ciphers, as just described, with moderate values of k can produce robust symmetric key encryption schemes, they are unfortunately difficult to implement.
For  and for a given mapping, Alice and Bob would need to maintain a table with 2  input values, which is an infeasible task.
Moreover, if Alice and Bob were to change keys, they would have to each regeneratethe table.
Thus, a full-table block cipher, providing predetermined mappings between all inputs andoutputs (as in the example above), is simply out of the question.k=3 23(=8) 8!=40,320 k=3 k=64 k k=64 k=64 64
Instead, block ciphers typically use functions that simulate randomly permuted tables.
An example (adapted from [Kaufman 1995] ) of such a function for  bits is shown in Figure 8.5.
The function first breaks a 64-bit block into 8 chunks, with each chunk consisting of 8 bits.
Each 8-bit chunk is processed by an 8-bit to 8-bit table, which is of manageable size.
For example, the first chunk is processed by the table denoted by T .
Next, the 8 output chunks are reassembled into a 64-bit block.
The positions of the 64 bits in the block are then scrambled (permuted) to produce a 64-bit output.
Thisoutput is fed back to the 64-bit input, where another cycle begins.
After n such cycles, the function provides a 64-bit block of ciphertext.
The purpose of the rounds is to make each input bit affect most (if not all) of the final output bits. (
If only one round were used, a given input bit would affect only 8 of the64 output bits.)
The key for this block cipher algorithm would be the eight permutation tables (assumingthe scramble function is publicly known).
Figure 8.5 An example of a block cipher Today there are a number of popular block ciphers, including DES (standing for Data Encryption Standard), 3DES, and AES (standing for Advanced Encryption Standard).
Each of these standards uses functions, rather than predetermined tables, along the lines of Figure 8.5 (albeit more complicated and specific to each cipher).
Each of these algorithms also uses a string of bits for a key.
For example, DES uses 64-bit blocks with a 56-bit key.
AES uses 128-bit blocks and can operate with keys that are 128,192, and 256 bits long.
An algorithm’s key determines the specific “mini-table” mappings andpermutations within the algorithm’s internals.
The brute-force attack for each of these ciphers is to cyclethrough all the keys, applying the decryption algorithm with each key.
Observe that with a key length of n, there are 2  possible keys.
NIST [NIST 2001] estimates that a machine that could crack 56-bit DES in one second (that is, try all 2  keys in one second) would take approximately 149 trillion years to crack a 128-bit AES key.k=64 1 n 56
Cipher-Block Chaining In computer networking applications, we typically need to encrypt long messages (or long streams of data).
If we apply a block cipher as described by simply chopping up the message into k-bit blocks and independently encrypting each block, a subtle but important problem occurs.
To see this, observe that two or more of the cleartext blocks can be identical.
For example, the cleartext in two or more blocks could be “HTTP/1.1”.
For these identical blocks, a block cipher would, of course, produce the sameciphertext.
An attacker could potentially guess the cleartext when it sees identical ciphertext blocks andmay even be able to decrypt the entire message by identifying identical ciphtertext blocks and using knowledge about the underlying protocol structure [Kaufman 1995] .
To address this problem, we can mix some randomness into the ciphertext so that identical plaintext blocks produce different ciphertext blocks.
To explain this idea, let m(i) denote the ith plaintext block, c(i) denote the ith ciphertext block, and  denote the exclusive-or (XOR) of two bit strings, a and b. (Recall that the  and , and the XOR of two bit strings is done on a bit-by-bit basis.
So, for example, .)
Also, denote the block-cipher encryption algorithm with key S as K .
The basic idea is as follows.
The sender creates a random k-bit number r(i) for the ith block and calculates .
Note that a new k-bit random number is chosen for each block.
The sender then sends c(1), r(1), c(2), r(2), c(3), r(3), and so on.
Since the receiver receives c(i) and r(i), it can recover each block of the plaintext by computing .
It is important to note that, although r(i) is sent in the clear and thus can be sniffed by Trudy, she cannot obtain the plaintext m(i), since she does not know the key K. Also note that if two plaintext blocks m(i) and m(j) are the same, the corresponding ciphertext blocks c(i) and c(j) will be different (as long as the random numbers r(i) and r(j) are different, which occurs with very high probability).
As an example, consider the 3-bit block cipher in Table 8.1.
Suppose the plaintext is 010010010.
If Alice encrypts this directly, without including the randomness, the resulting ciphertext becomes 101101101.
If Trudy sniffs this ciphertext, because each of the three cipher blocks is the same, she can correctlysurmise that each of the three plaintext blocks are the same.
Now suppose instead Alice generates therandom blocks , and  and uses the above technique to generate the ciphertext  and .
Note that the three ciphertext blocks are different even though the plaintext blocks are the same.
Alice then sends c(1), r(1), c(2), and r(2).
You should verify that Bob can obtain the original plaintext using the shared key K. The astute reader will note that introducing randomness solves one problem but creates another:namely, Alice must transmit twice as many bits as before.
Indeed, for each cipher bit, she must now alsosend a random bit, doubling the required bandwidth.
In order to have our cake and eat it too, blockciphers typically use a technique called Cipher Block Chaining (CBC) .
The basic idea is to send only one random value along with the very first message, and then have the sender and receiver use thea⊕b 0⊕0=1⊕1=0 0⊕1=1⊕0=1 10101010 ⊕11110000 =01011010 S c(i)=KS(m(i)⊕r(i)) m(i)=KS(c(i))⊕r(i) S r(1)=001, r(2)=111 r(3)=100 c(1)=100, c(2)=010, c(3)=000 S
computed coded blocks in place of the subsequent random number.
 Specifically, CBC operates as follows: 1.
Before encrypting the message (or the stream of data), the sender generates a random k-bit string, called the Initialization Vector (IV) .
Denote this initialization vector by c(0).
The sender sends the IV to the receiver in cleartext.
2.
For the first block, the sender calculates  that is, calculates the exclusive-or of the first block of cleartext with the IV.
It then runs the result through the block-cipher algorithm to get the corresponding ciphertext block; that is, .
The sender sends the encrypted block c(1) to the receiver.
3.
For the ith block, the sender generates the ith ciphertext block from  .
Let’s now examine some of the consequences of this approach.
First, the receiver will still be able to recover the original message.
Indeed, when the receiver receives c(i), it decrypts it with K  to obtain ; since the receiver also knows , it then obtains the cleartext block from .
Second, even if two cleartext blocks are identical, the corresponding ciphtertexts (almost always) will be different.
Third, although the sender sends the IV in the clear, an intruder will stillnot be able to decrypt the ciphertext blocks, since the intruder does not know the secret key, S. Finally, the sender only sends one overhead block (the IV), thereby negligibly increasing the bandwidth usage for long messages (consisting of hundreds of blocks).
As an example, let’s now determine the ciphertext for the 3-bit block cipher in Table 8.1 with plaintext 010010010 and .
The sender first uses the IV to calculate .
The sender then calculates  , and   The reader should verify that the receiver, knowing the IV and K can recover the original plaintext.
CBC has an important consequence when designing secure network protocols: we’ll need to provide a mechanism within the protocol to distribute the IV from sender to receiver.
We’ll see how this is done forseveral protocols later in this chapter.
8.2.2 Public Key Encryption For more than 2,000 years (since the time of the Caesar cipher and up to the 1970s), encryptedcommunication required that the two communicating parties share a common secret—the symmetrickey used for encryption and decryption.
One difficulty with this approach is that the two parties must somehow agree on the shared key; but to do so requires (presumably secure) communication!
Perhaps the parties could first meet and agree on the key in person (for example, two of Caesar’s centurions might meet at the Roman baths) and thereafter communicate with encryption.
In a networked world,m(1)⊕c(0), c(1)=KS(m(1)⊕c(0)) c(i)= KS(m(i)⊕c(i−1)) S s(i)=m(i)⊕c(i−1) c(i−1) m(i)=s(i)⊕c(i−1) IV=c(0)=001 c(1)=KS(m(1)⊕c(0))=100 c(2)= KS(m(2)⊕c(1))=KS(010⊕100)=000 C(3)=KS(m(3)⊕c(2))=KS(010⊕ 000)=101.S
however, communicating parties may never meet and may never converse except over the network.
Is it possible for two parties to communicate with encryption without having a shared secret key that is known in advance?
In 1976, Diffie and Hellman [Diffie 1976] demonstrated an algorithm (known now as Diffie-Hellman Key Exchange) to do just that—a radically different and marvelously elegant approach toward secure communication that has led to the development of today’s public key cryptography systems.
We’ll see shortly that public key cryptography systems also have several wonderful properties that make them useful not only Figure 8.6 Public key cryptography for encryption, but for authentication and digital signatures as well.
Interestingly, it has recently come to light that ideas similar to those in [Diffie 1976] and [RSA 1978]  had been independently developed in the early 1970s in a series of secret reports by researchers at the Communications-Electronics SecurityGroup in the United ­Kingdom [Ellis 1987].
As is often the case, great ideas can spring up independently in many places; fortunately, public key advances took place not only in private, but also in the public view, as well.
The use of public key cryptography is conceptually quite simple.
Suppose Alice wants to communicate with Bob.
As shown in Figure 8.6, rather than Bob and Alice sharing a single secret key (as in the case of symmetric key systems), Bob (the recipient of Alice’s messages) instead has two keys—a public key that is available to everyone  in the world (including Trudy the intruder) and a private key  that is known only to Bob.
We will use the notation  and  to refer to Bob’s public and private keys, respectively.
In order to communicate with Bob, Alice first fetches Bob’s public key.
Alice then encrypts her message, m, to Bob using Bob’s public key and a known (for example, standardized) encryption algorithm; that is, Alice computes .
Bob receives Alice’s encrypted message and uses his private key and a known (for example, standardized) decryption algorithm to decrypt Alice’s encrypted message.
That is, Bob computes .
We will see below that there are encryption/decryptionKB+ KB− KB−(m) KB−(KB+(m))
algorithms and techniques for choosing public and private keys such that ; that is, applying Bob’s public key, , to a message, m (to get ), and then applying Bob’s private key, , to the encrypted version of m (that is, computing ) gives back m. This is a remarkable result!
In this manner, Alice can use Bob’s publicly available key to send a secret message to Bob without either of them having to distribute any secret keys!
We will see shortly that we can interchange the public key and private key encryption and get the same remarkable result––that is, .
The use of public key cryptography is thus conceptually simple.
But two immediate worries may spring to mind.
A first concern is that although an intruder intercepting Alice’s encrypted message will see onlygibberish, the intruder knows both the key (Bob’s public key, which is available for all the world to see)and the algorithm that Alice used for encryption.
Trudy can thus mount a chosen-plaintext attack, usingthe known standardized encryption algorithm and Bob’s publicly available encryption key to encode anymessage she chooses!
Trudy might well try, for example, to encode messages, or parts of messages, that she suspects that Alice might send.
Clearly, if public key cryptography is to work, key selection and encryption/decryption must be done in such a way that it is impossible (or at least so hard as to benearly impossible) for an intruder to either determine Bob’s private key or somehow otherwise decrypt orguess Alice’s message to Bob.
A second concern is that since Bob’s encryption key is public, anyone can send an encrypted message to Bob, including Alice or someone claiming  to be Alice.
In the case of a single shared secret key, the fact that the sender knows the secret key implicitly identifies the sender to the receiver.
In the case of public key cryptography, however, this is no longer the case since anyonecan send an encrypted message to Bob using Bob’s publicly available key.
A digital signature, a topic we will study in Section 8.3, is needed to bind a sender to a message.
RSA While there may be many algorithms that address these concerns, the RSA ­algorithm (named after its founders, Ron Rivest, Adi Shamir, and Leonard Adleman) has become almost synonymous with public key cryptography.
Let’s first see how RSA works and then examine why it works.
RSA makes extensive use of arithmetic operations using modulo- n arithmetic.
So let’s briefly review modular arithmetic.
Recall that x mod n simply means the remainder of x when divided by n; so, for example, 19 mod .
In modular arithmetic, one performs the usual operations of addition, multiplication, and exponentiation.
However, the result of each operation is replaced by the integer remainder that is left when the result is divided by n. Adding and multiplying with modular arithmetic is facilitated with the following handy facts:KB−(KB+(m))=m KB+ KB−(m) KB− KB−(KB+(m)) KB−(B+(m))=KB+(KB−(m))=m 5=4 [ (a mod n)+(b mod n)]mod n=(a+b)mod n[ (a mod n)−(b mod n)]mod n=(a−b)mod n[ (a mod n)⋅(b mod n)]mod n=(a⋅b)mod n
It follows from the third fact that ( a mod n)   mod n, which is an identity that we will soon find very useful.
Now suppose that Alice wants to send to Bob an RSA-encrypted message, as shown in Figure 8.6.
In our discussion of RSA, let’s always keep in mind that a message is nothing but a bit pattern, and every bit pattern can be uniquely represented by an integer number (along with the length of the bit pattern).
For example, suppose a message is the bit pattern 1001; this message can be represented by thedecimal integer 9.
Thus, when encrypting a message with RSA, it is equivalent to encrypting the uniqueinteger number that represents the message.
There are two interrelated components of RSA: The choice of the public key and the private key The encryption and decryption algorithm To generate the public and private RSA keys, Bob performs the following steps: 1.
Choose two large prime numbers, p and q. How large should p and q be?
The larger the values, the more difficult it is to break RSA, but the longer it takes to perform the encoding and decoding.
RSA Laboratories recommends that the product of p and q be on the order of 1,024 bits.
For a discussion of how to find large prime numbers, see [Caldwell 2012].
2.
Compute  and .
3.
Choose a number, e, less than n, that has no common factors (other than 1) with z. (In this case, e and z are said to be relatively prime.)
The letter e is used since this value will be used in encryption.
4.
Find a number, d, such that  is exactly divisible (that is, with no ­remainder) by z. The letter d is used because this value will be used in decryption.
Put another way, given e, we choose d such that 5.
The public key that Bob makes available to the world, , is the pair of numbers (n, e); his private key, , is the pair of numbers (n, d).
The encryption by Alice and the decryption by Bob are done as follows: Suppose Alice wants to send Bob a bit pattern represented by the integer number m (with ).
To encode, Alice performs the exponentiation m, and then computes the integer remainder when m is divided by n. In other words, the encrypted value, c, of Alice’s plaintext message, m, isdn=ad n=pq z=(p−1)(q−1) ed−1 ed modz=1 KB+ KB− m<n e e c=memod n
The bit pattern corresponding to this ciphertext c is sent to Bob.
To decrypt the received ciphertext message, c, Bob computes which requires the use of his private key ( n, d).
Table 8.2 Alice’s RSA encryption, ,  Plaintext Letter m: numeric representation m Ciphertext  mod n l 12 248832 17 o 15 759375 15 v 22 5153632 22 e 5 3125 10 As a simple example of RSA, suppose Bob chooses  and .
­(Admittedly, these values are far too small to be secure.)
Then  and .
Bob chooses , since 5 and 24 have no common factors.
Finally, Bob chooses , since  (that is, ) is exactly divisible by 24.
Bob makes the two values,  and , public and keeps the value  secret.
Observing these two public values, suppose Alice now wants to send the letters l, o, v, and e to Bob.
Interpreting each letter as a number between 1 and 26 (with a being 1, and z being 26), Alice and Bob perform the encryption and decryption shown in Tables 8.2 and 8.3, respectively.
Note that in this example, we consider each of the four letters as a distinct message.
A more realistic example would be to convert the four letters into their 8-bit ASCII representations and then encrypt the integer corresponding to the resulting 32-bit bit pattern.(Such a realistic example generates numbers that are much too long to print in a textbook!)
Given that the “toy” example in Tables 8.2 and 8.3 has already produced some extremely large numbers, and given that we saw earlier that p and q should each be several hundred bits long, several practical issues regarding RSA come to mind.
How does one choose large prime numbers?
How does one then choose e and d?
How does one perform exponentiation with large numbers?
A discussion of these important issues is beyond the scope of this book; see [Kaufman 1995]  and the references therein for details.
Table 8.3 Bob’s RSA decryption, ,  Ciphertext c c m = c  mod n Plaintext Letterm=cdmod n e=5 n=35 ec=me p=5 q=7 n=35 z=24 e=5 d=29 5⋅29−1 ed−1 n=35 e=5 d=29 d=29 n=35 d d
17 4819685721067509150915091411825223071697 12 l 15 127834039403948858939111232757568359375 15 o 22 851643319086537701956194499721106030592 22 v 10 1000000000000000000000000000000 5 e Session Keys We note here that the exponentiation required by RSA is a rather time-consuming process.
By contrast, DES is at least 100 times faster in software and between 1,000 and 10,000 times faster in hardware [RSA Fast 2012] .
As a result, RSA is often used in practice in combination with symmetric key cryptography.
For example, if Alice wants to send Bob a large amount of encrypted data, she could do the following.
First Alice chooses a key that will be used to encode the data itself; this key is referred to as a session key , and is denoted by K .
Alice must inform Bob of the session key, since this is the shared ­symmetric key they will use with a symmetric key cipher (e.g., with DES or AES).
Aliceencrypts the session key using Bob’s public key, that is, computes  mod n. Bob receives the RSA-encrypted session key, c, and decrypts it to obtain the session key, K .
Bob now knows the session key that Alice will use for her encrypted data transfer.
Why Does RSA Work?
RSA encryption/decryption appears rather magical.
Why should it be that by applying the encryption algorithm and then the decryption algorithm, one recovers the original message?
In order to understand why RSA works, again denote , where p and q are the large prime numbers used in the RSA algorithm.
Recall that, under RSA encryption, a message (uniquely represented by an ­integer), m, is exponentiated to the power e using modulo- n arithmetic, that is, Decryption is performed by raising this value to the power d, again using modulo- n arithmetic.
The result of an encryption step followed by a decryption step is thus ( m mod n)  mod n. Let’s now see what we can say about this quantity.
As mentioned earlier, one important property of modulo arithmetic is ( a mod n) mod  mod n for any values a, n, and d. Thus, using  in this property, we haveS c=(KS)e S n=pq c=memod n e d d n=ad a=me (memod n)dmod n=medmod n
It therefore remains to show that .
Although we’re trying to remove some of the magic about why RSA works, to establish this, we’ll need to use a rather magical result from number theory here.
Specifically, we’ll need the result that says if p and q are prime, , and , then x mod n is the same as x    mod n [Kaufman 1995] .
Applying this result with  and  we have But remember that we have chosen e and d such that .
This gives us which is exactly the result we are looking for!
By first exponentiating to the power of e (that is, encrypting) and then exponentiating to the power of d (that is, ­decrypting), we obtain the original value, m. Even more wonderful is the fact that if we first exponentiate to the power of d and then exponentiate to the power of e—that is, we reverse the order of encryption and decryption, performing the decryption operation first and then applying the encryption operation—we also obtain the original value, m. This wonderful result follows immediately from the modular arithmetic: The security of RSA relies on the fact that there are no known algorithms for quickly factoring a number, in this case the public value n, into the primes p and q. If one knew p and q, then given the public value e, one could easily compute the secret key, d. On the other hand, it is not known whether or not there exist fast algorithms for factoring a number, and in this sense, the security of RSA is not guaranteed.
Another popular public-key encryption algorithm is the Diffie-Hellman algorithm, which we will briefly explore in the homework problems.
Diffie-Hellman is not as versatile as RSA in that it cannot be used toencrypt messages of arbitrary length; it can be used, however, to establish a symmetric session key,which is in turn used to encrypt messages.medmod n=m n=pq z=(p−1)(q−1) y (ymod z) x=m y=ed medmod n=m(edmod z)mod n edmod z=1 medmod n=m1mod n=m (mdmod n)emod n=mdemod n=medmod n=(memod n)dmod n
8.3 Message Integrity and Digital Signatures In the previous section we saw how encryption can be used to provide confidentiality to two communicating entities.
In this section we turn to the equally important cryptography topic of providingmessage integrity (also known as message ­authentication).
Along with message integrity, we will discuss two related topics in this section: digital signatures and end-point authentication.
We define the message integrity problem using, once again, Alice and Bob.
Suppose Bob receives a message (which may be encrypted or may be in plaintext) and he believes this message was sent byAlice.
To authenticate this message, Bob needs to verify: 1.
The message indeed originated from Alice.
2.
The message was not tampered with on its way to Bob.
We’ll see in Sections 8.4 through 8.7 that this problem of message integrity is a critical concern in just about all secure networking protocols.
As a specific example, consider a computer network using a link-state routing algorithm (such as OSPF) for determining routes between each pair of routers in the network (see Chapter 5).
In a link-state algorithm, each router needs to broadcast a link-state message to all other routers in the network.
A router’s link-state message includes a list of its directly connected neighbors and the direct costs tothese neighbors.
Once a router receives link-state messages from all of the other routers, it can create acomplete map of the network, run its least-cost routing algorithm, and configure its forwarding table.
Onerelatively easy attack on the routing algorithm is for Trudy to distribute bogus link-state messages withincorrect link-state information.
Thus the need for message integrity—when router B receives a link- state message from router A, router B should verify that router A actually created the message and, further, that no one tampered with the message in transit.
In this section, we describe a popular message integrity technique that is used by many secure networking protocols.
But before doing so, we need to cover another important topic in cryptography—cryptographic hash functions.
8.3.1 Cryptographic Hash Functions As shown in Figure 8.7, a hash function takes an input, m, and computes a fixed-size string H(m)
known as a hash.
The Internet checksum ( Chapter 3) and CRCs (Chapter 6) meet this definition.
A cryptographic hash function  is required to have the following additional property: It is computationally infeasible to find any two different messages x and y such that .
Informally, this property means that it is computationally infeasible for an intruder to substitute one message for another message that is protected by the hash Figure 8.7 Hash functions Figure 8.8 Initial message and fraudulent message have the same ­checksum!
function.
That is, if (m, H(m)) are the message and the hash of the message created by the sender, thenH(x)=H(y)
an intruder cannot forge the contents of another message, y, that has the same hash value as the original message.
Let’s convince ourselves that a simple checksum, such as the Internet checksum, would make a poor cryptographic hash function.
Rather than performing 1s complement arithmetic (as in the Internet checksum), let us compute a checksum by treating each character as a byte and adding the bytestogether using 4-byte chunks at a time.
Suppose Bob owes Alice $100.99 and sends an IOU to Alice consisting of the text string “ IOU100.99BOB. ”
The ASCII representation (in hexadecimal notation) for these letters is 49, 4F, 55, 31, 30, 30, 2E, 39, 39, 42, 4F, 42.
Figure 8.8 (top) shows that the 4-byte checksum for this message is B2 C1 D2 AC.
A slightly different message (and a much more costly one for Bob) is shown in the bottom half of Figure 8.8.
The messages “ IOU100.99BOB ” and “ IOU900.19BOB ” have the same checksum.
Thus, this simple checksum algorithm violates the requirement above.
Given the original data, it is simple to find another set of data with the same checksum.
Clearly, for security purposes, we are going to need a more powerful hash function than a checksum.
The MD5 hash algorithm of Ron Rivest [RFC 1321]  is in wide use today.
It computes a 128-bit hash in a four-step process consisting of a padding step (adding a one followed by enough zeros so that the length of the message satisfies certain conditions), an append step (appending a 64-bit representationof the message length before padding), an initialization of an accumulator, and a final looping step inwhich the message’s 16-word blocks are processed (mangled) in four rounds.
For a description of MD5 (including a C source code implementation) see [RFC 1321] .
The second major hash algorithm in use today is the Secure Hash Algorithm (SHA-1) [FIPS 1995].
This algorithm is based on principles similar to those used in the design of MD4 [RFC 1320] , the predecessor to MD5.
SHA-1, a US federal standard, is required for use whenever a cryptographic hash algorithm is needed for federal applications.
It produces a 160-bit message digest.
The longer output length makesSHA-1 more secure.
8.3.2 Message Authentication Code Let’s now return to the problem of message integrity.
Now that we understand hash functions, let’s takea first stab at how we might perform message integrity: 1.
Alice creates message m and calculates the hash H(m) (for example with SHA-1).
2.
Alice then appends H(m) to the message m, creating an extended message ( m, H(m)), and sends the extended message to Bob.
3.
Bob receives an extended message ( m, h) and calculates H(m).
If , Bob concludes that everything is fine.
This approach is obviously flawed.
Trudy can create a bogus message m´ in which she says she is Alice, calculate H(m´), and send Bob (m´, H(m´)).
When Bob receives the message, everything checks out in step 3, so Bob doesn’t suspect any funny ­business.
To perform message integrity, in addition to using cryptographic hash functions, Alice and Bob will need a shared secret s. This shared secret, which is nothing more than a string of bits, is called the authentication key.
Using this shared secret, message integrity can be performed as follows: 1.
Alice creates message m, concatenates s with m to create , and calculates the hash  (for example with SHA-1).
 is called the message authentication code (MAC).
2.
Alice then appends the MAC to the message m, creating an extended message , and sends the extended message to Bob.
3.
Bob receives an extended message ( m, h) and knowing s, calculates the MAC .
If , Bob concludes that everything is fine.
A summary of the procedure is shown in Figure 8.9.
Readers should note that the MAC here (standing for “message authentication code”) is not the same MAC used in link-layer protocols (standing for “medium access control”)!
One nice feature of a MAC is that it does not require an encryption algorithm.
Indeed, in many applications, including the link-state routing algorithm described earlier, communicating entities are onlyconcerned with message integrity and are not concerned with message confidentiality.
Using a MAC,the entities can authenticate Figure 8.9 Message authentication code (MAC)H(m)=h m+s H(m+s) H(m+s) (m, H(m+s)) H(m+s) H(m+s)=h
the messages they send to each other without having to integrate complex encryption algorithms into the integrity process.
As you might expect, a number of different standards for MACs have been proposed over the years.
The most popular standard today is HMAC, which can be used either with MD5 or SHA-1.
HMAC actually runs data and the authentication key through the hash function twice [Kaufman 1995 ; RFC 2104].
There still remains an important issue.
How do we distribute the shared authentication key to the communicating entities?
For example, in the link-state routing algorithm, we would somehow need todistribute the secret authentication key to each of the routers in the autonomous system. (
Note that therouters can all use the same authentication key.)
A network administrator could actually accomplish thisby physically visiting each of the routers.
Or, if the network administrator is a lazy guy, and if each router has its own public key, the network administrator could distribute the authentication key to any one of the routers by encrypting it with the router’s public key and then sending the encrypted key over thenetwork to the router.
8.3.3 Digital Signatures Think of the number of the times you’ve signed your name to a piece of paper during the last week.
Yousign checks, credit card receipts, legal documents, and letters.
Your signature attests to the fact that you(as opposed to someone else) have acknowledged and/or agreed with the document’s contents.
In adigital world, one often wants to indicate the owner or creator of a document, or to signify one’sagreement with a document’s content.
A digital signature  is a cryptographic technique for achieving these goals in a digital world.
Just as with handwritten signatures, digital signing should be done in a way that is verifiable and nonforgeable.
That is, it must be possible to prove that a document signed by an individual was indeed signed by that individual (the signature must be verifiable) and that only that individual could have signed the document (the signature cannot be forged).
Let’s now consider how we might design a digital signature scheme.
Observe that when Bob signs a message, Bob must put something on the message that is unique to him.
Bob could consider attaching a MAC for the signature, where the MAC is created by appending his key (unique to him) to themessage, and then taking the hash.
But for Alice to verify the signature, she must also have a copy ofthe key, in which case the key would not be unique to Bob.
Thus, MACs are not going to get the jobdone here.
Recall that with public-key cryptography, Bob has both a public and private key, with both of these keys being unique to Bob.
Thus, public-key cryptography is an excellent candidate for providing digital signatures.
Let us now examine how it is done.
Suppose that Bob wants to digitally sign a document, m. We can think of the document as a file or a message that Bob is going to sign and send.
As shown in Figure 8.10, to sign this document, Bob simply uses his private key, , to compute .
At first, it might seem odd that Bob is using his private key (which, as we saw in Section 8.2, was used to decrypt a message that had been encrypted with his public key) to sign a document.
But recall that encryption and decryption are nothing more than mathematical operations (exponentiation to the power of e or d in RSA; see Section 8.2) and recall that Bob’s goal is not to scramble or obscure the contents of the document, but rather to sign the document in a manner that is verifiable and nonforgeable.
Bob’s digital signature of the document is .
Does the digital signature  meet our requirements of being verifiable and nonforgeable?
Suppose Alice has m and .
She wants to prove in court (being Figure 8.10 Creating a digital signature for a document litigious) that Bob had indeed signed the document and was the only person who could have possibly signed the document.
Alice takes Bob’s public key, , and applies it to the digital signature, , associated with the document, m. That is, she computes , and voilà, with a dramatic flurry, she produces m, which exactly matches the original document!
Alice then argues that only Bob could have signed the document, for the following reasons: Whoever signed the message must have used the private key, , in computing the signature , such that .
The only person who could have known the private key, , is Bob.
Recall from our discussion ofKB− KB−(m) KB−(m) KB−(m) KB−(m) KB+ KB−(m) KB+(KB−(m)) KB− KB−(m) KB+(KB−(m))=m KB−
RSA in Section 8.2 that knowing the public key, , is of no help in learning the private key, .
Therefore, the only person who could know  is the person who generated the pair of keys, ( , ), in the first place, Bob. (
Note that this assumes, though, that Bob has not given  to anyone, nor has anyone stolen  from Bob.)
It is also important to note that if the original document, m, is ever modified to some alternate form, m´, the signature that Bob created for m will not be valid for m´, since  does not equal m´.
Thus we see that digital signatures also provide message integrity, allowing the receiver to verify that the message was unaltered as well as the source of the message.
One concern with signing data by encryption is that encryption and decryption are computationally expensive.
Given the overheads of encryption and decryption, signing data via completeencryption/decryption can be overkill.
A more efficient approach is to introduce hash functions into the digital signature.
Recall from ­Section 8.3.2 that a hash algorithm takes a message, m, of arbitrary length and computes a fixed-length “fingerprint” of the message, denoted by H(m).
Using a hash function, Bob signs the hash of a message rather than the message itself, that is, Bob calculates .
Since H(m) is generally much smaller than the original message m, the computational effort required to create the digital signature is substantially reduced.
In the context of Bob sending a message to Alice, Figure 8.11 provides a summary of the operational procedure of creating a digital signature.
Bob puts his original long message through a hash function.
He then digitally signs the resulting hash with his private key.
The original message (in cleartext) alongwith the digitally signed message digest (henceforth referred to as the digital signature) is then sent to Alice.
Figure 8.12 provides a summary of the operational procedure of the signature.
Alice applies the sender’s public key to the message to obtain a hash result.
Alice also applies the hash function to the cleartext message to obtain a second hash result.
If the two hashes match, then Alice can be sure aboutthe integrity and author of the message.
Before moving on, let’s briefly compare digital signatures with MACs, since they have parallels, but also have important subtle differences.
Both digital signatures andKB+ KB− KB− KB+ KB− KB− KB− KB+(KB−(m)) KB−(H(m))
Figure 8.11 Sending a digitally signed message MACs start with a message (or a document).
To create a MAC out of the message, we append an authentication key to the message, and then take the hash of the result.
Note that neither public key nor symmetric key encryption is involved in creating the MAC.
To create a digital signature, we first take thehash of the message and then encrypt the message with our private key (using public keycryptography).
Thus, a digital signature is a “heavier” technique, since it requires an underlying Public Key Infrastructure (PKI) with certification authorities as described below.
We’ll see in Section 8.4 that PGP—a popular secure e-mail system—uses digital signatures for message integrity.
We’ve seenalready that OSPF uses MACs for message integrity.
We’ll see in Sections 8.5 and 8.6 that MACs are also used for popular transport-layer and network-layer security protocols.
Public Key Certification An important application of digital signatures is public key certification , that is, certifying that a public key belongs to a specific entity.
Public key certification is used in many popular secure networking protocols, including IPsec and SSL.
To gain insight into this problem, let’s consider an Internet-commerce version of the classic “pizza prank.”
Alice is in the pizza delivery business and accepts orders
Figure 8.12 Verifying a signed message over the Internet.
Bob, a pizza lover, sends Alice a plaintext message that includes his home address and the type of pizza he wants.
In this message, Bob also includes a digital signature (that is, a signed hash of the original plaintext message) to prove to Alice that he is the true source of the message.
Toverify the signature, Alice obtains Bob’s public key (perhaps from a public key server or from the e-mailmessage) and checks the digital signature.
In this manner she makes sure that Bob, rather than someadolescent prankster, placed the order.
This all sounds fine until clever Trudy comes along.
As shown in Figure 8.13, Trudy is indulging in a prank.
She sends a message to Alice in which she says she is Bob, gives Bob’s home address, and orders a pizza.
In this message she also includes her (Trudy’s) public key, although Alice naturally assumes it is Bob’s public key.
Trudy also attaches a digital signature, which was created with her own (Trudy’s) private key.
After receiving the message, Alice applies Trudy’s public key (thinking that it isBob’s) to the digital signature and concludes that the plaintext message was
Figure 8.13 Trudy masquerades as Bob using public key cryptography indeed created by Bob.
Bob will be very surprised when the delivery person brings a pizza with pepperoni and anchovies to his home!
We see from this example that for public key cryptography to be useful, you need to be able to verify that you have the actual public key of the entity (person, router, browser, and so on) with whom youwant to communicate.
For example, when Alice wants to communicate with Bob using public keycryptography, she needs to verify that the public key that is supposed to be Bob’s is indeed Bob’s.
Binding a public key to a particular entity is typically done by a Certification Authority (CA), whose job is to validate identities and issue certificates.
A CA has the following roles: 1.
A CA verifies that an entity (a person, a router, and so on) is who it says it is.
There are no mandated procedures for how certification is done.
When dealing with a CA, one must trust the CA to have performed a suitably rigorous identity verification.
For example, if Trudy were able towalk into the Fly-by-Night
Figure 8.14 Bob has his public key certified by the CA CA and simply announce “I am Alice” and receive certificates associated with the identity of Alice, then one shouldn’t put much faith in public keys certified by the Fly-by-Night CA.
On the other hand, one might (or might not!)
be more willing to trust a CA that is part of a federal orstate program.
You can trust the identity associated with a public key only to the extent to whichyou can trust a CA and its identity verification techniques.
What a tangled web of trust we spin!
2.
Once the CA verifies the identity of the entity, the CA creates a certificate  that binds the public key of the entity to the identity.
The certificate contains the public key and globally uniqueidentifying information about the owner of the public key (for example, a human name or an IP address).
The certificate is digitally signed by the CA.
These steps are shown in Figure 8.14.
Let us now see how certificates can be used to combat pizza-ordering pranksters, like Trudy, and other undesirables.
When Bob places his order he also sends his CA-signed certificate.
Alice uses the CA’spublic key to check the validity of Bob’s certificate and extract Bob’s public key.
Both the International Telecommunication Union (ITU) and the IETF have developed standards for CAs.
ITU X.509 [ITU 2005a] specifies an authentication service as well as a specific syntax for certificates. [
RFC 1422]  describes CA-based key management for use with secure Internet e-mail.
It is compatible with X.509 but goes beyond X.509 by establishing procedures and conventions for a key management architecture.
Table 8.4 describes some of the important fields in a certificate.
Table 8.4 Selected fields in an X.509 and RFC 1422 public key
Field Name Description Version Version number of X.509 specification Serial numberCA-issued unique identifier for a certificate Signature Specifies the algorithm used by CA to sign this certificate IssuernameIdentity of CA issuing this certificate, in distinguished name (DN) [RFC 4514]  format Validity periodStart and end of period of validity for certificate Subject nameIdentity of entity whose public key is associated with this certificate, in DN format Subjectpublic keyThe subject’s public key as well indication of the public key algorithm (and algorithmparameters) to be used with this key
8.4 End-Point Authentication End-point authentication  is the process of one entity proving its identity to another entity over a computer network, for example, a user proving its identity to an e-mail server.
As humans, we authenticate each other in many ways: We recognize each ­other’s faces when we meet, we recognize each other’s voices on the telephone, we are authenticated by the customs official who checks usagainst the picture on our passport.
In this section, we consider how one party can authenticate another party when the two are communicating over a network.
We focus here on authenticating a “live” party, at the point in time whencommunication is actually occurring.
A concrete example is a user authenticating him or herself to an e-mail server.
This is a subtly different problem from proving that a message received at some point in the past did indeed come from that claimed sender, as studied in Section 8.3.
When performing authentication over the network, the communicating parties cannot rely on biometric information, such as a visual appearance or a voiceprint.
Indeed, we will see in our later case studies that it is often network elements such as routers and client/server processes that must authenticateeach other.
Here, authentication must be done solely on the basis of messages and data exchanged as part of an authentication protocol .
Typically, an authentication protocol would run before the two communicating parties run some other protocol (for example, a reliable data transfer protocol, a routing information exchange protocol, or an e-mail protocol).
The authentication protocol first establishes theidentities of the parties to each other’s satisfaction; only after authentication do the parties get down tothe work at hand.
As in the case of our development of a reliable data transfer (rdt) protocol in Chapter 3, we will find it instructive here to develop various versions of an authentication protocol, which we will call ap (authentication protocol), and poke holes in each version
Figure 8.15 Protocol ap1.0 and a failure scenario as we proceed. (
If you enjoy this stepwise evolution of a design, you might also enjoy [Bryant 1988] , which recounts a fictitious narrative between designers of an open-network authentication system, and their discovery of the many subtle issues involved.)
Let’s assume that Alice needs to authenticate herself to Bob.
8.4.1 Authentication Protocol ap1.0 Perhaps the simplest authentication protocol we can imagine is one where Alice simply sends a message to Bob saying she is Alice.
This protocol is shown in Figure 8.15.
The flaw here is obvious— there is no way for Bob actually to know that the person sending the message “I am Alice” is indeed Alice.
For example, Trudy (the intruder) could just as well send such a message.
8.4.2 Authentication Protocol ap2.0 If Alice has a well-known network address (e.g., an IP address) from which she always communicates,Bob could attempt to authenticate Alice by verifying that the source address on the IP datagram carryingthe authentication message matches Alice’s well-known address.
In this case, Alice would beauthenticated.
This might stop a very network-naive intruder from impersonating Alice, but it wouldn’tstop the determined student studying this book, or many others!
From our study of the network and data link layers, we know that it is not that hard (for example, if one had access to the operating system code and could build one’s own operating system kernel, as is the
case with Linux and several other freely available operating systems) to create an IP datagram, put whatever IP source address we want (for example, Alice’s well-known IP address) into the IP datagram, and send the datagram over the link-layer protocol to the first-hop router.
From then Figure 8.16 Protocol ap2.0 and a failure scenario on, the incorrectly source-addressed datagram would be dutifully forwarded to Bob.
This approach, shown in Figure 8.16, is a form of IP spoofing.
IP spoofing can be avoided if Trudy’s first-hop router is configured to forward only datagrams containing Trudy’s IP source address [RFC 2827] .
However, this capability is not universally deployed or enforced.
Bob would thus be foolish to assume that Trudy’s network manager (who might be Trudy herself) had configured Trudy’s first-hop router to forward onlyappropriately addressed datagrams.
8.4.3 Authentication Protocol ap3.0 One classic approach to authentication is to use a secret password.
The password is a shared secretbetween the authenticator and the person being authenticated.
Gmail, Facebook, telnet, FTP, and manyother services use password authentication.
In protocol ap3.0, Alice thus sends her secret password to Bob, as shown in Figure 8.17.
Since passwords are so widely used, we might suspect that protocol ap3.0 is fairly secure.
If so, we’d be wrong!
The security flaw here is clear.
If Trudy eavesdrops on Alice’s communication, then she can learn Alice’s password.
Lest you think this is unlikely, consider the fact that when you Telnet to anothermachine and log in, the login password is sent unencrypted to the Telnet server.
Someone connected tothe Telnet client or server’s LAN can possibly sniff (read and store) all packets transmitted on the LANand thus steal the login password.
In fact, this is a well-known approach for stealing passwords (see, for example, [Jimenez 1997] ).
Such a threat is obviously very real, so ap3.0 clearly won’t do.
8.4.4 Authentication Protocol ap3.1 Our next idea for fixing ap3.0 is naturally to encrypt the password.
By encrypting the password, we can prevent Trudy from learning Alice’s password.
If we assume Figure 8.17 Protocol ap3.0 and a failure scenario that Alice and Bob share a symmetric secret key,  then Alice can encrypt the password and send her identification message, “ I am Alice, ” and her encrypted password to Bob.
Bob then decrypts the password and, assuming the password is correct, authenticates Alice.
Bob feels comfortable inauthenticating Alice since Alice not only knows the password, but also knows the shared secret keyvalue needed to encrypt the password.
Let’s call this protocol ap3.1.
While it is true that ap3.1 prevents Trudy from learning Alice’s password, the use of cryptography here does not solve the authentication problem.
Bob is subject to a playback attack : Trudy need only eavesdrop on Alice’s communication, record the encrypted version of the password, and play back the encrypted version of the password to Bob to pretend that she is Alice.
The use of an encrypted password in ap3.1 doesn’t make the situation manifestly different from that of protocol ap3.0 in Figure 8.17.KA−B,
8.4.5 Authentication Protocol ap4.0 The failure scenario in Figure 8.17 resulted from the fact that Bob could not distinguish between the original authentication of Alice and the later playback of Alice’s original authentication.
That is, Bob could not tell if Alice was live (that is, was currently really on the other end of the connection) or whetherthe messages he was receiving were a recorded playback of a previous authentication of Alice.
The very (very) observant reader will recall that the three-way TCP handshake protocol needed to address the same problem—the server side of a TCP connection did not want to accept a connection if the received SYN segment was an old copy (retransmission) of a SYN segment from an earlier connection.
How did the TCP server side solve the problem of determining whether the client was really live?
Itchose an initial sequence number that had not been used in a very long time, sent that number to theclient, and then waited for the client to respond with an ACK segment containing that number.
We canadopt the same idea here for authentication purposes.
A nonce is a number that a protocol will use only once in a lifetime.
That is, once a protocol uses a nonce, it will never use that number again.
Our ap4.0 protocol uses a nonce as follows: 1.
Alice sends the message “ I am Alice ” to Bob.
2.
Bob chooses a nonce, R, and sends it to Alice.
3.
Alice encrypts the nonce using Alice and Bob’s symmetric secret key,  and sends the encrypted nonce,  (R), back to Bob.
As in protocol ap3.1, it is the fact that Alice knows  and uses it to encrypt a value that lets Bob know that the message he receives was generated by Alice.
The nonce is used to ensure that Alice is live.
4.
Bob decrypts the received message.
If the decrypted nonce equals the nonce he sent Alice, then Alice is authenticated.
Protocol ap4.0 is illustrated in Figure 8.18.
By using the once-in-a-lifetime value, R, and then checking the returned value,  (R), Bob can be sure that Alice is both who she says she is (since she knows the secret key value needed to encrypt R) and live (since she has encrypted the nonce, R, that Bob just created).
The use of a nonce and symmetric key cryptography forms the basis of ap4.0.
A natural question is whether we can use a nonce and public key cryptography (rather than symmetric key cryptography) to solve the authentication problem.
This issue is explored in the problems at the end of the chapter.
KA−B, KA−B KA−B KA−B
Figure 8.18 Protocol ap4.0 and a failure scenario
8.5 Securing E-Mail In previous sections, we examined fundamental issues in network security, including symmetric key and public key cryptography, end-point authentication, key distribution, message integrity, and digitalsignatures.
We are now going to examine how these tools are being used to provide security in theInternet.
Interestingly, it is possible to provide security services in any of the top four layers of the Internet protocol stack.
When security is provided for a specific application-layer protocol, the application usingthe protocol will enjoy one or more security services, such as confidentiality, authentication, or integrity.
When security is provided for a transport-layer protocol, all applications that use that protocol enjoy thesecurity services of the transport protocol.
When security is provided at the network layer on a host-to-host basis, all transport-layer segments (and hence all application-layer data) enjoy the security servicesof the network layer.
When security is provided on a link basis, then the data in all frames traveling overthe link receive the security services of the link.
In Sections 8.5 through 8.8, we examine how security tools are being used in the application, transport, network, and link layers.
Being consistent with the general structure of this book, we begin at the top of the protocol stack and discuss security at the application layer.
Our approach is to use a specificapplication, e-mail, as a case study for application-layer security.
We then move down the protocolstack.
We’ll examine the SSL protocol (which provides security at the transport layer), IPsec (whichprovides security at the network layer), and the security of the IEEE 802.11 wireless LAN protocol.
You might be wondering why security functionality is being provided at more than one layer in the Internet.
Wouldn’t it suffice simply to provide the security functionality at the network layer and be done with it?
There are two answers to this question.
First, although security at the network layer can offer“blanket coverage” by encrypting all the data in the datagrams (that is, all the transport-layer segments)and by authenticating all the source IP addresses, it can’t provide user-level security.
For example, acommerce site cannot rely on IP-layer security to authenticate a customer who is purchasing goods atthe commerce site.
Thus, there is a need for security functionality at higher layers as well as blanketcoverage at lower layers.
Second, it is generally easier to deploy new Internet services, includingsecurity services, at the higher layers of the protocol stack.
While waiting for security to be broadly deployed at the network layer, which is probably still many years in the future, many application developers “just do it” and introduce security functionality into their favorite applications.
A classicexample is Pretty Good Privacy (PGP), which provides secure e-mail (discussed later in this section).Requiring only client and server application code, PGP was one of the first security technologies to be broadly used in the Internet.
8.5.1 Secure E-Mail We now use the cryptographic principles of Sections 8.2 through 8.3 to create a secure e-mail system.
We create this high-level design in an incremental manner, at each step introducing new security services.
When designing a secure e-mail system, let us keep in mind the racy example introduced in Section 8.1—the love affair between Alice and Bob.
Imagine that Alice wants to send an e-mail message to Bob, and Trudy wants to intrude.
Before plowing ahead and designing a secure e-mail system for Alice and Bob, we should consider which security features would be most desirable for them.
First and foremost is confidentiality.
 As discussed in Section 8.1, neither Alice nor Bob wants Trudy to read Alice’s e-mail message.
The second feature that Alice and Bob would most likely want to see in the secure e-mail system is sender authentication .
In particular, when Bob receives the message “ I don’t love you anymore.
I never want to see you again.
Formerly yours, Alice, ” he would naturally want to be sure that the message came from Alice and not from Trudy.
Another feature that the two lovers wouldappreciate is message integrity , that is, assurance that the message Alice sends is not modified while en route to Bob.
Finally, the e-mail system should provide receiver authentication ; that is, Alice wants to make sure that she is indeed sending the letter to Bob and not to someone else (for example, Trudy) who is impersonating Bob.
So let’s begin by addressing the foremost concern, confidentiality.
The most straightforward way to provide confidentiality is for Alice to encrypt the message with symmetric key technology (such as DES or AES) and for Bob to decrypt the message on receipt.
As discussed in Section 8.2, if the symmetric key is long enough, and if only Alice and Bob have the key, then it is extremely difficult for anyone else (including Trudy) to read the message.
Although this approach is straightforward, it has the fundamental difficulty that we discussed in Section 8.2—distributing a symmetric key so that only Alice and Bob have copies of it.
So we naturally consider an alternative approach—public key cryptography (using, for example, RSA).
In the public key approach, Bob makes his public key publicly available (e.g., in a public key server or on his personal Web page), Alice encrypts her message with Bob’s public key, and shesends the encrypted message to Bob’s e-mail address.
When Bob receives the message, he simplydecrypts it with his private key.
Assuming that Alice knows for sure that the public key is Bob’s publickey, this approach is an excellent means to provide the desired confidentiality.
One problem, however,is that public key encryption is relatively inefficient, particularly for long messages.
To overcome the efficiency problem, let’s make use of a session key (discussed in Section 8.2.2).
In particular, Alice (1) selects a random symmetric session key, K, (2) encrypts her message, m, with the symmetric key, (3) encrypts the symmetric key with Bob’s public key, , (4) concatenates the S KB+
encrypted message and the encrypted symmetric key to form a “package,” and (5) sends the package to Bob’s Figure 8.19 Alice used a symmetric session key, K , to send a secret e-mail to Bob e-mail address.
The steps are illustrated in Figure 8.19. (
In this and the subsequent figures, the circled “” represents concatenation and the circled “ ” represents deconcatenation.)
When Bob receives the package, he (1) uses his private key, , to obtain the symmetric key, K , and (2) uses the symmetric key K  to decrypt the message m. Having designed a secure e-mail system that provides confidentiality, let’s now design another system that provides both sender authentication and message integrity.
We’ll suppose, for the moment, thatAlice and Bob are no longer concerned with confidentiality (they want to share their feelings witheveryone!),
and are concerned only about sender authentication and message integrity.
To accomplish this task, we use digital signatures and message digests, as described in Section 8.3.
Specifically, Alice (1) applies a hash function, H (for example, MD5), to her message, m, to obtain a message digest, (2) signs the result of the hash function with her private key, , to create a digital signature, (3) concatenates the original (unencrypted) message with the signature to create a package, and (4) sends the package to Bob’s e-mail address.
When Bob receives the package, he (1) applies Alice’s public key, , to the signed message digest and (2) compares the result of this operation with his own hash, H, of the message.
The steps are illustrated in Figure 8.20.
As discussed in Section 8.3, if the two results are the same, Bob can be pretty confident that the message came from Alice and is unaltered.
Now let’s consider designing an e-mail system that provides confidentiality, sender authentication, and message integrity.
This can be done by combining the procedures in Figures 8.19 and 8.20.
Alice first creates a preliminary package, exactly as in Figure 8.20, that consists of her original message along with a digitally signed hash of the message.
She then treats this preliminary package as a message in itself and sends this new message through the sender steps in Figure 8.19, creating a new package that is sent to Bob.
The steps applied by Alice are shown in Figure 8.21.
When Bob receives theS + − KB−S S KA− KA+
package, he first applies his side of Figure 8.19 and then his Figure 8.20 Using hash functions and digital signatures to provide ­sender authentication and message integrity side of Figure 8.20.
It should be clear that this design achieves the goal of providing confidentiality, sender authentication, and message integrity.
Note that, in this scheme, Alice uses public key cryptography twice: once with her own private key and once with Bob’s public key.
Similarly, Bob alsouses public key cryptography twice—once with his private key and once with Alice’s public key.
The secure e-mail design outlined in Figure 8.21 probably provides satisfactory security for most e-mail users for most occasions.
But there is still one important issue that remains to be addressed.
The design in Figure 8.21 requires Alice to obtain Bob’s public key, and requires Bob to obtain Alice’s public key.
The distribution of these public keys is a nontrivial problem.
For example, Trudy might masquerade as Bob and give Alice her own public key while saying that it is Bob’s public key, Figure 8.21 Alice uses symmetric key cyptography, public key ­cryptography, a hash function, and a digital signature to ­provide secrecy, sender authentication, and message integrity
enabling her to receive the message meant for Bob.
As we learned in Section 8.3, a popular approach for securely distributing public keys is to certify the public keys using a CA.
8.5.2 PGP Written by Phil Zimmermann in 1991, Pretty Good Privacy (PGP) is a nice example of an e-mail encryption scheme [PGPI 2016].
Versions of PGP are available in the public domain; for example, you can find the PGP software for your favorite platform as well as lots of interesting reading at the International PGP Home Page [PGPI 2016].
The PGP design is, in essence, the same as the design shown in Figure 8.21.
Depending on the version, the PGP software uses MD5 or SHA for calculating the message digest; CAST, triple-DES, or IDEA for symmetric key encryption; and RSA for the public key encryption.
When PGP is installed, the software creates a public key pair for the user.
The public key can be posted on the user’s Web site or placed in a public key server.
The private key is protected by the use of apassword.
The password has to be entered every time the user accesses the private key.
PGP gives theuser the option of digitally signing the message, encrypting the message, or both digitally signing and encrypting.
Figure 8.22 shows a PGP signed message.
This message appears after the MIME header.
The encoded data in the message is , that is, the digitally signed message digest.
As we discussed above, in order for Bob to verify the integrity of the message, he needs to have access to Alice’s public key.
Figure 8.23 shows a secret PGP message.
This message also appears after the MIME header.
Of course, the plaintext message is not included within the secret e-mail message.
When a sender (such as Alice) wants both confidentiality and integrity, PGP contains a message like that of Figure 8.23 within the message of Figure 8.22.
PGP also provides a mechanism for public key certification, but the mechanism is quite different from the more conventional CA.
PGP public keys are certified by KA−(H(m))
Figure 8.22 A PGP signed message Figure 8.23 A secret PGP message a web of trust.
Alice herself can certify any key/username pair when she believes the pair really belong together.
In addition, PGP permits Alice to say that she trusts another user to vouch for the authenticity of more keys.
Some PGP users sign each other’s keys by holding key-signing parties.
Users physicallygather, exchange ­public keys, and certify each other’s keys by signing them with their private keys.
8.6 Securing TCP Connections: SSL In the previous section, we saw how cryptographic techniques can provide confidentiality, data integrity, and end-point authentication to a specific application, namely, e-mail.
In this section, we’ll drop down alayer in the protocol stack and examine how cryptography can enhance TCP with security services,including confidentiality, data integrity, and end-point authentication.
This enhanced version of TCP iscommonly known as Secure Sockets Layer (SSL) .
A slightly modified version of SSL version 3, called Transport Layer Security (TLS) , has been standardized by the IETF [RFC 4346] .
The SSL protocol was originally designed by Netscape, but the basic ideas behind securing TCP had predated Netscape’s work (for example, see Woo [Woo 1994]).
Since its inception, SSL has enjoyed broad deployment.
SSL is supported by all popular Web browsers and Web servers, and it is used by Gmail and essentially all Internet commerce sites (including Amazon, eBay, and TaoBao).
Hundreds ofbillions of dollars are spent over SSL every year.
In fact, if you have ever purchased anything over theInternet with your credit card, the communication between your browser and the server for this purchase almost certainly went over SSL. (
You can identify that SSL is being used by your browser when the URL begins with https: rather than http.)
To understand the need for SSL, let’s walk through a typical Internet commerce scenario.
Bob is surfing the Web and arrives at the Alice Incorporated site, which is selling perfume.
The Alice Incorporated sitedisplays a form in which Bob is supposed to enter the type of perfume and quantity desired, his address,and his payment card number.
Bob enters this information, clicks on Submit, and expects to receive (viaordinary postal mail) the purchased perfumes; he also expects to receive a charge for his order in his next payment card statement.
This all sounds good, but if no security measures are taken, Bob could be in for a few surprises.
If no confidentiality (encryption) is used, an intruder could intercept Bob’s order and obtain his payment card information.
The intruder could then make purchases at Bob’s expense.
If no data integrity is used, an intruder could modify Bob’s order, having him purchase ten timesmore bottles of perfume than desired.
Finally, if no server authentication is used, a server could display Alice Incorporated’s famous logowhen in actuality the site maintained by Trudy, who is masquerading as Alice Incorporated.
After receiving Bob’s order, Trudy could take Bob’s money and run.
Or Trudy could carry out an identitytheft by collecting Bob’s name, address, and credit card number.
SSL addresses these issues by enhancing TCP with confidentiality, data integrity, server authentication, and client authentication.
SSL is often used to provide security to transactions that take place over HTTP.
However, because SSL secures TCP, it can be employed by any application that runs over TCP.
SSL provides a simpleApplication Programmer Interface (API) with sockets, which is similar and analogous to TCP’s API.When an application wants to employ SSL, the application includes SSL classes/libraries.
As shown in Figure 8.24, although SSL technically resides in the application layer, from the developer’s perspective it is a transport protocol that provides TCP’s services enhanced with security services.
8.6.1 The Big Picture We begin by describing a simplified version of SSL, one that will allow us to get a big-picture understanding of the why and how of SSL.
We will refer to this simplified Figure 8.24 Although SSL technically resides in the application layer, from the developer’s perspective it is a transport-layer ­protocol version of SSL as “almost-SSL.”
After describing almost-SSL, in the next subsection we’ll then describe the real SSL, filling in the details.
Almost-SSL (and SSL) has three phases: handshake , key derivation , and data transfer.
We now describe these three phases for a communication session between a client (Bob) and a server (Alice), with Alice having a private/public key pair and a certificate that binds her identity to her public key.
Handshake During the handshake phase, Bob needs to (a) establish a TCP connection with Alice, (b) verify that Alice is really  Alice, and (c) send Alice a master secret key, which will be used by both Alice and Bob to generate all the symmetric keys they need for the SSL session.
These three steps are shown in Figure 8.25.
Note that once the TCP connection is established, Bob sends Alice a hello message.
Alice then responds with her certificate, which contains her public key.
As discussed in Section 8.3, because the certificate has been certified by a CA, Bob knows for sure that the public key in the certificate belongs toAlice.
Bob then generates a Master Secret (MS) (which will only be used for this SSL session), encryptsthe MS with Alice’s public key to create the Encrypted Master Secret (EMS), and sends the EMS toAlice.
Alice decrypts the EMS with her private key to get the MS.
After this phase, both Bob and Alice(and no one else) know the master secret for this SSL session.
Figure 8.25 The almost-SSL handshake, beginning with a TCP ­connection Key Derivation In principle, the MS, now shared by Bob and Alice, could be used as the symmetric session key for all subsequent encryption and data integrity checking.
It is, however, generally considered safer for Aliceand Bob to each use different cryptographic keys, and also to use different keys for encryption andintegrity checking.
Thus, both Alice and Bob use the MS to generate four keys:  session encryption key for data sent from Bob to Alice  session MAC key for data sent from Bob to AliceEB=MB= EA=
session encryption key for data sent from Alice to Bob  session MAC key for data sent from Alice to Bob Alice and Bob each generate the four keys from the MS.
This could be done by simply slicing the MS into four keys. (
But in real SSL it is a little more complicated, as we’ll see.)
At the end of the key derivation phase, both Alice and Bob have all four keys.
The two encryption keys will be used to encrypt data; the two MAC keys will be used to verify the integrity of the data.
Data TransferNow that Alice and Bob share the same four session keys (E , M, E, and M ), they can start to send secured data to each other over the TCP connection.
Since TCP is a byte-stream protocol, a natural approach would be for SSL to encrypt application data on the fly and then pass the encrypted data on the fly to TCP.
But if we were to do this, where would we put the MAC for the integrity check?
Wecertainly do not want to wait until the end of the TCP session to verify the integrity of all of Bob’s datathat was sent over the entire session!
To address this issue, SSL breaks the data stream into records,appends a MAC to each record for integrity checking, and then encrypts the record .
To create the MAC, Bob inputs the record data along with the key M  into a hash function, as discussed in Section 8.3.
To encrypt the package record , Bob uses his session encryption key E .
This encrypted package is then passed to TCP for transport over the Internet.
Although this approach goes a long way, it still isn’t bullet-proof when it comes to providing data integrity for the entire message stream.
In particular, suppose Trudy is a woman-in-the-middle and has the abilityto insert, delete, and replace segments in the stream of TCP segments sent between Alice and Bob.
Trudy, for example, could capture two segments sent by Bob, reverse the order of the segments, adjust the TCP sequence numbers (which are not encrypted), and then send the two reverse-orderedsegments to Alice.
Assuming that each TCP segment encapsulates exactly one record, let’s now take alook at how Alice would process these segments.
1.
TCP running in Alice would think everything is fine and pass the two records to the SSL sublayer.
2.
SSL in Alice would decrypt the two records.
3.
SSL in Alice would use the MAC in each record to verify the data integrity of the two records.
4.
SSL would then pass the decrypted byte streams of the two records to the application layer; butthe complete byte stream received by Alice would not be in the correct order due to reversal of the records!
You are encouraged to walk through similar scenarios for when Trudy removes segments or whenTrudy replays segments.
MA= B B A A +MAC B +MACB
The solution to this problem, as you probably guessed, is to use sequence numbers.
SSL does this as follows.
Bob maintains a sequence number counter, which begins at zero and is incremented for each SSL record he sends.
Bob doesn’t actually include a sequence number in the record itself, but when hecalculates the MAC, he includes the sequence number in the MAC calculation.
Thus, the MAC is now a hash of the data plus the MAC key M  plus the current sequence number .
Alice tracks Bob’s sequence numbers, allowing her to verify the data integrity of a record by including the appropriate sequence number in the MAC calculation.
This use of SSL sequence numbers prevents Trudy from carrying out awoman-in-the-middle attack, such as reordering or replaying segments. (
Why?)
SSL RecordThe SSL record (as well as the almost-SSL record) is shown in Figure 8.26.
The record consists of a type field, version field, length field, data field, and MAC field.
Note that the first three fields are not encrypted.
The type field indicates whether the record is a handshake message or a message thatcontains application data.
It is also used to close the SSL connection, as discussed below.
SSL at thereceiving end uses the length field to extract the SSL records out of the incoming TCP byte stream.
Theversion field is self-explanatory.
8.6.2 A More Complete Picture The previous subsection covered the almost-SSL protocol; it served to give us a basic understanding ofthe why and how of SSL.
Now that we have a basic understanding of SSL, we can dig a little deeperand examine the essentials of the actual SSL protocol.
In parallel to reading this description of the SSLprotocol, you are encouraged to complete the Wireshark SSL lab, available at the textbook’s Web site.
Figure 8.26 Record format for SSL SSL Handshake SSL does not mandate that Alice and Bob use a specific symmetric key algorithm, a specific public-key algorithm, or a specific MAC.
Instead, SSL allows Alice and Bob to agree on the cryptographicalgorithms at the beginning of the SSL session, during the handshake phase.
Additionally, during thehandshake phase, Alice and Bob send nonces to each other, which are used in the creation of theB
session keys (E , M, E, and M ).
The steps of the real SSL handshake are as follows: 1.
The client sends a list of cryptographic algorithms it supports, along with a ­client nonce.
2.
From the list, the server chooses a symmetric algorithm (for example, AES), a public key algorithm (for example, RSA with a specific key length), and a MAC algorithm.
It sends back to the client its choices, as well as a certificate and a server nonce.
3.
The client verifies the certificate, extracts the server’s public key, generates a Pre-Master Secret (PMS), encrypts the PMS with the server’s public key, and sends the encrypted PMS to the server.
4.
Using the same key derivation function (as specified by the SSL standard), the client and server independently compute the Master Secret (MS) from the PMS and nonces.
The MS is then sliced up to generate the two encryption and two MAC keys.
Furthermore, when the chosensymmetric cipher employs CBC (such as 3DES or AES), then two Initialization Vectors (IVs)— one for each side of the connection—are also obtained from the MS.
Henceforth, all ­messages sent between client and server are encrypted and authenticated (with the MAC).
5.
The client sends a MAC of all the handshake messages.
6.
The server sends a MAC of all the handshake messages.
The last two steps protect the handshake from tampering.
To see this, observe that in step 1, the client typically offers a list of algorithms—some strong, some weak.
This list of algorithms is sent in cleartext, since the encryption algorithms and keys have not yet been agreed upon.
Trudy, as a woman-in-the- middle, could delete the stronger algorithms from the list, forcing the client to select a weak algorithm.
To prevent such a tampering attack, in step 5 the client sends a MAC of the concatenation of all thehandshake messages it sent and received.
The server can compare this MAC with the MAC of thehandshake messages it received and sent.
If there is an inconsistency, the server can terminate theconnection.
Similarly, the server sends a MAC of the handshake messages it has seen, allowing theclient to check for inconsistencies.
You may be wondering why there are nonces in steps 1 and 2.
Don’t sequence numbers suffice for preventing the segment replay attack?
The answer is yes, but they don’t alone prevent the “connection replay attack.”
Consider the following connection replay attack.
Suppose Trudy sniffs all messages between Alice and Bob.
The next day, Trudy masquerades as Bob and sends to Alice exactly the samesequence of messages that Bob sent to Alice on the previous day.
If Alice doesn’t use nonces, she willrespond with exactly the same sequence of messages she sent the previous day.
Alice will not suspectany funny business, as each message she receives will pass the integrity check.
If Alice is an e-commerce server, she will think that Bob is placing a second order (for exactly the same thing).
On theother hand, by including a nonce in the protocol, Alice will send different nonces for each TCP session, causing the encryption keys to be different on the two days.
Therefore, when Alice receives played-back SSL records from Trudy, the records will fail the integrity checks, and the bogus e-commerce transactionwill not succeed.
In summary, in SSL, nonces are used to defend against the “connection replay attack”B B A A
and sequence numbers are used to defend against replaying individual packets during an ongoing session.
Connection Closure At some point, either Bob or Alice will want to end the SSL session.
One approach would be to let Bob end the SSL session by simply terminating the underlying TCP connection—that is, by having Bob send a TCP FIN segment to Alice.
But such a naive design sets the stage for the truncation attack whereby Trudy once again gets in the middle of an ongoing SSL session and ends the session early with a TCP FIN.
If Trudy were to do this, Alice would think she received all of Bob’s data when ­actuality she only received a portion of it.
The solution to this problem is to indicate in the type field whether the recordserves to terminate the SSL session. (
Although the SSL type is sent in the clear, it is authenticated atthe receiver using the record’s MAC.)
By including such a field, if Alice were to receive a TCP FINbefore ­receiving a closure SSL record, she would know that something funny was going on.
This completes our introduction to SSL.
We’ve seen that it uses many of the cryptography principles discussed in Sections 8.2 and 8.3.
Readers who want to explore SSL on yet a deeper level can read Rescorla’s highly readable book on SSL [Rescorla 2001] .
8.7 Network-Layer Security: IPsec and Virtual Private Networks The IP security protocol, more commonly known as IPsec, provides security at the network layer.
IPsec secures IP datagrams between any two network-layer entities, including hosts and routers.
As we will soon describe, many institutions (corporations, government branches, non-profit organizations, and soon) use IPsec to create virtual private networks (VPNs) that run over the public Internet.
Before getting into the specifics of IPsec, let’s step back and consider what it means to provide confidentiality at the network layer.
With network-layer confidentiality between a pair of network entities(for example, between two routers, between two hosts, or between a router and a host), the sendingentity encrypts the payloads of all the datagrams it sends to the receiving entity.
The encrypted payloadcould be a TCP segment, a UDP segment, an ICMP message, and so on.
If such a network-layerservice were in place, all data sent from one entity to the other—including e-mail, Web pages, TCPhandshake messages, and management messages (such as ICMP and SNMP)—would be hidden fromany third party that might be sniffing the network.
For this reason, network-layer security is said to provide “blanket coverage.”
In addition to confidentiality, a network-layer security protocol could potentially provide other security services.
For example, it could provide source authentication, so that the receiving entity can verify thesource of the secured datagram.
A network-layer security protocol could provide data integrity, so thatthe receiving entity can check for any tampering of the datagram that may have occurred while thedatagram was in transit.
A network-layer security service could also provide replay-attack prevention,meaning that Bob could detect any duplicate datagrams that an attacker might insert.
We will soon seethat IPsec indeed provides mechanisms for all these security services, that is, for confidentiality, source authentication, data ­integrity, and replay-attack prevention.
8.7.1 IPsec and Virtual Private Networks (VPNs) An institution that extends over multiple geographical regions often desires its own IP network, so that its hosts and servers can send data to each other in a secure and confidential manner.
To achieve thisgoal, the institution could actually deploy a stand-alone physical network—including routers, links, and aDNS ­infrastructure—that is completely separate from the public Internet.
Such a disjoint network,dedicated to a particular institution, is called a private network.
Not surprisingly, a private network can be very costly, as the institution needs to purchase, install, and maintain its own physical networkinfrastructure.
Instead of deploying and maintaining a private network, many institutions today create VPNs over the existing public Internet.
With a VPN, the institution’s inter-office traffic is sent over the public Internetrather than over a physically independent network.
But to provide confidentiality, the inter-office traffic is encrypted before it enters the public Internet.
A simple example of a VPN is shown in Figure 8.27.
Here the institution consists of a headquarters, a branch office, and traveling salespersons that typically access the Internet from their hotel rooms. (
There is only one salesperson shown in the figure.)
In this VPN, whenever two hosts within headquarters send IP datagrams to each other or whenever two hostswithin the branch office want to communicate, they use good-old vanilla IPv4 (that is, without IPsecservices).
However, when two of the institution’s hosts Figure 8.27 Virtual private network (VPN) communicate over a path that traverses the public Internet, the traffic is encrypted before it enters the Internet.
To get a feel for how a VPN works, let’s walk through a simple example in the context of Figure 8.27.
When a host in headquarters sends an IP datagram to a salesperson in a hotel, the gateway router in headquarters converts the vanilla IPv4 datagram into an IPsec datagram and then forwards this IPsecdatagram into the Internet.
This IPsec datagram actually has a traditional IPv4 header, so that therouters in the public Internet process the datagram as if it were an ordinary IPv4 datagram—to them, the datagram is a perfectly ordinary datagram.
But, as shown Figure 8.27, the payload of the IPsec datagram includes an IPsec header, which is used for IPsec processing; furthermore, the payload of the
IPsec datagram is encrypted.
When the IPsec datagram arrives at the salesperson’s laptop, the OS in the laptop decrypts the payload (and provides other security services, such as verifying data integrity) and passes the unencrypted payload to the upper-layer protocol (for example, to TCP or UDP).
We have just given a high-level overview of how an institution can employ IPsec to create a VPN.
To see the forest through the trees, we have brushed aside many important details.
Let’s now take a closer look.
8.7.2 The AH and ESP Protocols IPsec is a rather complex animal—it is defined in more than a dozen RFCs.
Two important RFCs are RFC 4301, which describes the overall IP security architecture, and RFC 6071, which provides anoverview of the IPsec protocol suite.
Our goal in this textbook, as usual, is not simply to re-hash the dryand arcane RFCs, but instead take a more operational and pedagogic approach to describing theprotocols.
In the IPsec protocol suite, there are two principal protocols: the Authentication Header (AH) protocol and the Encapsulation Security Payload (ESP)  protocol.
When a source IPsec entity (typically a host or a router) sends secure datagrams to a destination entity (also a host or a router), it does so with either the AH protocol or the ESP protocol.
The AH protocol provides source authentication and data integrity but does not provide confidentiality.
The ESP protocol provides source authentication, data integrity, and confidentiality.
Because confidentiality is often critical for VPNs and other IPsec applications, the ESP protocol is much more widely used than the AH protocol.
In order to de-mystify IPsec and avoid much of its complication, we will henceforth focus exclusively on the ESP protocol.
Readers wanting to learn also about the AH protocol are encouraged to explore the RFCs and otheronline resources.
8.7.3 Security Associations IPsec datagrams are sent between pairs of network entities, such as between two hosts, between tworouters, or between a host and router.
Before sending IPsec datagrams from source entity to destinationentity, the source and destination entities create a network-layer logical connection.
This logicalconnection is called a security association (SA).
An SA is a simplex logical connection; that is, it is unidirectional from source to destination.
If both entities want to send secure datagrams to each other, then two SAs (that is, two logical connections) need to be established, one in each direction.
For example, consider once again the institutional VPN in Figure 8.27.
This institution consists of a
headquarters office, a branch office and, say, n traveling salespersons.
For the sake of example, let’s suppose that there is bi-directional IPsec traffic between headquarters and the branch office and bi- directional IPsec traffic between headquarters and the salespersons.
In this VPN, how many SAs arethere?
To answer this question, note that there are two SAs between the headquarters gateway routerand the branch-office gateway router (one in each direction); for each salesperson’s laptop, there aretwo SAs between the headquarters gateway router and the laptop (again, one in each direction).
So, in total, there are  SAs.
Keep in mind, however, that not all traffic sent into the Internet by the gateway routers or by the laptops will be IPsec secured.
 For example, a host in headquarters may want to access a Web server (such as Amazon or Google) in the public Internet.
Thus, the gateway router (and the laptops) will emit into the Internet both vanilla IPv4 ­datagrams and secured IPsec datagrams.
Figure 8.28 Security association (SA) from R1 to R2 Let’s now take a look “inside” an SA.
To make the discussion tangible and ­concrete, let’s do this in the context of an SA from router R1 to router R2 in Figure 8.28. (
You can think of Router R1 as the headquarters gateway router and Router R2 as the branch office gateway router from Figure 8.27.)
Router R1 will maintain state information about this SA, which will include: A 32-bit identifier for the SA, called the Security Parameter Index (SPI) The origin interface of the SA (in this case 200.168.1.100) and the destination interface of the SA (in this case 193.68.2.23) The type of encryption to be used (for example, 3DES with CBC) The encryption key The type of integrity check (for example, HMAC with MD5) The authentication key Whenever router R1 needs to construct an IPsec datagram for forwarding over this SA, it accesses this state information to determine how it should authenticate and encrypt the datagram.
Similarly, router R2 will maintain the same state information for this SA and will use this information to authenticate and decrypt any IPsec datagram that arrives from the SA.
An IPsec entity (router or host) often maintains state information for many SAs.
For example, in the VPN(2+2n)
example in Figure 8.27 with n salespersons, the headquarters gateway router maintains state information for  SAs.
An IPsec entity stores the state information for all of its SAs in its Security Association Database  (SAD), which is a data structure in the entity’s OS kernel.
8.7.4 The IPsec Datagram Having now described SAs, we can now describe the actual IPsec datagram.
IPsec has two different packet forms, one for the so-called tunnel mode  and the other for the so-called transport mode.
The tunnel mode, being more appropriate for VPNs, Figure 8.29 IPsec datagram format is more widely deployed than the transport mode.
In order to further de-mystify IPsec and avoid much of its complication, we henceforth focus exclusively on the tunnel mode.
Once you have a solid grip on the tunnel mode, you should be able to easily learn about the transport mode on your own.
The packet format of the IPsec datagram is shown in Figure 8.29.
You might think that packet formats are boring and insipid, but we will soon see that the IPsec datagram actually looks and tastes like a popular Tex-Mex delicacy!
Let’s examine the IPsec fields in the context of Figure 8.28.
Suppose router R1 receives an ordinary IPv4 datagram from host 172.16.1.17 (in the headquarters network) which is destined to host 172.16.2.48 (in the branch-office network).
Router R1 uses the ­following recipe to convert this “original IPv4 datagram” into an IPsec datagram: Appends to the back of the original IPv4 datagram (which includes the original header fields!)
an “ESP trailer” field Encrypts the result using the algorithm and key specified by the SA Appends to the front of this encrypted quantity a field called “ESP header”; the resulting package iscalled the “enchilada” Creates an authentication MAC over the whole enchilada  using the algorithm and key specified in(2+2n)
the SA Appends the MAC to the back of the enchilada forming the payload Finally, creates a brand new IP header with all the classic IPv4 header fields (together normally 20 bytes long), which it appends before the payload Note that the resulting IPsec datagram is a bona fide IPv4 datagram, with the traditional IPv4 header fields followed by a payload.
But in this case, the payload contains an ESP header, the original IP datagram, an ESP trailer, and an ESP authentication field (with the original datagram and ESP trailerencrypted).
The original IP datagram has 172.16.1.17 for the source IP address and 172.16.2.48 for thedestination IP address.
Because the IPsec datagram includes the original IP datagram, these addressesare included (and encrypted) as part of the payload of the IPsec packet.
But what about the source anddestination IP addresses that are in the new IP header, that is, in the left-most header of the IPsecdatagram?
As you might expect, they are set to the source and destination router interfaces at the twoends of the tunnels, namely, 200.168.1.100 and 193.68.2.23.
Also, the protocol number in this new IPv4header field is not set to that of TCP, UDP, or SMTP, but instead to 50, designating that this is an IPsec datagram using the ESP protocol.
After R1 sends the IPsec datagram into the public Internet, it will pass through many routers before reaching R2.
Each of these routers will process the datagram as if it were an ordinary datagram—theyare completely oblivious to the fact that the datagram is carrying IPsec-encrypted data.
For these publicInternet routers, because the destination IP address in the outer header is R2, the ultimate destination ofthe datagram is R2.
Having walked through an example of how an IPsec datagram is constructed, let’s now take a closer look at the ingredients in the enchilada.
We see in Figure 8.29 that the ESP trailer consists of three fields: padding; pad length; and next header.
Recall that block ciphers require the message to be encrypted to be an integer multiple of the block length.
Padding (consisting of meaningless bytes) isused so that when added to the original datagram (along with the pad length and next header fields), theresulting “message” is an integer number of blocks.
The pad-length field indicates to the receiving entityhow much padding was inserted (and thus needs to be removed).
The next header identifies the type(e.g.,
UDP) of data contained in the payload-data field.
The payload data (typically the original IPdatagram) and the ESP trailer are concatenated and then encrypted.
Appended to the front of this encrypted unit is the ESP header, which is sent in the clear and consists of two fields: the SPI and the sequence number field.
The SPI indicates to the receiving entity the SA towhich the datagram belongs; the receiving entity can then index its SAD with the SPI to determine theappropriate authentication/decryption algorithms and keys.
The sequence number field is used todefend against replay attacks.
The sending entity also appends an authentication MAC.
As stated earlier, the sending entity calculates
a MAC over the whole enchilada (consisting of the ESP header, the original IP datagram, and the ESP trailer—with the datagram and trailer being encrypted).
Recall that to calculate a MAC, the sender appends a secret MAC key to the enchilada and then calculates a fixed-length hash of the result.
When R2 receives the IPsec datagram, R2 observes that the destination IP address of the datagram is R2 itself.
R2 therefore processes the datagram.
Because the protocol field (in the left-most IP header) is 50, R2 sees that it should apply IPsec ESP processing to the datagram.
First, peering into the enchilada, R2 uses the SPI to determine to which SA the datagram belongs.
Second, it calculates theMAC of the enchilada and verifies that the MAC is consistent with the value in the ESP MAC field.
If it is,it knows that the enchilada comes from R1 and has not been tampered with.
Third, it checks thesequence-number field to verify that the datagram is fresh (and not a replayed datagram).
Fourth, it decrypts the encrypted unit using the decryption algorithm and key associated with the SA.
Fifth, itremoves padding and extracts the original, vanilla IP datagram.
And finally, sixth, it forwards the originaldatagram into the branch office network toward its ultimate destination.
Whew, what a complicated recipe, huh?
Well no one ever said that preparing and unraveling an enchilada was easy!
There is actually another important subtlety that needs to be addressed.
It centers on the following question: When R1 receives an (unsecured) datagram from a host in the headquarters network, and thatdatagram is destined to some destination IP address outside of headquarters, how does R1 knowwhether it should be converted to an IPsec datagram?
And if it is to be processed by IPsec, how doesR1 know which SA (of many SAs in its SAD) should be used to construct the IPsec datagram?
Theproblem is solved as follows.
Along with a SAD, the IPsec entity also maintains another data structurecalled the Security Policy Database (SPD).
The SPD indicates what types of datagrams (as a function of source IP address, destination IP address, and protocol type) are to be IPsec processed; and forthose that are to be IPsec processed, which SA should be used.
In a sense, the information in a SPDindicates “what” to do with an arriving datagram; the information in the SAD indicates “how” to do it.
Summary of IPsec Services So what services does IPsec provide, exactly?
Let us examine these services from the perspective of an attacker, say Trudy, who is a woman-in-the-middle, sitting somewhere on the path between R1 and R2 in Figure 8.28.
Assume throughout this ­discussion that Trudy does not know the authentication and encryption keys used by the SA.
What can and cannot Trudy do?
First, Trudy cannot see the original datagram.
If fact, not only is the data in the original datagram hidden from Trudy, but so is the protocolnumber, the source IP address, and the destination IP address.
For datagrams sent over the SA, Trudyonly knows that the datagram originated from some host in 172.16.1.0/24 and is destined to some hostin 172.16.2.0/24.
She does not know if it is carrying TCP, UDP, or ICMP data; she does not know if it iscarrying HTTP, SMTP, or some other type of application data.
This confidentiality thus goes a lot fartherthan SSL.
Second, suppose Trudy tries to tamper with a datagram in the SA by flipping some of its bits.
When this tampered datagram arrives at R2, it will fail the integrity check (using the MAC), thwarting
Trudy’s vicious attempts once again.
Third, suppose Trudy tries to masquerade as R1, creating a IPsec datagram with source 200.168.1.100 and destination 193.68.2.23.
Trudy’s attack will be futile, as this datagram will again fail the integrity check at R2.
Finally, because IPsec includes sequence numbers,Trudy will not be able create a successful replay attack.
In summary, as claimed at the beginning of thissection, IPsec provides—between any pair of devices that process packets through the network layer—confidentiality, source authentication, data integrity, and replay-attack prevention.
8.7.5 IKE: Key Management in IPsec When a VPN has a small number of end points (for example, just two routers as in Figure 8.28), the network administrator can manually enter the SA information (encryption/authentication algorithms andkeys, and the SPIs) into the SADs of the endpoints.
Such “manual keying” is clearly impractical for alarge VPN, which may consist of hundreds or even thousands of IPsec routers and hosts.
Large, geographically distributed deployments require an automated mechanism for creating the SAs.
IPsec does this with the Internet Key Exchange (IKE) protocol, specified in RFC 5996.
IKE has some similarities with the handshake in SSL (see Section 8.6).
Each IPsec entity has a certificate, which includes the entity’s public key.
As with SSL, the IKE protocol has the two entities exchange certificates, negotiate authentication and encryption algorithms, and securely exchange keymaterial for creating session keys in the IPsec SAs.
Unlike SSL, IKE employs two phases to carry outthese tasks.
Let’s investigate these two phases in the context of two routers, R1 and R2, in Figure 8.28.
The first phase consists of two exchanges of message pairs between R1 and R2: During the first exchange of messages, the two sides use Diffie-Hellman (see Homework Problems) to create a bi-directional IKE SA between the routers.
To keep us all confused, this bi-directional IKE SA is entirely different from the IPsec SAs discussed in Sections 8.6.3 and 8.6.4.
The IKE SA provides an authenticated and encrypted channel between the two routers.
During this first message-pair exchange, keys are established for encryption and authentication for the IKE SA.
Also established is a master secret that will be used to compute IPSec SA keys later in phase 2.
Observethat during this first step, RSA public and private keys are not used.
In particular, neither R1 nor R2reveals its identity by signing a message with its private key.
During the second exchange of messages, both sides reveal their identity to each other by signing their messages.
However, the identities are not revealed to a passive sniffer, since the messages are sent over the secured IKE SA channel.
Also during this phase, the two sides negotiate the IPsecencryption and authentication algorithms to be employed by the IPsec SAs.
In phase 2 of IKE, the two sides create an SA in each direction.
At the end of phase 2, the encryption
and authentication session keys are established on both sides for the two SAs.
The two sides can then use the SAs to send secured datagrams, as described in Sections 8.7.3 and 8.7.4.
The primary motivation for having two phases in IKE is computational cost—since the second phase doesn’t involve any public-key cryptography, IKE can generate a large number of SAs between the two IPsec entitieswith relatively little computational cost.
8.8 Securing Wireless LANs Security is a particularly important concern in wireless networks, where radio waves carrying frames can propagate far beyond the building containing the wireless base station and hosts.
In this section wepresent a brief introduction to wireless security.
For a more in-depth treatment, see the highly readable book by Edney and Arbaugh [Edney 2003].
The issue of security in 802.11 has attracted considerable attention in both technical circles and in the media.
While there has been considerable discussion, there has been little debate—there seems to beuniversal agreement that the original 802.11 specification contains a number of serious security flaws.
Indeed, public domain software can now be downloaded that exploits these holes, making those whouse the vanilla 802.11 security mechanisms as open to security attacks as users who use no securityfeatures at all.
In the following section, we discuss the security mechanisms initially standardized in the 802.11 specification, known collectively as Wired Equivalent Privacy (WEP) .
As the name suggests, WEP is meant to provide a level of security similar to that found in wired networks.
We’ll then discuss a few of the security holes in WEP and discuss the 802.11i standard, a fundamentally more secure version of802.11 adopted in 2004.
8.8.1 Wired Equivalent Privacy (WEP) The IEEE 802.11 WEP protocol was designed in 1999 to provide authentication and data encryptionbetween a host and a wireless access point (that is, base station) using a symmetric shared keyapproach.
WEP does not specify a key management algorithm, so it is assumed that the host andwireless access point have somehow agreed on the key via an out-of-band method.
Authentication iscarried out as ­follows: 1.
A wireless host requests authentication by an access point.
2.
The access point responds to the authentication request with a 128-byte nonce value.
3.
The wireless host encrypts the nonce using the symmetric key that it shares with the access point.
4.
The access point decrypts the host-encrypted nonce.
If the decrypted nonce matches the nonce value originally sent to the host, then the host is
authenticated by the access point.
The WEP data encryption algorithm is illustrated in Figure 8.30.
A secret 40-bit symmetric key, K , is assumed to be known by both a host and the access point.
In addition, a 24-bit Initialization Vector (IV) is appended to the 40-bit key to create a 64-bit key that will be used to encrypt a single frame.
The IV will Figure 8.30 802.11 WEP protocol change from one frame to another, and hence each frame will be encrypted with a different 64-bit key.
Encryption is performed as follows.
First a 4-byte CRC value (see Section 6.2) is computed for the data payload.
The payload and the four CRC bytes are then encrypted using the RC4 stream cipher.
We willnot cover the details of RC4 here (see [Schneier 1995]  and [Edney 2003] for details).
For our purposes, it is enough to know that when presented with a key value (in this case, the 64-bit ( K, IV) key), the RC4 algorithm produces a stream of key values,  that are used to encrypt the data and CRC value in a frame.
For practical purposes, we can think of these operations beingperformed a byte at a time.
Encryption is performed by XOR-ing the ith byte of data, d, with the ith key, , in the stream of key values generated by the ( K, IV) pair to produce the ith byte of ciphertext, c : The IV value changes from one frame to the next and is included in plaintext  in the header of each WEP-encrypted 802.11 frame, as shown in Figure 8.30.
The receiver takes the secret 40-bit symmetric key that it shares with the sender, appends the IV, and uses the resulting 64-bit key (which is identical to the key used by the sender to perform encryption) to decrypt the frame: Proper use of the RC4 algorithm requires that the same 64-bit key value never be used more than once.
Recall that the WEP key changes on a frame-by-frame basis.
For a given K (which changes rarely, if ever), this means that there are only 2  unique keys.
If these keys are chosen randomly, we can showS S k1IV,k2IV,k3IV,… i kiIVS i ci=di⊕kiIV di=ci⊕kiIV S 24
[Edney 2003] that the probability of having chosen the same IV value (and hence used the same 64-bit key) is more than 99 percent after only 12,000 frames.
With 1 Kbyte frame sizes and a data transmission rate of 11 Mbps, only a few seconds are needed before 12,000 frames are transmitted.
Furthermore, since the IV is transmitted in plaintext in the frame, an eavesdropper will know whenever aduplicate IV value is used.
To see one of the several problems that occur when a duplicate key is used, consider the following chosen-plaintext attack taken by Trudy against Alice.
Suppose that Trudy (possibly using IP spoofing)sends a request (for example, an HTTP or FTP request) to Alice to transmit a file with known content,  Trudy also observes the encrypted data  Since , if we XOR c with each side of this equality we have With this relationship, Trudy can use the known values of d and c  to compute .
The next time Trudy sees the same value of IV being used, she will know the key sequence  and will thus be able to decrypt the encrypted message.
There are several additional security concerns with WEP as well. [
Fluhrer 2001] described an attack exploiting a known weakness in RC4 when certain weak keys are chosen. [
Stubblefield 2002] discusses efficient ways to implement and exploit this attack.
Another concern with WEP involves the CRC bits shown in Figure 8.30 and transmitted in the 802.11 frame to detect altered bits in the payload.
However, an attacker who changes the encrypted content (e.g., substituting gibberish for the original encrypted data), computes a CRC over the substituted gibberish, and places the CRC into a WEP framecan produce an 802.11 frame that will be accepted by the receiver.
What is needed here are message integrity techniques such as those we studied in Section 8.3 to detect content tampering or substitution.
For more details of WEP security, see [Edney 2003; Wright 2015] and the ­references therein.
8.8.2 IEEE 802.11i Soon after the 1999 release of IEEE 802.11, work began on developing a new and improved version of 802.11 with stronger security mechanisms.
The new standard, known as 802.11i, underwent finalratification in 2004.
As we’ll see, while WEP provided relatively weak encryption, only a single way toperform authentication, and no key distribution mechanisms, IEEE 802.11i provides for much strongerforms of encryption, an extensible set of authentication mechanisms, and a key distribution mechanism.
In the following, we present an overview of 802.11i; an excellent (streaming audio) technical overview of 802.11i is [TechOnline 2012].
Figure 8.31 overviews the 802.11i framework.
In addition to the wireless client and access point,d1, d2, d3, d4,….
c1, c2, c3, c4,….
di=ci⊕kiIV i di⊕ci=kiIV i i kiIV k1IV,k2IV,k3IV,…
802.11i defines an authentication server with which the AP can communicate.
Separating the authentication server from the AP allows one authentication server to serve many APs, centralizing the (often sensitive) decisions Figure 8.31 802.11i: Four phases of operation regarding authentication and access within the single server, and keeping AP costs and complexity low.
802.11i operates in four phases: 1.
Discovery.
 In the discovery phase, the AP advertises its presence and the forms of authentication and encryption that can be provided to the wireless client node.
The client then requests the specific forms of authentication and encryption that it desires.
Although the clientand AP are already exchanging messages, the client has not yet been authenticated nor does it have an encryption key, and so several more steps will be required before the client can communicate with an arbitrary remote host over the wireless channel.
2.
Mutual authentication and Master Key (MK) generation.
 Authentication takes place between the wireless client and the authentication server.
In this phase, the access point acts essentiallyas a relay, forwarding messages between the client and the authentication server.
TheExtensible Authentication Protocol (EAP)  [RFC 3748]  defines the end-to-end message formats used in a simple request/response mode of interaction between the client and authentication server.
As shown in Figure 8.32, EAP messages are encapsulated using EAPoL (EAP over LAN, [IEEE 802.1X]) and sent over the 802.11 wireless link.
These EAP messages
are then decapsulated at the access point, and then re-encapsulated using the RADIUS protocol for transmission over UDP/IP to the authentication server.
While Figure 8.32 EAP is an end-to-end protocol.
EAP messages are encapsulated using EAPoL over the wireless link between the ­client and the access point, and using RADIUS over UDP/IP between the access point and the authentication server the RADIUS server and protocol [RFC 2865]  are not required by the 802.11i protocol, they are de facto standard components for 802.11i.
The recently standardized DIAMETER  protocol [RFC 3588]  is likely to replace RADIUS in the near future.
With EAP, the authentication server can choose one of a number of ways to perform authentication.
While 802.11i does not mandate a particular authentication method, the EAP- TLS authentication scheme [RFC 5216]  is often used.
EAP-TLS uses public key techniques (including nonce encryption and message digests) similar to those we studied in Section 8.3 to allow the client and the authentication server to mutually authenticate each other, and to derive a Master Key (MK) that is known to both parties.
3.
Pairwise Master Key (PMK) generation.
The MK is a shared secret known only to the client and the authentication server, which they each use to generate a second key, the Pairwise Master Key (PMK).
The authentication server then sends the PMK to the AP.
This is where wewanted to be!
The client and AP now have a shared key (recall that in WEP, the problem of key distribution was not addressed at all) and have mutually authenticated each other.
They’re just about ready to get down to business.
4.
Temporal Key (TK) generation.
With the PMK, the wireless client and AP can now generate additional keys that will be used for communication.
Of ­particular interest is the Temporal Key (TK), which will be used to perform the link-level encryption of data sent over the wireless link and to an arbitrary remote host.
802.11i provides several forms of encryption, including an AES-based encryption scheme and a
strengthened version of WEP encryption.
8.9 Operational Security: Firewalls and Intrusion Detection Systems We’ve seen throughout this chapter that the Internet is not a very safe place—bad guys are out there, wreaking all sorts of havoc.
Given the hostile nature of the Internet, let’s now consider an organization’snetwork and the network administrator who administers it.
From a network administrator’s point of view,the world divides quite neatly into two camps—the good guys (who belong to the organization’s network,and who should be able to access resources inside the organization’s network in a relatively unconstrained manner) and the bad guys (everyone else, whose access to network resources must be carefully scrutinized).
In many organizations, ranging from medieval castles to modern corporate officebuildings, there is a single point of entry/exit where both good guys and bad guys entering and leavingthe organization are security-checked.
In a castle, this was done at a gate at one end of the drawbridge;in a corporate building, this is done at the security desk.
In a computer network, when trafficentering/leaving a network is security-checked, logged, dropped, or forwarded, it is done by operationaldevices known as firewalls, intrusion detection systems (IDSs), and intrusion prevention systems (IPSs).
8.9.1 Firewalls A firewall is a combination of hardware and software that isolates an organization’s internal networkfrom the Internet at large, allowing some packets to pass and blocking others.
A firewall allows anetwork administrator to control access between the outside world and resources within theadministered network by managing the traffic flow to and from these resources.
A firewall has threegoals: All traffic from outside to inside, and vice versa, passes through the firewall.
 Figure 8.33 shows a firewall, sitting squarely at the boundary between the administered network and the rest ofthe Internet.
While large organizations may use multiple levels of firewalls or distributed firewalls [Skoudis 2006], locating a firewall at a single access point to the network, as shown in Figure 8.33, makes it easier to manage and enforce a security-access policy.
Only authorized traffic, as defined by the local security policy, will be allowed to pass.
 With all traffic entering and leaving the institutional network passing through the firewall, the firewall can restrict access to authorized traffic.
The firewall itself is immune to penetration.
 The firewall itself is a device connected to the network.
If not designed or installed properly, it can be compromised, in which case it provides only
a false sense of security (which is worse than no firewall at all!).
Figure 8.33 Firewall placement between the administered network and the outside world Cisco and Check Point are two of the leading firewall vendors today.
You can also easily create a firewall (packet filter) from a Linux box using iptables (public-domain software that is normally shipped with Linux).
Furthermore, as discussed in Chapters 4 and 5, firewalls are now frequently implemented in routers and controlled remotely using SDNs.
Firewalls can be classified in three categories: traditional packet filters, stateful filters, and application gateways.
We’ll cover each of these in turn in the following subsections.
Traditional Packet FiltersAs shown in Figure 8.33, an organization typically has a gateway router connecting its internal network to its ISP (and hence to the larger public Internet).
All traffic leaving and entering the internal network passes through this router, and it is at this router where packet filtering occurs.
A packet filter examines each datagram in isolation, determining whether the datagram should be allowed to pass orshould be dropped based on administrator-specific rules.
Filtering decisions are typically based on: IP source or destination address Protocol type in IP datagram field: TCP, UDP, ICMP, OSPF, and so on TCP or UDP source and destination port
Table 8.5 Policies and corresponding filtering rules for an organization’s network 130.207/16 with Web server at 130.207.244.203 Policy Firewall Setting No outside Web access.
Drop all outgoing packets to any IP address, port 80.
No incoming TCP connections, except those fororganization’s public Web server only.
Drop all incoming TCP SYN packets to anyIP except 130.207.244.203, port 80.
Prevent Web-radios from eating up theavailable bandwidth.
Drop all incoming UDP packets—except DNSpackets.
Prevent your network from being used for asmurf DoS attack.
Drop all ICMP ping packets going to a“broadcast” address (eg 130.207.255.255).
Prevent your network from being tracerouted.
Drop all outgoing ICMP TTL expired traffic.
TCP flag bits: SYN, ACK, and so on ICMP message type Different rules for datagrams leaving and entering the network Different rules for the different router interfaces A network administrator configures the firewall based on the policy of the organization.
The policy maytake user productivity and bandwidth usage into account as well as the security concerns of an organization.
Table 8.5 lists a number of possible polices an organization may have, and how they would be addressed with a packet filter.
For example, if the organization doesn’t want any incoming TCP connections except those for its public Web server, it can block all incoming TCP SYN segments exceptTCP SYN segments with destination port 80 and the destination IP address corresponding to the Webserver.
If the organization doesn’t want its users to monopolize access bandwidth with Internet radioapplications, it can block all not-critical UDP traffic (since Internet radio is often sent over UDP).
If theorganization doesn’t want its internal network to be mapped (tracerouted) by an outsider, it can block allICMP TTL expired messages leaving the organization’s network.
A filtering policy can be based on a combination of addresses and port numbers.
For example, a filtering router could forward all Telnet datagrams (those with a port number of 23) except those going to andcoming from a list of specific IP addresses.
This policy permits Telnet connections to and from hosts onthe allowed list.
Unfortunately, basing the policy on external addresses provides no protection against
datagrams that have had their source addresses spoofed.
Filtering can also be based on whether or not the TCP ACK bit is set.
This trick is quite useful if an organization wants to let its internal clients connect to external servers but wants to prevent externalclients from connecting to internal servers.
Table 8.6 An access control list for a router interface action source address dest address protocol source portdestportflagbit allow 222.22/16 outside of222.22/16TCP > 1023 80 any allow outside of222.22/16222.22/16 TCP 80 > 1023 ACK allow 222.22/16 outside of222.22/16UDP > 1023 53 — allow outside of222.22/16222.22/16 UDP 53 > 1023 — deny all all all all all all Recall from Section 3.5 that the first segment in every TCP connection has the ACK bit set to 0,whereas all the other segments in the connection have the ACK bit set to 1.
Thus, if an organizationwants to prevent external clients from initiating connections to internal servers, it simply filters allincoming segments with the ACK bit set to 0.
This policy kills all TCP connections originating from the outside, but permits connections originating internally.
Firewall rules are implemented in routers with access control lists, with each router interface having its own list.
An example of an access control list for an organization 222.22/16 is shown in Table 8.6.
This access control list is for an interface that connects the router to the organization’s external ISPs.
Rules are applied to each datagram that passes through the interface from top to bottom.
The first two rulestogether allow internal users to surf the Web: The first rule allows any TCP packet with destination port80 to leave the organization’s network; the second rule allows any TCP packet with source port 80 and the ACK bit set to enter the organization’s network.
Note that if an external source attempts to establish a TCP connection with an internal host, the connection will be blocked, even if the source or destinationport is 80.
The second two rules together allow DNS packets to enter and leave the organization’s
network.
In summary, this rather restrictive access control list blocks all traffic except Web traffic initiated from within the organization and DNS traffic. [
CERT Filtering 2012] provides a list of recommended port/protocol packet filterings to avoid a number of well-known security holes in existing network applications.
Stateful Packet Filters In a traditional packet filter, filtering decisions are made on each packet in isolation.
Stateful filters actually track TCP connections, and use this knowledge to make ­filtering decisions.
Table 8.7 Connection table for stateful filter source address dest address source port dest port 222.22.1.7 37.96.87.123 12699 80 222.22.93.2 199.1.205.23 37654 80 222.22.65.143 203.77.240.43 48712 80 To understand stateful filters, let’s reexamine the access control list in Table 8.6.
Although rather restrictive, the access control list in Table 8.6 nevertheless allows any packet arriving from the outside with ACK = 1 and source port 80 to get through the filter.
Such packets could be used by attackers in attempts to crash internal systems with malformed packets, carry out denial-of-service attacks, or mapthe internal network.
The naive solution is to block TCP ACK packets as well, but such an approachwould prevent the organization’s internal users from surfing the Web.
Stateful filters solve this problem by tracking all ongoing TCP connections in a connection table.
This is possible because the firewall can observe the beginning of a new connection by observing a three-way handshake (SYN, SYNACK, and ACK); and it can observe the end of a connection when it sees a FIN packet for the connection.
The firewall can also (conservatively) assume that the connection is overwhen it hasn’t seen any activity over the connection for, say, 60 seconds.
An example connection table for a firewall is shown in Table 8.7.
This connection table indicates that there are currently three ongoing TCP connections, all of which have been initiated from within the organization.
Additionally, the statefulfilter includes a new column, “check connection,” in its access control list, as shown in Table 8.8.
Note that Table 8.8 is identical to the access control list in Table 8.6, except now it indicates that the connection should be checked for two of the rules.
Let’s walk through some examples to see how the connection table and the extended access control list
work hand-in-hand.
Suppose an attacker attempts to send a malformed packet into the organization’s network by sending a datagram with TCP source port 80 and with the ACK flag set.
Further suppose that this packet has source port number 12543 and source IP address 150.23.23.155.
When this packet reaches the firewall, the firewall checks the access control list in Table 8.7, which indicates that the connection table must also be checked before permitting this packet to enter the organization’s network.
The firewall duly checks the connection table, sees that this packet is not part of an ongoing TCP connection, and rejects the packet.
As a second example, suppose that an internal user wants to surf an external Web site.
Because this user first sends a TCP SYN segment, the user’s TCP connection getsrecorded in the connection table.
When Table 8.8 Access control list for stateful filter action source address dest address protocol source portdestportflagbitcheckconxion allow 222.22/16 outside of222.22/16TCP > 1023 80 any allow outside of222.22/16222.22/16 TCP 80 >1023ACK X allow 222.22/16 outside of222.22/16UDP > 1023 53 — allow outside of222.22/16222.22/16 UDP 53 >1023— X deny all all all all all all the Web server sends back packets (with the ACK bit necessarily set), the firewall checks the table andsees that a corresponding connection is in progress.
The firewall will thus let these packets pass,thereby not interfering with the internal user’s Web surfing activity.
Application Gateway In the examples above, we have seen that packet-level filtering allows an organization to perform coarse-grain filtering on the basis of the contents of IP and TCP/UDP headers, including IP addresses, port numbers, and acknowledgment bits.
But what if an organization wants to provide a Telnet service to a restricted set of internal users (as opposed to IP addresses)?
And what if the organization wants suchprivileged users to authenticate themselves first before being allowed to create Telnet sessions to the
outside world?
Such tasks are beyond the capabilities of traditional and stateful filters.
Indeed, information about the identity of the internal users is application-layer data and is not included in the IP/TCP/UDP headers.
To have finer-level security, firewalls must combine packet filters with application gateways.
Application gateways look beyond the IP/TCP/UDP headers and make policy decisions based on application data.
An application gateway  is an application-specific server through which all application data (inbound and outbound) must pass.
Multiple application gateways can run on the same host, but each gateway is a separate server with its own processes.
To get some insight into application gateways, let’s design a firewall that allows only a restricted set of internal users to Telnet outside and prevents all external clients from Telneting inside.
Such a policy canbe accomplished by implementing Figure 8.34 Firewall consisting of an application gateway and a filter a combination of a packet filter (in a router) and a Telnet application gateway, as shown in Figure 8.34.
The router’s filter is configured to block all Telnet connections except those that originate from the IPaddress of the application gateway.
Such a filter configuration forces all outbound Telnet connections topass through the application gateway.
Consider now an internal user who wants to Telnet to the outsideworld.
The user must first set up a Telnet session with the application gateway.
An application running inthe gateway, which listens for incoming Telnet sessions, prompts the user for a user ID and password.
When the user supplies this information, the application gateway checks to see if the user has
permission to Telnet to the outside world.
If not, the Telnet connection from the internal user to the gateway is terminated by the gateway.
If the user has permission, then the gateway (1) prompts theuser for the host name of the external host to which the user wants to connect, (2) sets up a Telnetsession between the gateway and the external host, and (3) relays to the external host all data arrivingfrom the user, and relays to the user all data arriving from the external host.
Thus, the Telnet application gateway not only performs user authorization but also acts as a Telnet server and a Telnet client, relaying information between the user and the remote Telnet server.
Note that the filter will permit step 2because the gateway initiates the Telnet connection to the outside world.
CASE HISTORY ANONYMITY AND PRIVACY Suppose you want to visit a controversial Web site (for example, a political activist site) and you (1) don’t want to reveal your IP address to the Web site, (2) don’t want your local ISP (whichmay be your home or office ISP) to know that you are visiting the site, and (3) don’t want your local ISP to see the data you are exchanging with the site.
If you use the traditional approach of connecting directly to the Web site without any encryption, you fail on all three counts.
Even ifyou use SSL, you fail on the first two counts: Your source IP address is presented to the Website in every datagram you send; and the destination address of every packet you send caneasily be sniffed by your local ISP.
To obtain privacy and anonymity, you can instead use a combination of a trusted proxy server and SSL, as shown in Figure 8.35.
With this approach, you first make an SSL connection to the trusted proxy.
You then send, into this SSL connection, an HTTP request for a page at the desired site.
When the proxy receives the SSL-encrypted HTTP request, it decrypts the request and forwards the cleartext HTTP request to the Web site.
The Web site then responds to theproxy, which in turn forwards the response to you over SSL.
Because the Web site only sees theIP address of the proxy, and not of your client’s address, you are indeed obtaining anonymousaccess to the Web site.
And because all traffic between you and the proxy is encrypted, yourlocal ISP cannot invade your privacy by logging the site you visited or recording the data you are exchanging.
Many companies today (such as proxify .com) make available such proxy services.
Of course, in this solution, your proxy knows everything: It knows your IP address and the IP address of the site you’re surfing; and it can see all the traffic in ­cleartext exchanged between you and the Web site.
Such a solution, therefore, is only as good as the trustworthiness of the proxy.
A more robust approach, taken by the TOR anonymizing and privacy service, is to route your traffic through a series of non- ­colluding proxy servers [TOR 2016].
In particular, TOR allows independent ­individuals to contribute proxies to its proxy pool.
When a user connects to a server using TOR, TOR randomly chooses (from its proxy pool) a chain of three proxies and routes all traffic between client and server over the chain.
In this manner, assuming the proxiesdo not collude, no one knows that communication took place between your IP address and the
target Web site.
Furthermore, although cleartext is sent between the last proxy and the server, the last proxy doesn’t know what IP address is sending and receiving the cleartext.
Figure 8.35 Providing anonymity and privacy with a proxy Internal networks often have multiple application gateways, for example, gateways for Telnet, HTTP, FTP, and e-mail.
In fact, an organization’s mail server (see Section 2.3) and Web cache are application gateways.
Application gateways do not come without their disadvantages.
First, a different application gateway is needed for each application.
Second, there is a performance penalty to be paid, since all data will berelayed via the gateway.
This becomes a concern particularly when multiple users or applications areusing the same gateway machine.
Finally, the client software must know how to contact the gatewaywhen the user makes a request, and must know how to tell the application gateway what external serverto connect to.
8.9.2 Intrusion Detection Systems We’ve just seen that a packet filter (traditional and stateful) inspects IP, TCP, UDP, and ICMP headerfields when deciding which packets to let pass through the firewall.
However, to detect many attacktypes, we need to perform deep packet inspection, that is, look beyond the header fields and into the actual application data that the packets carry.
As we saw in Section 8.9.1, application gateways often do deep packet inspection.
But an application gateway only does this for a specific application.
Clearly, there is a niche for yet another device—a device that not only examines the headers of all packets passing through it (like a packet filter), but also performs deep packet inspection (unlike apacket filter).
When such a device observes a suspicious packet, or a suspicious series of packets, itcould prevent those packets from entering the organizational network.
Or, because the activity is only
deemed as suspicious, the device could let the packets pass, but send alerts to a network administrator, who can then take a closer look at the traffic and take appropriate actions.
A device that generates alerts when it observes potentially malicious traffic is called an intrusion detection system (IDS) .
A device that filters out suspicious traffic is called an intrusion prevention system (IPS) .
In this section we study both systems—IDS and IPS—together, since the most interesting technical aspect of thesesystems is how they detect suspicious traffic (and not whether they send alerts or drop packets).
We will henceforth collectively refer to IDS systems and IPS systems as IDS systems.
An IDS can be used to detect a wide range of attacks, including network mapping (emanating, for example, from nmap), port scans, TCP stack scans, DoS bandwidth-flooding attacks, worms and viruses, OS vulnerability attacks, and application vulnerability attacks. (
See Section 1.6 for a survey of network attacks.)
Today, thousands of organizations employ IDS systems.
Many of these deployed systems are proprietary, marketed by Cisco, Check Point, and other security equipment vendors.
Butmany of the deployed IDS systems are public-domain systems, such as the immensely popular Snort IDS system (which we’ll discuss shortly).
An organization may deploy one or more IDS sensors in its organizational network.
Figure 8.36 shows an organization that has three IDS sensors.
When multiple sensors are deployed, they typically work in concert, sending information about
Figure 8.36 An organization deploying a filter, an application gateway, and IDS sensors suspicious traffic activity to a central IDS processor, which collects and integrates the information and sends alarms to network administrators when deemed appropriate.
In Figure 8.36, the organization has partitioned its network into two regions: a high-security region, protected by a packet filter and an application gateway and monitored by IDS sensors; and a lower-security region—referred to as thedemilitarized zone (DMZ) —which is protected only by the packet filter, but also monitored by IDS sensors.
Note that the DMZ includes the organization’s servers that need to communicate with theoutside world, such as its public Web server and its authoritative DNS server.
You may be wondering at this stage, why multiple IDS sensors?
Why not just place one IDS sensor just behind the packet filter (or even integrated with the packet filter) in Figure 8.36?
We will soon see that an IDS not only needs to do deep packet inspection, but must also compare each passing packet with tens of thousands of “signatures”; this can be a significant amount of processing, particularly if theorganization receives gigabits/sec of traffic from the Internet.
By placing the IDS sensors further downstream, each sensor sees only a fraction of the organization’s traffic, and can more easily keep up.
Nevertheless, high-performance IDS and IPS systems are available today, and many organizations canactually get by with just one sensor located near its access router.
IDS systems are broadly classified as either signature-based systems  or ­anomaly-based systems .
A signature-based IDS maintains an extensive database of attack signatures.
Each signature is a set of rules pertaining to an intrusion activity.
A signature may simply be a list of characteristics about a singlepacket (e.g., source and destination port numbers, protocol type, and a specific string of bits in thepacket payload), or may relate to a series of packets.
The signatures are normally created by skilled network security engineers who research known attacks.
An organization’s network administrator can customize the signatures or add its own to the database.
Operationally, a signature-based IDS sniffs every packet passing by it, comparing each sniffed packet with the signatures in its database.
If a packet (or series of packets) matches a signature in thedatabase, the IDS generates an alert.
The alert could be sent to the network administrator in an e-mailmessage, could be sent to the network management system, or could simply be logged for futureinspection.
Signature-based IDS systems, although widely deployed, have a number of limitations.
Most importantly, they require previous knowledge of the attack to generate an accurate signature.
In otherwords, a signature-based IDS is completely blind to new attacks that have yet to be recorded.
Anotherdisadvantage is that even if a signature is matched, it may not be the result of an attack, so that a falsealarm is generated.
Finally, because every packet must be compared with an extensive collection ofsignatures, the IDS can become overwhelmed with processing and actually fail to detect many malicious
packets.
An anomaly-based IDS creates a traffic profile as it observes traffic in normal operation.
It then looks for packet streams that are statistically unusual, for example, an inordinate percentage of ICMP packets ora sudden exponential growth in port scans and ping sweeps.
The great thing about anomaly-based IDS systems is that they don’t rely on previous knowledge about existing attacks—that is, they canpotentially detect new, undocumented attacks.
On the other hand, it is an extremely challenging problem to distinguish between normal traffic and statistically unusual traffic.
To date, most IDS deployments are primarily signature-based, although some include some anomaly-based features.
SnortSnort is a public-domain, open source IDS with hundreds of thousands of existing deployments [Snort 2012 ; Koziol 2003].
It can run on Linux, UNIX, and Windows platforms.
It uses the generic sniffing interface libpcap, which is also used by Wireshark and many other packet sniffers.
It can easily handle 100 Mbps of traffic; for installations with gibabit/sec traffic rates, multiple Snort sensors may be needed.
To gain some insight into Snort, let’s take a look at an example of a Snort signature: alert icmp $EXTERNAL_NET any -> $HOME_NET any (msg:”ICMP PING NMAP”; dsize: 0; itype: 8;) This signature is matched by any ICMP packet that enters the organization’s network ( $HOME_NET ) from the outside ( $EXTERNAL_NET ), is of type 8 (ICMP ping), and has an empty payload (dsize = 0).
Since nmap (see Section 1.6) generates ping packets with these specific characteristics, this signature is designed to detect nmap ping sweeps.
When a packet matches this signature, Snort generates an alert that includes the message “ICMP PING NMAP” .
Perhaps what is most impressive about Snort is the vast community of users and security experts that maintain its signature database.
Typically within a few hours of a new attack, the Snort communitywrites and releases an attack signature, which is then downloaded by the hundreds of thousands ofSnort deployments distributed around the world.
Moreover, using the Snort signature syntax, networkadministrators can tailor the signatures to their own organization’s needs by either modifying existingsignatures or creating entirely new ones.
8.10 Summary In this chapter, we’ve examined the various mechanisms that our secret lovers, Bob and Alice, can use to communicate securely.
We’ve seen that Bob and Alice are interested in confidentiality (so they aloneare able to understand the contents of a transmitted message), end-point authentication (so they aresure that they are talking with each other), and message integrity (so they are sure that their messagesare not altered in transit).
Of course, the need for secure communication is not confined to secret lovers.
Indeed, we saw in Sections 8.5 through 8.8 that security can be used in various layers in a network architecture to protect against bad guys who have a large arsenal of possible attacks at hand.
The first part of this chapter presented various principles underlying secure communication.
In Section 8.2, we covered cryptographic techniques for encrypting and decrypting data, including symmetric key cryptography and public key cryptography.
DES and RSA were examined as specific case studies of these two major classes of cryptographic techniques in use in today’s networks.
In Section 8.3, we examined two approaches for providing message integrity: message authentication codes (MACs) and digital signatures.
The two approaches have a number of parallels.
Both use cryptographic hash functions and both techniques enable us to verify the source of the message as wellas the integrity of the message itself.
One important difference is that MACs do not rely on encryptionwhereas digital signatures require a public key infrastructure.
Both techniques are extensively used in practice, as we saw in Sections 8.5 through 8.8.
Furthermore, digital signatures are used to create digital certificates, which are important for verifying the validity of public keys.
In Section 8.4, we examined endpoint authentication and introduced nonces to defend against the replay attack.
In Sections 8.5 through 8.8 we examined several security networking protocols that enjoy extensive use in practice.
We saw that symmetric key cryptography is at the core of PGP, SSL, IPsec, and wireless security.
We saw that public key cryptography is crucial for both PGP and SSL.
We saw that PGP usesdigital signatures for message integrity, whereas SSL and IPsec use MACs.
Having now anunderstanding of the basic principles of cryptography, and having studied how these principles areactually used, you are now in position to design your own secure network protocols!
Armed with the techniques covered in Sections 8.2 through 8.8, Bob and Alice can communicate securely. (
One can only hope that they are networking students who have learned this material and can thus avoid having their tryst uncovered by Trudy!)
But confidentiality is only a small part of the network security picture.
As we learned in Section 8.9, increasingly, the focus in network security has been on securing the network infrastructure against a potential onslaught by the bad guys.
In the latter part of this chapter, we thus covered firewalls and IDS systems which inspect packets entering and leaving an
organization’s network.
This chapter has covered a lot of ground, while focusing on the most important topics in modern network security.
Readers who desire to dig deeper are encouraged to investigate the references cited in this chapter.
In particular, we recommend [Skoudis 2006] for attacks and operational security, [Kaufman 1995]  for cryptography and how it applies to network security, [Rescorla 2001]  for an in-depth but readable treatment of SSL, and [Edney 2003] for a thorough discussion of 802.11 security, including an insightful investigation into WEP and its flaws.
Homework Problems and Questions Chapter 8 Review Problems SECTION 8.1 SECTION 8.2 SECTIONS 8.3–8.4R1.
What are the differences between message confidentiality and message integrity?
Can you have confidentiality without integrity?
Can you have integrity without confidentiality?
Justify your answer.
R2.
Internet entities (routers, switches, DNS servers, Web servers, user end systems, and so on) often need to communicate securely.
Give three specific example pairs of Internet entities that may want secure communication.
R3.
From a service perspective, what is an important difference between a symmetric-key system and a public-key system?
R4.
Suppose that an intruder has an encrypted message as well as the decrypted version of that message.
Can the intruder mount a ciphertext-only attack, a known-plaintext attack, or a chosen- plaintext attack?
R5.
Consider an 8-block cipher.
How many possible input blocks does this cipher have?
How many possible mappings are there?
If we view each mapping as a key, then how many possible keys does this cipher have?
R6.
Suppose N people want to communicate with each of  other people using symmetric key encryption.
All communication between any two people, i and j, is visible to all other people in this group of N, and no other person in this group should be able to decode their communication.
How many keys are required in the system as a whole?
Now suppose that public key encryption is used.
How many keys are required in this case?
R7.
Suppose  , and  Use an identity of modular arithmetic to calculate in your head .
R8.
Suppose you want to encrypt the message 10101111 by encrypting the decimal number that corresponds to the message.
What is the decimal number?N−1 n=10,000, a=10,023 b=10,004. (
a⋅b)mod n
SECTIONS 8.5–8.8R9.
In what way does a hash provide a better message integrity check than a checksum (such as the Internet checksum)?
R10.
Can you “decrypt” a hash of a message to get the original message?
Explain your answer.
R11.
Consider a variation of the MAC algorithm ( Figure 8.9 ) where the sender sends  where  is the concatenation of H(m) and s. Is this variation flawed?
Why or why not?
R12.
What does it mean for a signed document to be verifiable and nonforgeable?
R13.
In what way does the public-key encrypted message hash provide a better digital signature than the public-key encrypted message?
R14.
Suppose certifier.com creates a certificate for foo.com.
Typically, the entire certificate would be encrypted with certifier.com’s public key.
True or false?
R15.
Suppose Alice has a message that she is ready to send to anyone who asks.
Thousands of people want to obtain Alice’s message, but each wants to be sure of the integrity of the message.
In this context, do you think a MAC-based or a digital-signature-based integrity scheme is more suitable?
Why?
R16.
What is the purpose of a nonce in an end-point authentication protocol?
R17.
What does it mean to say that a nonce is a once-in-a-lifetime value?
In whose lifetime?R18.
Is the message integrity scheme based on HMAC susceptible to playback attacks?
If so, how can a nonce be incorporated into the scheme to remove this susceptibility?(m, H(m)+s), H(m)+s R19.
Suppose that Bob receives a PGP message from Alice.
How does Bob know for sure that Alice created the message (rather than, say, Trudy)?
Does PGP use a MAC for message integrity?
R20.
In the SSL record, there is a field for SSL sequence numbers.
True or false?
R21.
What is the purpose of the random nonces in the SSL handshake?R22.
Suppose an SSL session employs a block cipher with CBC.
True or false: The server sends to the client the IV in the clear.
R23.
Suppose Bob initiates a TCP connection to Trudy who is pretending to be Alice.
During the handshake, Trudy sends Bob Alice’s certificate.
In what step of the SSL handshake algorithm will Bob discover that he is not communicating with Alice?
R24.
Consider sending a stream of packets from Host A to Host B using IPsec.
Typically, a new SA will be established for each packet sent in the stream.
True or false?
R25.
Suppose that TCP is being run over IPsec between headquarters and the branch office in Figure 8.28 .
If TCP retransmits the same packet, then the two corresponding packets sent by R1 packets will have the same sequence number in the ESP header.
True or false?
R26.
An IKE SA and an IPsec SA are the same thing.
True or false?
R27.
Consider WEP for 802.11.
Suppose that the data is 10101100 and the keystream is 1111000.
What is the resulting ciphertext?
SECTION 8.9 ProblemsR28.
In WEP, an IV is sent in the clear in every frame.
True or false?
R29.
Stateful packet filters maintain two data structures.
Name them and briefly describe what they do.
R30.
Consider a traditional (stateless) packet filter.
This packet filter may filter packets based on TCP flag bits as well as other header fields.
True or false?
R31.
In a traditional packet filter, each interface can have its own access control list.
True or false?
R32.
Why must an application gateway work in conjunction with a router filter to be effective?
R33.
Signature-based IDSs and IPSs inspect into the payloads of TCP and UDP segments.
True or false?
P1.
Using the monoalphabetic cipher in Figure 8.3 , encode the message “This is an easy problem.”
Decode the message “rmij’u uamu xyj.”
P2.
Show that Trudy’s known-plaintext attack, in which she knows the (ciphertext, plaintext) translation pairs for seven letters, reduces the number of possible substitutions to be checked in the example in Section 8.2.1 by approximately 109.
P3.
Consider the polyalphabetic system shown in Figure 8.4 .
Will a chosen-plaintext attack that is able to get the plaintext encoding of the message “The quick brown fox jumps over the lazy dog.”
be sufficient to decode all messages?
Why or why not?
P4.
Consider the block cipher in Figure 8.5 .
Suppose that each block cipher T simply reverses the order of the eight input bits (so that, for example, 11110000 becomes 00001111).
Furthersuppose that the 64-bit scrambler does not modify any bits (so that the output value of the mth bit is equal to the input value of the mth bit). (
a) With  and the original 64-bit input equal to 10100000 repeated eight times, what is the value of the output? (
b) Repeat part (a) but now change the last bit of the original 64-bit input from a 0 to a 1. (
c) Repeat parts (a) and (b) but nowsuppose that the 64-bit scrambler inverses the order of the 64 bits.
P5.
Consider the block cipher in Figure 8.5 .
For a given “key” Alice and Bob would need to keep eight tables, each 8 bits by 8 bits.
For Alice (or Bob) to store all eight tables, how many bits of storage are necessary?
How does this number compare with the number of bits required for afull-table 64-bit block cipher?
P6.
Consider the 3-bit block cipher in Table 8.1 .
Suppose the plaintext is 100100100. (
a) Initially assume that CBC is not used.
What is the resulting ciphertext? (
b) Suppose Trudy sniffs the ciphertext.
Assuming she knows that a 3-bit block cipher without CBC is being employed (but doesn’t know the specific cipher), what can she surmise? (
c) Now suppose that CBC is usedi n=3
with .
What is the resulting ciphertext?
P7. (
a) Using RSA, choose  and , and encode the word “dog” by encrypting each letter separately.
Apply the decryption algorithm to the encrypted version to recover the original plaintext message. (
b) Repeat part (a) but now encrypt “dog” as one message m. P8.
Consider RSA with  and .
a. What are n and z?
b. Let e be 3.
Why is this an acceptable choice for e?
c. Find d such that  (mod z) and .
d. Encrypt the message  using the key (n, e).
Let c denote the corresponding ciphertext.
Show all work.
Hint: To simplify the calculations, use the fact: P9.
In this problem, we explore the Diffie-Hellman (DH) public-key encryption algorithm, which allows two entities to agree on a shared key.
The DH algorithm makes use of a large prime number p and another large number g less than p. Both p and g are made public (so that an attacker would know them).
In DH, Alice and Bob each independently choose secret keys, S and S , respectively.
Alice then computes her public key, T, by raising g to S  and then taking mod p. Bob similarly computes his own public key T by raising g to S  and then taking mod p. Alice and Bob then exchange their public keys over the Internet.
Alice then calculates the sharedsecret key S by raising T  to S  and then taking mod p. Similarly, Bob calculates the shared key S′ by raising T  to S  and then taking mod p. a. Prove that, in general, Alice and Bob obtain the same symmetric key, that is, prove .
b. With p = 11 and g = 2, suppose Alice and Bob choose private keys  and , respectively.
Calculate Alice’s and Bob’s public keys, T and T .
Show all work.
c. Following up on part (b), now calculate S as the shared symmetric key.
Show all work.
d. Provide a timing diagram that shows how Diffie-Hellman can be attacked by a man-in- the-middle.
The timing diagram should have three vertical lines, one for Alice, one for Bob, and one for the attacker Trudy.
P10.
Suppose Alice wants to communicate with Bob using symmetric key cryptography using a session key K .
In Section 8.2 , we learned how public-key cryptography can be used to distribute the session key from Alice to Bob.
In this problem, we explore how the session key can be distributed—without public key cryptography—using a key distribution center (KDC).
The KDC is a server that shares a unique secret symmetric key with each registered user.
For Alice and Bob, denote these keys by K  and K .
Design a scheme that uses the KDC to distribute K  to Alice and Bob.
Your scheme should use three messages to distribute the session key: a message from Alice to the KDC; a message from the KDC to Alice; and finally a messagefrom Alice to Bob.
The first message is K  (A, B).
Using the notation, K , K , S, A, and B answer the following questions.
IV=111 p=3 q=11 p=5 q=11 de=1 d<160 m=8 [ (a mod n)⋅(b mod n)]mod n=(a⋅b)modn A B A A B B B A A B S=S′ SA=5 SB=12 A B S A-KDC B-KDC S A-KDC A-KDC B-KDC
a. What is the second message?
b. What is the third message?
P11.
Compute a third message, different from the two messages in Figure 8.8 , that has the same checksum as the messages in Figure 8.8 .
P12.
Suppose Alice and Bob share two secret keys: an authentication key S and a symmetric encryption key S. Augment Figure 8.9 so that both integrity and confidentiality are provided.
P13.
In the BitTorrent P2P file distribution protocol (see Chapter 2 ), the seed breaks the file into blocks, and the peers redistribute the blocks to each other.
Without any protection, an attacker can easily wreak havoc in a torrent by masquerading as a benevolent peer and sending bogus blocks to a small subset of peers in the torrent.
These unsuspecting peers then redistribute thebogus blocks to other peers, which in turn redistribute the bogus blocks to even more peers.
Thus, it is critical for BitTorrent to have a mechanism that allows a peer to verify the integrity of ablock, so that it doesn’t redistribute bogus blocks.
Assume that when a peer joins a torrent, it initially gets a .torrent file from a fully trusted source.
Describe a simple scheme that allows peers to verify the integrity of blocks.
P14.
The OSPF routing protocol uses a MAC rather than digital signatures to provide message integrity.
Why do you think a MAC was chosen over digital signatures?
P15.
Consider our authentication protocol in Figure 8.18 in which Alice authenticates herself to Bob, which we saw works well (i.e., we found no flaws in it).
Now suppose that while Alice is authenticating herself to Bob, Bob must authenticate himself to Alice.
Give a scenario by which Trudy, pretending to be Alice, can now authenticate herself to Bob as Alice. (
Hint: Consider that the sequence of operations of the protocol, one with Trudy initiating and one with Bob initiating, can be arbitrarily interleaved.
Pay particular attention to the fact that both Bob and Alice will usea nonce, and that if care is not taken, the same nonce can be used maliciously.)
P16.
A natural question is whether we can use a nonce and public key cryptography to solve the end-point authentication problem in Section 8.4 .
Consider the following natural protocol: (1) Alice sends the message “ I am Alice ” to Bob. (
2) Bob chooses a nonce, R, and sends it to Alice. (
3) Alice uses her private key to encrypt the nonce and sends the resulting value to Bob. (
4) Bob applies Alice’s public key to the received message.
Thus, Bob computes R and authenticates Alice.
a. Diagram this protocol, using the notation for public and private keys employed in the textbook.
b. Suppose that certificates are not used.
Describe how Trudy can become a “woman-in-the-middle” by intercepting Alice’s messages and then ­pretending to be Alice to Bob.
P17.
Figure 8.19 shows the operations that Alice must perform with PGP to provide confidentiality, authentication, and integrity.
Diagram the corresponding operations that Bob must perform on the package received from Alice.
P18.
Suppose Alice wants to send an e-mail to Bob.
Bob has a public-private key pair1 2
, and Alice has Bob’s certificate.
But Alice does not have a public, private key pair.
Alice and Bob (and the entire world) share the same hash function .
a. In this situation, is it possible to design a scheme so that Bob can verify that Alice created the message?
If so, show how with a block diagram for Alice and Bob.
b. Is it possible to design a scheme that provides confidentiality for sending the messagefrom Alice to Bob?
If so, show how with a block diagram for Alice and Bob.
P19.
Consider the Wireshark output below for a portion of an SSL session.
a. Is Wireshark packet 112 sent by the client or server?
b. What is the server’s IP address and port number?
c. Assuming no loss and no retransmissions, what will be the sequence number of the nextTCP segment sent by the client?
d. How many SSL records does Wireshark packet 112 contain?
e. Does packet 112 contain a Master Secret or an Encrypted Master Secret or neither?
f. Assuming that the handshake type field is 1 byte and each length field is 3 bytes, whatare the values of the first and last bytes of the Master Secret (or Encrypted Master Secret)?
g. The client encrypted handshake message takes into account how many SSL records?
h. The server encrypted handshake message takes into account how many SSL records?
P20.
In Section 8.6.1 , it is shown that without sequence numbers, Trudy (a woman-in-the middle) can wreak havoc in an SSL session by interchanging TCP segments.
Can Trudy do something similar by deleting a TCP segment?
What does she need to do to succeed at the deletion attack?
What effect will it have?(KB+,KB−) H(⋅)
(Wireshark screenshot reprinted by permission of the Wireshark Foundation.)
P21.
Suppose Alice and Bob are communicating over an SSL session.
Suppose an attacker, who does not have any of the shared keys, inserts a bogus TCP segment into a packet stream with correct TCP checksum and sequence numbers (and correct IP addresses and port numbers).
Will SSL at the receiving side accept the bogus packet and pass the payload to thereceiving application?
Why or why not?
P22.
The following true/false questions pertain to Figure 8.28 .
a. When a host in 172.16.1/24 sends a datagram to an Amazon.com server, the router R1 will encrypt the datagram using IPsec.
b. When a host in 172.16.1/24 sends a datagram to a host in 172.16.2/24, the router R1 will change the source and destination address of the IP datagram.
c. Suppose a host in 172.16.1/24 initiates a TCP connection to a Web server in172.16.2/24.
As part of this connection, all datagrams sent by R1 will have protocol number 50 in the left-most IPv4 header field.
d. Consider sending a TCP segment from a host in 172.16.1/24 to a host in 172.16.2/24.
Suppose the acknowledgment for this segment gets lost, so that TCP resends the segment.
Because IPsec uses sequence numbers, R1 will not resend the TCP segment.
P23.
Consider the example in Figure 8.28 .
Suppose Trudy is a woman-in-the-middle, who can insert datagrams into the stream of datagrams going from R1 and R2.
As part of a replay attack, Trudy sends a duplicate copy of one of the datagrams sent from R1 to R2.
Will R2 decrypt theduplicate datagram and forward it into the branch-office network?
If not, describe in detail howR2 detects the duplicate datagram.
P24.
Consider the following pseudo-WEP protocol.
The key is 4 bits and the IV is 2 bits.
The IV is appended to the end of the key when generating the keystream.
Suppose that the shared secret key is 1010.
The keystreams for the four possible inputs are as follows:101000: 0010101101010101001011010100100 . . .
101001: 1010011011001010110100100101101 . .
.101010: 0001101000111100010100101001111 . .
.101011: 1111101010000000101010100010111 . . .
Suppose all messages are 8 bits long.
Suppose the ICV (integrity check) is 4 bits long, and is calculated by XOR-ing the first 4 bits of data with the last 4 bits of data.
Suppose the pseudo-WEP packet consists of three fields: first the IV field, then the message field, and last the ICVfield, with some of these fields encrypted.
a. We want to send the message  using the  and using WEP.
What will be the values in the three WEP fields?
b. Show that when the receiver decrypts the WEP packet, it recovers the message and the ICV.
c. Suppose Trudy intercepts a WEP packet (not necessarily with the ) and wants to modify it before forwarding it to the receiver.
Suppose Trudy flips the first ICV bit.
Assuming that Trudy does not know the keystreams for any of the IVs, what other bit(s)must Trudy also flip so that the received packet passes the ICV check?
d. Justify your answer by modifying the bits in the WEP packet in part (a), decrypting the resulting packet, and verifying the integrity check.
P25.
Provide a filter table and a connection table for a stateful firewall that is as restrictive as possible but accomplishes the following: a. Allows all internal users to establish Telnet sessions with external hosts.
b. Allows external users to surf the company Web site at 222.22.0.12.
c. But otherwise blocks all inbound and outbound traffic.
The internal network is 222.22/16.
In your solution, suppose that the connection table is currently caching three connections, all from inside to outside.
You’ll need to inventappropriate IP addresses and port numbers.
P26.
Suppose Alice wants to visit the Web site activist.com using a TOR-like ­service.
This service uses two non-colluding proxy servers, Proxy1 and Proxy2.
Alice first obtains them=10100000 IV=11 IV=11
Wireshark Lab In this lab (available from the book Web site), we investigate the Secure Sockets Layer (SSL) protocol.
Recall from Section 8.6 that SSL is used for securing a TCP connection, and that it is extensively used in practice for secure Internet transactions.
In this lab, we will focus on the SSL records sent over the TCP connection.
We will attempt to delineate and classify each of the records, with a goal ofunderstanding the why and how for each record.
We investigate the various SSL record types as well asthe fields in the SSL messages.
We do so by analyzing a trace of the SSL records sent between yourhost and an e-commerce server.
IPsec Lab In this lab (available from the book Web site), we will explore how to create IPsec SAs between linux boxes.
You can do the first part of the lab with two ordinary linux boxes, each with one Ethernet adapter.
But for the second part of the lab, you will need four linux boxes, two of which having two Ethernetadapters.
In the second half of the lab, you will create IPsec SAs using the ESP protocol in the tunnelmode.
You will do this by first manually creating the SAs, and then by having IKE create the SAs.
AN INTERVIEW WITH… Steven M. Bellovin Steven M. Bellovin joined the faculty at Columbia University after many years at the NetworkServices Research Lab at AT&T Labs Research in Florham Park, New Jersey.
His focus is onnetworks, security, and why the two are incompatible.
In 1995, he was awarded the UsenixLifetime Achievement Award for his work in the creation of Usenet, the first newsgroup exchange network that linked two or more computers and allowed users to share informationcertificates (each containing a public key) for Proxy1 and Proxy2 from some central server.
Denote  and  for the encryption/decryption with public and private RSA keys.
a. Using a timing diagram, provide a protocol (as simple as possible) that enables Alice toestablish a shared session key S with Proxy1.
Denote S (m) for encryption/decryption of data m with the shared key S .
b. Using a timing diagram, provide a protocol (as simple as possible) that allows Alice toestablish a shared session key S with Proxy2 without revealing her IP address to Proxy2.
c. Assume now that shared keys S and S  are now established.
Using a timing diagram, provide a protocol (as simple as possible and not using public-key cryptography ) that allows Alice to request an html page from activist.com without revealing her IP address to Proxy2 and without revealing to Proxy1 which site she is visiting .
Your diagram should end with an HTTP request arriving at activist.com.
K1+(),K2+(),K1−(), K2−() 1 1 1 2 1 2
and join in discussions.
Steve is also an elected member of the National Academy of Engineering.
He received his BA from Columbia University and his PhD from the University ofNorth Carolina at Chapel Hill.
What led you to specialize in the networking security area?
This is going to sound odd, but the answer is simple: It was fun.
My background was in systems programming and systems administration, which leads fairly naturally to security.
AndI’ve always been interested in communications, ranging back to part-time systemsprogramming jobs when I was in college.
My work on security continues to be motivated by two things—a desire to keep computers useful, which means that their function can’t be corrupted by attackers, and a desire to protectprivacy.
What was your vision for Usenet at the time that you were developing it?
And now?
We originally viewed it as a way to talk about computer science and computer programming around the country, with a lot of local use for administrative matters, for-sale ads, and so on.
Infact, my original prediction was one to two messages per day, from 50–100 sites at the most—ever.
But the real growth was in people-related topics, including—but not limited to—humaninteractions with computers.
My favorite newsgroups, over the years, have been things likerec.woodworking, as well as sci.crypt.
To some extent, netnews has been displaced by the Web.
Were I to start designing it today, it would look very different.
But it still excels as a way to reach a very broad audience that isinterested in the topic, without having to rely on particular Web sites.
Has anyone inspired you professionally?
In what ways?
Professor Fred Brooks—the founder and original chair of the computer science department at the University of North Carolina at Chapel Hill, the manager of the team that developed the IBM S/360 and OS/360, and the author of The Mythical Man-Month —was a tremendous influence on my career.
More than anything else, he taught outlook and trade-offs—how to look at problems in the context of the real world (and how much messier the real world is than a theorist would like), and how to balance competing interests in designing a solution.
Most computer work is engineering—the art of making the right trade-offs to satisfy many contradictory objectives.
What is your vision for the future of networking and security?
Thus far, much of the security we have has come from isolation.
A firewall, for example, works by cutting off access to certain machines and services.
But we’re in an era of increasingconnectivity—it’s gotten harder to isolate things.
Worse yet, our production systems require farmore separate pieces, interconnected by networks.
Securing all that is one of our biggestchallenges.
What would you say have been the greatest advances in security?
How much further do we have to go?
At least scientifically, we know how to do cryptography.
That’s been a big help.
But most security problems are due to buggy code, and that’s a much harder problem.
In fact, it’s the oldestunsolved problem in computer science, and I think it will remain that way.
The challenge isfiguring out how to secure systems when we have to build them out of insecure components.
We can already do that for reliability in the face of hardware failures; can we do the same for security?
Do you have any advice for students about the Internet and networking security?
Learning the mechanisms is the easy part.
Learning how to “think paranoid” is harder.
You have to remember that probability distributions don’t apply—the attackers can and will find improbableconditions.
And the details matter—a lot.
Chapter 9 Multimedia Networking While lounging in bed or riding buses and subways, people in all corners of the world are currently using the Internet to watch movies and television shows on demand.
Internet movie and television distributioncompanies such as Netflix and Amazon in North America and Youku and Kankan in China havepractically become household names.
But people are not only watching Internet videos, they are usingsites like YouTube to upload and distribute their own user-generated content, becoming Internet video producers as well as consumers.
Moreover, network applications such as Skype, Google Talk, and WeChat (enormously popular in China) allow people to not only make “telephone calls” over theInternet, but to also enhance those calls with video and multi-person conferencing.
In fact, we predictthat by the end of the current decade most of the video consumption and voice conversations will takeplace end-to-end over the Internet, more typically to wireless devices connected to the Internet viacellular and WiFi access networks.
Traditional telephony and broadcast television are quickly becomingobsolete.
We begin this chapter with a taxonomy of multimedia applications in Section 9.1.
We’ll see that a multimedia application can be classified as either streaming stored audio/video , conversational voice/video-over-IP , or streaming live audio/video .
We’ll see that each of these classes of applications has its own unique service requirements that differ significantly from those of traditional elastic applications such as e-mail, Web browsing, and remote login.
In Section 9.2, we’ll examine video streaming in some detail.
We’ll explore many of the underlying principles behind video streaming, including client buffering, prefetching, and adapting video quality to available bandwidth.
In Section 9.3, we investigate conversational voice and video, which, unlike elastic applications, are highly sensitive to end-to-end delay but can tolerate occasional loss of data.
Here we’ll examine how techniques such as adaptive playout, forward error correction, and error concealment can mitigate against network-induced packet loss and delay.
We’ll also examine Skype as a case study.
In Section 9.4, we’ll study RTP and SIP, two popular protocols for real-time conversational voice and video applications.
In Section 9.5, we’ll investigate mechanisms within the network that can be used to distinguish one class of traffic (e.g., delay-sensitive applications such as conversational voice) from another (e.g., elastic applications suchas browsing Web pages), and provide differentiated service among multiple classes of traffic.
9.1 Multimedia Networking Applications We define a multimedia network application as any network application that employs audio or video.
In this section, we provide a taxonomy of multimedia applications.
We’ll see that each class of applicationsin the taxonomy has its own unique set of service requirements and design issues.
But before diving intoan in-depth discussion of Internet multimedia applications, it is useful to consider the intrinsiccharacteristics of the audio and video media themselves.
9.1.1 Properties of Video Perhaps the most salient characteristic of video is its high bit rate .
Video distributed over the Internet typically ranges from 100 kbps for low-quality video conferencing to over 3 Mbps for streaming high-definition movies.
To get a sense of how video bandwidth demands compare with those of other Internetapplications, let’s briefly consider three different users, each using a different Internet application.
Ourfirst user, Frank, is going quickly through photos posted on his friends’ Facebook pages.
Let’s assume that Frank is looking at a new photo every 10 seconds, and that photos are on average 200 Kbytes in size. (
As usual, throughout this discussion we make the simplifying assumption that ) Our second user, Martha, is streaming music from the Internet (“the cloud”) to her smartphone.
Let’sassume Martha is using a service such as Spotify to listen to many MP3 songs, one after the other,each encoded at a rate of 128 kbps.
Our third user, Victor, is watching a video that has been encoded at2 Mbps.
Finally, let’s suppose that the session length for all three users is 4,000 seconds (approximately 67 minutes).
Table 9.1 compares the bit rates and the total bytes transferred for these three users.
We see that video streaming consumes by far the most bandwidth, having a bit rate of more than ten times greater than that of the Facebook and music-streaming applications.
Therefore, when design Table 9.1 Comparison of bit-rate requirements of three Internet applications Bit rate Bytes transferred in 67 min Facebook Frank 160 kbps 80 Mbytes Martha Music 128 kbps 64 Mbytes Victor Video 2 Mbps 1 Gbyte1 Kbyte=8,000 bits.
ing networked video applications, the first thing we must keep in mind is the high bit-rate requirements of video.
Given the popularity of video and its high bit rate, it is perhaps not surprising that Cisco predicts [Cisco 2015]  that streaming and stored video will be approximately 80 percent of global consumer Internet traffic by 2019.
Another important characteristic of video is that it can be compressed, thereby trading off video quality with bit rate.
A video is a sequence of images, typically being displayed at a constant rate, for example,at 24 or 30 images per second.
An uncompressed, digitally encoded image consists of an array ofpixels, with each pixel encoded into a number of bits to represent luminance and color.
There are two types of redundancy in video, both of which can be exploited by video compression .
Spatial redundancy  is the redundancy within a given image.
Intuitively, an image that consists of mostly white space has a high degree of redundancy and can be efficiently compressed without significantlysacrificing image quality.
Temporal redundancy  reflects repetition from image to subsequent image.
If, for example, an image and the subsequent image are exactly the same, there is no reason to re-encode the subsequent image; it is instead more efficient simply to indicate during encoding that the subsequentimage is exactly the same.
Today’s off-the-shelf compression algorithms can compress a video toessentially any bit rate desired.
Of course, the higher the bit rate, the better the image quality and thebetter the overall user viewing experience.
We can also use compression to create multiple versions of the same video, each at a different quality level.
For example, we can use compression to create, say, three versions of the same video, at rates of 300 kbps, 1 Mbps, and 3 Mbps.
Users can then decide which version they want to watch as a function of their current available bandwidth.
Users with high-speed Internet connections might choose the 3 Mbps version; users watching the video over 3G with a smartphone might choose the 300 kbps version.
Similarly, the video in a video conference application can be compressed “on-the-fly” to provide the bestvideo quality given the available end-to-end bandwidth between conversing users.
9.1.2 Properties of Audio Digital audio (including digitized speech and music) has significantly lower bandwidth requirements thanvideo.
Digital audio, however, has its own unique properties that must be considered when designingmultimedia network applications.
To understand these properties, let’s first consider how analog audio (which humans and musical instruments generate) is converted to a digital signal: The analog audio signal is sampled at some fixed rate, for example, at 8,000 samples per second.
The value of each sample will be some real number.
Each of the samples is then rounded to one of a finite number of values.
This operation is referred toas quantization .
The number of such finite values—called quantization values—is typically a power
of two, for example, 256 quantization values.
Each of the quantization values is represented by a fixed number of bits.
For example, if there are 256 quantization values, then each value—and hence each audio sample—is represented by one byte.
The bit representations of all the samples are then concatenated together to form the digitalrepresentation of the signal.
As an example, if an analog audio signal is sampled at 8,000 samples per second and each sample is quantized and represented by 8 bits, then the resulting digital signal will have a rate of 64,000 bits per second.
For playback through audio speakers, the digital signalcan then be converted back—that is, decoded—to an analog signal.
However, the decoded analogsignal is only an approximation of the original signal, and the sound quality may be noticeablydegraded (for example, high-frequency sounds may be missing in the decoded signal).
Byincreasing the sampling rate and the number of quantization values, the decoded signal can betterapproximate the original analog signal.
Thus (as with video), there is a trade-off between the qualityof the decoded signal and the bit-rate and storage requirements of the digital signal.
The basic encoding technique that we just described is called pulse code modulation (PCM) .
Speech encoding often uses PCM, with a sampling rate of 8,000 samples per second and 8 bits per sample,resulting in a rate of 64 kbps.
The audio compact disk (CD) also uses PCM, with a sampling rate of44,100 samples per second with 16 bits per sample; this gives a rate of 705.6 kbps for mono and 1.411Mbps for stereo.
PCM-encoded speech and music, however, are rarely used in the Internet.
Instead, as with video, compression techniques are used to reduce the bit rates of the stream.
Human speech can be compressed to less than 10 kbps and still be intelligible.
A popular compression technique for near CD- quality stereo music is MPEG 1 layer 3, more commonly known as MP3.
MP3 encoders can compress to many different rates; 128 kbps is the most common encoding rate and produces very little sounddegradation.
A related standard is Advanced Audio Coding (AAC) , which has been popularized by Apple.
As with video, multiple versions of a prerecorded audio stream can be created, each at a differentbit rate.
Although audio bit rates are generally much less than those of video, users are generally much more sensitive to audio glitches than video glitches.
Consider, for example, a video conference taking place over the Internet.
If, from time to time, the video signal is lost for a few seconds, the video conference can likely proceed without too much user frustration.
If, however, the audio signal is frequently lost, the users may have to terminate the session.
9.1.3 Types of Multimedia Network Applications The Internet supports a large variety of useful and entertaining multimedia applications.
In this subsection, we classify multimedia applications into three broad categories: (i) streaming stored
audio/video , (ii) conversational voice/video-over-IP , and (iii) streaming live audio/video .
As we will soon see, each of these application categories has its own set of service requirements and design issues.
Streaming Stored Audio and Video To keep the discussion concrete, we focus here on streaming stored video, which typically combines video and audio components.
Streaming stored audio (such as Spotify’s streaming music service) is very similar to streaming stored video, although the bit rates are typically much lower.
In this class of applications, the underlying medium is prerecorded video, such as a movie, a television show, a prerecorded sporting event, or a prerecorded user-generated video (such as those commonlyseen on YouTube).
These prerecorded videos are placed on servers, and users send requests to the servers to view the videos on demand .
Many Internet companies today provide streaming video, including YouTube (Google), Netflix, Amazon, and Hulu.
Streaming stored video has three key distinguishing features.
Streaming.
In a streaming stored video application, the client typically begins video playout within a few seconds after it begins receiving the video from the server.
This means that the client will be playing out from one location in the video while at the same time receiving later parts of the videofrom the server.
This technique, known as streaming, avoids having to download the entire video file (and incurring a potentially long delay) before playout begins.
Interactivity.
Because the media is prerecorded, the user may pause, reposition forward, reposition backward, fast-forward, and so on through the video content.
The time from when the user makes such a request until the action manifests itself at the client should be less than a few seconds foracceptable responsiveness.
Continuous playout.
 Once playout of the video begins, it should proceed according to the original timing of the recording.
Therefore, data must be received from the server in time for its playout at theclient; otherwise, users experience video frame freezing (when the client waits for the delayedframes) or frame skipping (when the client skips over delayed frames).
By far, the most important performance measure for streaming video is average throughput.
In order toprovide continuous playout, the network must provide an average throughput to the streaming application that is at least as large the bit rate of the video itself.
As we will see in Section 9.2, by using buffering and prefetching, it is possible to provide continuous playout even when the throughputfluctuates, as long as the average throughput (averaged over 5–10 seconds) remains above the video rate [Wang 2008].
For many streaming video applications, prerecorded video is stored on, and streamed from, a CDN rather than from a single data center.
There are also many P2P video streaming applications for which the video is stored on users’ hosts (peers), with different chunks of video arriving from different peers
that may spread around the globe.
Given the prominence of Internet video streaming, we will explore video streaming in some depth in Section 9.2, paying particular attention to client buffering, prefetching, adapting quality to bandwidth availability, and CDN distribution.
Conversational Voice- and Video-over-IP Real-time conversational voice over the Internet is often referred to as Internet telephony , since, from the user’s perspective, it is similar to the traditional circuit-switched telephone service.
It is also commonly called Voice-over-IP (VoIP).
Conversational video is similar, except that it includes the video of the participants as well as their voices.
Most of today’s voice and video conversational systems allowusers to create conferences with three or more participants.
Conversational voice and video are widelyused in the Internet today, with the Internet companies Skype, QQ, and Google Talk boasting hundredsof millions of daily users.
In our discussion of application service requirements in Chapter 2 (Figure 2.4), we identified a number of axes along which application requirements can be classified.
Two of these axes—timing considerations and tolerance of data loss—are particularly important for conversational voice and videoapplications.
Timing considerations are important because audio and video conversational applicationsare highly delay-sensitive.
For a conversation with two or more interacting speakers, the delay from when a user speaks or moves until the action is manifested at the other end should be less than a fewhundred milliseconds.
For voice, delays smaller than 150 milliseconds are not perceived by a humanlistener, delays between 150 and 400 milliseconds can be acceptable, and delays exceeding 400milliseconds can result in frustrating, if not completely unintelligible, voice conversations.
On the other hand, conversational multimedia applications are loss-tolerant—occasional loss only causes occasional glitches in audio/video playback, and these losses can often be partially or fully concealed.
These delay-sensitive but loss-tolerant characteristics are clearly different from those ofelastic data applications such as Web browsing, e-mail, social networks, and remote login.
For elasticapplications, long delays are annoying but not particularly harmful; the completeness and integrity of thetransferred data, however, are of paramount importance.
We will explore conversational voice and video in more depth in Section 9.3, paying particular attention to how adaptive playout, forward error correction, and error concealment can mitigate against network-induced packet loss and delay.
Streaming Live Audio and Video This third class of applications is similar to traditional broadcast radio and television, except that transmission takes place over the Internet.
These applications allow a user to receive a live radio or television transmission—such as a live sporting event or an ongoing news event—transmitted from any corner of the world.
Today, thousands of radio and television stations around the world are broadcastingcontent over the Internet.
Live, broadcast-like applications often have many users who receive the same audio/video program at the same time.
In the Internet today, this is typically done with CDNs ( Section 2.6).
As with streaming stored multimedia, the network must provide each live multimedia flow with an average throughput that is larger than the video consumption rate.
Because the event is live, delay can also be an issue,although the timing constraints are much less stringent than those for conversational voice.
Delays of up to ten seconds or so from when the user chooses to view a live transmission to when playout begins can be tolerated.
We will not cover streaming live media in this book because many of the techniques usedfor streaming live media—initial buffering delay, adaptive bandwidth use, and CDN distribution—aresimilar to those for streaming stored media.
9.2 Streaming Stored Video For streaming video applications, prerecorded videos are placed on servers, and users send requests to these servers to view the videos on demand.
The user may watch the video from beginning to endwithout interruption, may stop watching the video well before it ends, or interact with the video bypausing or repositioning to a future or past scene.
Streaming video systems can be classified into three categories: UDP streaming , HTTP streaming , and adaptive HTTP streaming  (see Section 2.6).
Although all three types of systems are used in practice, the majority of today’s systems employ HTTP streaming and adaptive HTTP streaming.
A common characteristic of all three forms of video streaming is the extensive use of client-side application buffering to mitigate the effects of varying end-to-end delays and varying amounts ofavailable bandwidth between server and client.
For streaming video (both stored and live), usersgenerally can tolerate a small several-second initial delay between when the client requests a video andwhen video playout begins at the client.
Consequently, when the video starts to arrive at the client, the client need not immediately begin playout, but can instead build up a reserve of video in an application buffer.
Once the client has built up a reserve of several seconds of buffered-but-not-yet-played video, the client can then begin video playout.
There are two important advantages provided by such client buffering .
First, client-side buffering can absorb variations in server-to-client delay.
If a particular piece of video data is delayed, as long as it arrives before the reserve of received-but-not-yet-played video isexhausted, this long delay will not be noticed.
Second, if the server-to-client bandwidth briefly dropsbelow the video consumption rate, a user can continue to enjoy continuous playback, again as long asthe client application buffer does not become completely drained.
Figure 9.1 illustrates client-side buffering.
In this simple example, suppose that video is encoded at a fixed bit rate, and thus each video block contains video frames that are to be played out over the same fixed amount of time, Δ. The server transmits the first video block at t , the second block at  the third block at  and so on.
Once the client begins playout, each block should be played out Δ time units after the previous block in order to reproduce the timing of the original recorded video.
Because of the variable end-to-end network delays, different video blocks experience different delays.
The first video block arrives at the client at t and the second block arrives at t. The network delay for the ith block is the horizontal distance between the time the block was transmitted by the server and the time it is received at the client; note that the network delay varies from one video block to another.
In this example, if the client were to begin playout as soon as the first block arrived at t, then the second block would not have arrived in time to be played out at out at .
In this case, video playout would either have to stall (waiting for block 2 to arrive) or block 2 could be skipped—both resulting in undesirable0 t0+Δ, t0+2Δ, 1 2 1 t1+Δ
playout impairments.
Instead, if the client were to delay the start of playout until t, when blocks 1 through 6 have all arrived, periodic playout can proceed with all blocks having been received before their playout time.
Figure 9.1 Client playout delay in video streaming 9.2.1 UDP Streaming We only briefly discuss UDP streaming here, referring the reader to more in-depth discussions of the protocols behind these systems where appropriate.
With UDP streaming, the server transmits video at arate that matches the client’s video consumption rate by clocking out the video chunks over UDP at asteady rate.
For example, if the video consumption rate is 2 Mbps and each UDP packet carries 8,000bits of video, then the server would transmit one UDP packet into its socket every .
As we learned in Chapter 3, because UDP does not employ a congestion-control mechanism, the server can push packets into the network at the consumption rate ofthe video without the rate-control restrictions of TCP.
UDP streaming typically uses a small client-sidebuffer, big enough to hold less than a second of video.
Before passing the video chunks to UDP, the server will encapsulate the video chunks within transport packets specially designed for transporting audio and video, using the Real-Time Transport Protocol (RTP) [RFC 3550]  or a similar (possibly proprietary) scheme.
We delay our coverage of RTP until Section 9.3, where we discuss RTP in the context of conversational voice and video systems.
Another distinguishing property of UDP streaming is that in addition to the server-to-client video stream, the client and server also maintain, in parallel, a separate control connection over which the client sendscommands regarding session state changes (such as pause, resume, reposition, and so on).
The Real-3 (8000  bits)/(2 Mbps)=4 msec
Time Streaming Protocol (RTSP) [RFC 2326] , explained in some detail in the Web site for this textbook, is a popular open protocol for such a control connection.
Although UDP streaming has been employed in many open-source systems and proprietary products, it suffers from three significant drawbacks.
First, due to the unpredictable and varying amount of availablebandwidth between server and client, constant-rate UDP streaming can fail to provide continuous playout.
For example, consider the scenario where the video consumption rate is 1 Mbps and the server-to-client available bandwidth is usually more than 1 Mbps, but every few minutes the availablebandwidth drops below 1 Mbps for several seconds.
In such a scenario, a UDP streaming system thattransmits video at a constant rate of 1 Mbps over RTP/UDP would likely provide a poor user experience,with freezing or skipped frames soon after the available bandwidth falls below 1 Mbps.
The seconddrawback of UDP streaming is that it requires a media control server, such as an RTSP server, toprocess client-to-server interactivity requests and to track client state (e.g., the client’s playout point in the video, whether the video is being paused or played, and so on) for each ongoing client session.
This increases the overall cost and complexity of deploying a large-scale video-on-demand system.
The third drawback is that many firewalls are configured to block UDP traffic, preventing the users behind thesefirewalls from receiving UDP video.
9.2.2 HTTP Streaming In HTTP streaming, the video is simply stored in an HTTP server as an ordinary file with a specific URL.When a user wants to see the video, the client establishes a TCP connection with the server and issuesan HTTP GET request for that URL.
The server then sends the video file, within an HTTP responsemessage, as quickly as possible, that is, as quickly as TCP congestion control and flow control willallow.
On the client side, the bytes are collected in a client application buffer.
Once the number of bytes in this buffer exceeds a predetermined threshold, the client application begins playback—specifically, it periodically grabs video frames from the client application buffer, decompresses the frames, anddisplays them on the user’s screen.
We learned in Chapter 3 that when transferring a file over TCP, the server-to-client transmission rate can vary significantly due to TCP’s congestion control mechanism.
In particular, it is not uncommon for the transmission rate to vary in a “saw-tooth” manner associated with TCP congestion control.
Furthermore, packets can also be significantly delayed due to TCP’s retransmission mechanism.
Because of these characteristics of TCP, the conventional wisdom in the 1990s was that video streaming would never work well over TCP.
Over time, however, designers of streaming video systemslearned that TCP’s congestion control and reliable-data transfer mechanisms do not necessarilypreclude continuous playout when client buffering and prefetching (discussed in the next section) areused.
The use of HTTP over TCP also allows the video to traverse firewalls and NATs more easily (which are often configured to block most UDP traffic but to allow most HTTP traffic).
Streaming over HTTP also obviates the need for a media control server, such as an RTSP server, reducing the cost of a large-scale deployment over the Internet.
Due to all of these advantages, most video streaming applicationstoday—including YouTube and Netflix—use HTTP streaming (over TCP) as its underlying streamingprotocol.
Prefetching Video As we just learned, client-side buffering can be used to mitigate the effects of varying end-to-end delays and varying available bandwidth.
In our earlier example in Figure 9.1, the server transmits video at the rate at which the video is to be played out.
However, for streaming stored video, the client can attempt to download the video at a rate higher  than the consumption rate, thereby prefetching  video frames that are to be consumed in the future.
This prefetched video is naturally stored in the client application buffer.
Such prefetching occurs naturally with TCP streaming, since TCP’s congestion avoidance mechanism will attempt to use all of the available bandwidth between server and client.
To gain some insight into prefetching, let’s take a look at a simple example.
Suppose the video consumption rate is 1 Mbps but the network is capable of delivering the video from server to client at aconstant rate of 1.5 Mbps.
Then the client will not only be able to play out the video with a very small playout delay, but will also be able to increase the amount of buffered video data by 500 Kbits everysecond.
In this manner, if in the future the client receives data at a rate of less than 1 Mbps for a briefperiod of time, the client will be able to continue to provide continuous playback due to the reserve in its buffer. [
Wang 2008] shows that when the average TCP throughput is roughly twice the media bit rate, streaming over TCP results in minimal starvation and low buffering delays.
Client Application Buffer and TCP BuffersFigure 9.2 illustrates the interaction between client and server for HTTP streaming.
At the server side, the portion of the video file in white has already been sent into the server’s socket, while the darkened portion is what remains to be sent.
After “passing through the socket door,” the bytes are placed in the TCP send buffer before being transmitted into the Internet, as described in Chapter 3.
In Figure 9.2, because the TCP send buffer at the server side is shown to be full, the server is momentarily prevented from sending more bytes from the video file into the socket.
On the client side, the client application(media player) reads bytes from the TCP receive buffer (through its client socket) and places the bytesinto the client application buffer.
At the same time, the client application periodically grabs video framesfrom the client application buffer, decompresses the frames, and displays them on the user’s screen.
Note that if the client application buffer is larger than the video file, then the whole process of movingbytes from the server’s storage to the client’s application buffer is equivalent to an ordinary file downloadover HTTP—the client simply pulls the video off the server as fast as TCP will allow!
Figure 9.2 Streaming stored video over HTTP/TCP Consider now what happens when the user pauses the video during the streaming process.
During the pause period, bits are not removed from the client application buffer, even though bits continue to enter the buffer from the server.
If the client application buffer is finite, it may eventually become full, which willcause “back pressure” all the way back to the server.
Specifically, once the client application bufferbecomes full, bytes can no longer be removed from the client TCP receive buffer, so it too becomes full.
Once the client receive TCP buffer becomes full, bytes can no longer be removed from the server TCPsend buffer, so it also becomes full.
Once the TCP becomes full, the server cannot send any more bytesinto the socket.
Thus, if the user pauses the video, the server may be forced to stop transmitting, inwhich case the server will be blocked until the user resumes the video.
In fact, even during regular playback (that is, without pausing), if the client application buffer becomes full, back pressure will cause the TCP buffers to become full, which will force the server to reduce its rate.
To determine the resulting rate, note that when the client application removes f bits, it creates room for f bits in the client application buffer, which in turn allows the server to send f additional bits.
Thus, the server send rate can be no higher than the video consumption rate at the client.
Therefore, a full client application buffer indirectly imposes a limit on the rate that video can be sent from server to client when streaming over HTTP.
Analysis of Video Streaming Some simple modeling will provide more insight into initial playout delay and freezing due to application buffer depletion.
As shown in Figure 9.3, let B denote the size
Figure 9.3 Analysis of client-side buffering for video streaming (in bits) of the client’s application buffer, and let Q denote the number of bits that must be buffered before the client application begins playout. (
Of course, ) Let r denote the video consumption rate —the rate at which the client draws bits out of the client application buffer during playback.
So, for example, if the video’s frame rate is 30 frames/sec, and each (compressed) frame is 100,000 bits, then .
To see the forest through the trees, we’ll ignore TCP’s send and receive buffers.
Let’s assume that the server sends bits at a constant rate x whenever the client buffer is not full. (
This is a gross simplification, since TCP’s send rate varies due to congestion control; we’ll examine more realistic time-dependent rates x(t) in the problems at the end of this chapter.)
Suppose at time  the application buffer is empty and video begins arriving to the client application buffer.
We now ask at what time  does playout begin?
And while we are at it, at what time  does the client application buffer become full?
First, let’s determine t , the time when Q bits have entered the application buffer and playout begins.
Recall that bits arrive to the client application buffer at rate x and no bits are removed from this buffer before playout begins.
Thus, the amount of time required to build up Q bits (the initial buffering delay) is .
Now let’s determine t, the point in time when the client application buffer becomes full.
We first observe that if  (that is, if the server send rate is less than the video consumption rate), then the client buffer will never become full!
Indeed, starting at time t, the buffer will be depleted at rate r and will only be filled at rate .
Eventually the client buffer will empty out entirely, at which time the video will freeze on the screen while the client buffer waits another t seconds to build up Q bits of video.
Thus, when theQ<B. r=3 Mbps t=0, t=tp t=tf p tp=Q/x f x<r p x<r p
available rate in the network is less than the video rate, playout will alternate between periods of continuous playout and periods of freezing.
 In a homework problem, you will be asked to determine the length of each continuous playout and freezing period as a function of Q, r, and x. Now let’s determine t for when  In this case, starting at time t, the buffer increases from Q to B at rate  since bits are being depleted at rate r but are arriving at rate x, as shown in Figure 9.3.
Given these hints, you will be asked in a homework problem to determine t, the time the client buffer becomes full.
Note that when the available rate in the network is more than the video rate, after the initial buffering delay, the user willenjoy continuous playout until the video ends.
Early Termination and Repositioning the VideoHTTP streaming systems often make use of the HTTP byte-range header in the HTTP GET request message, which specifies the specific range of bytes the client currently wants to retrieve from thedesired video.
This is particularly useful when the user wants to reposition (that is, jump) to a future point in time in the video.
When the user repositions to a new position, the client sends a new HTTP request, indicating with the byte-range header from which byte in the file should the server send data.
When the server receives the new HTTP request, it can forget about any earlier request and insteadsend bytes beginning with the byte indicated in the byte-range request.
While we are on the subject of repositioning, we briefly mention that when a user repositions to a future point in the video or terminates the video early, some prefetched-but-not-yet-viewed data transmitted bythe server will go unwatched—a waste of network bandwidth and server resources.
For example, suppose that the client buffer is full with B bits at some time t  into the video, and at this time the user repositions to some instant  into the video, and then watches the video to completion from that point on.
In this case, all B bits in the buffer will be unwatched and the bandwidth and server resources that were used to transmit those B bits have been completely wasted.
There is significant wasted bandwidth in the Internet due to early termination, which can be quite costly, particularly for wireless links [Ihm 2011].
For this reason, many streaming systems use only a moderate-size client application buffer, or will limit the amount of prefetched video using the byte-range header in HTTP requests [Rao 2011] .
Repositioning and early termination are analogous to cooking a large meal, eating only a portion of it,and throwing the rest away, thereby wasting food.
So the next time your parents criticize you for wastingfood by not eating all your dinner, you can quickly retort by saying they are wasting bandwidth andserver resources when they reposition while watching movies over the Internet!
But, of course, twowrongs do not make a right—both food and bandwidth are not to be wasted!
In Sections 9.2.1 and 9.2.2, we covered UDP streaming and HTTP streaming, respectively.
A third type of streaming is Dynamic Adaptive Streaming over HTTP (DASH), which uses multiple versions of thef x>r.p x−r f 0 t>t0+B/r
video, each compressed at a different rate.
DASH is discussed in detail in Section 2.6.2.
CDNs are often used to distribute stored and live video.
CDNs are discussed in detail in Section 2.6.3.
9.3 Voice-over-IP Real-time conversational voice over the Internet is often referred to as Internet telephony , since, from the user’s perspective, it is similar to the traditional circuit-switched telephone service.
It is also commonly called Voice-over-IP (VoIP).
In this section we describe the principles and protocols underlying VoIP.
Conversational video is similar in many respects to VoIP, except that it includes thevideo of the participants as well as their voices.
To keep the discussion focused and concrete, we focus here only on voice in this section rather than combined voice and video.
9.3.1 Limitations of the Best-Effort IP Service The Internet’s network-layer protocol, IP, provides best-effort service.
That is to say the service makes its best effort to move each datagram from source to destination as quickly as possible but makes nopromises whatsoever about getting the packet to the destination within some delay bound or about a limit on the percentage of packets lost.
The lack of such guarantees poses significant challenges to thedesign of real-time conversational applications, which are acutely sensitive to packet delay, jitter, and loss.
In this section, we’ll cover several ways in which the performance of VoIP over a best-effort network can be enhanced.
Our focus will be on application-layer techniques, that is, approaches that do not requireany changes in the network core or even in the transport layer at the end hosts.
To keep the discussionconcrete, we’ll discuss the limitations of best-effort IP service in the context of a specific VoIP example.
The sender generates bytes at a rate of 8,000 bytes per second; every 20 msecs the sender gathersthese bytes into a chunk.
A chunk and a special header (discussed below) are encapsulated in a UDP segment, via a call to the socket interface.
Thus, the number of bytes in a chunk is  and a UDP segment is sent every 20 msecs.
If each packet makes it to the receiver with a constant end-to-end delay, then packets arrive at the receiver periodically every 20 msecs.
In these ideal conditions, the receiver can simply play back eachchunk as soon as it arrives.
But unfortunately, some packets can be lost and most packets will not havethe same end-to-end delay, even in a lightly congested Internet.
For this reason, the receiver must takemore care in determining (1) when to play back a chunk, and (2) what to do with a missing chunk.
Packet Loss(20 msecs)⋅(8,000 bytes/sec)=160 bytes,
Consider one of the UDP segments generated by our VoIP application.
The UDP segment is encapsulated in an IP datagram.
As the datagram wanders through the network, it passes through router buffers (that is, queues) while waiting for transmission on outbound links.
It is possible that one ormore of the buffers in the path from sender to receiver is full, in which case the arriving IP datagram maybe discarded, never to arrive at the receiving application.
Loss could be eliminated by sending the packets over TCP (which provides for reliable data transfer) rather than over UDP.
However, retransmission mechanisms are often considered unacceptable forconversational real-time audio applications such as VoIP, because they increase end-to-end delay [Bolot 1996].
Furthermore, due to TCP congestion control, packet loss may result in a reduction of the TCP sender’s transmission rate to a rate that is lower than the receiver’s drain rate, possibly leading to buffer starvation.
This can have a severe impact on voice intelligibility at the receiver.
For these reasons, most existing VoIP applications run over UDP by default. [
Baset 2006]  reports that UDP is used by Skype unless a user is behind a NAT or firewall that blocks UDP segments (in which case TCP is used).
But losing packets is not necessarily as disastrous as one might think.
Indeed, packet loss rates between 1 and 20 percent can be tolerated, depending on how voice is encoded and transmitted, andon how the loss is concealed at the receiver.
For example, forward error correction (FEC) can helpconceal packet loss.
We’ll see below that with FEC, redundant information is transmitted along with the original information so that some of the lost original data can be recovered from the redundantinformation.
Nevertheless, if one or more of the links between sender and receiver is severelycongested, and packet loss exceeds 10 to 20 percent (for example, on a wireless link), then there isreally nothing that can be done to achieve acceptable audio quality.
Clearly, best-effort service has its limitations.
End-to-End Delay End-to-end delay  is the accumulation of transmission, processing, and queuing delays in routers; propagation delays in links; and end-system processing delays.
For real-time conversational applications, such as VoIP, end-to-end delays smaller than 150 msecs are not perceived by a humanlistener; delays between 150 and 400 msecs can be acceptable but are not ideal; and delays exceeding400 msecs can seriously hinder the interactivity in voice conversations.
The receiving side of a VoIP application will typically disregard any packets that are delayed more than a certain threshold, for example, more than 400 msecs.
Thus, packets that are delayed by more than the threshold areeffectively lost.
Packet Jitter A crucial component of end-to-end delay is the varying queuing delays that a packet experiences in the network’s routers.
Because of these varying delays, the time from when a packet is generated at the
source until it is received at the receiver can fluctuate from packet to packet, as shown in Figure 9.1.
This phenomenon is called jitter.
As an example, consider two consecutive packets in our VoIP application.
The sender sends the second packet 20 msecs after sending the first packet.
But at the receiver, the spacing between these packets can become greater than 20 msecs.
To see this, supposethe first packet arrives at a nearly empty queue at a router, but just before the second packet arrives atthe queue a large number of packets from other sources arrive at the same queue.
Because the first packet experiences a small queuing delay and the second packet suffers a large queuing delay at this router, the first and second packets become spaced by more than 20 msecs.
The spacing betweenconsecutive packets can also become less than 20 msecs.
To see this, again consider two consecutivepackets.
Suppose the first packet joins the end of a queue with a large number of packets, and thesecond packet arrives at the queue before this first packet is transmitted and before any packets fromother sources arrive at the queue.
In this case, our two packets find themselves one right after the otherin the queue.
If the time it takes to transmit a packet on the router’s outbound link is less than 20 msecs,then the spacing between first and second packets becomes less than 20 msecs.
The situation is analogous to driving cars on roads.
Suppose you and your friend are each driving in your own cars from San Diego to Phoenix.
Suppose you and your friend have similar driving styles, and that you both drive at 100 km/hour, traffic permitting.
If your friend starts out one hour before you,depending on intervening traffic, you may arrive at Phoenix more or less than one hour after your friend.
If the receiver ignores the presence of jitter and plays out chunks as soon as they arrive, then the resulting audio quality can easily become unintelligible at the receiver.
Fortunately, jitter can often beremoved by using sequence numbers, timestamps , and a playout delay, as discussed below.
9.3.2 Removing Jitter at the Receiver for Audio For our VoIP application, where packets are being generated periodically, the receiver should attempt toprovide periodic playout of voice chunks in the presence of random network jitter.
This is typically doneby combining the following two mechanisms: Prepending each chunk with a timestamp.
 The sender stamps each chunk with the time at which the chunk was generated.
Delaying playout of chunks at the receiver.
 As we saw in our earlier discussion of Figure 9.1, the playout delay of the received audio chunks must be long enough so that most of the packets arereceived before their scheduled playout times.
This playout delay can either be fixed throughout theduration of the audio session or vary adaptively during the audio session lifetime.
We now discuss how these three mechanisms, when combined, can alleviate or even eliminate theeffects of jitter.
We examine two playback strategies: fixed playout delay and adaptive playout delay.
Fixed Playout Delay With the fixed-delay strategy, the receiver attempts to play out each chunk exactly q msecs after the chunk is generated.
So if a chunk is timestamped at the sender at time t, the receiver plays out the chunk at time  assuming the chunk has arrived by that time.
Packets that arrive after their scheduled playout times are discarded and considered lost.
What is a good choice for q?
VoIP can support delays up to about 400 msecs, although a more satisfying conversational experience is achieved with smaller values of q. On the other hand, if q is made much smaller than 400 msecs, then many packets may miss their scheduled playback times dueto the network-induced packet jitter.
Roughly speaking, if large variations in end-to-end delay are typical, it is preferable to use a large q; on the other hand, if delay is small and variations in delay are also small, it is preferable to use a small q, perhaps less than 150 msecs.
The trade-off between the playback delay and packet loss is illustrated in Figure 9.4.
The figure shows the times at which packets are generated and played Figure 9.4 Packet loss for different fixed playout delays out for a single talk spurt.
Two distinct initial playout delays are considered.
As shown by the leftmost staircase, the sender generates packets at regular intervals—say, every 20 msecs.
The first packet in this talk spurt is received at time r. As shown in the figure, the arrivals of subsequent packets are not evenly spaced due to the network jitter.
For the first playout schedule, the fixed initial playout delay is set to  With this schedule, the fourtht+q, p−r.
packet does not arrive by its scheduled playout time, and the receiver considers it lost.
For the second playout schedule, the fixed initial playout delay is set to  For this schedule, all packets arrive before their scheduled playout times, and there is therefore no loss.
Adaptive Playout Delay The previous example demonstrates an important delay-loss trade-off that arises when designing a playout strategy with fixed playout delays.
By making the initial playout delay large, most packets willmake their deadlines and there will therefore be negligible loss; however, for conversational servicessuch as VoIP, long delays can become bothersome if not intolerable.
Ideally, we would like the playoutdelay to be minimized subject to the constraint that the loss be below a few percent.
The natural way to deal with this trade-off is to estimate the network delay and the variance of the network delay, and to adjust the playout delay accordingly at the beginning of each talk spurt.
Thisadaptive adjustment of playout delays at the beginning of the talk spurts will cause the sender’s silent periods to be compressed and elongated; however, compression and elongation of silence by a small amount is not noticeable in speech.
Following [Ramjee 1994] , we now describe a generic algorithm that the receiver can use to adaptively adjust its playout delays.
To this end, let  the timestamp of the ith packet  the time the packet was generated by the sender  the time packet i is received by receiver  the time packet i is played at receiver The end-to-end network delay of the ith packet is  Due to network jitter, this delay will vary from packet to packet.
Let d  denote an estimate of the average  network delay upon reception of the ith packet.
This estimate is constructed from the timestamps as follows: where u is a fixed constant (for example, ).
Thus d  is a smoothed average of the observed network delays  The estimate places more weight on the recently observed network delays than on the observed network delays of the distant past.
This form of estimate should not be completely unfamiliar; a similar idea is used to estimate round-trip times in TCP, as discussed in Chapter 3.
Let v denote an estimate of the average deviation of the delay from the estimated average delay.
This estimate is also constructed from the timestamps:p′−r.
ti= = ri= pi= ri−ti.
i di=(1−u)di−1+u(ri−ti) u=0.01 i r1−t1,…,ri−ti.
i vi=(1−u)vi−1+u| ri−ti−di|
The estimates d  and v  are calculated for every packet received, although they are used only to determine the playout point for the first packet in any talk spurt.
Once having calculated these estimates, the receiver employs the following algorithm for the playout of packets.
If packet i is the first packet of a talk spurt, its playout time, p, is computed as: where K is a positive constant (for example, ).
The purpose of the Kv  term is to set the playout time far enough into the future so that only a small fraction of the arriving packets in the talk spurt will be lost due to late arrivals.
The playout point for any subsequent packet in a talk spurt is computed as an offsetfrom the point in time when the first packet in the talk spurt was played out.
In particular, let be the length of time from when the first packet in the talk spurt is generated until it is played out.
If packet j also belongs to this talk spurt, it is played out at time The algorithm just described makes perfect sense assuming that the receiver can tell whether a packet is the first packet in the talk spurt.
This can be done by examining the signal energy in each received packet.
9.3.3 Recovering from Packet Loss We have discussed in some detail how a VoIP application can deal with packet jitter.
We now briefly describe several schemes that attempt to preserve acceptable audio quality in the presence of packetloss.
Such schemes are called loss recovery schemes .
Here we define packet loss in a broad sense: A packet is lost either if it never arrives at the receiver or if it arrives after its scheduled playout time.
OurVoIP example will again serve as a context for describing loss recovery schemes.
As mentioned at the beginning of this section, retransmitting lost packets may not be feasible in a real- time conversational application such as VoIP.
Indeed, retransmitting a packet that has missed itsplayout deadline serves absolutely no purpose.
And retransmitting a packet that overflowed a routerqueue cannot normally be accomplished quickly enough.
Because of these considerations, VoIPapplications often use some type of loss anticipation scheme.
Two types of loss anticipation schemesare forward error correction (FEC) and interleaving.i i i pi=ti+di+Kvi K=4 i qi=pi−ti pj=tj+qi
Forward Error Correction (FEC) The basic idea of FEC is to add redundant information to the original packet stream.
For the cost of marginally increasing the transmission rate, the redundant information can be used to reconstruct approximations or exact versions of some of the lost packets.
Following [Bolot 1996] and [Perkins 1998] , we now outline two simple FEC mechanisms.
The first mechanism sends a redundant encoded chunk after every n chunks.
The redundant chunk is obtained by exclusive OR-ing the n original chunks [Shacham 1990] .
In this manner if any one packet of the group of  packets is lost, the receiver can fully reconstruct the lost packet.
But if two or more packets in a group are lost, the receiver cannot reconstruct the lost packets.
By keeping , the group size, small, a large fraction of the lost packets can be recovered when loss is not excessive.
However, the smaller the group size, the greater the relative increase of the transmission rate.
In particular, the transmission rate increases by a factor of 1/ n, so that, if  then the transmission rate increases by 33 percent.
Furthermore, this simple scheme increases the playout delay, as the receiver must wait to receive the entire group of packets before itcan begin playout.
For more practical details about how FEC works for multimedia transport see [RFC 5109] .
The second FEC mechanism is to send a lower-resolution audio stream as the redundant information.
For example, the sender might create a nominal audio stream and a corresponding low-resolution, low-bit rate audio stream. (
The nominal stream could be a PCM encoding at 64 kbps, and the lower-qualitystream could be a GSM encoding at 13 kbps.)
The low-bit rate stream is referred to as the redundant stream.
As shown in Figure 9.5, the sender constructs the nth packet by taking the nth chunk from the nominal stream and appending to it the st chunk from the redundant stream.
In this manner, whenever there is nonconsecutive packet loss, the receiver can conceal the loss by playing out the low- bit rate encoded chunk that arrives with the subsequent packet.
Of course, low-bit rate chunks givelower quality than the nominal chunks.
However, a stream of mostly high-quality chunks, occasional low-quality chunks, and no missing chunks gives good overall audio quality.
Note that in this scheme, thereceiver only has to receive two packets before playback, so that the increased playout delay is small.
Furthermore, if the low-bit rate encoding is much less than the nominal encoding, then the marginalincrease in the transmission rate will be small.
In order to cope with consecutive loss, we can use a simple variation.
Instead of appending just the st low-bit rate chunk to the nth nominal chunk, the sender can append the st and nd low- bit rate chunk, or append the st and rd low-bit rate chunk, and so on.
By appending more low- bit rate chunks to each nominal chunk, the audio quality at the receiver becomes acceptable for a wider variety of harsh best-effort environments.
On the other hand, the additional chunks increase thetransmission bandwidth and the playout delay.n+1 n+1 n=3, (n−1) (n−1) (n−1) (n−2) (n−1) (n−3)
Figure 9.5 Piggybacking lower-quality redundant information Interleaving As an alternative to redundant transmission, a VoIP application can send interleaved audio.
As shown in Figure 9.6, the sender resequences units of audio data before transmission, so that originally adjacent units are separated by a certain distance in the transmitted stream.
Interleaving can mitigate the effect of packet losses.
If, for example, units are 5 msecs in length and chunks are 20 msecs (that is, four unitsper chunk), then the first chunk could contain units 1, 5, 9, and 13; the second chunk could contain units 2, 6, 10, and 14; and so on.
Figure 9.6 shows that the loss of a single packet from an interleaved stream results in multiple small gaps in the reconstructed stream, as opposed to the single large gap that would occur in a noninterleaved stream.
Interleaving can significantly improve the perceived quality of an audio stream [Perkins 1998] .
It also has low overhead.
The obvious disadvantage of interleaving is that it increases latency.
This limits its use for conversational applications such as VoIP, although it can perform well for streaming storedaudio.
A major advantage of interleaving is that it does not increase the bandwidth requirements of astream.
Error Concealment Error concealment schemes attempt to produce a replacement for a lost packet that is similar to the original.
As discussed in [Perkins 1998] , this is possible since audio
Figure 9.6 Sending interleaved audio signals, and in particular speech, exhibit large amounts of short-term self-similarity.
As such, these techniques work for relatively small loss rates (less than 15 percent), and for small packets (4–40 msecs).
When the loss length approaches the length of a phoneme (5–100 msecs) these techniquesbreak down, since whole phonemes may be missed by the listener.
Perhaps the simplest form of receiver-based recovery is packet repetition.
Packet repetition replaces lost packets with copies of the packets that arrived immediately before the loss.
It has low computationalcomplexity and performs reasonably well.
Another form of receiver-based recovery is interpolation,which uses audio before and after the loss to interpolate a suitable packet to cover the loss.
Interpolation performs somewhat better than packet repetition but is significantly more computationally intensive [Perkins 1998] .
9.3.4 Case Study: VoIP with Skype Skype is an immensely popular VoIP application with over 50 million accounts active on a daily basis.
In addition to providing host-to-host VoIP service, Skype offers host-to-phone services, phone-to-hostservices, and multi-party host-to-host video conferencing services. (
Here, a host is again any Internetconnected IP device, including PCs, tablets, and smartphones.)
Skype was acquired by Microsoft in2011.
Because the Skype protocol is proprietary, and because all Skype’s control and media packets are encrypted, it is difficult to precisely determine how Skype operates.
Nevertheless, from the Skype Web site and several measurement studies, researchers have learned how Skype generally works [Baset 2006 ; Guha 2006; Chen 2006; Suh 2006; Ren 2006 ; Zhang X 2012].
For both voice and video, the Skype clients have at their disposal many different codecs, which are capable of encoding the media at a wide range of rates and qualities.
For example, video rates for Skype have been measured to be as low as 30 kbps for a low-quality session up to almost 1 Mbps for a high quality session [Zhang X 2012].Typically, Skype’s audio quality is better than the “POTS” (Plain Old Telephone Service) quality providedby the wire-line phone system. (
Skype codecs typically sample voice at 16,000 samples/sec or higher,which provides richer tones than POTS, which samples at 8,000/sec.)
By default, Skype sends audioand video packets over UDP.
However, control packets are sent over TCP, and media packets are alsosent over TCP when firewalls block UDP streams.
Skype uses FEC for loss recovery for both voice andvideo streams sent over UDP.
The Skype client also adapts the audio and video streams it sends to current network conditions, by changing video quality and FEC overhead [Zhang X 2012].
Skype uses P2P techniques in a number of innovative ways, nicely illustrating how P2P can be used in applications that go beyond content distribution and file sharing.
As with instant messaging, host-to-hostInternet telephony is inherently P2P since, at the heart of the application, pairs of users (that is, peers)communicate with each other in real time.
But Skype also employs P2P techniques for two otherimportant functions, namely, for user location and for NAT traversal.
Figure 9.7 Skype peers As shown in Figure 9.7, the peers (hosts) in Skype are organized into a hierarchical overlay network, with each peer classified as a super peer or an ordinary peer.
Skype maintains an index that maps Skype usernames to current IP addresses (and port numbers).
This index is distributed over the superpeers.
When Alice wants to call Bob, her Skype client searches the distributed index to determine Bob’s current IP address.
Because the Skype protocol is proprietary, it is currently not known how the index mappings are organized across the super peers, although some form of DHT organization is verypossible.
P2P techniques are also used in Skype relays , which are useful for establishing calls between hosts in home networks.
Many home network configurations provide access to the Internet through NATs, as discussed in Chapter 4.
Recall that a NAT prevents a host from outside the home network from initiating a connection to a host within the home network.
If both Skype callers have NATs, then there is a problem—neither can accept a call initiated by the other, making a call seemingly impossible.
The clever use of super peers and relays nicely solves this problem.
Suppose that when Alice signs in, she is assigned to a non-NATed super peer and initiates a session to that super peer. (
Since Alice is initiatingthe session, her NAT permits this session.)
This session allows Alice and her super peer to exchangecontrol messages.
The same happens for Bob when he signs in.
Now, when Alice wants to call Bob, sheinforms her super peer, who in turn informs Bob’s super peer, who in turn informs Bob of Alice’sincoming call.
If Bob accepts the call, the two super peers select a third non-NATed super peer—therelay peer—whose job will be to relay data between Alice and Bob.
Alice’s and Bob’s super peers then instruct Alice and Bob respectively to initiate a session with the relay.
As shown in Figure 9.7, Alice then sends voice packets to the relay over the Alice-to-relay connection (which was initiated by Alice), and the relay then forwards these packets over the relay-to-Bob connection (which was initiated by Bob); packets from Bob to Alice flow over these same two relay connections in reverse.
And voila!—Bob and Alice have an end-to-end connection even though neither can accept a session originating from outside.
Up to now, our discussion on Skype has focused on calls involving two persons.
Now let’s examine multi-party audio conference calls.
With  participants, if each user were to send a copy of its audio stream to each of the  other users, then a total of  audio streams would need to be sent into the network to support the audio conference.
To reduce this bandwidth usage, Skype employs a clever distribution technique.
Specifically, each user sends its audio stream to the conference initiator.
The conference initiator combines the audio streams into one stream (basically by adding all the audiosignals together) and then sends a copy of each combined stream to each of the other  participants.
In this manner, the number of streams is reduced to  For ordinary two-person video conversations, Skype routes the call peer-to-peer, unless NAT traversal is required, in which case thecall is relayed through a non-NATed peer, as described earlier.
For a video conference call involving  participants, due to the nature of the video medium, Skype does not combine the call into oneN>2 N−1 N(N−1) N−1 2(N−1).
N>2
stream at one location and then redistribute the stream to all the participants, as it does for voice calls.
Instead, each participant’s video stream is routed to a server cluster (located in Estonia as of 2011), which in turn relays to each participant the  streams of the  other participants [Zhang X 2012].
You may be wondering why each participant sends a copy to a server rather than directly sending acopy of its video stream to each of the other  participants?
Indeed, for both approaches,  video streams are being collectively received by the N participants in the conference.
The reason is, because upstream link bandwidths are significantly lower than downstream link bandwidths in mostaccess links, the upstream links may not be able to support the  streams with the P2P approach.
VoIP systems such as Skype, WeChat, and Google Talk introduce new privacy concerns.
Specifically,when Alice and Bob communicate over VoIP, Alice can sniff Bob’s IP address and then use geo-location services [MaxMind 2016; Quova 2016] to determine Bob’s current location and ISP (for example, his work or home ISP).
In fact, with Skype it is possible for Alice to block the transmission of certain packets during call establishment so that she obtains Bob’s current IP address, say every hour, without Bob knowing that he is being tracked and without being on Bob’s contact list.
Furthermore, the IP address discovered from Skype can be correlated with IP addresses found in BitTorrent, so that Alice can determine the files that Bob is downloading [LeBlond 2011].
Moreover, it is possible to partially decrypt a Skype call by doing a traffic analysis of the packet sizes in a stream [White 2011].N−1 N−1 N−1 N(N−1) N−1
9.4 Protocols for Real-Time Conversational Applications Real-time conversational applications, including VoIP and video conferencing, are compelling and very popular.
It is therefore not surprising that standards bodies, such as the IETF and ITU, have been busyfor many years (and continue to be busy!)
at hammering out standards for this class of applications.
With the appropriate standards in place for real-time conversational applications, independentcompanies are creating new products that interoperate with each other.
In this section we examine RTP and SIP for real-time conversational applications.
Both standards are enjoying widespread implementation in industry products.
9.4.1 RTP In the previous section, we learned that the sender side of a VoIP application appends header fields tothe audio chunks before passing them to the transport layer.
These header fields include sequencenumbers and timestamps.
Since most multimedia networking applications can make use of sequencenumbers and timestamps, it is convenient to have a standardized packet structure that includes fields foraudio/video data, sequence number, and timestamp, as well as other potentially useful fields.
RTP, defined in RFC 3550, is such a standard.
RTP can be used for transporting common formats such as PCM, ACC, and MP3 for sound and MPEG and H.263 for video.
It can also be used for transportingproprietary sound and video formats.
Today, RTP enjoys widespread implementation in many productsand research prototypes.
It is also complementary to other important real-time interactive protocols,such as SIP.
In this section, we provide an introduction to RTP.
We also encourage you to visit Henning Schulzrinne’s RTP site [Schulzrinne-RTP 2012], which provides a wealth of information on the subject.
Also, you may want to visit the RAT site [RAT 2012], which documents VoIP application that uses RTP.
RTP Basics RTP typically runs on top of UDP.
The sending side encapsulates a media chunk within an RTP packet, then encapsulates the packet in a UDP segment, and then hands the segment to IP.
The receiving side extracts the RTP packet from the UDP segment, then extracts the media chunk from the RTP packet,and then passes the chunk to the media player for decoding and rendering.
As an example, consider the use of RTP to transport voice.
Suppose the voice source is PCM-encoded
(that is, sampled, quantized, and digitized) at 64 kbps.
Further suppose that the application collects the encoded data in 20-msec chunks, that is, 160 bytes in a chunk.
The sending side precedes each chunk of the audio data with an RTP header that includes the type of audio encoding, a sequence number, and a timestamp.
The RTP header is normally 12 bytes.
The audio chunk along with the RTP headerform the RTP packet.
The RTP packet is then sent into the UDP socket interface.
At the receiver side,the application receives the RTP packet from its socket interface.
The application extracts the audio chunk from the RTP packet and uses the header fields of the RTP packet to properly decode and play back the audio chunk.
If an application incorporates RTP—instead of a proprietary scheme to provide payload type, sequence numbers, or timestamps—then the application will more easily interoperate with other networkedmultimedia applications.
For example, if two different companies develop VoIP software and they bothincorporate RTP into their product, there may be some hope that a user using one of the VoIP products will be able to communicate with a user using the other VoIP product.
In Section 9.4.2, we’ll see that RTP is often used in conjunction with SIP, an important standard for Internet telephony.
It should be emphasized that RTP does not provide any mechanism to ensure timely delivery of data or provide other quality-of-service (QoS) guarantees; it does not even guarantee delivery of packets orprevent out-of-order delivery of packets.
Indeed, RTP encapsulation is seen only at the end systems.
Routers do not distinguish between IP datagrams that carry RTP packets and IP datagrams that don’t.
RTP allows each source (for example, a camera or a microphone) to be assigned its own independent RTP stream of packets.
For example, for a video conference between two participants, four RTP streams could be opened—two streams for transmitting the audio (one in each direction) and two streams for transmitting the video (again, one in each direction).
However, many popular encodingtechniques—including MPEG 1 and MPEG 2—bundle the audio and video into a single stream duringthe encoding process.
When the audio and video are bundled by the encoder, then only one RTPstream is generated in each direction.
RTP packets are not limited to unicast applications.
They can also be sent over one-to-many and many- to-many multicast trees.
For a many-to-many multicast session, all of the session’s senders and sources typically use the same multicast group for sending their RTP streams.
RTP multicast streams belonging together, such as audio and video streams emanating from multiple senders in a video conferenceapplication, belong to an RTP session.
Figure 9.8 RTP header fields
RTP Packet Header Fields As shown in Figure 9.8, the four main RTP packet header fields are the payload type, sequence number, timestamp, and source identifier fields.
The payload type field in the RTP packet is 7 bits long.
For an audio stream, the payload type field is used to indicate the type of audio encoding (for example, PCM, adaptive delta modulation, linear predictive encoding) that is being used.
If a sender decides to change the encoding in the middle of asession, the sender can inform the receiver of the change through this payload type field.
The sendermay want to change the encoding in order to increase the audio quality or to decrease the RTP stream bit rate.
Table 9.2 lists some of the audio payload types currently supported by RTP.
For a video stream, the payload type is used to indicate the type of video encoding (for example, motion JPEG, MPEG 1, MPEG 2, H.261).
Again, the sender can change video encoding on the fly during a session.
Table 9.3 lists some of the video payload types currently supported by RTP.
The other important fields are the following: Sequence number field.
 The sequence number field is 16 bits long.
The sequence number increments by one for each RTP packet sent, and may be used by the receiver to detect packet loss and to restore packet sequence.
For example, if the receiver side of the application receives astream of RTP packets with a gap between sequence numbers 86 and 89, then the receiver knows that packets 87 and 88 are missing.
The receiver can then attempt to conceal the lost data.
Timestamp field.
The timestamp field is 32 bits long.
It reflects the sampling instant of the first byte in the RTP data packet.
As we saw in the preceding section, the receiver can use timestamps to remove packet jitter introduced in the network and to provide synchronous playout at the receiver.
The timestamp is derived from a sampling clock at the sender.
As an example, for audio the timestamp clock increments by one for each sampling period (for example, each 125 μsec for an 8 kHz sampling clock); if the audio application generates chunks consisting of 160 encoded samples, then the timestamp increases by 160 for each RTP packet when the source is active.
The timestampclock continues to increase at a constant rate even if the source is inactive.
Synchronization source identifier (SSRC).
 The SSRC field is 32 bits long.
It identifies the source of the RTP stream.
Typically, each stream in an RTP session has a distinct SSRC.
The SSRC is notthe IP address of the sender, but instead is a number that the source assigns randomly when thenew stream is started.
The probability that two streams get assigned the same SSRC is very small.
Should this happen, the two sources pick a new SSRC value.
Table 9.2 Audio payload types supported by RTP Payload-Type Number Audio Format Sampling Rate Rate 0 PCM μ-law 8 kHz 64 kbps
1 1016 8 kHz 4.8 kbps 3 GSM 8 kHz 13 kbps 7 LPC 8 kHz 2.4 kbps 9 G.722 16 kHz 48–64 kbps 14 MPEG Audio 90 kHz — 15 G.728 8 kHz 16 kbps Table 9.3 Some video payload types supported by RTP Payload-Type Number Video Format 26 Motion JPEG 31 H.261 32 MPEG 1 video 33 MPEG 2 video 9.4.2 SIP The Session Initiation Protocol (SIP), defined in [RFC 3261 ; RFC 5411] , is an open and lightweight protocol that does the following: It provides mechanisms for establishing calls between a caller and a callee over an IP network.
It allows the caller to notify the callee that it wants to start a call.
It allows the participants to agree on media encodings.
It also allows participants to end calls.
It provides mechanisms for the caller to determine the current IP address of the callee.
Users do not have a single, fixed IP address because they may be assigned addresses dynamically (using DHCP) and because they may have multiple IP devices, each with a different IP address.
It provides mechanisms for call management, such as adding new media streams during the call,
changing the encoding during the call, inviting new participants during the call, call transfer, and call holding.
Setting Up a Call to a Known IP Address To understand the essence of SIP, it is best to take a look at a concrete example.
In this example, Alice is at her PC and she wants to call Bob, who is also working at his PC.
Alice’s and Bob’s PCs are both equipped with SIP-based software for making and receiving phone calls.
In this initial example, we’ll assume that Alice knows the IP address of Bob’s PC.
Figure 9.9 illustrates the SIP call-establishment process.
In Figure 9.9, we see that an SIP session begins when Alice sends Bob an INVITE message, which resembles an HTTP request message.
This INVITE message is sent over UDP to the well-known port 5060 for SIP. (
SIP messages can also be sent over TCP.)
The INVITE message includes an identifierfor Bob (bob@193.64.210.89), an indication of Alice’s current IP address, an indication that Alice desires to receive audio, which is to be encoded in format AVP 0 (PCM encoded μ-law) and
Figure 9.9 SIP call establishment when Alice knows Bob’s IP address encapsulated in RTP, and an indication that she wants to receive the RTP packets on port 38060.
After receiving Alice’s INVITE message, Bob sends an SIP response message, which resembles an HTTP response message.
This response SIP message is also sent to the SIP port 5060.
Bob’s responseincludes a 200 OK as well as an indication of his IP address, his desired encoding and packetization forreception, and his port number to which the audio packets should be sent.
Note that in this exampleAlice and Bob are going to use different audio-encoding mechanisms: Alice is asked to encode heraudio with GSM whereas Bob is asked to encode his audio with PCM μ-law.
After receiving Bob’s response, Alice sends Bob an SIP acknowledgment message.
After this SIP transaction, Bob and Alice can talk. (
For visual convenience, Figure 9.9 shows Alice talking after Bob, but in truth they would normally talk at the same time.)
Bob will encode and packetize the audio as requested and send the audio packets to port number 38060 at IP address 167.180.112.24.
Alice will also encode and packetize the audio as requested and send the audio packets to port number 48753 at IP address 193.64.210.89.
From this simple example, we have learned a number of key characteristics of SIP.
First, SIP is an out- of-band protocol: The SIP messages are sent and received in sockets that are different from those usedfor sending and receiving the media data.
Second, the SIP messages themselves are ASCII-readableand resemble HTTP messages.
Third, SIP requires all messages to be acknowledged, so it can runover UDP or TCP.
In this example, let’s consider what would happen if Bob does not have a PCM μ-law codec for encoding audio.
In this case, instead of responding with 200 OK, Bob would likely respond with a 606 Not Acceptable and list in the message all the codecs he can use.
Alice would then choose one of the listedcodecs and send another INVITE message, this time advertising the chosen codec.
Bob could alsosimply reject the call by sending one of many possible rejection reply codes. (
There are many suchcodes, including “busy,” “gone,” “payment required,” and “forbidden.”)
SIP Addresses In the previous example, Bob’s SIP address is sip:bob@193.64.210.89.
However, we expect many—if not most—SIP addresses to resemble e-mail addresses.
For example, Bob’s address might be sip:bob@domain.com .
When Alice’s SIP device sends an INVITE message, the message would include this e-mail-like address; the SIP infrastructure would then route the message to the IP device that Bob is currently using (as we’ll discuss below).
Other possible forms for the SIP address could beBob’s legacy phone number or simply Bob’s first/middle/last name (assuming it is unique).
An interesting feature of SIP addresses is that they can be included in Web pages, just as people’s e- mail addresses are included in Web pages with the mailto URL.
For example, suppose Bob has a
personal homepage, and he wants to provide a means for visitors to the homepage to call him.
He could then simply include the URL sip:bob@domain.com .
When the visitor clicks on the URL, the SIP application in the visitor’s device is launched and an INVITE message is sent to Bob.
SIP Messages In this short introduction to SIP, we’ll not cover all SIP message types and headers.
Instead, we’ll take a brief look at the SIP INVITE message, along with a few common header lines.
Let us again suppose thatAlice wants to initiate a VoIP call to Bob, and this time Alice knows only Bob’s SIP address, bob@domain.com , and does not know the IP address of the device that Bob is currently using.
Then her message might look something like this: INVITE sip:bob@domain.com  SIP/2.0 Via: SIP/2.0/UDP 167.180.112.24 From: sip:alice@hereway.com To: sip:bob@domain.com Call-ID: a2e3a@pigeon.hereway.com Content-Type: application/sdp Content-Length: 885 c=IN IP4 167.180.112.24 m=audio 38060 RTP/AVP 0 The INVITE line includes the SIP version, as does an HTTP request message.
Whenever an SIPmessage passes through an SIP device (including the device that originates the message), it attaches aVia header, which indicates the IP address of the device. (
We’ll see soon that the typical INVITEmessage passes through many SIP devices before reaching the callee’s SIP application.)
Similar to ane-mail message, the SIP message includes a From header line and a To header line.
The message includes a Call-ID, which uniquely identifies the call (similar to the message-ID in e-mail).
It includes a Content-Type header line, which defines the format used to describe the content contained in the SIPmessage.
It also includes a Content-Length header line, which provides the length in bytes of thecontent in the message.
Finally, after a carriage return and line feed, the message contains the content.
In this case, the content provides information about Alice’s IP address and how Alice wants to receivethe audio.
Name Translation and User Location In the example in Figure 9.9, we assumed that Alice’s SIP device knew the IP address where Bob could
be contacted.
But this assumption is quite unrealistic, not only because IP addresses are often dynamically assigned with DHCP, but also because Bob may have multiple IP devices (for example, different devices for his home, work, and car).
So now let us suppose that Alice knows only Bob’s e-mail address, bob@domain.com , and that this same address is used for SIP-based calls.
In this case, Alice needs to obtain the IP address of the device that the user bob@domain.com  is currently using.
To find this out, Alice creates an INVITE message that begins with INVITE bob@domain.com  SIP/2.0 and sends this message to an SIP proxy.
The proxy will respond with an SIP reply that might include the IP address of the device that bob@domain.com  is currently using.
Alternatively, the reply might include the IP address of Bob’s voicemail box, or it might include a URL of a Web page (that says “Bob is sleeping.
Leave me alone!”).
Also, the result returned by the proxy might depend on the caller: If the callis from Bob’s wife, he might accept the call and supply his IP address; if the call is from Bob’s mother-in-law, he might respond with the URL that points to the I-am-sleeping Web page!
Now, you are probably wondering, how can the proxy server determine the current IP address for bob@domain.com ?
To answer this question, we need to say a few words about another SIP device, the SIP registrar.
Every SIP user has an associated registrar.
Whenever a user launches an SIP application on a device, the application sends an SIP register message to the registrar, informing theregistrar of its current IP address.
For example, when Bob launches his SIP application on his PDA, theapplication would send a message along the lines of: REGISTER sip:domain.com SIP/2.0 Via: SIP/2.0/UDP 193.64.210.89 From: sip:bob@domain.com To: sip:bob@domain.com Expires: 3600 Bob’s registrar keeps track of Bob’s current IP address.
Whenever Bob switches to a new SIP device, the new device sends a new register message, indicating the new IP address.
Also, if Bob remains atthe same device for an extended period of time, the device will send refresh register messages,indicating that the most recently sent IP address is still valid. (
In the example above, refresh messagesneed to be sent every 3600 seconds to maintain the address at the registrar server.)
It is worth noting that the registrar is analogous to a DNS authoritative name server: The DNS server translates fixed host names to fixed IP addresses; the SIP registrar translates fixed human identifiers (for example, bob@domain.com ) to dynamic IP addresses.
Often SIP registrars and SIP proxies are run on the same host.
Now let’s examine how Alice’s SIP proxy server obtains Bob’s current IP address.
From the preceding discussion we see that the proxy server simply needs to forward Alice’s INVITE message to Bob’s registrar/proxy.
The registrar/proxy could then forward the message to Bob’s current SIP device.
Finally,
Bob, having now received Alice’s INVITE message, could send an SIP response to Alice.
As an example, consider Figure 9.10, in which jim@umass.edu, currently working on 217.123.56.89, wants to initiate a Voice-over-IP (VoIP) session with keith@upenn.edu , currently working on 197.87.54.21.
The following steps are taken: Figure 9.10 Session initiation, involving SIP proxies and registrars (1) Jim sends an INVITE message to the umass SIP proxy. (
2) The proxy does a DNS lookup on the SIP registrar upenn.edu (not shown in diagram) and then forwards the message to the registrar server. (
3) Because keith@upenn.edu  is no longer registered at the upenn registrar, the upenn registrar sends a redirect response, indicating that it should try keith@nyu.edu. (
4) The umass proxy sends an INVITE message to the NYU SIP registrar. (
5) The NYU registrar knows the IP address of keith@upenn.edu and forwards the INVITE message to the host 197.87.54.21, which is running Keith’s SIP client. (
6–8) An SIP response is sent back through registrars/proxies to the SIP client on 217.123.56.89. (
9) Media issent directly between the two clients. (
There is also an SIP acknowledgment message, which is notshown.)
Our discussion of SIP has focused on call initiation for voice calls.
SIP, being a signaling protocol for initiating and ending calls in general, can be used for video conference calls as well as for text-based
sessions.
In fact, SIP has become a fundamental component in many instant messaging applications.
Readers desiring to learn more about SIP are encouraged to visit Henning Schulzrinne’s SIP Web site [Schulzrinne-SIP 2016].
In particular, on this site you will find open source software for SIP clients and servers [SIP Software 2016].
9.5 Network Support for Multimedia In Sections 9.2 through 9.4, we learned how application-level mechanisms such as client buffering, prefetching, adapting media quality to available bandwidth, adaptive playout, and loss mitigation techniques can be used by multimedia applications to improve a multimedia application’s performance.
We also learned how content distribution networks and P2P overlay networks can be used to provide a system-level  approach for delivering multimedia content.
These techniques and approaches are all designed to be used in today’s best-effort Internet.
Indeed, they are in use today precisely because the Internet provides only a single, best-effort class of service.
But as designers of computer networks, we can’t help but ask whether the network (rather than the applications or application-level infrastructure alone) might provide mechanisms to support multimedia content delivery.
As we’ll see shortly, the answer is, of course, “yes”!
But we’ll also see that a number of these new network-level mechanismshave yet to be widely deployed.
This may be due to their complexity and to the fact that application-leveltechniques together with best-effort service and properly dimensioned network resources (for example,bandwidth) can indeed provide a “good-enough” (even if not-always-perfect) end-to-end multimedia delivery service.
Table 9.4 summarizes three broad approaches towards providing network-level support for multimedia applications.
Making the best of best-effort service.
The application-level mechanisms and infrastructure that we studied in Sections 9.2 through 9.4 can be successfully used in a well-dimensioned network where packet loss and excessive end-to-end delay rarely occur.
When demand increases areforecasted, the ISPs deploy additional bandwidth and switching capacity to continue to ensure satisfactory delay and packet-loss performance [Huang 2005].
We’ll discuss such network dimensioning  further in Section 9.5.1.
Differentiated service.
 Since the early days of the Internet, it’s been envisioned that different types of traffic (for example, as indicated in the Type-of-Service field in the IP4v packet header) could be provided with different classes of service, rather than a single one-size-fits-all best-effort service.
With differentiated service , one type of traffic might be given strict priority over another class of traffic when both types of traffic are queued at a router.
For example, packets belonging to a real-time conversational application might be given priority over other packets due to their stringent delayconstraints.
Introducing differentiated service into the network will require new mechanisms forpacket marking (indicating a packet’s class of service), packet scheduling, and more.
We’ll cover differentiated service, and new network mechanisms needed to implement this service, in Sections 9.5.2 and 9.5.3.
Table 9.4 Three network-level approaches to supporting multimedia applications Approach Granularity Guarantee Mechanisms Complexity Deployment to date Making the best of best- effort serviceall traffic treated equallynone, or softapplication-layersupport, CDNs, overlays, network-level resourceprovisioningminimal everywhere Differentiatedservicedifferentclasses oftraffic treated differentlynone, orsoftpacket marking,policing,schedulingmedium some Per-connectionQuality-of-Service (QoS)Guaranteeseachsource-destinationflows treateddifferentlysoft orhard, onceflow isadmittedpacket marking,policing,scheduling; calladmission andsignalinglight little Per-connection Quality-of-Service (QoS) Guarantees.
 With per-connection QoS guarantees, each instance of an application explicitly reserves end-to-end bandwidth and thus has a guaranteedend-to-end performance.
A hard guarantee means the application will receive its requested quality of service (QoS) with certainty.
A soft guarantee means the application will receive its requested quality of service with high probability.
For example, if a user wants to make a VoIP call from Host A to Host B, the user’s VoIP application reserves bandwidth explicitly in each link along a routebetween the two hosts.
But permitting applications to make reservations and requiring the network tohonor the reservations requires some big changes.
First, we need a protocol that, on behalf of theapplications, reserves link bandwidth on the paths from the senders to their receivers.
Second, we’llneed new scheduling policies in the router queues so that per-connection bandwidth reservationscan be honored.
Finally, in order to make a reservation, the applications must give the network adescription of the traffic that they intend to send into the network and the network will need to policeeach application’s traffic to make sure that it abides by that description.
These mechanisms, when combined, require new and complex software in hosts and routers.
Because per-connection QoS guaranteed service has not seen significant deployment, we’ll cover these mechanisms only briefly in Section 9.5.4.
9.5.1 Dimensioning Best-Effort Networks Fundamentally, the difficulty in supporting multimedia applications arises from their stringent performance requirements—low end-to-end packet delay, delay jitter, and loss—and the fact that packetdelay, delay jitter, and loss occur whenever the network becomes congested.
A first approach toimproving the quality of multimedia applications—an approach that can often be used to solve just aboutany problem where resources are constrained—is simply to “throw money at the problem” and thus simply avoid resource contention.
In the case of networked multimedia, this means providing enough link capacity throughout the network so that network congestion, and its consequent packet delay andloss, never (or only very rarely) occurs.
With enough link capacity, packets could zip through today’sInternet without queuing delay or loss.
From many perspectives this is an ideal situation—multimediaapplications would perform perfectly, users would be happy, and this could all be achieved with nochanges to Internet’s best-effort architecture.
The question, of course, is how much capacity is “enough” to achieve this nirvana, and whether the costs of providing “enough” bandwidth are practical from a business standpoint to the ISPs.
The question of how much capacity to provide at network links in a given topology to achieve a given level ofperformance is often known as bandwidth provisioning .
The even more complicated problem of how to design a network topology (where to place routers, how to interconnect routers with links, and whatcapacity to assign to links) to achieve a given level of end-to-end performance is a network designproblem often referred to as network dimensioning .
Both bandwidth provisioning and network dimensioning are complex topics, well beyond the scope of this textbook.
We note here, however, thatthe following issues must be addressed in order to predict application-level performance between twonetwork end points, and thus provision enough capacity to meet an application’s performance requirements.
Models of traffic demand between network end points.
 Models may need to be specified at both the call level (for example, users “arriving” to the network and starting up end-to-end applications) and at the packet level (for example, packets being generated by ongoing applications).
Note thatworkload may change over time.
Well-defined performance requirements.
For example, a performance requirement for supporting delay-sensitive traffic, such as a conversational multimedia application, might be that the probability that the end-to-end delay of the packet is greater than a maximum tolerable delay be less than some small value [Fraleigh 2003].
Models to predict end-to-end performance for a given workload model, and techniques to find a minimal cost bandwidth allocation that will result in all user requirements being met.
Here, researchers are busy developing performance models that can quantify performance for a givenworkload, and optimization techniques to find minimal-cost bandwidth allocations meeting performance requirements.
Given that today’s best-effort Internet could (from a technology standpoint) support multimedia traffic at an appropriate performance level if it were dimensioned to do so, the natural question is why today’sInternet doesn’t do so.
The answers are primarily economic and organizational.
From an economicstandpoint, would users be willing to pay their ISPs enough for the ISPs to install sufficient bandwidth tosupport multimedia applications over a best-effort Internet?
The organizational issues are perhaps even more daunting.
Note that an end-to-end path between two multimedia end points will pass through the networks of multiple ISPs.
From an organizational standpoint, would these ISPs be willing to cooperate(perhaps with revenue sharing) to ensure that the end-to-end path is properly dimensioned to support multimedia applications?
For a perspective on these economic and organizational issues, see [Davies 2005] .
For a perspective on provisioning tier-1 backbone networks to support delay-sensitive traffic, see [Fraleigh 2003].
9.5.2 Providing Multiple Classes of Service Perhaps the simplest enhancement to the one-size-fits-all best-effort service in today’s Internet is to divide traffic into classes, and provide different levels of service to these different classes of traffic.
Forexample, an ISP might well want to provide a higher class of service to delay-sensitive Voice-over-IP orteleconferencing traffic (and charge more for this service!)
than to elastic traffic such as e-mail or HTTP.Alternatively, an ISP may simply want to provide a higher quality of service to customers willing to pay more for this improved service.
A number of residential wired-access ISPs and cellular wireless-access ISPs have adopted such tiered levels of service—with platinum-service subscribers receiving betterperformance than gold- or silver-service subscribers.
We’re all familiar with different classes of service from our everyday lives—first-class airline passengers get better service than business-class passengers, who in turn get better service than those of us whofly economy class; VIPs are provided immediate entry to events while everyone else waits in line; eldersare revered in some countries and provided seats of honor and the finest food at a table.
It’s important to note that such differential service is provided among aggregates of traffic, that is, among classes of traffic, not among individual connections.
For example, all first-class passengers are handled the same(with no first-class passenger receiving any better treatment than any other first-class passenger), justas all VoIP packets would receive the same treatment within the network, independent of the particularend-to-end connection to which they belong.
As we will see, by dealing with a small number of trafficaggregates, rather than a large number of individual connections, the new network mechanisms required to provide better-than-best service can be kept relatively simple.
The early Internet designers clearly had this notion of multiple classes of service in mind.
Recall the type-of-service (ToS) field in the IPv4 header discussed in Chapter 4.
IEN123 [ISI 1979] describes the ToS field also present in an ancestor of the IPv4 datagram as follows: “The Type of Service [field]
provides an indication of the abstract parameters of the quality of service desired.
These parameters are to be used to guide the selection of the actual service parameters when transmitting a datagram through a particular network.
Several networks offer service precedence, which somehow treats highprecedence traffic as more important that other traffic.”
More than four decades ago, the vision ofproviding different levels of service to different classes of traffic was clear!
However, it’s taken us anequally long period of time to realize this vision.
Motivating Scenarios Let’s begin our discussion of network mechanisms for providing multiple classes of service with a few motivating scenarios.
Figure 9.11 shows a simple network scenario in which two application packet flows originate on Hosts H1 and H2 on one LAN and are destined for Hosts H3 and H4 on another LAN.
The routers on the two LANs are connected by a 1.5 Mbps link.
Let’s assume the LAN speeds are significantly higher than 1.5 Mbps, and focus on the output queue of router R1; it is here that packet delay and packet loss will occur if the aggregate sending rate of H1 and H2 exceeds 1.5 Mbps.
Let’s further suppose that a 1 Mbpsaudio application (for example, a CD-quality audio call) shares the Figure 9.11 Competing audio and HTTP applications 1.5 Mbps link between R1 and R2 with an HTTP Web-browsing application that is downloading a Web page from H2 to H4.
In the best-effort Internet, the audio and HTTP packets are mixed in the output queue at R1 and (typically) transmitted in a first-in-first-out (FIFO) order.
In this scenario, a burst of packets from the Web
server could potentially fill up the queue, causing IP audio packets to be excessively delayed or lost due to buffer overflow at R1.
How should we solve this potential problem?
Given that the HTTP Web- browsing application does not have time constraints, our intuition might be to give strict priority to audiopackets at R1.
Under a strict priority scheduling discipline, an audio packet in the R1 output buffer wouldalways be transmitted before any HTTP packet in the R1 output buffer.
The link from R1 to R2 wouldlook like a dedicated link of 1.5 Mbps to the audio traffic, with HTTP traffic using the R1-to-R2 link only when no audio traffic is queued.
In order for R1 to distinguish between the audio and HTTP packets in its queue, each packet must be marked as belonging to one of these two classes of traffic.
This was theoriginal goal of the type-of-service (ToS) field in IPv4.
As obvious as this might seem, this then is ourfirst insight into mechanisms needed to provide multiple classes of traffic: Insight 1: Packet marking allows a router to distinguish among packets belonging to different classes of traffic.
Note that although our example considers a competing multimedia and elastic flow, the same insight applies to the case that platinum, gold, and silver classes of service are implemented—a packet-marking mechanism is still needed to indicate that class of service to which a packet belongs.
Now suppose that the router is configured to give priority to packets marked as belonging to the 1 Mbps audio application.
Since the outgoing link speed is 1.5 Mbps, even though the HTTP packets receivelower priority, they can still, on average, receive 0.5 Mbps of transmission service.
But what happens ifthe audio application starts sending packets at a rate of 1.5 Mbps or higher (either maliciously or due toan error in the application)?
In this case, the HTTP packets will starve, that is, they will not receive any service on the R1-to-R2 link.
Similar problems would occur if multiple applications (for example, multiple audio calls), all with the same class of service as the audio application, were sharing the link’sbandwidth; they too could collectively starve the FTP session.
Ideally, one wants a degree of isolationamong classes of traffic so that one class of traffic can be protected from the other.
This protection couldbe implemented at different places in the network—at each and every router, at first entry to thenetwork, or at inter-domain network boundaries.
This then is our second insight: Insight 2:  It is desirable to provide a degree of traffic isolation  among classes so that one class is not adversely affected by another class of traffic that misbehaves.
We’ll examine several specific mechanisms for providing such isolation among traffic classes.
We note here that two broad approaches can be taken.
First, it is possible to perform traffic policing , as shown in Figure 9.12.
If a traffic class or flow must meet certain criteria (for example, that the audio flow not exceed a peak rate of 1 Mbps), then a policing mechanism can be put into place to ensure that these criteria are indeed observed.
If the policed application misbehaves, the policing mechanism will takesome action (for example, drop or delay packets that are in violation of the criteria) so that the trafficactually entering the network conforms to the criteria.
The leaky bucket mechanism that we’ll examine
shortly is perhaps the most widely used policing mechanism.
In Figure 9.12, the packet classification and marking mechanism (Insight 1) and the policing mechanism (Insight 2) are both implemented together at the network’s edge, either in the end system or at an edge router.
A complementary approach for providing isolation among traffic classes is for the link-level packet- scheduling mechanism to explicitly allocate a fixed amount of link bandwidth to each class.
For example, the audio class could be allocated 1 Mbps at R1, and the HTTP class could be allocated 0.5 Mbps.
In this case, the audio and Figure 9.12 Policing (and marking) the audio and HTTP traffic classes
Figure 9.13 Logical isolation of audio and HTTP traffic classes HTTP flows see a logical link with capacity 1.0 and 0.5 Mbps, respectively, as shown in Figure 9.13.
With strict enforcement of the link-level allocation of bandwidth, a class can use only the amount of bandwidth that has been allocated; in particular, it cannot utilize bandwidth that is not currently beingused by others.
For example, if the audio flow goes silent (for example, if the speaker pauses andgenerates no audio packets), the HTTP flow would still not be able to transmit more than 0.5 Mbps overthe R1-to-R2 link, even though the audio flow’s 1 Mbps bandwidth allocation is not being used at thatmoment.
Since bandwidth is a “use-it-or-lose-it” resource, there is no reason to prevent HTTP trafficfrom using bandwidth not used by the audio traffic.
We’d like to use bandwidth as efficiently as possible,never wasting it when it could be otherwise used.
This gives rise to our third insight: Insight 3:  While providing isolation among classes or flows, it is desirable to use resources (for example, link bandwidth and buffers) as efficiently as possible.
Recall from our discussion in Sections 1.3 and 4.2 that packets belonging to various network flows are multiplexed and queued for transmission at the output buffers associated with a link.
The manner in which queued packets are selected for transmission on the link is known as the link-scheduling discipline , and was discussed in detail in Section 4.2.
Recall that in Section 4.2 three link-scheduling disciplines were discussed, namely, FIFO, priority queuing, and Weighted Fair Queuing (WFQ).
We’ll see soon see that WFQ will play a particularly important role for isolating the traffic classes.
The Leaky Bucket One of our earlier insights was that policing, the regulation of the rate at which a class or flow (we will assume the unit of policing is a flow in our discussion below) is allowed to inject packets into the
network, is an important QoS mechanism.
But what aspects of a flow’s packet rate should be policed?
We can identify three important policing criteria, each differing from the other according to the time scale over which the packet flow is policed: Average rate.
 The network may wish to limit the long-term average rate (packets per time interval) at which a flow’s packets can be sent into the network.
A crucial issue here is the interval of timeover which the average rate will be policed.
A flow whose average rate is limited to 100 packets persecond is more constrained than a source that is limited to 6,000 packets per minute, even though both have the same average rate over a long enough interval of time.
For example, the latter constraint would allow a flow to send 1,000 packets in a given second-long interval of time, while theformer constraint would disallow this sending behavior.
Peak rate.
 While the average-rate constraint limits the amount of traffic that can be sent into the network over a relatively long period of time, a peak-rate constraint limits the maximum number ofpackets that can be sent over a shorter period of time.
Using our example above, the network maypolice a flow at an average rate of 6,000 packets per minute, while limiting the flow’s peak rate to 1,500 packets per second.
Burst size.
The network may also wish to limit the maximum number of packets (the “burst” of packets) that can be sent into the network over an extremely short interval of time.
In the limit, as the interval length approaches zero, the burst size limits the number of packets that can beinstantaneously sent into the network.
Even though it is physically impossible to instantaneously send multiple packets into the network (after all, every link has a physical transmission rate that cannot be exceeded!),
the abstraction of a maximum burst size is a useful one.
The leaky bucket mechanism is an abstraction that can be used to characterize these policing limits.
As shown in Figure 9.14, a leaky bucket consists of a bucket that can hold up to b tokens.
Tokens are added to this bucket as follows.
New tokens, which may potentially be added to the bucket, are alwaysbeing generated at a rate of r tokens per second. (
We assume here for simplicity that the unit of time is a second.)
If the bucket is filled with less than b tokens when a token is generated, the newly generated token is added to the bucket; otherwise the newly generated token is ignored, and the token bucket remains full with b tokens.
Let us now consider how the leaky bucket can be used to police a packet flow.
Suppose that before a packet is transmitted into the network, it must first remove a token from the token bucket.
If the tokenbucket is empty, the packet must wait for
Figure 9.14 The leaky bucket policer a token. (
An alternative is for the packet to be dropped, although we will not consider that option here.)
Let us now consider how this behavior polices a traffic flow.
Because there can be at most b tokens in the bucket, the maximum burst size for a leaky-bucket-policed flow is b packets.
Furthermore, because the token generation rate is r, the maximum number of packets that can enter the network of any interval of time of length t is  Thus, the token-generation rate, r, serves to limit the long-term average rate at which packets can enter the network.
It is also possible to use leaky buckets (specifically, two leaky buckets in series) to police a flow’s peak rate in addition to the long-term average rate; see thehomework problems at the end of this chapter.
Leaky Bucket  Weighted Fair Queuing  Provable Maximum Delay in a Queue Let’s close our discussion on policing by showing how the leaky bucket and WFQ can be combined to provide a bound on the delay through a router’s queue. (
Readers who have forgotten about WFQ are encouraged to review WFQ, which is covered in Section 4.2.)
Let’s consider a router’s output link that multiplexes n flows, each policed by a leaky bucket with parameters b and  using WFQ scheduling.
We use the term flow here loosely to refer to the set of packets that are not distinguished from each other by the scheduler.
In practice, a flow might be comprised of traffic from a single end-to-end connection or a collection of many such connections, see Figure 9.15.
Recall from our discussion of WFQ that each flow, i, is guaranteed to receive a share of the link bandwidth equal to at least  where R is the transmissionrt+b.
+ = i ri,i=1,…,n, R⋅wi/(∑ wj),
Figure 9.15 n multiplexed leaky bucket flows with WFQ scheduling rate of the link in packets/sec.
What then is the maximum delay that a packet will experience while waiting for service in the WFQ (that is, after passing through the leaky bucket)?
Let us focus on flow 1.
Suppose that flow 1’s token bucket is initially full.
A burst of b packets then arrives to the leaky bucket policer for flow 1.
These packets remove all of the tokens (without wait) from the leaky bucket and thenjoin the WFQ waiting area for flow 1.
Since these b packets are served at a rate of at least  packet/sec, the last of these packets will then have a maximum delay, d, until its transmission is completed, where The rationale behind this formula is that if there are b packets in the queue and packets are being serviced (removed) from the queue at a rate of at least  packets per second, then the amount of time until the last bit of the last packet is transmitted cannot be more than .
A homework problem asks you to prove that as long as  then d  is indeed the maximum delay that any packet in flow 1 will ever experience in the WFQ queue.
9.5.3 Diffserv Having seen the motivation, insights, and specific mechanisms for providing multiple classes of service, let’s wrap up our study of approaches toward proving multiple classes of service with an example—the Internet Diffserv architecture [RFC 2475 ; Kilkki 1999].
Diffserv provides service differentiation—that is, the ability to handle different classes of traffic in different ways within the Internet in a scalable manner.1 1 R⋅wi/(∑ wj) max dmax=b1R⋅w1/∑ wj 1 R⋅w1/(∑ wj) b1/(R⋅w1/(∑ wj)) r1<R⋅w1/(∑ wj),max
The need for scalability arises from the fact that millions of simultaneous source-destination traffic flows may be present at a backbone router.
We’ll see shortly that this need is met by placing only simple functionality within the network core, with more complex control operations being implemented at thenetwork’s edge.
Let’s begin with the simple network shown in Figure 9.16.
We’ll describe one possible use of Diffserv here; other variations are possible, as described in RFC 2475 .
The Diffserv architecture consists of two sets of functional elements: Edge functions: Packet classification and traffic conditioning.
 At the incoming edge of the network (that is, at either a Diffserv-capable host that generates traffic or at the first Diffserv-capable router that the traffic passes through), arriving packets are marked.
More specifically, the differentiated service (DS) field in the IPv4 or IPv6 packet header is set to some value [RFC 3260] .
The definition of the DS field is intended to supersede the earlier definitions of the IPv4 type-of-service field and the IPv6 traffic class fields that we discussed in Chapter 4.
For example, in Figure 9.16, packets being sent from H1 to H3 might be marked at R1, while packets being sent from H2 to H4 might be marked at R2.
The mark that a packet receives identifies the class of traffic to which it belongs.
Different classes of traffic will then receive different service within the core network.
Figure 9.16 A simple Diffserv network example Core function: Forwarding.
 When a DS-marked packet arrives at a Diffserv-capable router, the packet is forwarded onto its next hop according to the so-called per-hop behavior (PHB) associatedwith that packet’s class.
The per-hop behavior influences how a router’s buffers and link bandwidth are shared among the competing classes of traffic.
A crucial tenet of the Diffserv architecture is that
a router’s per-hop behavior will be based only on packet markings, that is, the class of traffic to which a packet belongs.
Thus, if packets being sent from H1 to H3 in Figure 9.16 receive the same marking as packets being sent from H2 to H4, then the network routers treat these packets as an aggregate, without distinguishing whether the packets originated at H1 or H2.
For example, R3would not distinguish between packets from H1 and H2 when forwarding these packets on to R4.
Thus, the Diffserv architecture obviates the need to keep router state for individual source- destination pairs—a critical consideration in making Diffserv scalable.
An analogy might prove useful here.
At many large-scale social events (for example, a large publicreception, a large dance club or discothèque, a concert, or a football game), people entering the eventreceive a pass of one type or another: VIP passes for Very Important People; over-21 passes for peoplewho are 21 years old or older (for example, if alcoholic drinks are to be served); backstage passes atconcerts; press passes for reporters; even an ordinary pass for the Ordinary Person.
These passes are typically distributed upon entry to the event, that is, at the edge of the event.
It is here at the edge where computationally intensive operations, such as paying for entry, checking for the appropriate type ofinvitation, and matching an invitation against a piece of identification, are performed.
Furthermore, theremay be a limit on the number of people of a given type that are allowed into an event.
If there is such alimit, people may have to wait before entering the event.
Once inside the event, one’s pass allows oneto receive differentiated service at many locations around the event—a VIP is provided with free drinks,a better table, free food, entry to exclusive rooms, and fawning service.
Conversely, an ordinary personis excluded from certain areas, pays for drinks, and receives only basic service.
In both cases, theservice received within the event depends solely on the type of one’s pass.
Moreover, all people within a class are treated alike.
Figure 9.17 provides a logical view of the classification and marking functions within the edge router.
Packets arriving to the edge router are first classified.
The classifier selects packets based on the values of one or more packet header fields (for example, source address, destination address, source port,destination port, and protocol ID) and steers the packet to the appropriate marking function.
As notedabove, a packet’s marking is carried in the DS field in the packet header.
In some cases, an end user may have agreed to limit its packet-sending rate to conform to a declared traffic profile.
The traffic profile might contain a limit on the peak rate, as well as the burstiness of thepacket flow, as we saw previously with the leaky bucket mechanism.
As long as the user sends packetsinto the network in a way that conforms to the negotiated traffic profile, the packets receive their priority
Figure 9.17 A simple Diffserv network example marking and are forwarded along their route to the destination.
On the other hand, if the traffic profile is violated, out-of-profile packets might be marked differently, might be shaped (for example, delayed so that a maximum rate constraint would be observed), or might be dropped at the network edge.
The role of the metering function , shown in Figure 9.17, is to compare the incoming packet flow with the negotiated traffic profile and to determine whether a packet is within the negotiated traffic profile.
The actual decision about whether to immediately remark, forward, delay, or drop a packet is a policy issue determined by the network administrator and is not specified in the Diffserv architecture.
So far, we have focused on the marking and policing functions in the Diffserv architecture.
The second key component of the Diffserv architecture involves the per-hop behavior (PHB) performed by Diffserv- capable routers.
PHB is rather cryptically, but carefully, defined as “a description of the externally observable forwarding behavior of a Diffserv node applied to a particular Diffserv behavior aggregate” [RFC 2475] .
Digging a little deeper into this definition, we can see several important considerations embedded within: A PHB can result in different classes of traffic receiving different performance (that is, different externally observable forwarding behaviors).
While a PHB defines differences in performance (behavior) among classes, it does not mandate anyparticular mechanism for achieving these behaviors.
As long as the externally observable performance criteria are met, any implementation mechanism and any buffer/bandwidth allocationpolicy can be used.
For example, a PHB would not require that a particular packet-queuing discipline (for example, a priority queue versus a WFQ queue versus a FCFS queue) be used to achieve a particular behavior.
The PHB is the end, to which resource allocation and implementationmechanisms are the means.
Differences in performance must be observable and hence measurable.
Two PHBs have been defined: an expedited forwarding (EF) PHB [RFC 3246]  and an assured forwarding (AF) PHB [RFC 2597] .
The expedited forwarding  PHB specifies that the departure rate of a class of traffic from a router must equal or exceed a configured rate.
The assured forwarding PHB divides traffic into four classes, where each AF class is guaranteed to be provided with some minimum amount of bandwidth and buffering.
Let’s close our discussion of Diffserv with a few observations regarding its service model.
First, we have implicitly assumed that Diffserv is deployed within a single administrative domain, but typically an end-to-end service must be fashioned from multiple ISPs sitting between communicating end systems.
Inorder to provide end-to-end Diffserv service, all the ISPs between the end systems must not onlyprovide this service, but most also cooperate and make settlements in order to offer end customers trueend-to-end service.
Without this kind of cooperation, ISPs directly selling Diffserv service to customerswill find themselves repeatedly saying: “Yes, we know you paid extra, but we don’t have a serviceagreement with the ISP that dropped and delayed your traffic.
I’m sorry that there were so many gaps in your VoIP call!”
Second, if Diffserv were actually in place and the network ran at only moderate load, most of the time there would be no perceived difference between a best-effort service and a Diffservservice.
Indeed, end-to-end delay is usually dominated by access rates and router hops rather than byqueuing delays in the routers.
Imagine the unhappy Diffserv customer who has paid more for premiumservice but finds that the best-effort service being provided to others almost always has the sameperformance as premium service!
9.5.4 Per-Connection Quality-of-Service (QoS) Guarantees: Resource Reservation and Call Admission In the previous section, we have seen that packet marking and policing, traffic isolation, and link-level scheduling can provide one class of service with better performance than another.
Under certainscheduling disciplines, such as priority scheduling, the lower classes of traffic are essentially “invisible”to the highest-priority class of traffic.
With proper network dimensioning, the highest class of service canindeed achieve extremely low packet loss and delay—essentially circuit-like performance.
But can the network guarantee  that an ongoing flow in a high-priority traffic class will continue to receive such service throughout the flow’s duration using only the mechanisms that we have described so far?
It cannot.
In this section, we’ll see why yet additional network mechanisms and protocols are requiredwhen a hard service guarantee is provided to individual connections.
Let’s return to our scenario from Section 9.5.2 and consider two 1 Mbps audio applications transmitting their packets over the 1.5 Mbps link, as shown in Figure 9.18.
The combined data rate of the two flows (2 Mbps) exceeds the link capacity.
Even with classification and marking, isolation of flows, and sharing of unused bandwidth (of which there is none), this is clearly a losing proposition.
There is simply not
enough bandwidth to accommodate the needs of both applications at Figure 9.18 Two competing audio applications overloading the R1-to-R2 link the same time.
If the two applications equally share the bandwidth, each application would lose 25 percent of its transmitted packets.
This is such an unacceptably low QoS that both audio applications are completely unusable; there’s no need even to transmit any audio packets in the first place.
Given that the two applications in Figure 9.18 cannot both be satisfied simultaneously, what should the network do?
Allowing both to proceed with an unusable QoS wastes network resources on application flows that ultimately provide no utility to the end user.
The answer is hopefully clear—one of theapplication flows should be blocked (that is, denied access to the network), while the other should beallowed to proceed on, using the full 1 Mbps needed by the application.
The telephone network is anexample of a network that performs such call blocking—if the required resources (an end-to-end circuit in the case of the telephone network) cannot be allocated to the call, the call is blocked (prevented from entering the network) and a busy signal is returned to the user.
In our example, there is no gain inallowing a flow into the network if it will not receive a sufficient QoS to be considered usable.
Indeed,there is a cost to admitting a flow that does not receive its needed QoS, as network resources are beingused to support a flow that provides no utility to the end user.
By explicitly admitting or blocking flows based on their resource requirements, and the source requirements of already-admitted flows, the network can guarantee that admitted flows will be able to receive their requested QoS. Implicit in the need to provide a guaranteed QoS to a flow is the need for the flow to declare its QoS requirements.
This process of having a flow declare its QoS requirement,and then having the network either accept the flow (at the required QoS) or block the flow is referred toas the call admission  process.
This then is our fourth insight (in addition to the three earlier insights from Section 9.5.2,) into the mechanisms needed to provide QoS.
Insight 4:  If sufficient resources will not always be available, and QoS is to be guaranteed , a call admission process is needed in which flows declare their QoS requirements and are then either admitted to the network (at the required QoS) or blocked from the network (if the required QoS cannotbe provided by the network).
Our motivating example in Figure 9.18 highlights the need for several new network mechanisms and protocols if a call (an end-to-end flow) is to be guaranteed a given quality of service once it begins: Resource reservation.
 The only way to guarantee  that a call will have the resources (link bandwidth, buffers) needed to meet its desired QoS is to explicitly allocate those resources to the call—a process known in networking parlance as resource reservation .
Once resources are reserved, the call has on-demand access to these resources throughout its duration, regardless of the demands of all other calls.
If a call reserves and receives a guarantee of x Mbps of link bandwidth, and never transmits at a rate greater than x, the call will see loss- and delay-free performance.
Call admission.
 If resources are to be reserved, then the network must have a mechanism for calls to request and reserve resources.
Since resources are not infinite, a call making a call admission request will be denied admission, that is, be blocked, if the requested resources are not available.
Such a call admission is performed by the telephone network—we request resources when we dial a number.
If the circuits (TDMA slots) needed to complete the call are available, the circuits are allocated and the call is completed.
If the circuits are not available, then the call is blocked, and wereceive a busy signal.
A blocked call can try again to gain admission to the network, but it is notallowed to send traffic into the network until it has successfully completed the call admissionprocess.
Of course, a router that allocates link bandwidth should not allocate more than is availableat that link.
Typically, a call may reserve only a fraction of the link’s bandwidth, and so a router mayallocate link bandwidth to more than one call.
However, the sum of the allocated bandwidth to allcalls should be less than the link capacity if hard quality of service guarantees are to be provided.
Call setup signaling.
 The call admission process described above requires that a call be able to reserve sufficient resources at each and every network router on its source-to-destination path toensure that its end-to-end QoS requirement is met.
Each router must determine the local resourcesrequired by the session, consider the amounts of its resources that are already committed to other ongoing sessions, and determine whether it has sufficient resources to satisfy the per-hop QoS requirement of the session at this router without violating local QoS guarantees made to an already-admitted session.
A signaling protocol is needed to coordinate these various activities—the per-hopallocation of local resources, as well as the overall end-to-end decision of whether or not the call hasbeen able to reserve suf
Figure 9.19 The call setup process ficient resources at each and every router on the end-to-end path.
This is the job of the call setup protocol , as shown in Figure 9.19.
The RSVP protocol  [Zhang 1993, RFC 2210]  was proposed for this purpose within an Internet architecture for providing quality-of-service guarantees.
In ATM networks, the Q2931b protocol [Black 1995]  carries this information among the ATM network’s switches and end point.
Despite a tremendous amount of research and development, and even products that provide for per- connection quality of service guarantees, there has been almost no extended deployment of suchservices.
There are many possible reasons.
First and foremost, it may well be the case that the simple application-level mechanisms that we studied in Sections 9.2 through 9.4, combined with proper network dimensioning ( Section 9.5.1) provide “good enough” best-effort network service for multimedia applications.
In addition, the added complexity and cost of deploying and managing a network that provides per-connection quality of service guarantees may be judged by ISPs to be simply too highgiven predicted customer revenues for that service.
9.6 Summary Multimedia networking is one of the most exciting developments in the Internet today.
People throughout the world less and less time in front of their televisions, and are instead use their smartphones anddevices to receive audio and video trans missions, both live and prerecorded.
Moreover, with sites like YouTube, users have become producers as well as consumers of multimedia Internet content.
Inaddition to video distribution, the Internet is also being used to transport phone calls.
In fact, over the next 10 years, the Internet, along with wireless Internet access, may make the traditional circuit- switched telephone system a thing of the past.
VoIP not only provides phone service inexpensively, butalso provides numerous value-added services, such as video conferencing, online directory services,voice messaging, and integration into social networks such as Facebook and WeChat.
In Section 9.1, we described the intrinsic characteristics of video and voice, and then classified multimedia applications into three categories: (i) streaming stored audio/video, (ii) conversational voice/video-over-IP, and (iii) streaming live audio/video.
In Section 9.2, we studied streaming stored video in some depth.
For streaming video applications, prerecorded videos are placed on servers, and users send requests to these servers to view the videos on demand.
We saw that streaming video systems can be classified into two categories: UDP streamingand HTTP.
We observed that the most important performance measure for streaming video is averagethroughput.
In Section 9.3, we examined how conversational multimedia applications, such as VoIP, can be designed to run over a best-effort network.
For conversational multimedia, timing considerations are important because conversational applications are highly delay-sensitive.
On the other hand, conversational multimedia applications are loss—tolerant—occasional loss only causes occasionalglitches in audio/video playback, and these losses can often be partially or fully concealed.
We saw howa combination of client buffers, packet sequence numbers, and timestamps can greatly alleviate theeffects of network-induced jitter.
We also surveyed the technology behind Skype, one of the leading voice- and video-over-IP companies.
In Section 9.4, we examined two of the most important standardized protocols for VoIP, namely, RTP and SIP.
In Section 9.5, we introduced how several network mechanisms (link-level scheduling disciplines and traffic policing) can be used to provide differentiated service among several classes of traffic.
Homework Problems and Questions Chapter 9 Review Questions SECTION 9.1 SECTION 9.2 SECTION 9.3R1.
Reconstruct Table 9.1 for when Victor Video is watching a 4 Mbps video, Facebook Frank is looking at a new 100 Kbyte image every 20 seconds, and Martha Music is listening to 200 kbps audio stream.
R2.
There are two types of redundancy in video.
Describe them, and discuss how they can be exploited for efficient compression.
R3.
Suppose an analog audio signal is sampled 16,000 times per second, and each sample is quantized into one of 1024 levels.
What would be the resulting bit rate of the PCM digital audio signal?
R4.
Multimedia applications can be classified into three categories.
Name and describe each category.
R5.
Streaming video systems can be classified into three categories.
Name and briefly describe each of these categories.
R6.
List three disadvantages of UDP streaming.
R7.
With HTTP streaming, are the TCP receive buffer and the client’s application buffer the same thing?
If not, how do they interact?
R8.
Consider the simple model for HTTP streaming.
Suppose the server sends bits at a constant rate of 2 Mbps and playback begins when 8 million bits have been received.
What is the initial buffering delay t?p R9.
What is the difference between end-to-end delay and packet jitter?
What are the causes of packet jitter?
R10.
Why is a packet that is received after its scheduled playout time considered lost?
R11.
Section 9.3 describes two FEC schemes.
Briefly summarize them.
Both schemes increase the transmission rate of the stream by adding overhead.
Does interleaving also increase the
SECTION 9.4 Problemstransmission rate?
R12.
How are different RTP streams in different sessions identified by a receiver?
How are different streams from within the same session identified?
R13.
What is the role of a SIP registrar?
How is the role of an SIP registrar different from that of a home agent in Mobile IP?
P1.
Consider the figure below.
Similar to our discussion of Figure 9.1 , suppose that video is encoded at a fixed bit rate, and thus each video block contains video frames that are to be played out over the same fixed amount of time, Δ. The server transmits the first video block at t , the second block at  the third block at  and so on.
Once the client begins playout, each block should be played out Δ time units after the previous block.
a. Suppose that the client begins playout as soon as the first block arrives at t. In the figure below, how many blocks of video (including the first block) will have arrived at the client in time for their playout?
Explain how you arrived at your answer.
b. Suppose that the client begins playout now at .
How many blocks of video (including the first block) will have arrived at the client in time for their playout?
Explain how you arrived at your answer.
c. In the same scenario at (b) above, what is the largest number of blocks that is ever stored in the client buffer, awaiting playout?
Explain how you arrived at your answer.
d. What is the smallest playout delay at the client, such that every video block has arrived intime for its playout?
Explain how you arrived at your answer.0 t0+Δ, t0+2Δ, 1 t1+Δ
P2.
Recall the simple model for HTTP streaming shown in Figure 9.3 .
Recall that B denotes the size of the client’s application buffer, and Q denotes the number of bits that must be buffered before the client application begins playout.
Also r denotes the video consumption rate.
Assume that the server sends bits at a constant rate x whenever the client buffer is not full.
a. Suppose that  As discussed in the text, in this case playout will alternate between periods of continuous playout and periods of freezing.
Determine the length of each continuous playout and freezing period as a function of Q, r, and x. b. Now suppose that  At what time  does the client application buffer become full?
P3.
Recall the simple model for HTTP streaming shown in Figure 9.3 .
Suppose the buffer size is infinite but the server sends bits at variable rate x(t).
Specifically, suppose x(t) has the following saw-tooth shape.
The rate is initially zero at time  and linearly climbs to H at time  It then repeats this pattern again and again, as shown in the figure below.
a. What is the server’s average send rate?
b. Suppose that  so that the client starts playback as soon as it receives a video frame.
What will happen?
c. Now suppose  and  Determine as a function of Q, H, and T the time at which playback first begins.
d. Suppose  and  Prove there will be no freezing after the initial playout delay.
e. Suppose  Find the smallest value of Q such that there will be no freezing after the initial playback delay.
f. Now suppose that the buffer size B is finite.
Suppose  As a function of Q, B, T, and H, determine the time  when the client application buffer first becomes full.
P4.
Recall the simple model for HTTP streaming shown in Figure 9.3 .
Suppose the client application buffer is infinite, the server sends at the constant rate x, and the video consumptionx<r. x>r. t=tf t=0 t=T. Q=0, Q>0 HT/2≥Q. H>2r Q=HT/2.
H>2r.
H>2r.
t=tf r<x.
rate is r with  Also suppose playback begins immediately.
Suppose that the user terminates the video early at time  At the time of termination, the server stops sending bits (if it hasn’t already sent all the bits in the video).
a. Suppose the video is infinitely long.
How many bits are wasted (that is, sent but not viewed)?
b. Suppose the video is T seconds long with  How many bits are wasted (that is, sent but not viewed)?
P5.
Consider a DASH system (as discussed in Section 2.6 ) for which there are N video versions (at N different rates and qualities) and N audio versions (at N different rates and qualities).
Suppose we want to allow the player to choose at any time any of the N video versions and any of the N audio versions.
a. If we create files so that the audio is mixed in with the video, so server sends only onemedia stream at given time, how many files will the server need to store (each a different URL)?
b. If the server instead sends the audio and video streams separately and has the client synchronize the streams, how many files will the server need to store?
P6.
In the VoIP example in Section 9.3 , let h be the total number of header bytes added to each chunk, including UDP and IP header.
a. Assuming an IP datagram is emitted every 20 msecs, find the transmission rate in bits per second for the datagrams generated by one side of this application.
b. What is a typical value of h when RTP is used?
P7.
Consider the procedure described in Section 9.3 for estimating average delay d. Suppose that  Let  be the most recent sample delay, let  be the next most recent sample delay, and so on.
a. For a given audio application suppose four packets have arrived at the receiver with sample delays  and  Express the estimate of delay d in terms of the four samples.
b. Generalize your formula for n sample delays.
c. For the formula in part (b), let n approach infinity and give the resulting formula.
Comment on why this averaging procedure is called an exponential moving average.
P8.
Repeat parts (a) and (b) in Question P7 for the estimate of average delay deviation.
P9.
For the VoIP example in Section 9.3 , we introduced an online procedure (exponential moving average) for estimating delay.
In this problem we will examine an alternative procedure.
Let t be the timestamp of the ith packet received; let r  be the time at which the ith packet is received.
Let d  be our estimate of average delay after receiving the nth packet.
After the first packet is received, we set the delay estimate equal to t=E. T>E. i u=0.1.
r1−t1 r2−t2 r4−t4, r3−t3, r2−t2, r1−t1.
i i n d1=r1−t1.
a. Suppose that we would like  for all n. Give a recursive formula for d  in terms of  and t .
b. Describe why for Internet telephony, the delay estimate described in Section 9.3 is more appropriate than the delay estimate outlined in part (a).
P10.
Compare the procedure described in Section 9.3 for estimating average delay with the procedure in Section 3.5 for estimating round-trip time.
What do the procedures have in common?
How are they different?
P11.
Consider the figure below (which is similar to Figure 9.3 ).
A sender begins sending packetized audio periodically at  The first packet arrives at the receiver at  a. What are the delays (from sender to receiver, ignoring any playout delays) of packets 2 through 8?
Note that each vertical and horizontal line segment in the figure has a length of 1, 2, or 3 time units.
b. If audio playout begins as soon as the first packet arrives at the receiver at  which of the first eight packets sent will not arrive in time for playout?
c. If audio playout begins at  which of the first eight packets sent will not arrive in time for playout?
d. What is the minimum playout delay at the receiver that results in all of the first eight packets arriving in time for their playout?
P12.
Consider again the figure in P11, showing packet audio transmission and reception times.
a. Compute the estimated delay for packets 2 through 8, using the formula for d from Section 9.3.2 .
Use a value of .dn=(r1−t1+r2−t2+⋯+rn−tn)/n n dn−1, rn,n t=1.
t=8.
t=8, t=9, i u=0.1
b. Compute the estimated deviation of the delay from the estimated average for packets 2 through 8, using the formula for v from Section 9.3.2 .
Use a value of .
P13.
Recall the two FEC schemes for VoIP described in Section 9.3 .
Suppose the first scheme generates a redundant chunk for every four original chunks.
Suppose the second scheme uses a low-bit rate encoding whose transmission rate is 25 percent of the transmission rate of thenominal stream.
a. How much additional bandwidth does each scheme require?
How much playback delay does each scheme add?
b. How do the two schemes perform if the first packet is lost in every group of five packets?Which scheme will have better audio quality?
c. How do the two schemes perform if the first packet is lost in every group of two packets?Which scheme will have better audio quality?
P14.
a. Consider an audio conference call in Skype with  participants.
Suppose each participant generates a constant stream of rate r bps.
How many bits per second will the call initiator need to send?
How many bits per second will each of the other  participants need to send?
What is the total send rate, aggregated over all participants?
b. Repeat part (a) for a Skype video conference call using a central server.
c. Repeat part (b), but now for when each peer sends a copy of its video stream to each of the  other peers.
P15.
a. Suppose we send into the Internet two IP datagrams, each carrying a different UDPsegment.
The first datagram has source IP address A1, destination IP address B, source port P1, and destination port T. The second datagram has source IP address A2,destination IP address B, source port P2, and destination port T. Suppose that A1 is different from A2 and that P1 is different from P2.
Assuming that both datagrams reach their final destination, will the two UDP datagrams be received by the same socket?
Whyor why not?
b. Suppose Alice, Bob, and Claire want to have an audio conference call using SIP and RTP.
For Alice to send and receive RTP packets to and from Bob and Claire, is only one UDP socket sufficient (in addition to the socket needed for the SIP messages)?
If yes,then how does Alice’s SIP client distinguish between the RTP packets received from Bob and Claire?
P16.
True or false: a. If stored video is streamed directly from a Web server to a media player, then the application is using TCP as the underlying transport protocol.i u=0.1 N>2 N−1 N−1
b. When using RTP, it is possible for a sender to change encoding in the middle of a session.
c. All applications that use RTP must use port 87.
d. If an RTP session has a separate audio and video stream for each sender, then the audio and video streams use the same SSRC.
e. In differentiated services, while per-hop behavior defines differences in performanceamong classes, it does not mandate any particular mechanism for achieving these performances.
f. Suppose Alice wants to establish an SIP session with Bob.
In her INVITE message she includes the line: m=audio 48753 RTP/AVP 3 (AVP 3 denotes GSM audio).
Alice has therefore indicated in this message that she wishes to send GSM audio.
g. Referring to the preceding statement, Alice has indicated in her INVITE message that she will send audio to port 48753.
h. SIP messages are typically sent between SIP entities using a default SIP port number.
i. In order to maintain registration, SIP clients must periodically send REGISTERmessages.
j. SIP mandates that all SIP clients support G.711 audio encoding.
P17.
Consider the figure below, which shows a leaky bucket policer being fed by a stream ofpackets.
The token buffer can hold at most two tokens, and is initially full at  New tokens arrive at a rate of one token per slot.
The output link speed is such that if two packets obtain tokens at the beginning of a time slot, they can both go to the output link in the same slot.
Thetiming details of the system are as follows: A. Packets (if any) arrive at the beginning of the slot.
Thus in the figure, packets 1, 2, and 3 arrive in slot 0.
If there are already packets in the queue, then the arriving packets join the end of the queue.
Packets proceed towards the front of the queue in a FIFO manner.
B. After the arrivals have been added to the queue, if there are any queued packets, one or two of those packets (depending on the number of available tokens) will each remove a token from the token buffer and go to the output link during that slot.
Thus, packets 1 andt=0.
Programming Assignment In this lab, you will implement a streaming video server and client.
The client will use the real-time streaming protocol (RTSP) to control the actions of the server.
The server will use the real-time protocol(RTP) to packetize the video for transport over UDP.
You will be given Python code that partially implements RTSP and RTP at the client and server.
Your job will be to complete both the client and server code.
When you are finished, you will have created a client-server application that does thefollowing:2 each remove a token from the buffer (since there are initially two tokens) and go to the output link during slot 0.
C. A new token is added to the token buffer if it is not full, since the token generation rate isr = 1 token/slot.
D. Time then advances to the next time slot, and these steps repeat.
Answer the following questions: a. For each time slot, identify the packets that are in the queue and the number of tokens in the bucket, immediately after the arrivals have been processed (step 1 above) but before any of the packets have passed through the queue and removed a token.
Thus, for the  time slot in the example above, packets 1, 2, and 3 are in the queue, and there are two tokens in the buffer.
b. For each time slot indicate which packets appear on the output after the token(s) have been removed from the queue.
Thus, for the  time slot in the example above, packets 1 and 2 appear on the output link from the leaky buffer during slot 0.
P18.
Repeat P17 but assume that  Assume again that the bucket is initially full.
P19.
Consider P18 and suppose now that  and that  as before.
Will your answer to the question above change?
P20.
Consider the leaky bucket policer that polices the average rate and burst size of a packet flow.
We now want to police the peak rate, p, as well.
Show how the output of this leaky bucket policer can be fed into a second leaky bucket policer so that the two leaky buckets in seriespolice the average rate, peak rate, and burst size.
Be sure to give the bucket size and tokengeneration rate for the second policer.
P21.
A packet flow is said to conform to a leaky bucket specification ( r, b) with burst size b and average rate r if the number of packets that arrive to the leaky bucket is less than  packets in every interval of time of length t for all t. Will a packet flow that conforms to a leaky bucket specification ( r, b) ever have to wait at a leaky bucket policer with parameters r and b?
Justify your answer.
P22.
Show that as long as  then d  is indeed the maximum delay that any packet in flow 1 will ever experience in the WFQ queue.t=0 t=0 r=2.
r=3 b=2 rt+b r1<Rw1/(∑ wj), max
The client sends SETUP, PLAY, PAUSE, and TEARDOWN RTSP commands, and the server responds to the commands.
When the server is in the playing state, it periodically grabs a stored JPEG frame, packetizes theframe with RTP, and sends the RTP packet into a UDP socket.
The client receives the RTP packets, removes the JPEG frames, decompresses the frames, andrenders the frames on the client’s monitor.
The code you will be given implements the RTSP protocol in the server and the RTP depacketization in the client.
The code also takes care of displaying the transmitted video.
You will need to implement RTSP in the client and RTP server.
This programming assignment will significantly enhance thestudent’s understanding of RTP, RTSP, and streaming video.
It is highly recommended.
The assignmentalso suggests a number of optional exercises, including implementing the RTSP DESCRIBE commandat both client and server.
You can find full details of the assignment, as well as an overview of the RTSP protocol, at the Web site www.pearsonhighered.com/ cs-resources .
AN INTERVIEW WITH . . .
Henning Schulzrinne Henning Schulzrinne is a professor, chair of the Department of Computer Science, and head ofthe Internet Real-Time Laboratory at Columbia University.
He is the co-author of RTP, RTSP,SIP, and GIST—key protocols for audio and video communications over the Internet.
Henningreceived his BS in electrical and industrial engineering at TU Darmstadt in Germany, his MS inelectrical and computer engineering at the University of Cincinnati, and his PhD in electricalengineering at the University of Massachusetts, Amherst.
What made you decide to specialize in multimedia networking?
This happened almost by accident.
As a PhD student, I got involved with DARTnet, an experimental network spanning the United States with T1 lines.
DARTnet was used as a provingground for multicast and Internet real-time tools.
That led me to write my first audio tool, NeVoT.Through some of the DARTnet participants, I became involved in the IETF, in the then-nascent
Audio Video Transport working group.
This group later ended up standardizing RTP.
What was your first job in the computer industry?
What did it entail?
My first job in the computer industry was soldering together an Altair computer kit when I was a high school student in Livermore, California.
Back in Germany, I started a little consultingcompany that devised an address management program for a travel agency—storing data oncassette tapes for our TRS-80 and using an IBM Selectric typewriter with a home-brew hardware interface as a printer.
My first real job was with AT&T Bell Laboratories, developing a network emulator for constructing experimental networks in a lab environment.
What are the goals of the Internet Real-Time Lab?
Our goal is to provide components and building blocks for the Internet as the single future communications infrastructure.
This includes developing new protocols, such as GIST (fornetwork-layer signaling) and LoST (for finding resources by location), or enhancing protocolsthat we have worked on earlier, such as SIP, through work on rich presence, peer-to-peersystems, next-generation emergency calling, and service creation tools.
Recently, we have alsolooked extensively at wireless systems for VoIP, as 802.11b and 802.11n networks and maybe WiMax networks are likely to become important last-mile technologies for telephony.
We are also trying to greatly improve the ability of users to diagnose faults in the complicated tangle ofproviders and equipment, using a peer-to-peer fault diagnosis system called DYSWIS (Do YouSee What I See).
We try to do practically relevant work, by building prototypes and open source systems, by measuring performance of real systems, and by contributing to IETF standards.
What is your vision for the future of multimedia networking?
We are now in a transition phase; just a few years shy of when IP will be the universal platform for multimedia services, from IPTV to VoIP.
We expect radio, telephone, and TV to be available even during snowstorms and earthquakes, so when the Internet takes over the role of these dedicated networks, users will expect the same level of reliability.
We will have to learn to design network technologies for an ecosystem of competing carriers, service and content providers, serving lots of technically untrained users and defending themagainst a small, but destructive, set of malicious and criminal users.
Changing protocols isbecoming increasingly hard.
They are also becoming more complex, as they need to take intoaccount competing business interests, security, privacy, and the lack of transparency ofnetworks caused by firewalls and network address translators.
Since multimedia networking is becoming the foundation for almost all of consumer
entertainment, there will be an emphasis on managing very large networks, at low cost.
Users will expect ease of use, such as finding the same content on all of their devices.
Why does SIP have a promising future?
As the current wireless network upgrade to 3G networks proceeds, there is the hope of a single multimedia signaling mechanism spanning all types of networks, from cable modems, tocorporate telephone networks and public wireless networks.
Together with software radios, this will make it possible in the future that a single device can be used on a home network, as a cordless BlueTooth phone, in a corporate network via 802.11 and in the wide area via 3Gnetworks.
Even before we have such a single universal wireless device, the personal mobilitymechanisms make it possible to hide the differences between networks.
One identifier becomesthe universal means of reaching a person, rather than remembering or passing around half adozen technology- or location-specific telephone numbers.
SIP also breaks apart the provision of voice (bit) transport from voice services.
It now becomes technically possible to break apart the local telephone monopoly, where one company provides neutral bit transport, while others provide IP “dial tone” and the classical telephone services, such as gateways, call forwarding, and caller ID.
Beyond multimedia signaling, SIP offers a new service that has been missing in the Internet: event notification.
We have approximated such services with HTTP kludges and e-mail, but thiswas never very satisfactory.
Since events are a common abstraction for distributed systems, thismay simplify the construction of new services.
Do you have any advice for students entering the networking field?
Networking bridges disciplines.
It draws from electrical engineering, all aspects of computer science, operations research, statistics, economics, and other disciplines.
Thus, networkingresearchers have to be familiar with subjects well beyond protocols and routing algorithms.
Given that networks are becoming such an important part of everyday life, students wanting to make a difference in the field should think of the new resource constraints in networks: humantime and effort, rather than just bandwidth or storage.
Work in networking research can be immensely satisfying since it is about allowing people to communicate and exchange ideas, one of the essentials of being human.
The Internet hasbecome the third major global infrastructure, next to the transportation system and energydistribution.
Almost no part of the economy can work without high-performance networks, sothere should be plenty of opportunities for the foreseeable future.
References A note on URLs.
 In the references below, we have provided URLs for Web pages, Web-only documents, and other material that has not been published in a conference or journal (when we have been able to locate a URL for such material).
We have not provided URLs for conference and journal publications, as these documents can usually be located via a search engine, from the conference Web site (e.g., papers in all ACM SIGCOMM conferences and workshops can be located via http:/ /www.acm.org/ sigcomm), or via a digital library subscription.
While all URLs provided below were valid (and tested) in Jan. 2016, URLs can become out of date.
Please consult the online version of this book ( www.pearsonhighered .com/ cs-resources ) for an up-to-date bibliography.
A note on Internet Request for Comments (RFCs): Copies of Internet RFCs are available at many sites.
The RFC Editor of the Internet Society (the body that overseesthe RFCs) maintains the site, http:/ /www.rfc-editor.org .
This site allows you to search for a specific RFC by title, number, or authors, and will show updates to any RFCs listed.
Internet RFCs can be updated or obsoleted by later RFCs.
Our favorite site for getting RFCs is the original source— http:/ /www.rfc-editor.org . [
3GPP 2016] Third Generation Partnership Project homepage, http:/ /www.3gpp.org/ [Abramson 1970]  N. Abramson, “The Aloha System—Another Alternative for Computer Communications,” Proc.
1970 Fall Joint Computer Conference, AFIPS Conference , p. 37, 1970. [
Abramson 1985]  N. Abramson, “Development of the Alohanet,” IEEE Transactions on Information Theory , Vol.
IT-31, No.
3 (Mar. 1985), pp.
119–123. [
Abramson 2009]  N. Abramson, “The Alohanet—Surfing for Wireless Data,” IEEE Communications Magazine , Vol.
47, No.
12, pp.
21–25. [
Adhikari 2011a]  V. K. Adhikari, S. Jain, Y. Chen, Z. L. Zhang, “Vivisecting YouTube: An Active Measurement Study,” Technical Report, University of Minnesota, 2011. [
Adhikari 2012] V. K. Adhikari, Y. Gao, F. Hao, M. Varvello, V. Hilt, M. Steiner, Z. L. Zhang, “Unreeling Netflix: Understanding and Improving Multi-CDN Movie Delivery,” Technical Report, University of Minnesota, 2012. [
Afanasyev 2010]  A. Afanasyev, N. Tilley, P. Reiher, L. Kleinrock, “Host-to-Host Congestion Control for TCP,” IEEE Communications Surveys & Tutorials , Vol.
12, No.
3, pp.
304–342. [
Agarwal 2009]  S. Agarwal, J. Lorch, “Matchmaking for Online Games and Other Latency-sensitive P2P Systems,” Proc.
2009 ACM SIGCOMM. [
Ager 2012]  B. Ager, N. Chatzis, A. Feldmann, N. Sarrar, S. Uhlig, W. Willinger, “Anatomy of a Large European ISP,” Sigcomm, 2012.
[Ahn 1995] J. S. Ahn, P. B. Danzig, Z. Liu, and Y. Yan, “Experience with TCP Vegas: Emulation and Experiment,” Proc.
1995 ACM SIGCOMM (Boston, MA, Aug. 1995), pp.
185–195. [
Akamai 2016]  Akamai homepage , http:/ /www.akamai.com [Akella 2003]  A. Akella, S. Seshan, A. Shaikh, “An Empirical Evaluation of Wide-Area Internet Bottlenecks,” Proc.
2003 ACM Internet Measurement Conference  (Miami, FL, Nov. 2003). [
Akhshabi 2011] S. Akhshabi, A. C. Begen, C. Dovrolis, “An Experimental Evaluation of Rate-Adaptation Algorithms in Adaptive Streaming over HTTP,” Proc.
2011 ACM Multimedia Systems Conf . [
Akyildiz 2010] I. Akyildiz, D. Gutierrex-Estevez, E. Reyes, “The Evolution to 4G Cellular Systems, LTE Advanced,” Physical Communication , Elsevier, 3 (2010), 217–244. [
Albitz 1993] P. Albitz and C. Liu, DNS and BIND , O’Reilly & Associates, Petaluma, CA, 1993. [
Al-Fares 2008]  M. Al-Fares, A. Loukissas, A. Vahdat, “A Scalable, Commodity Data Center Network Architecture,” Proc.
2008 ACM SIGCOMM. [
Amazon 2014] J. Hamilton, “AWS: Innovation at Scale, YouTube video, https://www.youtube.com/ watch?v=JIQETrFC_SQ [Anderson 1995] J. B. Andersen, T. S. Rappaport, S. Yoshida, “Propagation Measurements and Models for Wireless Communications Channels,” IEEE Communications Magazine , (Jan. 1995), pp.
42–49. [
Alizadeh 2010] M. Alizadeh, A. Greenberg, D. Maltz, J. Padhye, P. Patel, B. Prabhakar, S. Sengupta, M. Sridharan. “
Data center TCP (DCTCP),” ACM SIGCOMM 2010 Conference , ACM, New York, NY, USA, pp.
63–74. [
Allman 2011] E. Allman, “The Robustness Principle Reconsidered: Seeking a Middle Ground,” Communications of the ACM , Vol.
54, No.
8 (Aug. 2011), pp.
40–45. [
Appenzeller 2004] G. Appenzeller, I. Keslassy, N. McKeown, “Sizing Router Buffers,” Proc.
2004 ACM SIGCOMM (Portland, OR, Aug. 2004). [
ASO-ICANN 2016]  The Address Supporting Organization homepage, http:/ /www.aso.icann.org [AT&T 2013] “AT&T Vision Alignment Challenge Technology Survey,” AT&T Domain 2.0 Vision White Paper, November 13, 2013.
[Atheros 2016]  Atheros Communications Inc., “Atheros AR5006 WLAN Chipset Product Bulletins,” http://www.atheros.com/pt/AR5006Bulletins.htm [Ayanoglu 1995] E. Ayanoglu, S. Paul, T. F. La Porta, K. K. Sabnani, R. D. Gitlin, “AIRMAIL: A Link-Layer Protocol for Wireless Networks,” ACM ACM/Baltzer Wireless Networks Journal , 1: 47–60, Feb. 1995. [
Bakre 1995]  A. Bakre, B. R. Badrinath, “I-TCP: Indirect TCP for Mobile Hosts,” Proc.
1995 Int.
Conf.
on Distributed Computing Systems (ICDCS)  (May 1995), pp.
136–143. [
Balakrishnan 1997]  H. Balakrishnan, V. Padmanabhan, S. Seshan, R. Katz, “A Comparison of Mechanisms for Improving TCP Performance Over Wireless Links,” IEEE/ACM Transactions on Networking  Vol.
5, No.
6 (Dec. 1997). [
Balakrishnan 2003]  H. Balakrishnan, F. Kaashoek, D. Karger, R. Morris, I. Stoica, “Looking Up Data in P2P Systems,” Communications of the ACM , Vol.
46, No.
2 (Feb. 2003), pp.
43–48. [
Baldauf 2007] M. Baldauf, S. Dustdar, F. Rosenberg, “A Survey on Context-Aware Systems,” Int.
J. Ad Hoc and Ubiquitous Computing , Vol.
2, No.
4 (2007), pp.
263–277. [
Baran 1964]  P. Baran, “On Distributed Communication Networks,” IEEE Transactions on Communication Systems , Mar. 1964.
Rand Corporation Technical report with the same title (Memorandum RM-3420-PR, 1964).
http:/ /www.rand.org/ publications/ RM/RM3420/ [Bardwell 2004] J. Bardwell, “You Believe You Understand What You Think I Said . . .
The Truth About 802.11 Signal and Noise Metrics: A Discussion Clarifying Often- Misused 802.11 WLAN Terminologies,” http:/ /www.connect802.com/download/ techpubs/2004/you_believe_D100201.pdf [Barford 2009]  P. Barford, N. Duffield, A. Ron, J. Sommers, “Network Performance Anomaly Detection and Localization,” Proc.
2009 IEEE INFOCOM (Apr. 2009). [
Baronti 2007] P. Baronti, P. Pillai, V. Chook, S. Chessa, A. Gotta, Y. Hu, “Wireless Sensor Networks: A Survey on the State of the Art and the 802.15.4 and ZigBee Standards,” Computer Communications , Vol.
30, No.
7 (2007), pp.
1655–1695. [
Baset 2006]  S. A. Basset and H. Schulzrinne, “An Analysis of the Skype Peer-to-Peer Internet Telephony Protocol,” Proc.
2006 IEEE INFOCOM (Barcelona, Spain, Apr. 2006). [
BBC 2001]  BBC news online “A Small Slice of Design,” Apr. 2001, http:/ /news.bbc.co.uk/ 2/hi/science/nature/1264205.stm [Beheshti 2008]  N. Beheshti, Y. Ganjali, M. Ghobadi, N. McKeown, G. Salmon, “Experimental Study of Router Buffer Sizing,” Proc.
ACM Internet Measurement Conference (Oct. 2008, Vouliagmeni, Greece).
[Bender 2000]  P. Bender, P. Black, M. Grob, R. Padovani, N. Sindhushayana, A. Viterbi, “CDMA/HDR: A Bandwidth-Efficient High-Speed Wireless Data Service for Nomadic Users,” IEEE Commun.
Mag.,
Vol.
38, No.
7 (July 2000), pp.
70–77. [
Berners-Lee 1989]  T. Berners-Lee, CERN, “Information Management: A Proposal,” Mar. 1989, May 1990.
http:/ /www.w3.org/ History/1989/proposal .html [Berners-Lee 1994]  T. Berners-Lee, R. Cailliau, A. Luotonen, H. Frystyk Nielsen, A. Secret, “The World-Wide Web,” Communications of the ACM , Vol.
37, No.
8 (Aug. 1994), pp.
76–82. [
Bertsekas 1991]  D. Bertsekas, R. Gallagher, Data Networks, 2nd Ed. ,
Prentice Hall, Englewood Cliffs, NJ, 1991. [
Biersack 1992]  E. W. Biersack, “Performance Evaluation of Forward Error Correction in ATM Networks,” Proc.
1999 ACM SIGCOMM (Baltimore, MD, Aug. 1992), pp.
248–257. [
BIND 2016]  Internet Software Consortium page on BIND, http://www.isc.org/bind.html [Bisdikian 2001] C. Bisdikian, “An Overview of the Bluetooth Wireless Technology,” IEEE Communications Magazine , No.
12 (Dec. 2001), pp.
86–94. [
Bishop 2003] M. Bishop, Computer Security: Art and Science , Boston: Addison Wesley, Boston MA, 2003. [
Black 1995]  U. Black, ATM Volume I: Foundation for Broadband Networks , Prentice Hall, 1995. [
Black 1997]  U. Black, ATM Volume II: Signaling in Broadband Networks , Prentice Hall, 1997. [
Blumenthal 2001] M. Blumenthal, D. Clark, “Rethinking the Design of the Internet: The End-to-end Arguments vs. the Brave New World,” ACM Transactions on Internet Technology , Vol.
1, No.
1 (Aug. 2001), pp.
70–109. [
Bochman 1984]  G. V. Bochmann, C. A. Sunshine, “Formal Methods in Communication Protocol Design,” IEEE Transactions on Communications , Vol.
28, No.
4 (Apr. 1980) pp.
624–631. [
Bolot 1996] J-C. Bolot, A. Vega-Garcia, “Control Mechanisms for Packet Audio in the Internet,” Proc.
1996 IEEE INFOCOM, pp.
232–239. [
Bosshart 2013]  P. Bosshart, G. Gibb, H. Kim, G. Varghese, N. McKeown, M. Izzard, F. Mujica, M. Horowitz, “Forwarding Metamorphosis: Fast Programmable Match-Action Processing in Hardware for SDN,” ACM SIGCOMM Comput.
Commun.
Rev. 43, 4 (Aug. 2013), 99–110 .
[Bosshart 2014]  P. Bosshart, D. Daly, G. Gibb, M. Izzard, N. McKeown, J. Rexford, C. Schlesinger, D. Talayco, A. Vahdat, G. Varghese, D. Walker, “P4: Programming Protocol-Independent Packet Processors,” ACM SIGCOMM Comput.
Commun.
Rev. 44, 3 (July 2014), pp.
87–95. [
Brakmo 1995]  L. Brakmo, L. Peterson, “TCP Vegas: End to End Congestion Avoidance on a Global Internet,” IEEE Journal of Selected Areas in Communications , Vol.
13, No.
8 (Oct. 1995), pp.
1465–1480. [
Bryant 1988]  B. Bryant, “Designing an Authentication System: A Dialogue in Four Scenes,” http:/ /web.mit.edu/ kerberos/www/ dialogue.html [Bush 1945] V. Bush, “As We May Think,” The Atlantic Monthly , July 1945.
http:/ /www.theatlantic.com/ unbound/ flashbks/computer/bushf.htm [Byers 1998]  J. Byers, M. Luby, M. Mitzenmacher, A. Rege, “A Digital Fountain Approach to Reliable Distribution of Bulk Data,” Proc.
1998 ACM SIGCOMM (Vancouver, Canada, Aug. 1998), pp.
56–67. [
Caesar 2005a]  M. Caesar, D. Caldwell, N. Feamster, J. Rexford, A. Shaikh, J. van der Merwe, “Design and implementation of a Routing Control Platform,” Proc.
Networked Systems Design and Implementation  (May 2005). [
Caesar 2005b]  M. Caesar, J. Rexford, “BGP Routing Policies in ISP Networks,” IEEE Network Magazine , Vol.
19, No.
6 (Nov. 2005). [
Caldwell 2012] C. Caldwell, “The Prime Pages,” http:/ /www.utm.edu/ research/ primes/prove [Cardwell 2000] N. Cardwell, S. Savage, T. Anderson, “Modeling TCP Latency,” Proc.
2000 IEEE INFOCOM (Tel-Aviv, Israel, Mar. 2000). [
Casado 2007]  M. Casado, M. Freedman, J. Pettit, J. Luo, N. McKeown, S. Shenker, “Ethane: Taking Control of the Enterprise,” Proc.
ACM SIGCOMM ’07, New York, pp.
1–12.
See also IEEE/ACM Trans.
Networking, 17, 4 (Aug. 2007), pp.
270–1283. [
Casado 2009]  M. Casado, M. Freedman, J. Pettit, J. Luo, N. Gude, N. McKeown, S. Shenker, “Rethinking Enterprise Network Control,” IEEE/ACM Transactions on Networking (ToN) , Vol.
17, No.
4 (Aug. 2009), pp.
1270–1283. [
Casado 2014]  M. Casado, N. Foster, A. Guha, “Abstractions for Software-Defined Networks,” Communications of the ACM , Vol.
57 No.
10, (Oct. 2014), pp.
86–95. [
Cerf 1974]  V. Cerf, R. Kahn, “A Protocol for Packet Network Interconnection,” IEEE Transactions on Communications Technology , Vol.
COM-22, No.
5, pp.
627–641. [
CERT 2001–09 ] CERT, “Advisory 2001–09: Statistical Weaknesses in TCP/IP Initial Sequence Numbers,” http:/ /www.cert.org/ advisories/CA-2001-09.html
[CERT 2003–04]  CERT, “CERT Advisory CA-2003-04 MS-SQL Server Worm,” http:/ /www.cert.org/ advisories/CA-2003-04.html [CERT 2016]  CERT, http:/ /www.cert.org [CERT Filtering 2012] CERT, “Packet Filtering for Firewall Systems,” http://www.cert.org/ tech_tips/packet_filtering.html [Cert SYN 1996]  CERT, “Advisory CA-96.21: TCP SYN Flooding and IP Spoofing Attacks,” http:/ /www.cert.org/ advisories/CA-1998-01.html [Chandra 2007]  T. Chandra, R. Greisemer, J. Redstone, “Paxos Made Live: an Engineering Perspective,” Proc.
of 2007 ACM Symposium on Principles of Distributed Computing (PODC), pp.
398–407. [
Chao 2001] H. J. Chao, C. Lam, E. Oki, Broadband Packet Switching Technologies—A Practical Guide to ATM Switches and IP Routers , John Wiley & Sons, 2001. [
Chao 2011] C. Zhang, P. Dunghel, D. Wu, K. W. Ross, “Unraveling the BitTorrent Ecosystem,” IEEE Transactions on Parallel and Distributed Systems , Vol.
22, No.
7 (July 2011). [
Chen 2000] G. Chen, D. Kotz, “A Survey of Context-Aware Mobile Computing Research,” Technical Report TR2000-381 , Dept.
of Computer Science, Dartmouth College, Nov. 2000.
http:/ /www.cs.dartmouth.edu/ reports/TR2000-381.pdf [Chen 2006] K.-T. Chen, C.-Y. Huang, P. Huang, C.-L. Lei, “Quantifying Skype User Satisfaction,” Proc.
2006 ACM SIGCOMM (Pisa, Italy, Sept. 2006). [
Chen 2011] Y. Chen, S. Jain, V. K. Adhikari, Z. Zhang, “Characterizing Roles of Front-End Servers in End-to-End Performance of Dynamic Content Distribution,” Proc.
2011 ACM Internet Measurement Conference  (Berlin, Germany, Nov. 2011). [
Cheswick 2000]  B. Cheswick, H. Burch, S. Branigan, “Mapping and Visualizing the Internet,” Proc.
2000 Usenix Conference  (San Diego, CA, June 2000). [
Chiu 1989] D. Chiu, R. Jain, “Analysis of the Increase and Decrease Algorithms for Congestion Avoidance in Computer Networks,” Computer Networks and ISDN Systems , Vol.
17, No.
1, pp.
1–14.
http:/ /www.cs.wustl.edu/ ~jain/ papers/cong_av.htm [Christiansen 2001]  M. Christiansen, K. Jeffay, D. Ott, F. D. Smith, “Tuning Red for Web Traffic,” IEEE/ACM Transactions on Networking , Vol.
9, No.
3 (June 2001), pp.
249–264. [
Chuang 2005] S. Chuang, S. Iyer, N. McKeown, “Practical Algorithms for Performance Guarantees in Buffered Crossbars,” Proc.
2005 IEEE INFOCOM.
[Cisco 802.11ac 2014]  Cisco Systems, “802.11ac: The Fifth Generation of Wi-Fi,” Technical White Paper, Mar. 2014. [
Cisco 7600 2016]  Cisco Systems, “Cisco 7600 Series Solution and Design Guide,” http:/ /www.cisco.com/ en/US/products/hw/ routers/ps368/prod_technical_ reference09186a0080092246.html [Cisco 8500 2012]  Cisco Systems Inc., “Catalyst 8500 Campus Switch Router Architecture,” http://www.cisco.com/univercd/cc/td/doc/product/l3sw/8540/rel_12_0/w5_6f/softcnfg/1cfg8500.pdf [Cisco 12000 2016]  Cisco Systems Inc., “Cisco XR 12000 Series and Cisco 12000 Series Routers,” http:/ /www.cisco.com/ en/US/products/ps6342/index.html [Cisco 2012]  Cisco 2012, Data Centers, http:/ /www.cisco.com/ go/dce [Cisco 2015]  Cisco Visual Networking Index: Forecast and Methodology, 2014–2019, White Paper, 2015. [
Cisco 6500 2016]  Cisco Systems, “Cisco Catalyst 6500 Architecture White Paper,” http:/ /www.cisco.com/ c/en/us/products/collateral/switches/ catalyst-6500-series- switches/prod_white_paper0900aecd80673385.html [Cisco NAT 2016]  Cisco Systems Inc., “How NAT Works,” http://www.cisco.com/en/US/tech/tk648/tk361/technologies_tech_note09186a0080094831.shtml [Cisco QoS 2016] Cisco Systems Inc., “Advanced QoS Services for the Intelligent Internet,” http://www.cisco.com/warp/public/cc/pd/iosw/ioft/ioqo/tech/qos_wp.htm [Cisco Queue 2016] Cisco Systems Inc., “Congestion Management Overview,” http:/ /www.cisco.com/ en/US/docs/ios/12_2/qos/configuration/ guide/ qcfconmg.html [Cisco SYN 2016]  Cisco Systems Inc., “Defining Strategies to Protect Against TCP SYN Denial of Service Attacks,” http:/ /www.cisco.com/ en/US/tech/tk828/ technologies_tech_note09186a00800f67d5.shtml [Cisco TCAM 2014]  Cisco Systems Inc., “CAT 6500 and 7600 Series Routers and Switches TCAM Allocation Adjustment Procedures,” http:/ /www.cisco.com/ c/en/us/ support/ docs/switches/catalyst-6500-series-switches/ 117712-problemsolution-cat6500-00.html [Cisco VNI 2015] Cisco Systems Inc., “Visual Networking Index,” http:/ /www.cisco.com/ web/solutions/ sp/vni/vni_forecast_highlights/ index.html [Clark 1988]  D. Clark, “The Design Philosophy of the DARPA Internet Protocols,” Proc.
1988 ACM SIGCOMM (Stanford, CA, Aug. 1988).
[Cohen 1977] D. Cohen, “Issues in Transnet Packetized Voice Communication,” Proc.
Fifth Data Communications Symposium  (Snowbird, UT, Sept. 1977), pp.
6–13. [
Cookie Central 2016]  Cookie Central homepage, http:/ /www.cookiecentral.com/  n_cookie_faq.htm [Cormen 2001]  T. H. Cormen, Introduction to Algorithms,  2nd Ed.,
MIT Press, Cambridge, MA, 2001. [
Crow 1997] B. Crow, I. Widjaja, J. Kim, P. Sakai, “IEEE 802.11 Wireless Local Area Networks,” IEEE Communications Magazine  (Sept. 1997), pp.
116–126. [
Cusumano 1998] M. A. Cusumano, D. B. Yoffie, Competing on Internet Time: Lessons from Netscape and Its Battle with Microsoft , Free Press, New York, NY, 1998. [
Czyz 2014]  J. Czyz, M. Allman, J. Zhang, S. Iekel-Johnson, E. Osterweil, M. Bailey, “Measuring IPv6 Adoption,” Proc.
ACM SIGCOMM 2014, ACM, New York, NY, USA, pp.
87–98. [
Dahlman 1998]  E. Dahlman, B. Gudmundson, M. Nilsson, J. Sköld, “UMTS/IMT-2000 Based on Wideband CDMA,” IEEE Communications Magazine  (Sept. 1998), pp.
70–80. [
Daigle 1991] J. N. Daigle, Queuing Theory for Telecommunications , Addison-Wesley, Reading, MA, 1991. [
DAM 2016]  Digital Attack Map, http:/ /www.digitalattackmap.com [Davie 2000]  B. Davie and Y. Rekhter, MPLS: Technology and Applications , Morgan Kaufmann Series in Networking, 2000. [
Davies 2005]  G. Davies, F. Kelly, “Network Dimensioning, Service Costing, and Pricing in a Packet-Switched Environment,” Telecommunications Policy , Vol.
28, No.
4, pp.
391–412. [
DEC 1990]  Digital Equipment Corporation, “In Memoriam: J. C. R. Licklider 1915–1990,” SRC Research Report 61, Aug. 1990.
http://www.memex.org/ licklider.pdf [DeClercq 2002]  J. DeClercq, O. Paridaens, “Scalability Implications of Virtual Private Networks,” IEEE Communications Magazine , Vol.
40, No.
5 (May 2002), pp.
151–157. [
Demers 1990]  A. Demers, S. Keshav, S. Shenker, “Analysis and Simulation of a Fair Queuing Algorithm,” Internetworking: Research and Experience,  Vol.
1, No.
1 (1990), pp.
3–26.
[dhc 2016] IETF Dynamic Host Configuration working group homepage, http:/ /www.ietf.org/ html.charters/dhc-charter.html [Dhungel 2012] P. Dhungel, K. W. Ross, M. Steiner.,
Y. Tian, X. Hei, “Xunlei: Peer-Assisted Download Acceleration on a Massive Scale,” Passive and Active Measurement Conference (PAM) 2012 , Vienna, 2012. [
Diffie 1976] W. Diffie, M. E. Hellman, “New Directions in Cryptography,” IEEE Transactions on Information Theory , Vol IT-22 (1976), pp.
644–654. [
Diggavi 2004] S. N. Diggavi, N. Al-Dhahir, A. Stamoulis, R. Calderbank, “Great Expectations: The Value of Spatial Diversity in Wireless Networks,” Proceedings of the IEEE, Vol.
92, No.
2 (Feb. 2004). [
Dilley 2002] J. Dilley, B. Maggs, J. Parikh, H. Prokop, R. Sitaraman, B. Weihl, “Globally Distributed Content Delivert,” IEEE Internet Computing (Sept.–Oct.
2002). [
Diot 2000] C. Diot, B. N. Levine, B. Lyles, H. Kassem, D. Balensiefen, “Deployment Issues for the IP Multicast Service and Architecture,” IEEE Network, Vol.
14, No.
1 (Jan./Feb.
2000) pp.
78–88. [
Dischinger 2007] M. Dischinger, A. Haeberlen, K. Gummadi, S. Saroiu, “Characterizing residential broadband networks,” Proc.
2007 ACM Internet Measurement Conference , pp.
24–26. [
Dmitiropoulos 2007] X. Dmitiropoulos, D. Krioukov, M. Fomenkov, B. Huffaker, Y. Hyun, K. C. Claffy, G. Riley, “AS Relationships: Inference and Validation,” ACM Computer Communication Review  (Jan. 2007). [
DOCSIS 2011] Data-Over-Cable Service Interface Specifications, DOCSIS 3.0: MAC and Upper Layer Protocols Interface Specification, CM-SP-MULPIv3.0-I16-110623, 2011. [
Dodge 2016] M. Dodge, “An Atlas of Cyberspaces,” http://www.cybergeography.org/ atlas/isp_maps.html [Donahoo 2001] M. Donahoo, K. Calvert, TCP/IP Sockets in C: Practical Guide for Programmers, Morgan Kaufman, 2001. [
DSL 2016] DSL Forum homepage, http:/ /www.dslforum.org/ [Dhunghel 2008] P. Dhungel, D. Wu, B. Schonhorst, K.W. Ross, “A Measurement Study of Attacks on BitTorrent Leechers,” 7th International Workshop on Peer-to-Peer Systems (IPTPS 2008) (Tampa Bay, FL, Feb. 2008). [
Droms 2002]  R. Droms, T. Lemon, The DHCP Handbook  (2nd Edition), SAMS Publishing, 2002.
[Edney 2003] J. Edney and W. A. Arbaugh, Real 802.11 Security: Wi-Fi Protected Access and 802.11i , Addison-Wesley Professional, 2003. [
Edwards 2011] W. K. Edwards, R. Grinter, R. Mahajan, D. Wetherall, “Advancing the State of Home Networking,” Communications of the ACM , Vol.
54, No.
6 (June 2011), pp.
62–71. [
Ellis 1987] H. Ellis, “The Story of Non-Secret Encryption,” http://jya.com/ellisdoc.htm [Erickson 2013]  D. Erickson, “ The Beacon Openflow Controller,” 2nd ACM SIGCOMM Workshop on Hot Topics in Software Defined Networking  (HotSDN ’13).
ACM, New York, NY, USA, pp.
13–18. [
Ericsson 2012]  Ericsson, “The Evolution of Edge,” http://www.ericsson.com/technology/whitepapers/broadband/evolution_of_EDGE.shtml [Facebook 2014] A. Andreyev, “Introducing Data Center Fabric, the Next-Generation Facebook Data Center Network,” https://code.facebook.com/posts/360346274145943/introducing-data-center-fabric-the-next-generation-facebook-data-center-network [Faloutsos 1999] C. Faloutsos, M. Faloutsos, P. Faloutsos, “What Does the Internet Look Like?
Empirical Laws of the Internet Topology,” Proc.
1999 ACM SIGCOMM (Boston, MA, Aug. 1999). [
Farrington 2010] N. Farrington, G. Porter, S. Radhakrishnan, H. Bazzaz, V. Subramanya, Y. Fainman, G. Papen, A. Vahdat, “Helios: A Hybrid Electrical/Optical Switch Architecture for Modular Data Centers,” Proc.
2010 ACM SIGCOMM. [
Feamster 2004]  N. Feamster, H. Balakrishnan, J. Rexford, A. Shaikh, K. van der Merwe, “The Case for Separating Routing from Routers,” ACM SIGCOMM Workshop on Future Directions in Network Architecture , Sept. 2004. [
Feamster 2004]  N. Feamster, J. Winick, J. Rexford, “A Model for BGP Routing for Network Engineering,” Proc.
2004 ACM SIGMETRICS (New York, NY, June 2004). [
Feamster 2005]  N. Feamster, H. Balakrishnan, “Detecting BGP Configuration Faults with Static Analysis,” NSDI (May 2005). [
Feamster 2013]  N. Feamster, J. Rexford, E. Zegura, “The Road to SDN,” ACM Queue, Volume 11, Issue 12, (Dec. 2013). [
Feldmeier 1995]  D. Feldmeier, “Fast Software Implementation of Error Detection Codes,” IEEE/ACM Transactions on Networking , Vol.
3, No.
6 (Dec. 1995), pp.
640–652.
[Ferguson 2013] A. Ferguson, A. Guha, C. Liang, R. Fonseca, S. Krishnamurthi, “Participatory Networking: An API for Application Control of SDNs,” Proceedings ACM SIGCOMM 2013, pp.
327–338. [
Fielding 2000]  R. Fielding, “Architectural Styles and the Design of Network-based Software Architectures,” 2000.
PhD Thesis, UC Irvine, 2000. [
FIPS 1995] Federal Information Processing Standard, “Secure Hash Standard,” FIPS Publication 180-1.
http:/ /www.itl.nist.gov/ fipspubs/ fip180-1.htm [Floyd 1999] S. Floyd, K. Fall, “Promoting the Use of End-to-End Congestion Control in the Internet,” IEEE/ACM Transactions on Networking , Vol.
6, No.
5 (Oct. 1998), pp.
458–472. [
Floyd 2000] S. Floyd, M. Handley, J. Padhye, J. Widmer, “Equation-Based Congestion Control for Unicast Applications,” Proc.
2000 ACM SIGCOMM (Stockholm, Sweden, Aug. 2000). [
Floyd 2001] S. Floyd, “A Report on Some Recent Developments in TCP Congestion Control,” IEEE Communications Magazine  (Apr. 2001). [
Floyd 2016] S. Floyd, “References on RED (Random Early Detection) Queue Management,” http:/ /www.icir.org/ floyd/ red.html [Floyd Synchronization 1994]  S. Floyd, V. Jacobson, “Synchronization of Periodic Routing Messages,” IEEE/ACM Transactions on Networking , Vol.
2, No.
2 (Apr. 1997) pp.
122–136. [
Floyd TCP 1994] S. Floyd, “TCP and Explicit Congestion Notification,” ACM SIGCOMM Computer Communications Review , Vol.
24, No.
5 (Oct. 1994), pp.
10–23. [
Fluhrer 2001] S. Fluhrer, I. Mantin, A. Shamir, “Weaknesses in the Key Scheduling Algorithm of RC4,” Eighth Annual Workshop on Selected Areas in Cryptography (Toronto, Canada, Aug. 2002). [
Fortz 2000] B. Fortz, M. Thorup, “Internet Traffic Engineering by Optimizing OSPF Weights,” Proc.
2000 IEEE INFOCOM (Tel Aviv, Israel, Apr. 2000). [
Fortz 2002] B. Fortz, J. Rexford, M. Thorup, “Traffic Engineering with Traditional IP Routing Protocols,” IEEE Communication Magazine  (Oct. 2002). [
Fraleigh 2003] C. Fraleigh, F. Tobagi, C. Diot, “Provisioning IP Backbone Networks to Support Latency Sensitive Traffic,” Proc.
2003 IEEE INFOCOM (San Francisco, CA, Mar. 2003). [
Frost 1994] J. Frost, “BSD Sockets: A Quick and Dirty Primer,” http://world.std .com/~jimf/papers/sockets/sockets.html
[FTC 2015] Internet of Things: Privacy and Security in a Connected World, Federal Trade Commission, 2015, https://www.ftc.gov/ system/ files/documents/reports/ federal-trade-commission-staff-report-november-2013-workshop-entitled-internet-things-privacy/ 150127iotrpt.pdf [FTTH 2016] Fiber to the Home Council, http:/ /www.ftthcouncil.org/ [Gao 2001] L. Gao, J. Rexford, “Stable Internet Routing Without Global Coordination,” IEEE/ACM Transactions on Networking , Vol.
9, No.
6 (Dec. 2001), pp.
681–692. [
Gartner 2014]  Gartner report on Internet of Things, http:/ /www.gartner.com/  technology/ research/ internet-of-things [Gauthier 1999] L. Gauthier, C. Diot, and J. Kurose, “End-to-End Transmission Control Mechanisms for Multiparty Interactive Applications on the Internet,” Proc.
1999 IEEE INFOCOM (New York, NY, Apr. 1999). [
Gember-Jacobson 2014]  A. Gember-Jacobson, R. Viswanathan, C. Prakash, R. Grandl, J. Khalid, S. Das, A. Akella, “OpenNF: Enabling Innovation in Network Function Control,” Proc.
ACM SIGCOMM 2014, pp.
163–174. [
Goodman 1997] David J. Goodman, Wireless Personal Communications Systems , Prentice-Hall, 1997. [
Google IPv6 2015] Google Inc. “IPv6 Statistics,” https://www.google.com/ intl/en/ipv6/statistics.html [Google Locations 2016]  Google data centers.
http:/ /www.google.com/ corporate/datacenter/locations.html [Goralski 1999] W. Goralski, Frame Relay for High-Speed Networks , John Wiley, New York, 1999. [
Greenberg 2009a]  A. Greenberg, J. Hamilton, D. Maltz, P. Patel, “The Cost of a Cloud: Research Problems in Data Center Networks,” ACM Computer Communications Review  (Jan. 2009). [
Greenberg 2009b] A. Greenberg, N. Jain, S. Kandula, C. Kim, P. Lahiri, D. Maltz, P. Patel, S. Sengupta, “VL2: A Scalable and Flexible Data Center Network,” Proc.
2009 ACM SIGCOMM. [
Greenberg 2011]  A. Greenberg, J. Hamilton, N. Jain, S. Kandula, C. Kim, P. Lahiri, D. Maltz, P. Patel, S. Sengupta, “VL2: A Scalable and Flexible Data Center Network,” Communications of the ACM , Vol.
54, No.
3 (Mar. 2011), pp.
95–104. [
Greenberg 2015] A. Greenberg, “SDN for the Cloud,” Sigcomm 2015 Keynote Address, http:/ /conferences.sigcomm.org/sigcomm/2015/pdf/ papers/keynote.pdf
[Griffin 2012] T. Griffin, “Interdomain Routing Links,” http:/ /www.cl.cam.ac.uk/ ~tgg22/interdomain/ [Gude 2008] N. Gude, T. Koponen, J. Pettit, B. Pfaff, M. Casado, N. McKeown, and S. Shenker, “NOX: Towards an Operating System for Networks,”  ACM SIGCOMM Computer Communication Review, July 2008. [
Guha 2006] S. Guha, N. Daswani, R. Jain, “An Experimental Study of the Skype Peer-to-Peer VoIP System,” Proc.
Fifth Int.
Workshop on P2P Systems  (Santa Barbara, CA, 2006). [
Guo 2005] L. Guo, S. Chen, Z. Xiao, E. Tan, X. Ding, X. Zhang, “Measurement, Analysis, and Modeling of BitTorrent-Like Systems,” Proc.
2005 ACM Internet Measurement Conference . [
Guo 2009] C. Guo, G. Lu, D. Li, H. Wu, X. Zhang, Y. Shi, C. Tian, Y. Zhang, S. Lu, “BCube: A High Performance, Server-centric Network Architecture for Modular Data Centers,” Proc.
2009 ACM SIGCOMM. [
Gupta 2001] P. Gupta, N. McKeown, “Algorithms for Packet Classification,” IEEE Network Magazine , Vol.
15, No.
2 (Mar./Apr.
2001), pp.
24–32. [
Gupta 2014] A. Gupta, L. Vanbever, M. Shahbaz, S. Donovan, B. Schlinker, N. Feamster, J. Rexford, S. Shenker, R. Clark, E. Katz-Bassett, “SDX: A Software Defined Internet Exchange, “ Proc.
ACM SIGCOMM 2014 (Aug. 2014), pp.
551–562. [
Ha 2008]  S. Ha, I. Rhee, L. Xu, “CUBIC: A New TCP-Friendly High-Speed TCP Variant,” ACM SIGOPS Operating System Review, 2008. [
Halabi 2000] S. Halabi, Internet Routing Architectures,  2nd Ed.,
Cisco Press, 2000. [
Hanabali 2005] A. A. Hanbali, E. Altman, P. Nain, “A Survey of TCP over Ad Hoc Networks,” IEEE Commun.
Surveys and Tutorials, Vol.
7, No.
3 (2005), pp.
22–36. [
Hei 2007]  X. Hei, C. Liang, J. Liang, Y. Liu, K. W. Ross, “A Measurement Study of a Large-scale P2P IPTV System,” IEEE Trans.
on Multimedia  (Dec. 2007). [
Heidemann 1997]  J. Heidemann, K. Obraczka, J. Touch, “Modeling the Performance of HTTP over Several Transport Protocols,” IEEE/ACM Transactions on Networking , Vol.
5, No.
5 (Oct. 1997), pp.
616–630. [
Held 2001] G. Held, Data Over Wireless Networks: Bluetooth, WAP, and Wireless LANs , McGraw-Hill, 2001. [
Holland 2001]  G. Holland, N. Vaidya, V. Bahl, “A Rate-Adaptive MAC Protocol for Multi-Hop Wireless Networks,” Proc.
2001 ACM Int.
Conference of Mobile Computing and
Networking (Mobicom01)  (Rome, Italy, July 2001). [
Hollot 2002] C.V. Hollot, V. Misra, D. Towsley, W. Gong, “Analysis and Design of Controllers for AQM Routers Supporting TCP Flows,” IEEE Transactions on Automatic Control , Vol.
47, No.
6 (June 2002), pp.
945–959. [
Hong 2013] C. Hong, S, Kandula, R. Mahajan, M.Zhang, V. Gill, M. Nanduri, R. Wattenhofer, “Achieving High Utilization with Software-driven WAN,” ACM SIGCOMM Conference  (Aug. 2013), pp.15–26. [
Huang 2002] C. Haung, V. Sharma, K. Owens, V. Makam, “Building Reliable MPLS Networks Using a Path Protection Mechanism,” IEEE Communications Magazine , Vol.
40, No.
3 (Mar. 2002), pp.
156–162. [
Huang 2005] Y. Huang, R. Guerin, “Does Over-Provisioning Become More or Less Efficient as Networks Grow Larger?,”
Proc.
IEEE Int.
Conf.
Network Protocols (ICNP) (Boston MA, Nov. 2005). [
Huang 2008] C. Huang, J. Li, A. Wang, K. W. Ross, “Understanding Hybrid CDN-P2P: Why Limelight Needs Its Own Red Swoosh,” Proc.
2008 NOSSDAV, Braunschweig, Germany. [
Huitema 1998]  C. Huitema, IPv6: The New Internet Protocol, 2nd Ed.,
Prentice Hall, Englewood Cliffs, NJ, 1998. [
Huston 1999a] G. Huston, “Interconnection, Peering, and Settlements—Part I,” The Internet Protocol Journal , Vol.
2, No.
1 (Mar. 1999). [
Huston 2004] G. Huston, “NAT Anatomy: A Look Inside Network Address Translators,” The Internet Protocol Journal , Vol.
7, No.
3 (Sept. 2004). [
Huston 2008a] G. Huston, “Confronting IPv4 Address Exhaustion,” http:/ /www.potaroo.net/ ispcol/ 2008-10/ v4depletion.html [Huston 2008b] G. Huston, G. Michaelson, “IPv6 Deployment: Just where are we?”
http:/ /www.potaroo.net/ ispcol/ 2008-04/ ipv6.html [Huston 2011a] G. Huston, “A Rough Guide to Address Exhaustion,” The Internet Protocol Journal,  Vol.
14, No.
1 (Mar. 2011). [
Huston 2011b] G. Huston, “Transitioning Protocols,” The Internet Protocol Journal , Vol.
14, No.
1 (Mar. 2011). [
IAB 2016]  Internet Architecture Board homepage, http:/ /www.iab.org/
[IANA Protocol Numbers 2016]  Internet Assigned Numbers Authority, Protocol Numbers, http:/ /www.iana.org/ assignments/protocol-numbers/ protocol-numbers.xhtml [IBM 1997] IBM Corp., IBM Inside APPN - The Essential Guide to the Next-Generation SNA , SG24-3669-03, June 1997. [
ICANN 2016]  The Internet Corporation for Assigned Names and Numbers homepage, http:/ /www.icann.org [IEEE 802 2016]  IEEE 802 LAN/MAN Standards Committee homepage, http:/ /www.ieee802.org/ [IEEE 802.11 1999]  IEEE 802.11, “1999 Edition (ISO/IEC 8802-11: 1999) IEEE Standards for Information Technology—Telecommunications and Information Exchange Between Systems—Local and Metropolitan Area Network—Specific Requirements—Part 11: Wireless LAN Medium Access Control (MAC) and Physical Layer (PHY) Specification,” http://standards.ieee.org/getieee802/download/802.11-1999.pdf [IEEE 802.11ac 2013]  IEEE, “802.11ac-2013—IEEE Standard for Information technology—Telecommunications and Information Exchange Between Systems—Local and Metropolitan Area Networks—Specific Requirements—Part 11: Wireless LAN Medium Access Control (MAC) and Physical Layer (PHY) Specifications—Amendment 4: Enhancements for Very High Throughput for Operation in Bands Below 6 GHz.” [
IEEE 802.11n 2012] IEEE, “IEEE P802.11—Task Group N—Meeting Update: Status of 802.11n,” http://grouper.ieee.org/ groups/ 802/11/Reports/tgn_update .htm [IEEE 802.15 2012]  IEEE 802.15 Working Group for WPAN homepage, http:/ /grouper.ieee.org/ groups/ 802/15/. [IEEE 802.15.4 2012]  IEEE 802.15 WPAN Task Group 4, http:/ /www.ieee802.org/15/pub/ TG4.html [IEEE 802.16d 2004] IEEE, “IEEE Standard for Local and Metropolitan Area Networks, Part 16: Air Interface for Fixed Broadband Wireless Access Systems,” http:/ / standards.ieee.org/getieee802/ download/ 802.16-2004.pdf [IEEE 802.16e 2005]  IEEE, “IEEE Standard for Local and Metropolitan Area Networks, Part 16: Air Interface for Fixed and Mobile Broadband Wireless Access Systems, Amendment 2: Physical and Medium Access Control Layers for Combined Fixed and Mobile Operation in Licensed Bands and Corrigendum 1,” http:/ / standards.ieee.org/getieee802/ download/ 802.16e-2005.pdf [IEEE 802.1q 2005] IEEE, “IEEE Standard for Local and Metropolitan Area Networks: Virtual Bridged Local Area Networks,” http:/ /standards.ieee.org/ getieee802/ download/ 802.1Q-2005.pdf [IEEE 802.1X] IEEE Std 802.1X-2001 Port-Based Network Access Control, http://standards.ieee.org/reading/ieee/std_public/description/lanman/ 802.1x-2001_desc.html
[IEEE 802.3 2012] IEEE, “IEEE 802.3 CSMA/CD (Ethernet),” http:/ /grouper.ieee.org/ groups/ 802/3/ [IEEE 802.5 2012] IEEE, IEEE 802.5 homepage, http://www.ieee802.org/5/ www8025org/ [IETF 2016] Internet Engineering Task Force homepage, http:/ /www.ietf.org [Ihm 2011] S. Ihm, V. S. Pai, “Towards Understanding Modern Web Traffic,” Proc.
2011 ACM Internet Measurement Conference  (Berlin). [
IMAP 2012] The IMAP Connection, http://www.imap.org/[Intel 2016] Intel Corp., “Intel 710 Ethernet Adapter,” http:/ /www.intel.com/  content/ www/ us/en/ethernet-products/converged-network-adapters/ethernet-xl710 .html [Internet2 Multicast 2012] Internet2 Multicast Working Group homepage, http://www.internet2.edu/multicast/ [ISC 2016] Internet Systems Consortium homepage, http:/ /www.isc.org [ISI 1979] Information Sciences Institute, “DoD Standard Internet Protocol,” Internet Engineering Note 123 (Dec. 1979), http:/ /www.isi.edu/ in-notes/ ien/ ien123.txt [ISO 2016] International Organization for Standardization homepage, International Organization for Standardization, http:/ /www.iso.org/ [ISO X.680 2002] International Organization for Standardization, “X.680: ITU-T Recommendation X.680 (2002) Information Technology—Abstract Syntax Notation One (ASN.1): Specification of Basic Notation,” http:/ /www.itu.int/ ITU-T/ studygroups/ com17/languages/X.680-0207.pdf [ITU 1999] Asymmetric Digital Subscriber Line (ADSL) Transceivers.
ITU-T G.992.1, 1999.[ITU 2003] Asymmetric Digital Subscriber Line (ADSL) Transceivers—Extended Bandwidth ADSL2 (ADSL2Plus).
ITU-T G.992.5, 2003. [
ITU 2005a] International Telecommunication Union, “ITU-T X.509, The Directory: Public-key and attribute certificate frameworks” (Aug. 2005). [
ITU 2006] ITU, “G.993.1: Very High Speed Digital Subscriber Line Transceivers (VDSL),” https://www.itu.int/rec/T-REC-G.993.1-200406-I/en, 2006.[ITU 2015] “Measuring the Information Society Report,” 2015, http:/ /www.itu.int/ en/ITU-D/Statistics/Pages/publications/ mis2015.aspx
[ITU 2012] The ITU homepage, http:/ /www.itu.int/ [ITU-T Q.2931 1995] International Telecommunication Union, “Recommendation Q.2931 (02/95)—Broadband Integrated Services Digital Network (B-ISDN)— Digital Subscriber Signalling System No.
2 (DSS 2)—User-Network Interface (UNI)—Layer 3 Specification for Basic Call/Connection Control.” [
IXP List 2016] List of IXPs, Wikipedia, https://en.wikipedia.org/ wiki/ List_of_ Internet_exchange_points [Iyengar 2015]  J. Iyengar, I. Swett, “QUIC: A UDP-Based Secure and Reliable Transport for HTTP/2,” Internet Draft draft-tsvwg-quic-protocol-00, June 2015. [
Iyer 2008]  S. Iyer, R. R. Kompella, N. McKeown, “Designing Packet Buffers for Router Line Cards,” IEEE Transactions on Networking , Vol.
16, No.
3 (June 2008), pp.
705–717. [
Jacobson 1988] V. Jacobson, “Congestion Avoidance and Control,” Proc.
1988 ACM SIGCOMM (Stanford, CA, Aug. 1988), pp.
314–329. [
Jain 1986] R. Jain, “A Timeout-Based Congestion Control Scheme for Window Flow-Controlled Networks,” IEEE Journal on Selected Areas in Communications SAC-4 , 7 (Oct. 1986). [
Jain 1989] R. Jain, “A Delay-Based Approach for Congestion Avoidance in Interconnected Heterogeneous Computer Networks,” ACM SIGCOMM Computer Communications Review , Vol.
19, No.
5 (1989), pp.
56–71. [
Jain 1994] R. Jain, FDDI Handbook: High-Speed Networking Using Fiber and Other Media , Addison-Wesley, Reading, MA, 1994. [
Jain 1996] R. Jain.
S. Kalyanaraman, S. Fahmy, R. Goyal, S. Kim, “Tutorial Paper on ABR Source Behavior,” ATM Forum/96-1270, Oct. 1996.
http://www.cse.wustl.edu/ ~jain/ atmf/ftp/atm96-1270.pdf [Jain 2013] S. Jain, A. Kumar, S. Mandal, J. Ong, L. Poutievski, A. Singh, S.Venkata, J. Wanderer, J. Zhou, M. Zhu, J. Zolla, U. Hölzle, S. Stuart, A, Vahdat, “B4: Experience with a Globally Deployed Software Defined Wan,” ACM SIGCOMM 2013, pp.
3–14. [
Jaiswal 2003]  S. Jaiswal, G. Iannaccone, C. Diot, J. Kurose, D. Towsley, “Measurement and Classification of Out-of-Sequence Packets in a Tier-1 IP backbone,” Proc.
2003 IEEE INFOCOM. [
Ji 2003] P. Ji, Z. Ge, J. Kurose, D. Towsley, “A Comparison of Hard-State and Soft-State Signaling Protocols,” Proc.
2003 ACM SIGCOMM (Karlsruhe, Germany, Aug. 2003).
[Jimenez 1997]  D. Jimenez, “Outside Hackers Infiltrate MIT Network, Compromise Security,” The Tech, Vol.
117, No 49 (Oct. 1997), p. 1, http://www-tech.mit.edu/ V117/ N49/hackers.49n.html [Jin 2004] C. Jin, D. X. We, S. Low, “FAST TCP: Motivation, Architecture, Algorithms, Performance,” Proc.
2004 IEEE INFOCOM (Hong Kong, Mar. 2004). [
Juniper Contrail 2016] Juniper Networks, “Contrail,” http:/ /www.juniper.net/ us/en/products-services/ sdn/contrail/ [Juniper MX2020 2015]  Juniper Networks, “MX2020 and MX2010 3D Universal Edge Routers,” www.juniper.net/us/en/local/pdf/.../1000417-en.pdf [Kaaranen 2001]  H. Kaaranen, S. Naghian, L. Laitinen, A. Ahtiainen, V. Niemi, Networks: Architecture, Mobility and Services , New York: John Wiley & Sons, 2001. [
Kahn 1967] D. Kahn, The Codebreakers: The Story of Secret Writing , The Macmillan Company, 1967. [
Kahn 1978] R. E. Kahn, S. Gronemeyer, J. Burchfiel, R. Kunzelman, “Advances in Packet Radio Technology,” Proc.
1978 IEEE INFOCOM, 66, 11 (Nov. 1978). [
Kamerman 1997]  A. Kamerman, L. Monteban, “WaveLAN-II: A High– Performance Wireless LAN for the Unlicensed Band,” Bell Labs Technical Journal  (Summer 1997), pp.
118–133. [
Kar 2000]  K. Kar, M. Kodialam, T. V. Lakshman, “Minimum Interference Routing of Bandwidth Guaranteed Tunnels with MPLS Traffic Engineering Applications,” IEEE J. Selected Areas in Communications  (Dec. 2000). [
Karn 1987]  P. Karn, C. Partridge, “Improving Round-Trip Time Estimates in Reliable Transport Protocols,” Proc.
1987 ACM SIGCOMM. [
Karol 1987]  M. Karol, M. Hluchyj, A. Morgan, “Input Versus Output Queuing on a Space-Division Packet Switch,” IEEE Transactions on Communications, Vol.
35, No.
12 (Dec.1987), pp.
1347–1356. [
Kaufman 1995]  C. Kaufman, R. Perlman, M. Speciner, Network Security, Private Communication in a Public World , Prentice Hall, Englewood Cliffs, NJ, 1995. [
Kelly 1998]  F. P. Kelly, A. Maulloo, D. Tan, “Rate Control for Communication Networks: Shadow Prices, Proportional Fairness and Stability,” J. Operations Res.
Soc.,
Vol.
49, No.
3 (Mar. 1998), pp.
237–252. [
Kelly 2003]  T. Kelly, “Scalable TCP: Improving Performance in High Speed Wide Area Networks,” ACM SIGCOMM Computer Communications Review , Volume 33, No.
2 (Apr. 2003), pp.83–91.
[Kilkki 1999] K. Kilkki, Differentiated Services for the Internet , Macmillan Technical Publishing, Indianapolis, IN, 1999. [
Kim 2005]  H. Kim, S. Rixner, V. Pai, “Network Interface Data Caching,” IEEE Transactions on Computers , Vol.
54, No.
11 (Nov. 2005), pp.
1394–1408. [
Kim 2008]  C. Kim, M. Caesar, J. Rexford, “Floodless in SEATTLE: A Scalable Ethernet Architecture for Large Enterprises,” Proc.
2008 ACM SIGCOMM (Seattle, WA, Aug. 2008). [
Kleinrock 1961]  L. Kleinrock, “Information Flow in Large Communication Networks,” RLE Quarterly Progress Report, July 1961. [
Kleinrock 1964]  L. Kleinrock, 1964 Communication Nets: Stochastic Message Flow and Delay , McGraw-Hill, New York, NY, 1964. [
Kleinrock 1975]  L. Kleinrock, Queuing Systems, Vol.
1, John Wiley, New York, 1975. [
Kleinrock 1975b] L. Kleinrock, F. A. Tobagi, “Packet Switching in Radio Channels: Part I—Carrier Sense Multiple-Access Modes and Their Throughput-Delay Characteristics,” IEEE Transactions on Communications , Vol.
23, No.
12 (Dec. 1975), pp.
1400–1416. [
Kleinrock 1976]  L. Kleinrock, Queuing Systems, Vol.
2, John Wiley, New York, 1976. [
Kleinrock 2004]  L. Kleinrock, “The Birth of the Internet,” http://www.lk.cs.ucla.edu/ LK/Inet/birth.html [Kohler 2006] E. Kohler, M. Handley, S. Floyd, “DDCP: Designing DCCP: Congestion Control Without Reliability,” Proc.
2006 ACM SIGCOMM (Pisa, Italy, Sept. 2006). [
Kolding 2003] T. Kolding, K. Pedersen, J. Wigard, F. Frederiksen, P. Mogensen, “High Speed Downlink Packet Access: WCDMA Evolution,” IEEE Vehicular Technology Society News  (Feb. 2003), pp.
4–10. [
Koponen 2010] T. Koponen, M. Casado, N. Gude, J. Stribling, L. Poutievski, M. Zhu, R. Ramanathan, Y. Iwata, H. Inoue, T. Hama, S. Shenker, “Onix: A Distributed Control Platform for Large-Scale Production Networks,” 9th USENIX conference on Operating systems design and implementation  (OSDI’10), pp.
1–6. [
Koponen 2011] T. Koponen, S. Shenker, H. Balakrishnan, N. Feamster, I. Ganichev, A. Ghodsi, P. B. Godfrey, N. McKeown, G. Parulkar, B. Raghavan, J. Rexford, S. Arianfar, D. Kuptsov, “Architecting for Innovation,” ACM Computer Communications Review , 2011. [
Korhonen 2003] J. Korhonen, Introduction to 3G Mobile Communications , 2nd ed.,
Artech House, 2003.
[Koziol 2003] J. Koziol, Intrusion Detection with Snort , Sams Publishing, 2003. [
Kreutz 2015]  D. Kreutz, F.M.V. Ramos, P. Esteves Verissimo, C. Rothenberg, S. Azodolmolky, S. Uhlig, “Software-Defined Networking: A Comprehensive Survey,” Proceedings of the IEEE, Vol.
103, No.
1 (Jan. 2015), pp.
14-76.
This paper is also being updated at https://github.com/ SDN-Survey/ latex/wiki [Krishnamurthy 2001]  B. Krishnamurthy, J. Rexford, Web Protocols and Practice: HTTP/ 1.1, Networking Protocols, and Traffic Measurement , Addison-Wesley, Boston, MA, 2001. [
Kulkarni 2005] S. Kulkarni, C. Rosenberg, “Opportunistic Scheduling: Generalizations to Include Multiple Constraints, Multiple Interfaces, and Short Term Fairness,” Wireless Networks , 11 (2005), 557–569. [
Kumar 2006]  R. Kumar, K.W. Ross, “Optimal Peer-Assisted File Distribution: Single and Multi-Class Problems,” IEEE Workshop on Hot Topics in Web Systems and Technologies  (Boston, MA, 2006). [
Labovitz 1997] C. Labovitz, G. R. Malan, F. Jahanian, “Internet Routing Instability,” Proc.
1997 ACM SIGCOMM (Cannes, France, Sept. 1997), pp.
115–126. [
Labovitz 2010] C. Labovitz, S. Iekel-Johnson, D. McPherson, J. Oberheide, F. Jahanian, “Internet Inter-Domain Traffic,” Proc.
2010 ACM SIGCOMM. [
Labrador 1999] M. Labrador, S. Banerjee, “Packet Dropping Policies for ATM and IP Networks,” IEEE Communications Surveys , Vol.
2, No.
3 (Third Quarter 1999), pp.
2–14. [
Lacage 2004]  M. Lacage, M.H. Manshaei, T. Turletti, “IEEE 802.11 Rate Adaptation: A Practical Approach,” ACM Int.
Symposium on Modeling, Analysis, and Simulation of Wireless and Mobile Systems (MSWiM)  (Venice, Italy, Oct. 2004). [
Lakhina 2004] A. Lakhina, M. Crovella, C. Diot, “Diagnosing Network-Wide Traffic Anomalies,” Proc.
2004 ACM SIGCOMM. [
Lakhina 2005] A. Lakhina, M. Crovella, C. Diot, “Mining Anomalies Using Traffic Feature Distributions,” Proc.
2005 ACM SIGCOMM. [
Lakshman 1997]  T. V. Lakshman, U. Madhow, “The Performance of TCP/IP for Networks with High Bandwidth-Delay Products and Random Loss,” IEEE/ACM Transactions on Networking , Vol.
5, No.
3 (1997), pp.
336–350. [
Lakshman 2004]  T. V. Lakshman, T. Nandagopal, R. Ramjee, K. Sabnani, T. Woo, “The SoftRouter Architecture,” Proc.
3nd ACM Workshop on Hot Topics in Networks (Hotnets-III), Nov. 2004.
[Lam 1980]  S. Lam, “A Carrier Sense Multiple Access Protocol for Local Networks,” Computer Networks , Vol.
4 (1980), pp.
21–32. [
Lamport 1989] L. Lamport, “The Part-Time Parliament,” Technical Report 49, Systems Research Center, Digital Equipment Corp., Palo Alto, Sept. 1989. [
Lampson 1983] Lampson, Butler W. “Hints for computer system design,” ACM SIGOPS Operating Systems Review, Vol.
17, No.
5, 1983. [
Lampson 1996] B. Lampson, “How to Build a Highly Available System Using Consensus,” Proc.
10th International Workshop on Distributed Algorithms  (WDAG ’96), Özalp Babaoglu and Keith Marzullo (Eds.),
Springer-Verlag, pp.
1–17. [
Lawton 2001] G. Lawton, “Is IPv6 Finally Gaining Ground?”
IEEE Computer Magazine  (Aug. 2001), pp.
11–15. [
LeBlond 2011] S. Le Blond, C. Zhang, A. Legout, K. Ross, W. Dabbous.
2011, “I know where you are and what you are sharing: exploiting P2P communications to invade users’ privacy.”
2011 ACM Internet Measurement Conference , ACM, New York, NY, USA, pp.
45–60. [
Leighton 2009] T. Leighton, “Improving Performance on the Internet,” Communications of the ACM , Vol.
52, No.
2 (Feb. 2009), pp.
44–51. [
Leiner 1998] B. Leiner, V. Cerf, D. Clark, R. Kahn, L. Kleinrock, D. Lynch, J. Postel, L. Roberts, S. Woolf, “A Brief History of the Internet,” http://www.isoc.org/internet/history/brief.html [Leung 2006] K. Leung, V. O.K. Li, “TCP in Wireless Networks: Issues, Approaches, and Challenges,” IEEE Commun.
Surveys and Tutorials , Vol.
8, No.
4 (2006), pp.
64–79. [
Levin 2012] D. Levin, A. Wundsam, B. Heller, N. Handigol, A. Feldmann, “Logically Centralized?:
State Distribution Trade-offs in Software Defined Networks,” Proc.
First Workshop on Hot Topics in Software Defined Networks  (Aug. 2012), pp.
1–6. [
Li 2004] L. Li, D. Alderson, W. Willinger, J. Doyle, “A First-Principles Approach to Understanding the Internet’s Router-Level Topology,” Proc.
2004 ACM SIGCOMM (Portland, OR, Aug. 2004). [
Li 2007] J. Li, M. Guidero, Z. Wu, E. Purpus, T. Ehrenkranz, “BGP Routing Dynamics Revisited.”
ACM Computer Communication Review  (Apr. 2007). [
Li 2015] S.Q. Li, “Building Softcom Ecosystem Foundation,” Open Networking Summit, 2015. [
Lin 2001] Y. Lin, I. Chlamtac, Wireless and Mobile Network Architectures , John Wiley and Sons, New York, NY, 2001.
[Liogkas 2006] N. Liogkas, R. Nelson, E. Kohler, L. Zhang, “Exploiting BitTorrent for Fun (but Not Profit),” 6th International Workshop on Peer-to-Peer Systems (IPTPS 2006). [
Liu 2003] J. Liu, I. Matta, M. Crovella, “End-to-End Inference of Loss Nature in a Hybrid Wired/Wireless Environment,” Proc.
WiOpt’03: Modeling and Optimization in Mobile, Ad Hoc and Wireless Networks . [
Locher 2006] T. Locher, P. Moor, S. Schmid, R. Wattenhofer, “Free Riding in BitTorrent is Cheap,” Proc.
ACM HotNets 2006 (Irvine CA, Nov. 2006). [
Lui 2004] J. Lui, V. Misra, D. Rubenstein, “On the Robustness of Soft State Protocols,” Proc.
IEEE Int.
Conference on Network Protocols (ICNP ’04 ), pp.
50–60. [
Mahdavi 1997] J. Mahdavi, S. Floyd, “TCP-Friendly Unicast Rate-Based Flow Control,” unpublished note (Jan. 1997). [
MaxMind 2016] http://www.maxmind.com/ app/ip-location [Maymounkov 2002]  P. Maymounkov, D. Mazières. “
Kademlia: A Peer-to-Peer Information System Based on the XOR Metric.”
Proceedings of the 1st International Workshop on Peerto-Peer Systems (IPTPS ‘02)  (Mar. 2002), pp.
53–65. [
McKeown 1997a]  N. McKeown, M. Izzard, A. Mekkittikul, W. Ellersick, M. Horowitz, “The Tiny Tera: A Packet Switch Core,” IEEE Micro Magazine  (Jan.–Feb.
1997). [
McKeown 1997b] N. McKeown, “A Fast Switched Backplane for a Gigabit Switched Router,” Business Communications Review,  Vol.
27, No.
12.
http://tiny- tera.stanford.edu/~nickm/papers/cisco_fasts_wp.pdf [McKeown 2008] N. McKeown, T. Anderson, H. Balakrishnan, G. Parulkar, L. Peterson, J. Rexford, S. Shenker, J. Turner.
2008.
OpenFlow: Enabling Innovation in Campus Networks.
SIGCOMM Comput.
Commun.
Rev. 38, 2 (Mar. 2008), pp.
69–74. [
McQuillan 1980] J. McQuillan, I. Richer, E. Rosen, “The New Routing Algorithm for the Arpanet,” IEEE Transactions on Communications,  Vol.
28, No.
5 (May 1980), pp.
711–719. [
Metcalfe 1976]  R. M. Metcalfe, D. R. Boggs. “
Ethernet: Distributed Packet Switching for Local Computer Networks,” Communications of the Association for Computing Machinery,  Vol.
19, No.
7 (July 1976), pp.
395–404. [
Meyers 2004]  A. Myers, T. Ng, H. Zhang, “Rethinking the Service Model: Scaling Ethernet to a Million Nodes ,” ACM Hotnets Conference , 2004.
[MFA Forum 2016] IP/MPLS Forum homepage, http://www.ipmplsforum.org/ [Mockapetris 1988 ] P. V. Mockapetris, K. J. Dunlap, “Development of the Domain Name System,” Proc.
1988 ACM SIGCOMM (Stanford, CA, Aug. 1988). [
Mockapetris 2005]  P. Mockapetris, Sigcomm Award Lecture, video available at http:/ /www.postel.org/ sigcomm [Molinero-Fernandez 2002] P. Molinaro-Fernandez, N. McKeown, H. Zhang, “Is IP Going to Take Over the World (of Communications)?”
Proc.
2002 ACM Hotnets. [
Molle 1987] M. L. Molle, K. Sohraby, A. N. Venetsanopoulos, “Space-Time Models of Asynchronous CSMA Protocols for Local Area Networks,” IEEE Journal on Selected Areas in Communications,  Vol.
5, No.
6 (1987), pp.
956–968. [
Moore 2001] D. Moore, G. Voelker, S. Savage, “Inferring Internet Denial of Service Activity,” Proc.
2001 USENIX Security Symposium  (Washington, DC, Aug. 2001). [
Motorola 2007] Motorola, “Long Term Evolution (LTE): A Technical Overview,” http://www.motorola.com/staticfiles/Business/Solutions/Industry%20Solutions/Service%20Providers/Wireless%20Operators/LTE/_Document/Static%20Files/6834_MotDoc_New.pdf [Mouly 1992] M. Mouly, M. Pautet, The GSM System for Mobile Communications , Cell and Sys, Palaiseau, France, 1992. [
Moy 1998] J. Moy, OSPF: Anatomy of An Internet Routing Protocol, Addison-Wesley, Reading, MA, 1998. [
Mukherjee 1997]  B. Mukherjee, Optical Communication Networks , McGraw-Hill, 1997. [
Mukherjee 2006]  B. Mukherjee, Optical WDM Networks, Springer, 2006. [
Mysore 2009]  R. N. Mysore, A. Pamboris, N. Farrington, N. Huang, P. Miri, S. Radhakrishnan, V. Subramanya, A. Vahdat, “PortLand: A Scalable Fault-Tolerant Layer 2 Data Center Network Fabric,” Proc.
2009 ACM SIGCOMM. [
Nahum 2002]  E. Nahum, T. Barzilai, D. Kandlur, “Performance Issues in WWW Servers,” IEEE/ACM Transactions on Networking , Vol 10, No.
1 (Feb. 2002). [
Netflix Open Connect 2016] Netflix Open Connect CDN, 2016, https:// openconnect.netflix.com/ [Netflix Video 1] Designing Netflix’s Content Delivery System, D. Fulllager, 2014, https://www.youtube.com/ watch?v=LkLLpYdDINA
[Netflix Video 2] Scaling the Netflix Global CDN, D. Temkin, 2015, https://www .youtube.com/ watch?v=tbqcsHg-Q_o [Neumann 1997]  R. Neumann, “Internet Routing Black Hole,” The Risks Digest: Forum on Risks to the Public in Computers and Related Systems,  Vol.
19, No.
12 (May 1997).
http:/ /catless.ncl.ac.uk/Risks/19.12.html#subj1.1 [Neville-Neil 2009]  G. Neville-Neil, “Whither Sockets?”
Communications of the ACM,  Vol.
52, No.
6 (June 2009), pp.
51–55. [
Nicholson 2006] A Nicholson, Y. Chawathe, M. Chen, B. Noble, D. Wetherall, “Improved Access Point Selection,” Proc.
2006 ACM Mobisys Conference  (Uppsala Sweden, 2006). [
Nielsen 1997]  H. F. Nielsen, J. Gettys, A. Baird-Smith, E. Prud’hommeaux, H. W. Lie, C. Lilley, “Network Performance Effects of HTTP/1.1, CSS1, and PNG,” W3C Document, 1997 (also appears in Proc.
1997 ACM SIGCOM (Cannes, France, Sept 1997), pp.
155–166. [
NIST 2001] National Institute of Standards and Technology, “Advanced Encryption Standard (AES),” Federal Information Processing Standards 197, Nov. 2001, http:/ / csrc.nist.gov/publications/ fips/fips197/fips-197.pdf [NIST IPv6 2015] US National Institute of Standards and Technology, “Estimating IPv6 & DNSSEC Deployment SnapShots,” http:/ /fedv6-deployment.antd.nist.gov/ snap- all.html [Nmap 2012]  Nmap homepage, http:/ /www.insecure.com/ nmap [Nonnenmacher 1998]  J. Nonnenmacher, E. Biersak, D. Towsley, “Parity-Based Loss Recovery for Reliable Multicast Transmission,” IEEE/ACM Transactions on Networking,  Vol.
6, No.
4 (Aug. 1998), pp.
349–361. [
Nygren 2010]  Erik Nygren, Ramesh K. Sitaraman, and Jennifer Sun, “The Akamai Network: A Platform for High-performance Internet Applications,” SIGOPS Oper.
Syst.
Rev. 44, 3 (Aug. 2010), 2–19. [
ONF 2016] Open Networking Foundation, Technical Library, https://www.opennetworking.org/ sdn-resources/technical-library [ONOS 2016] Open Network Operating System (ONOS), “Architecture Guide,” https://wiki.onosproject.org/display/ONOS/Architecture+Guide , 2016. [
OpenFlow 2009]  Open Network Foundation, “OpenFlow Switch Specification 1.0.0, TS-001,” https://www.opennetworking.org/ images/stories/downloads/ sdn- resources/ onf-specifications/ openflow/ openflow-spec-v1.0.0.pdf
[OpenDaylight Lithium 2016]  OpenDaylight, “Lithium,” https://www.opendaylight.org/lithium [OSI 2012] International Organization for Standardization homepage, http:/ /www.iso.org/ iso/en/ISOOnline.frontpage [Osterweil 2012] E. Osterweil, D. McPherson, S. DiBenedetto, C. Papadopoulos, D. Massey, “Behavior of DNS Top Talkers,” Passive and Active Measurement Conference , 2012. [
Padhye 2000]  J. Padhye, V. Firoiu, D. Towsley, J. Kurose, “Modeling TCP Reno Performance: A Simple Model and Its Empirical Validation,” IEEE/ACM Transactions on Networking,  Vol.
8 No.
2 (Apr. 2000), pp.
133–145. [
Padhye 2001]  J. Padhye, S. Floyd, “On Inferring TCP Behavior,” Proc.
2001 ACM SIGCOMM (San Diego, CA, Aug. 2001). [
Palat 2009]  S. Palat, P. Godin, “The LTE Network Architecture: A Comprehensive Tutorial,” in LTE—The UMTS Long Term Evolution: From Theory to Practice.
Also available as a standalone Alcatel white paper . [
Panda 2013] A. Panda, C. Scott, A. Ghodsi, T. Koponen, S. Shenker, “CAP for Networks,” Proc.
ACM HotSDN ’13 , pp.
91–96. [
Parekh 1993]  A. Parekh, R. Gallagher, “A Generalized Processor Sharing Approach to Flow Control in Integrated Services Networks: The Single-Node Case,” IEEE/ACM Transactions on Networking,  Vol.
1, No.
3 (June 1993), pp.
344–357. [
Partridge 1992]  C. Partridge, S. Pink, “An Implementation of the Revised Internet Stream Protocol (ST-2),” Journal of Internetworking: Research and Experience , Vol.
3, No.
1 (Mar. 1992). [
Partridge 1998]  C. Partridge, et al. “
A Fifty Gigabit per second IP Router,” IEEE/ACM Transactions on Networking,  Vol.
6, No.
3 (Jun. 1998), pp.
237–248. [
Pathak 2010]  A. Pathak, Y. A. Wang, C. Huang, A. Greenberg, Y. C. Hu, J. Li, K. W. Ross, “Measuring and Evaluating TCP Splitting for Cloud Services,” Passive and Active Measurement (PAM) Conference  (Zurich, 2010). [
Perkins 1994]  A. Perkins, “Networking with Bob Metcalfe,” The Red Herring Magazine  (Nov. 1994). [
Perkins 1998]  C. Perkins, O. Hodson, V. Hardman, “A Survey of Packet Loss Recovery Techniques for Streaming Audio,” IEEE Network Magazine  (Sept./Oct.
1998), pp.
40–47.
[Perkins 1998b]  C. Perkins, Mobile IP: Design Principles and Practice , Addison-Wesley, Reading, MA, 1998. [
Perkins 2000]  C. Perkins, Ad Hoc Networking , Addison-Wesley, Reading, MA, 2000. [
Perlman 1999]  R. Perlman, Interconnections: Bridges, Routers, Switches, and Internetworking Protocols,  2nd ed.,
Addison-Wesley Professional Computing Series, Reading, MA, 1999. [
PGPI 2016] The International PGP homepage, http:/ /www.pgpi.org [Phifer 2000] L. Phifer, “The Trouble with NAT,” The Internet Protocol Journal,  Vol.
3, No.
4 (Dec. 2000), http://www.cisco.com/warp/public/759/ipj_3-4/ipj_ 3-4_nat.html [Piatek 2007]  M. Piatek, T. Isdal, T. Anderson, A. Krishnamurthy, A. Venkataramani, “Do Incentives Build Robustness in Bittorrent?,”
Proc.
NSDI (2007). [
Piatek 2008]  M. Piatek, T. Isdal, A. Krishnamurthy, T. Anderson, “One Hop Reputations for Peer-to-peer File Sharing Workloads,” Proc.
NSDI (2008). [
Pickholtz 1982] R. Pickholtz, D. Schilling, L. Milstein, “Theory of Spread Spectrum Communication—a Tutorial,” IEEE Transactions on Communications,  Vol.
30, No.
5 (May 1982), pp.
855–884. [
PingPlotter 2016] PingPlotter homepage, http:/ /www.pingplotter.com [Piscatello 1993] D. Piscatello, A. Lyman Chapin, Open Systems Networking,  Addison-Wesley, Reading, MA, 1993. [
Pomeranz 2010]  H. Pomeranz, “Practical, Visual, Three-Dimensional Pedagogy for Internet Protocol Packet Header Control Fields,” https://righteousit.wordpress.com/ 2010/06/27/practical-visual-three-dimensional-pedagogy-for-internet-protocol-packet-header-control-fields/ , June 2010. [
Potaroo 2016] “Growth of the BGP Table–1994 to Present,” http:/ /bgp.potaroo.net/ [PPLive 2012] PPLive homepage, http://www.pplive.com [Qazi 2013] Z. Qazi, C. Tu, L. Chiang, R. Miao, V. Sekar, M. Yu, “SIMPLE-fying Middlebox Policy Enforcement Using SDN,” ACM SIGCOMM Conference (Aug. 2013), pp.
27–38. [
Quagga 2012] Quagga, “Quagga Routing Suite,” http:/ /www.quagga.net/
[Quittner 1998] J. Quittner, M. Slatalla, Speeding the Net: The Inside Story of Netscape and How It Challenged Microsoft,  Atlantic Monthly Press, 1998. [
Quova 2016] www.quova.com [Ramakrishnan 1990]  K. K. Ramakrishnan, R. Jain, “A Binary Feedback Scheme for Congestion Avoidance in Computer Networks,” ACM Transactions on Computer Systems, Vol.
8, No.
2 (May 1990), pp.
158–181. [
Raman 1999]  S. Raman, S. McCanne, “A Model, Analysis, and Protocol Framework for Soft State-based Communication,” Proc.
1999 ACM SIGCOMM (Boston, MA, Aug. 1999). [
Raman 2007]  B. Raman, K. Chebrolu, “Experiences in Using WiFi for Rural Internet in India,” IEEE Communications Magazine,  Special Issue on New Directions in Networking Technologies in Emerging Economies (Jan. 2007). [
Ramaswami 2010]  R. Ramaswami, K. Sivarajan, G. Sasaki, Optical Networks: A Practical  Perspective , Morgan Kaufman Publishers, 2010. [
Ramjee 1994]  R. Ramjee, J. Kurose, D. Towsley, H. Schulzrinne, “Adaptive Playout Mechanisms for Packetized Audio Applications in Wide-Area Networks,” Proc.
1994 IEEE INFOCOM. [
Rao 2011]  A. S. Rao, Y. S. Lim, C. Barakat, A. Legout, D. Towsley, W. Dabbous, “Network Characteristics of Video Streaming Traffic,” Proc.
2011 ACM CoNEXT  (Tokyo). [
Ren 2006]  S. Ren, L. Guo, X. Zhang, “ASAP: An AS-Aware Peer-Relay Protocol for High Quality VoIP,” Proc.
2006 IEEE ICDCS (Lisboa, Portugal, July 2006). [
Rescorla 2001]  E. Rescorla, SSL and TLS: Designing and Building Secure Systems,  Addison-Wesley, Boston, 2001. [
RFC 001] S. Crocker, “Host Software,” RFC 001 (the very first RFC!).[RFC 768] J. Postel, “User Datagram Protocol,” RFC 768, Aug. 1980. [
RFC 791] J. Postel, “Internet Protocol: DARPA Internet Program Protocol Specification,” RFC 791, Sept. 1981.[RFC 792] J. Postel, “Internet Control Message Protocol,” RFC 792, Sept. 1981.
[RFC 793] J. Postel, “Transmission Control Protocol,” RFC 793, Sept. 1981. [
RFC 801] J. Postel, “NCP/TCP Transition Plan,” RFC 801, Nov. 1981. [
RFC 826] D. C. Plummer, “An Ethernet Address Resolution Protocol—or— Converting Network Protocol Addresses to 48-bit Ethernet Address for Transmission on Ethernet Hardware,” RFC 826, Nov. 1982. [
RFC 829] V. Cerf, “Packet Satellite Technology Reference Sources,” RFC 829, Nov. 1982.[RFC 854] J. Postel, J. Reynolds, “TELNET Protocol Specification,” RFC 854, May 1993.[RFC 950] J. Mogul, J. Postel, “Internet Standard Subnetting Procedure,” RFC 950, Aug. 1985. [
RFC 959] J. Postel and J. Reynolds, “File Transfer Protocol (FTP),” RFC 959, Oct. 1985. [
RFC 1034]  P. V. Mockapetris, “Domain Names—Concepts and Facilities,” RFC 1034, Nov. 1987. [
RFC 1035]  P. Mockapetris, “Domain Names—Implementation and Specification,” RFC 1035, Nov. 1987. [
RFC 1058]  C. L. Hendrick, “Routing Information Protocol,” RFC 1058, June 1988. [
RFC 1071]  R. Braden, D. Borman, and C. Partridge, “Computing the Internet Checksum,” RFC 1071, Sept. 1988. [
RFC 1122]  R. Braden, “Requirements for Internet Hosts—Communication Layers,” RFC 1122, Oct. 1989. [
RFC 1123]  R. Braden, ed., “
Requirements for Internet Hosts—Application and Support,” RFC-1123, Oct. 1989. [
RFC 1142]  D. Oran, “OSI IS-IS Intra-Domain Routing Protocol,” RFC 1142, Feb. 1990. [
RFC 1190]  C. Topolcic, “Experimental Internet Stream Protocol: Version 2 (ST-II),” RFC 1190, Oct. 1990. [
RFC 1256]  S. Deering, “ICMP Router Discovery Messages,” RFC 1256, Sept. 1991.
[RFC 1320]  R. Rivest, “The MD4 Message-Digest Algorithm,” RFC 1320, Apr. 1992. [
RFC 1321]  R. Rivest, “The MD5 Message-Digest Algorithm,” RFC 1321, Apr. 1992. [
RFC 1323]  V. Jacobson, S. Braden, D. Borman, “TCP Extensions for High Performance,” RFC 1323, May 1992. [
RFC 1422]  S. Kent, “Privacy Enhancement for Internet Electronic Mail: Part II: Certificate-Based Key Management,” RFC 1422. [
RFC 1546]  C. Partridge, T. Mendez, W. Milliken, “Host Anycasting Service,” RFC 1546, 1993. [
RFC 1584]  J. Moy, “Multicast Extensions to OSPF,” RFC 1584, Mar. 1994. [
RFC 1633]  R. Braden, D. Clark, S. Shenker, “Integrated Services in the Internet Architecture: an Overview,” RFC 1633, June 1994. [
RFC 1636]  R. Braden, D. Clark, S. Crocker, C. Huitema, “Report of IAB Workshop on Security in the Internet Architecture,” RFC 1636, Nov. 1994. [
RFC 1700]  J. Reynolds, J. Postel, “Assigned Numbers,” RFC 1700, Oct. 1994. [
RFC 1752]  S. Bradner, A. Mankin, “The Recommendations for the IP Next Generation Protocol,” RFC 1752, Jan. 1995. [
RFC 1918]  Y. Rekhter, B. Moskowitz, D. Karrenberg, G. J. de Groot, E. Lear, “Address Allocation for Private Internets,” RFC 1918, Feb. 1996. [
RFC 1930]  J. Hawkinson, T. Bates, “Guidelines for Creation, Selection, and Registration of an Autonomous System (AS),” RFC 1930, Mar. 1996. [
RFC 1939]  J. Myers, M. Rose, “Post Office Protocol—Version 3,” RFC 1939, May 1996. [
RFC 1945]  T. Berners-Lee, R. Fielding, H. Frystyk, “Hypertext Transfer Protocol—HTTP/1.0,” RFC 1945, May 1996. [
RFC 2003]  C. Perkins, “IP Encapsulation Within IP,” RFC 2003, Oct. 1996. [
RFC 2004]  C. Perkins, “Minimal Encapsulation Within IP,” RFC 2004, Oct. 1996.
[RFC 2018]  M. Mathis, J. Mahdavi, S. Floyd, A. Romanow, “TCP Selective Acknowledgment Options,” RFC 2018, Oct. 1996. [
RFC 2131]  R. Droms, “Dynamic Host Configuration Protocol,” RFC 2131, Mar. 1997. [
RFC 2136]  P. Vixie, S. Thomson, Y. Rekhter, J. Bound, “Dynamic Updates in the Domain Name System,” RFC 2136, Apr. 1997. [
RFC 2205]  R. Braden, Ed.,
L. Zhang, S. Berson, S. Herzog, S. Jamin, “Resource ReSerVation Protocol (RSVP)—Version 1 Functional Specification,” RFC 2205, Sept. 1997. [
RFC 2210]  J. Wroclawski, “The Use of RSVP with IETF Integrated Services,” RFC 2210, Sept. 1997. [
RFC 2211]  J. Wroclawski, “Specification of the Controlled-Load Network Element Service,” RFC 2211, Sept. 1997. [
RFC 2215]  S. Shenker, J. Wroclawski, “General Characterization Parameters for Integrated Service Network Elements,” RFC 2215, Sept. 1997. [
RFC 2326]  H. Schulzrinne, A. Rao, R. Lanphier, “Real Time Streaming Protocol (RTSP),” RFC 2326, Apr. 1998. [
RFC 2328]  J. Moy, “OSPF Version 2,” RFC 2328, Apr. 1998. [
RFC 2420]  H. Kummert, “The PPP Triple-DES Encryption Protocol (3DESE),” RFC 2420, Sept. 1998. [
RFC 2453]  G. Malkin, “RIP Version 2,” RFC 2453, Nov. 1998. [
RFC 2460]  S. Deering, R. Hinden, “Internet Protocol, Version 6 (IPv6) Specification,” RFC 2460, Dec. 1998. [
RFC 2475]  S. Blake, D. Black, M. Carlson, E. Davies, Z. Wang, W. Weiss, “An Architecture for Differentiated Services,” RFC 2475, Dec. 1998. [
RFC 2578]  K. McCloghrie, D. Perkins, J. Schoenwaelder, “Structure of Management Information Version 2 (SMIv2),” RFC 2578, Apr. 1999. [
RFC 2579]  K. McCloghrie, D. Perkins, J. Schoenwaelder, “Textual Conventions for SMIv2,” RFC 2579, Apr. 1999. [
RFC 2580]  K. McCloghrie, D. Perkins, J. Schoenwaelder, “Conformance Statements for SMIv2,” RFC 2580, Apr. 1999.
[RFC 2597]  J. Heinanen, F. Baker, W. Weiss, J. Wroclawski, “Assured Forwarding PHB Group,” RFC 2597, June 1999. [
RFC 2616]  R. Fielding, J. Gettys, J. Mogul, H. Frystyk, L. Masinter, P. Leach, T. Berners-Lee, R. Fielding, “Hypertext Transfer Protocol—HTTP/1.1,” RFC 2616, June 1999. [
RFC 2663]  P. Srisuresh, M. Holdrege, “IP Network Address Translator (NAT) Terminology and Considerations,” RFC 2663. [
RFC 2702]  D. Awduche, J. Malcolm, J. Agogbua, M. O’Dell, J. McManus, “Requirements for Traffic Engineering Over MPLS,” RFC 2702, Sept. 1999. [
RFC 2827]  P. Ferguson, D. Senie, “Network Ingress Filtering: Defeating Denial of Service Attacks which Employ IP Source Address Spoofing,” RFC 2827, May 2000. [
RFC 2865]  C. Rigney, S. Willens, A. Rubens, W. Simpson, “Remote Authentication Dial In User Service (RADIUS),” RFC 2865, June 2000. [
RFC 3007]  B. Wellington, “Secure Domain Name System (DNS) Dynamic Update,” RFC 3007, Nov. 2000. [
RFC 3022]  P. Srisuresh, K. Egevang, “Traditional IP Network Address Translator (Traditional NAT),” RFC 3022, Jan. 2001. [
RFC 3022]  P. Srisuresh, K. Egevang, “Traditional IP Network Address Translator (Traditional NAT),” RFC 3022, Jan. 2001. [
RFC 3031]  E. Rosen, A. Viswanathan, R. Callon, “Multiprotocol Label Switching Architecture,” RFC 3031, Jan. 2001. [
RFC 3032]  E. Rosen, D. Tappan, G. Fedorkow, Y. Rekhter, D. Farinacci, T. Li, A. Conta, “MPLS Label Stack Encoding,” RFC 3032, Jan. 2001. [
RFC 3168]  K. Ramakrishnan, S. Floyd, D. Black, “The Addition of Explicit Congestion Notification (ECN) to IP,” RFC 3168, Sept. 2001. [
RFC 3209]  D. Awduche, L. Berger, D. Gan, T. Li, V. Srinivasan, G. Swallow, “RSVP-TE: Extensions to RSVP for LSP Tunnels,” RFC 3209, Dec. 2001. [
RFC 3221]  G. Huston, “Commentary on Inter-Domain Routing in the Internet,” RFC 3221, Dec. 2001. [
RFC 3232]  J. Reynolds, “Assigned Numbers: RFC 1700 Is Replaced by an On-line Database,” RFC 3232, Jan. 2002. [
RFC 3234]  B. Carpenter, S. Brim, “Middleboxes: Taxonomy and Issues,” RFC 3234, Feb. 2002.
[RFC 3246]  B. Davie, A. Charny, J.C.R. Bennet, K. Benson, J.Y. Le Boudec, W. Courtney, S. Davari, V. Firoiu, D. Stiliadis, “An Expedited Forwarding PHB (Per-Hop Behavior),” RFC 3246, Mar. 2002. [
RFC 3260]  D. Grossman, “New Terminology and Clarifications for Diffserv,” RFC 3260, Apr. 2002. [
RFC 3261]  J. Rosenberg, H. Schulzrinne, G. Carmarillo, A. Johnston, J. Peterson, R. Sparks, M. Handley, E. Schooler, “SIP: Session Initiation Protocol,” RFC 3261, July 2002. [
RFC 3272]  J. Boyle, V. Gill, A. Hannan, D. Cooper, D. Awduche, B. Christian, W. S. Lai, “Overview and Principles of Internet Traffic Engineering,” RFC 3272, May 2002. [
RFC 3286]  L. Ong, J. Yoakum, “An Introduction to the Stream Control Transmission Protocol (SCTP),” RFC 3286, May 2002. [
RFC 3346]  J. Boyle, V. Gill, A. Hannan, D. Cooper, D. Awduche, B. Christian, W. S. Lai, “Applicability Statement for Traffic Engineering with MPLS,” RFC 3346, Aug. 2002. [
RFC 3390]  M. Allman, S. Floyd, C. Partridge, “Increasing TCP’s Initial Window,” RFC 3390, Oct. 2002. [
RFC 3410]  J. Case, R. Mundy, D. Partain, “Introduction and Applicability Statements for Internet Standard Management Framework,” RFC 3410, Dec. 2002. [
RFC 3414]  U. Blumenthal and B. Wijnen, “User-based Security Model (USM) for Version 3 of the Simple Network Management Protocol (SNMPv3),” RFC 3414, Dec. 2002. [
RFC 3416]  R. Presuhn, J. Case, K. McCloghrie, M. Rose, S. Waldbusser, “Version 2 of the Protocol Operations for the Simple Network Management Protocol (SNMP),” Dec. 2002. [
RFC 3439]  R. Bush, D. Meyer, “Some Internet Architectural Guidelines and Philosophy,” RFC 3439, Dec. 2003. [
RFC 3447]  J. Jonsson, B. Kaliski, “Public-Key Cryptography Standards (PKCS) #1: RSA Cryptography Specifications Version 2.1,” RFC 3447, Feb. 2003. [
RFC 3468]  L. Andersson, G. Swallow, “The Multiprotocol Label Switching (MPLS) Working Group Decision on MPLS Signaling Protocols,” RFC 3468, Feb. 2003. [
RFC 3469]  V. Sharma, Ed.,
F. Hellstrand, Ed, “Framework for Multi-Protocol Label Switching (MPLS)-based Recovery,” RFC 3469, Feb. 2003.
ftp://ftp.rfc-editor.org/in- notes/rfc3469.txt [RFC 3501]  M. Crispin, “Internet Message Access Protocol—Version 4rev1,” RFC 3501, Mar. 2003.
[RFC 3550]  H. Schulzrinne, S. Casner, R. Frederick, V. Jacobson, “RTP: A Transport Protocol for Real-Time Applications,” RFC 3550, July 2003. [
RFC 3588]  P. Calhoun, J. Loughney, E. Guttman, G. Zorn, J. Arkko, “Diameter Base Protocol,” RFC 3588, Sept. 2003. [
RFC 3649]  S. Floyd, “HighSpeed TCP for Large Congestion Windows,” RFC 3649, Dec. 2003. [
RFC 3746]  L. Yang, R. Dantu, T. Anderson, R. Gopal, “Forwarding and Control Element Separation (ForCES) Framework,” Internet, RFC 3746, Apr. 2004. [
RFC 3748]  B. Aboba, L. Blunk, J. Vollbrecht, J. Carlson, H. Levkowetz, Ed., “
Extensible Authentication Protocol (EAP),” RFC 3748, June 2004. [
RFC 3782]  S. Floyd, T. Henderson, A. Gurtov, “The NewReno Modification to TCP’s Fast Recovery Algorithm,” RFC 3782, Apr. 2004. [
RFC 4213]  E. Nordmark, R. Gilligan, “Basic Transition Mechanisms for IPv6 Hosts and Routers,” RFC 4213, Oct. 2005. [
RFC 4271]  Y. Rekhter, T. Li, S. Hares, Ed., “
A Border Gateway Protocol 4 (BGP-4),” RFC 4271, Jan. 2006. [
RFC 4272]  S. Murphy, “BGP Security Vulnerabilities Analysis,” RFC 4274, Jan. 2006. [
RFC 4291]  R. Hinden, S. Deering, “IP Version 6 Addressing Architecture,” RFC 4291, Feb. 2006. [
RFC 4340]  E. Kohler, M. Handley, S. Floyd, “Datagram Congestion Control Protocol (DCCP),” RFC 4340, Mar. 2006. [
RFC 4443]  A. Conta, S. Deering, M. Gupta, Ed., “
Internet Control Message Protocol (ICMPv6) for the Internet Protocol Version 6 (IPv6) Specification,” RFC 4443, Mar. 2006. [
RFC 4346]  T. Dierks, E. Rescorla, “The Transport Layer Security (TLS) Protocol Version 1.1,” RFC 4346, Apr. 2006. [
RFC 4514]  K. Zeilenga, Ed., “
Lightweight Directory Access Protocol (LDAP): String Representation of Distinguished Names,” RFC 4514, June 2006. [
RFC 4601]  B. Fenner, M. Handley, H. Holbrook, I. Kouvelas, “Protocol Independent Multicast—Sparse Mode (PIM-SM): Protocol Specification (Revised),” RFC 4601, Aug. 2006.
[RFC 4632]  V. Fuller, T. Li, “Classless Inter-domain Routing (CIDR): The Internet Address Assignment and Aggregation Plan,” RFC 4632, Aug. 2006. [
RFC 4960]  R. Stewart, ed., “
Stream Control Transmission Protocol,” RFC 4960, Sept. 2007. [
RFC 4987]  W. Eddy, “TCP SYN Flooding Attacks and Common Mitigations,” RFC 4987, Aug. 2007. [
RFC 5000]  RFC editor, “Internet Official Protocol Standards,” RFC 5000, May 2008. [
RFC 5109]  A. Li (ed.), “
RTP Payload Format for Generic Forward Error Correction,” RFC 5109, Dec. 2007. [
RFC 5216]  D. Simon, B. Aboba, R. Hurst, “The EAP-TLS Authentication Protocol,” RFC 5216, Mar. 2008. [
RFC 5218]  D. Thaler, B. Aboba, “What Makes for a Successful Protocol?,”
RFC 5218, July 2008. [
RFC 5321]  J. Klensin, “Simple Mail Transfer Protocol,” RFC 5321, Oct. 2008. [
RFC 5322]  P. Resnick, Ed., “
Internet Message Format,” RFC 5322, Oct. 2008. [
RFC 5348]  S. Floyd, M. Handley, J. Padhye, J. Widmer, “TCP Friendly Rate Control (TFRC): Protocol Specification,” RFC 5348, Sept. 2008. [
RFC 5389]  J. Rosenberg, R. Mahy, P. Matthews, D. Wing, “Session Traversal Utilities for NAT (STUN),” RFC 5389, Oct. 2008. [
RFC 5411]  J Rosenberg, “A Hitchhiker’s Guide to the Session Initiation Protocol (SIP),” RFC 5411, Feb. 2009. [
RFC 5681]  M. Allman, V. Paxson, E. Blanton, “TCP Congestion Control,” RFC 5681, Sept. 2009. [
RFC 5944]  C. Perkins, Ed., “
IP Mobility Support for IPv4, Revised,” RFC 5944, Nov. 2010. [
RFC 6265]  A Barth, “HTTP State Management Mechanism,” RFC 6265, Apr. 2011. [
RFC 6298]  V. Paxson, M. Allman, J. Chu, M. Sargent, “Computing TCP’s Retransmission Timer,” RFC 6298, June 2011.
[RFC 7020]  R. Housley, J. Curran, G. Huston, D. Conrad, “The Internet Numbers Registry System,” RFC 7020, Aug. 2013. [
RFC 7094]  D. McPherson, D. Oran, D. Thaler, E. Osterweil, “Architectural Considerations of IP Anycast,” RFC 7094, Jan. 2014. [
RFC 7323]  D. Borman, R. Braden, V. Jacobson, R. Scheffenegger (ed.), “
TCP Extensions for High Performance,” RFC 7323, Sept. 2014. [
RFC 7540]  M. Belshe, R. Peon, M. Thomson (Eds), “Hypertext Transfer Protocol Version 2 (HTTP/2),” RFC 7540, May 2015. [
Richter 2015]  P. Richter, M. Allman, R. Bush, V. Paxson, “A Primer on IPv4 Scarcity,” ACM SIGCOMM Computer Communication Review,  Vol.
45, No.
2 (Apr. 2015), pp.
21–32. [
Roberts 1967]  L. Roberts, T. Merril, “Toward a Cooperative Network of Time-Shared Computers,” AFIPS Fall Conference  (Oct. 1966). [
Rodriguez 2010] R. Rodrigues, P. Druschel, “Peer-to-Peer Systems,” Communications of the ACM,  Vol.
53, No.
10 (Oct. 2010), pp.
72–82. [
Rohde 2008] Rohde, Schwarz, “UMTS Long Term Evolution (LTE) Technology Introduction,” Application Note 1MA111. [
Rom 1990]  R. Rom, M. Sidi, Multiple Access Protocols: Performance and Analysis,  Springer-Verlag, New York, 1990. [
Root Servers 2016]  Root Servers home page, http:/ /www.root-servers.org/ [RSA 1978]  R. Rivest, A. Shamir, L. Adelman, “A Method for Obtaining Digital Signatures and Public-key Cryptosystems,” Communications of the ACM,  Vol.
21, No.
2 (Feb. 1978), pp.
120–126. [
RSA Fast 2012]  RSA Laboratories, “How Fast Is RSA?”
http:/ /www.rsa.com/rsalabs/node.asp?id=2215 [RSA Key 2012]  RSA Laboratories, “How Large a Key Should Be Used in the RSA Crypto System?”
http:/ /www.rsa.com/rsalabs/node.asp?id=2218 [Rubenstein 1998] D. Rubenstein, J. Kurose, D. Towsley, “Real-Time Reliable Multicast Using Proactive Forward Error Correction,” Proceedings of NOSSDAV ‘98 (Cambridge, UK, July 1998). [
Ruiz-Sanchez 2001] M. Ruiz-Sánchez, E. Biersack, W. Dabbous, “Survey and Taxonomy of IP Address Lookup Algorithms,” IEEE Network Magazine,  Vol.
15, No.
2 (Mar./Apr.
2001), pp.
8–23.
[Saltzer 1984]  J. Saltzer, D. Reed, D. Clark, “End-to-End Arguments in System Design,” ACM Transactions on Computer Systems (TOCS), Vol.
2, No.
4 (Nov. 1984). [
Sandvine 2015] “Global Internet Phenomena Report, Spring 2011,” http://www.sandvine.com/news/globalbroadbandtrends.asp , 2011. [
Sardar 2006]  B. Sardar, D. Saha, “A Survey of TCP Enhancements for Last-Hop Wireless Networks,” IEEE Commun.
Surveys and Tutorials,  Vol.
8, No.
3 (2006), pp.
20–34. [
Saroiu 2002] S. Saroiu, P. K. Gummadi, S. D. Gribble, “A Measurement Study of Peer-to-Peer File Sharing Systems,” Proc.
of Multimedia Computing and Networking (MMCN) (2002). [
Sauter 2014]  M. Sauter, From GSM to LTE-Advanced, John Wiley and Sons, 2014. [
Savage 2015]  D. Savage, J. Ng, S. Moore, D. Slice, P. Paluch, R. White, “Enhanced Interior Gateway Routing Protocol,” Internet Draft, draft-savage-eigrp-04.txt, Aug. 2015. [
Saydam 1996]  T. Saydam, T. Magedanz, “From Networks and Network Management into Service and Service Management,” Journal of Networks and System Management,  Vol.
4, No.
4 (Dec. 1996), pp.
345–348. [
Schiller 2003] J. Schiller, Mobile Communications  2nd edition, Addison Wesley, 2003. [
Schneier 1995]  B. Schneier, Applied Cryptography: Protocols, Algorithms, and Source Code in C,  John Wiley and Sons, 1995. [
Schulzrinne-RTP 2012] Henning Schulzrinne’s RTP site, http:/ /www.cs.columbia .edu/ ~hgs/rtp [Schulzrinne-SIP 2016] Henning Schulzrinne’s SIP site, http:/ /www.cs.columbia.edu/ ~hgs/sip [Schwartz 1977]  M. Schwartz, Computer-Communication Network Design and Analysis , Prentice-Hall, Englewood Cliffs, NJ, 1997. [
Schwartz 1980]  M. Schwartz, Information, Transmission, Modulation, and Noise,  McGraw Hill, New York, NY 1980. [
Schwartz 1982]  M. Schwartz, “Performance Analysis of the SNA Virtual Route Pacing Control,” IEEE Transactions on Communications,  Vol.
30, No.
1 (Jan. 1982), pp.
172–184.
[Scourias 2012]  J. Scourias, “Overview of the Global System for Mobile Communications: GSM.”
http://www.privateline.com/PCS/GSM0.html [SDNHub 2016]  SDNHub, “App Development Tutorials,” http:/ /sdnhub.org/  tutorials/ [Segaller 1998]  S. Segaller, Nerds 2.0.1, A Brief History of the Internet, TV Books, New York, 1998. [
Sekar 2011 ] V. Sekar, S. Ratnasamy, M. Reiter, N. Egi, G. Shi, “ The Middlebox Manifesto: Enabling Innovation in Middlebox Deployment,” Proc.
10th ACM Workshop on Hot Topics in Networks (HotNets),  Article 21, 6 pages. [
Serpanos 2011]  D. Serpanos, T. Wolf, Architecture of Network Systems, Morgan Kaufmann Publishers, 2011. [
Shacham 1990]  N. Shacham, P. McKenney, “Packet Recovery in High-Speed Networks Using Coding and Buffer Management,” Proc.
1990 IEEE INFOCOM (San Francisco, CA, Apr. 1990), pp.
124–131. [
Shaikh 2001] A. Shaikh, R. Tewari, M. Agrawal, “On the Effectiveness of DNS-based Server Selection,” Proc.
2001 IEEE INFOCOM. [
Singh 1999] S. Singh, The Code Book: The Evolution of Secrecy from Mary, Queen of Scotsto Quantum Cryptography,  Doubleday Press, 1999. [
Singh 2015] A. Singh, J. Ong,.
Agarwal, G. Anderson, A. Armistead, R. Banno, S. Boving, G. Desai, B. Felderman, P. Germano, A. Kanagala, J. Provost, J. Simmons, E. Tanda, J. Wanderer, U. Hölzle, S. Stuart, A. Vahdat, “Jupiter Rising: A Decade of Clos Topologies and Centralized Control in Google’s Datacenter Network,” Sigcomm, 2015. [
SIP Software 2016] H. Schulzrinne Software Package site, http:/ /www.cs.columbia.edu/ IRT/software [Skoudis 2004] E. Skoudis, L. Zeltser, Malware: Fighting Malicious Code , Prentice Hall, 2004. [
Skoudis 2006] E. Skoudis, T. Liston, Counter Hack Reloaded: A Step-by-Step Guide to Computer Attacks and Effective Defenses (2nd Edition) , Prentice Hall, 2006. [
Smith 2009] J. Smith, “Fighting Physics: A Tough Battle,” Communications of the ACM,  Vol.
52, No.
7 (July 2009), pp.
60–65. [
Snort 2012] Sourcefire Inc., Snort homepage, http://http://www.snort.org/ [Solensky 1996]  F. Solensky, “IPv4 Address Lifetime Expectations,” in IPng: Internet Protocol Next Generation  (S. Bradner, A. Mankin, ed.),
Addison-Wesley, Reading, MA,
1996. [
Spragins 1991] J. D. Spragins, Telecommunications Protocols and Design,  Addison-Wesley, Reading, MA, 1991. [
Srikant 2004]  R. Srikant, The Mathematics of Internet Congestion Control,  Birkhauser, 2004 [Steinder 2002] M. Steinder, A. Sethi, “Increasing Robustness of Fault Localization Through Analysis of Lost, Spurious, and Positive Symptoms,” Proc.
2002 IEEE INFOCOM. [
Stevens 1990]  W. R. Stevens, Unix Network Programming,  Prentice-Hall, Englewood Cliffs, NJ. [
Stevens 1994]  W. R. Stevens, TCP/IP Illustrated, Vol.
1: The Protocols, Addison-Wesley, Reading, MA, 1994. [
Stevens 1997]  W.R. Stevens, Unix Network Programming, Volume 1: Networking APIs-Sockets and XTI,  2nd edition, Prentice-Hall, Englewood Cliffs, NJ, 1997. [
Stewart 1999]  J. Stewart, BGP4: Interdomain Routing in the Internet,  Addison-Wesley, 1999. [
Stone 1998] J. Stone, M. Greenwald, C. Partridge, J. Hughes, “Performance of Checksums and CRC’s Over Real Data,” IEEE/ACM Transactions on Networking,  Vol.
6, No.
5 (Oct. 1998), pp.
529–543. [
Stone 2000] J. Stone, C. Partridge, “When Reality and the Checksum Disagree,” Proc.
2000 ACM SIGCOMM (Stockholm, Sweden, Aug. 2000). [
Strayer 1992]  W. T. Strayer, B. Dempsey, A. Weaver, XTP: The Xpress Transfer Protocol, Addison-Wesley, Reading, MA, 1992. [
Stubblefield 2002] A. Stubblefield, J. Ioannidis, A. Rubin, “Using the Fluhrer, Mantin, and Shamir Attack to Break WEP,” Proceedings of 2002 Network and Distributed Systems Security Symposium  (2002), pp.
17–22. [
Subramanian 2000] M. Subramanian, Network Management: Principles and Practice,  Addison-Wesley, Reading, MA, 2000. [
Subramanian 2002] L. Subramanian, S. Agarwal, J. Rexford, R. Katz, “Characterizing the Internet Hierarchy from Multiple Vantage Points,” Proc.
2002 IEEE INFOCOM. [
Sundaresan 2006]  K.Sundaresan, K. Papagiannaki, “The Need for Cross-layer Information in Access Point Selection,” Proc.
2006 ACM Internet Measurement Conference (Rio De Janeiro, Oct. 2006).
[Suh 2006] K. Suh, D. R. Figueiredo, J. Kurose and D. Towsley, “Characterizing and Detecting Relayed Traffic: A Case Study Using Skype,” Proc.
2006 IEEE INFOCOM (Barcelona, Spain, Apr. 2006). [
Sunshine 1978] C. Sunshine, Y. Dalal, “Connection Management in Transport Protocols,” Computer Networks,  North-Holland, Amsterdam, 1978. [
Tariq 2008] M. Tariq, A. Zeitoun, V. Valancius, N. Feamster, M. Ammar, “Answering What-If Deployment and Configuration Questions with WISE,” Proc.
2008 ACM SIGCOMM (Aug. 2008). [
TechnOnLine 2012] TechOnLine, “Protected Wireless Networks,” online webcast tutorial, http://www.techonline.com/community/tech_topic/internet/21752 [Teixeira 2006]  R. Teixeira, J. Rexford, “Managing Routing Disruptions in Internet Service Provider Networks,” IEEE Communications Magazine  (Mar. 2006). [
Think 2012] Technical History of Network Protocols, “Cyclades,” http://www.cs.utexas.edu/users/chris/think/Cyclades/index.shtml [Tian 2012] Y. Tian, R. Dey, Y. Liu, K. W. Ross, “China’s Internet: Topology Mapping and Geolocating,” IEEE INFOCOM Mini-Conference 2012  (Orlando, FL, 2012). [
TLD list 2016] TLD list maintained by Wikipedia, https://en.wikipedia.org/ wiki/ List_of_Internet_top-level_domains [Tobagi 1990] F. Tobagi, “Fast Packet Switch Architectures for Broadband Integrated Networks,” Proc.
1990 IEEE INFOCOM,  Vol.
78, No.
1 (Jan. 1990), pp.
133–167. [
TOR 2016] Tor: Anonymity Online, http://www.torproject.org [Torres 2011]  R. Torres, A. Finamore, J. R. Kim, M. M. Munafo, S. Rao, “Dissecting Video Server Selection Strategies in the YouTube CDN,” Proc.
2011 Int.
Conf.
on Distributed Computing Systems . [
Tourrilhes 2014] J. Tourrilhes, P. Sharma, S. Banerjee, J. Petit, “SDN and Openflow Evolution: A Standards Perspective,” IEEE Computer Magazine,  Nov. 2014, pp.
22–29. [
Turner 1988] J. S. Turner, “Design of a Broadcast packet switching network,” IEEE Transactions on Communications,  Vol.
36, No.
6 (June 1988), pp.
734–743. [
Turner 2012] B. Turner, “2G, 3G, 4G Wireless Tutorial,” http://blogs.nmscommunications.com/communications/2008/10/2g-3g-4g-wireless-tutorial.html
[UPnP Forum 2016] UPnP Forum homepage, http:/ /www.upnp.org/ [van der Berg 2008]  R. van der Berg, “How the ’Net Works: An Introduction to Peering and Transit,” http:/ /arstechnica.com/guides/ other/peering-and-transit.ars [van der Merwe 1998]  J. van der Merwe, S. Rooney, I. Leslie, S. Crosby, “The Tempest: A Practical Framework for Network Programmability,” IEEE Network, Vol.
12, No.
3 (May 1998), pp.
20–28. [
Varghese 1997]  G. Varghese, A. Lauck, “Hashed and Hierarchical Timing Wheels: Efficient Data Structures for Implementing a Timer Facility,” IEEE/ACM Transactions on Networking,  Vol.
5, No.
6 (Dec. 1997), pp.
824–834. [
Vasudevan 2012]  S. Vasudevan, C. Diot, J. Kurose, D. Towsley, “Facilitating Access Point Selection in IEEE 802.11 Wireless Networks,” Proc.
2005 ACM Internet Measurement Conference , (San Francisco CA, Oct. 2005). [
Villamizar 1994] C. Villamizar, C. Song. “
High Performance tcp in ansnet,” ACM SIGCOMM Computer Communications Review , Vol.
24, No.
5 (1994), pp.
45–60. [
Viterbi 1995] A. Viterbi, CDMA: Principles of Spread Spectrum Communication,  Addison-Wesley, Reading, MA, 1995. [
Vixie 2009] P. Vixie, “What DNS Is Not,” Communications of the ACM,  Vol.
52, No.
12 (Dec. 2009), pp.
43–47. [
Wakeman 1992]  I. Wakeman, J. Crowcroft, Z. Wang, D. Sirovica, “Layering Considered Harmful,” IEEE Network (Jan. 1992), pp.
20–24. [
Waldrop 2007] M. Waldrop, “Data Center in a Box,” Scientific American  (July 2007). [
Wang 2004] B. Wang, J. Kurose, P. Shenoy, D. Towsley, “Multimedia Streaming via TCP: An Analytic Performance Study,” Proc.
2004 ACM Multimedia Conference  (New York, NY, Oct. 2004). [
Wang 2008] B. Wang, J. Kurose, P. Shenoy, D. Towsley, “Multimedia Streaming via TCP: An Analytic Performance Study ,” ACM Transactions on Multimedia Computing Communications and Applications (TOMCCAP) , Vol.
4, No.
2 (Apr. 2008), p. 16.
1–22. [
Wang 2010] G. Wang, D. G. Andersen, M. Kaminsky, K. Papagiannaki, T. S. E. Ng, M. Kozuch, M. Ryan, “c-Through: Part-time Optics in Data Centers,” Proc.
2010 ACM SIGCOMM. [
Wei 2006] W. Wei, C. Zhang, H. Zang, J. Kurose, D. Towsley, “Inference and Evaluation of Split-Connection Approaches in Cellular Data Networks,” Proc.
Active and Passive Measurement Workshop  (Adelaide, Australia, Mar. 2006).
[Wei 2007] D. X. Wei, C. Jin, S. H. Low, S. Hegde, “FAST TCP: Motivation, Architecture, Algorithms, Performance,” IEEE/ACM Transactions on Networking  (2007). [
Weiser 1991]  M. Weiser, “The Computer for the Twenty-First Century,” Scientific American  (Sept. 1991): 94–10.
http://www.ubiq.com/ hypertext/weiser/ SciAmDraft3.html [White 2011] A. White, K. Snow, A. Matthews, F. Monrose, “Hookt on fon-iks: Phonotactic Reconstruction of Encrypted VoIP Conversations,” IEEE Symposium on Security and Privacy , Oakland, CA, 2011. [
Wigle.net 2016] Wireless Geographic Logging Engine, http:/ /www.wigle.net [Wiki Satellite 2016] Satellite Internet access, https://en.wikipedia.org/ wiki/ Satellite_Internet_access [Wireshark 2016]  Wireshark homepage, http:/ /www.wireshark.org [Wischik 2005] D. Wischik, N. McKeown, “Part I: Buffer Sizes for Core Routers,” ACM SIGCOMM Computer Communications Review , Vol.
35, No.
3 (July 2005). [
Woo 1994] T. Woo, R. Bindignavle, S. Su, S. Lam, “SNP: an interface for secure network programming,” Proc.
1994 Summer USENIX (Boston, MA, June 1994), pp.
45–58. [
Wright 2015] J. Wright, J. Wireless Security Secrets & Solutions,  3e, “Hacking Exposed Wireless,” McGraw-Hill Education, 2015. [
Wu 2005] J. Wu, Z. M. Mao, J. Rexford, J. Wang, “Finding a Needle in a Haystack: Pinpointing Significant BGP Routing Changes in an IP Network,” Proc.
USENIX NSDI (2005). [
Xanadu 2012] Xanadu Project homepage, http:/ /www.xanadu.com/ [Xiao 2000] X. Xiao, A. Hannan, B. Bailey, L. Ni, “Traffic Engineering with MPLS in the Internet,” IEEE Network (Mar./Apr.
2000). [
Xu 2004] L. Xu, K Harfoush, I. Rhee, “Binary Increase Congestion Control (BIC) for Fast Long-Distance Networks,” IEEE INFOCOM 2004, pp.
2514–2524. [
Yavatkar 1994]  R. Yavatkar, N. Bhagwat, “Improving End-to-End Performance of TCP over Mobile Internetworks,” Proc.
Mobile 94 Workshop on Mobile Computing Systems and Applications  (Dec. 1994).
[YouTube 2009] YouTube 2009, Google container data center tour, 2009. [
YouTube 2016] YouTube Statistics, 2016, https://www.youtube.com/ yt/press/ statistics.html [Yu 2004] Yu, Fang, H. Katz, Tirunellai V. Lakshman. “
Gigabit Rate Packet Pattern-Matching Using TCAM,” Proc.
2004 Int.
Conf.
Network Protocols, pp.
174–183. [
Yu 2011] M. Yu, J. Rexford, X. Sun, S. Rao, N. Feamster, “A Survey of VLAN Usage in Campus Networks,” IEEE Communications Magazine,  July 2011. [
Zegura 1997] E. Zegura, K. Calvert, M. Donahoo, “A Quantitative Comparison of Graph-based Models for Internet Topology,” IEEE/ACM Transactions on Networking,  Vol.
5, No.
6, (Dec. 1997).
See also http://www.cc.gatech.edu/projects/gtim  for a software package that generates networks with a transit-stub structure. [
Zhang 1993] L. Zhang, S. Deering, D. Estrin, S. Shenker, D. Zappala, “RSVP: A New Resource Reservation Protocol,” IEEE Network Magazine,  Vol.
7, No.
9 (Sept. 1993), pp.
8–18. [
Zhang 2007] L. Zhang, “A Retrospective View of NAT,” The IETF Journal, Vol.
3, Issue 2 (Oct. 2007). [
Zhang 2015] G. Zhang, W. Liu, X. Hei, W. Cheng, “Unreeling Xunlei Kankan: Understanding Hybrid CDN-P2P Video-on-Demand Streaming,” IEEE Transactions on Multimedia , Vol.
17, No.
2, Feb. 2015. [
Zhang X 2102] X. Zhang, Y. Xu, Y. Liu, Z. Guo, Y. Wang, “Profiling Skype Video Calls: Rate Control and Video Quality,” IEEE INFOCOM  (Mar. 2012). [
Zink 2009] M. Zink, K. Suh, Y. Gu, J. Kurose, “Characteristics of YouTube Network Traffic at a Campus Network—Measurements, Models, and Implications,” Computer Networks, Vol.
53, No.
4, pp.
501–514, 2009.
Index
Network SecurityIntroductionChapter 1
Learning Objective•Introduce the security requirements •confidentiality•integrity•availability•Describe the X.800 security architecture for OSI
Network Security Requirements
Computer NetworkSecurity•Definition: The protection afforded to an automated information system in order toattain the application objectives to preserving the integrity, availability, and confidentialityof information system resources (includes hardware, software, firmware, information/data, and telecommunications).
 -NIST Computer Security Handbook
Confidentiality•Dataconfidentiality: Assures that private or confidential information is not made available or disclosed to unauthorizedindividuals; •Privacy: Assures that individual's control or influence what information related to them may be collected and stored and by whomand to whomthat information may be disclosed•i.e.,
student grade information
Integrity•Data integrity: Assures that data (both stored and in transmitted packets) and programs are changed only in a specifiedand authorizedmanner;•System integrity: Assures that a system performs its intended function in an unimpaired manner, free from deliberate or inadvertent unauthorizedmanipulation of the system•i.e.,
a hospital patient’s allergy information
Availability•Availability: Assures that systems work promptly, and service is not denied to authorized users, ensuring timelyand reliableaccess to and use of information•i.e.,
denial of service attack
Other security requirements•Authenticity•Accountability•tracible data source, •fault isolation•intrusion detection and prevention, •recovery and legal action•system must keep records of their activities to permit later forensic analysis to trace security breaches or to aid in transaction disputes
Question•What security requirements does a blockchain system have achieved?
A Hyperledger
Properties of Random Numbers•Randomness•Uniformity•distribution of bits in the sequence should be uniform •Independence•no one subsequence in the sequence can be inferred from the others •Unpredictable•satisfies the "next-bit test“
Entropy•A measure of uncertainty•In other words, a measure of how unpredictable the outcomes are•High entropy= unpredictable outcomes = desirable in cryptography•The uniform distribution has the highest entropy (every outcome equally likely, e.g. fair coin toss)•Usually measured in bits (so 3 bits of entropy = uniform, random distribution over 8 values) Entropy of an information source
True random numbers generators•Several sources of randomness –natural sources of randomness•decay times of radioactive materials•electrical noise from a resistor or semiconductor•radio channel or audible noise•keyboard timings•disk electrical activity•mouse movements•Physical unclonable function (PUF)•Some are better than others
Combining sources of randomness•Suppose r1, r2, …, rkare random numbers from different sources.
E.g.,r1 = electrical noise from a resistor or semiconductorr2 = sample of hip-hop music on radior3 = clock on computerb = r1⊕r2⊕…⊕rkIf any one of r1, r2, …, rkis truly random, then so is bMany poor sources + 1 good source = good entropy
Pseudorandom Number Generators (PRNGs)•True randomness is expensive•Pseudorandom number generator(PRNGs): An algorithm that uses a little bit of true randomness to generate a lot of random-looking output•Also called deterministic random bit generators(DRBGs)•PRNGs are deterministic: Output is generated according to a set algorithm•However, for an attacker who can’t see the internal state, the output is computationally indistinguishablefrom true randomness
Security attack•Definition: any action that compromises the security of information owned by an organization•Two types of security attacks•Passive attack•Active attack
Passive attack•i.e.eavesdropping on or monitoring of transmissions•Goal: obtain information being transmitted•release of message contents•traffic analysis –a promiscuous sniffer•Very difficult to detect –no alteration of the data•But easy to prevent, why?
Active attack•active attack includes:•replay•Modification of messages•Denial of service•Masquerade
Example: two points communication •Generic types of attacks Eavedropping
Example of modification attack in 6LoWPAN
Example: a group of attackers
Know Your Threat Model•Threat model:A model of who your attacker is and what resources they have•One of the best ways to counter an attacker is to attack their reasons
Example: adversary model•“The adversary is assumed to be intelligent and has limited number of resources.
Before capturing the nodes, it exploits the various vulnerabilities of the networks.
It knows the topology of the network, routing information.
It aims to capture the sink node so as todisrupt the whole traffic.
If it is not able to capture the sink node, it will capture the nearby nodes of the sink.
It tries to disrupt the whole traffic of the network with minimum number of captured nodes.
It is also assumed that the adversary tends to attack more on the nodes closer to the data sink than nodes that are far away”
No class on Wednesday•No class on Wednesday (Sept 18, 2024) due to theJob fair.
Wish you good luck!•Reminder toformaproject group by Sept. 9th, 2024
Project •Task1: OnDemand Professor Q&A Bot •Your task is to build a Q&A Bot over private data that answers questions about the network security course using the open-sourcealternatives to ChatGPTthat can be run on your local machine.
Data privacy can be compromised when sending data over the internet, so it is mandatory to keep it on your local system.
•Your Q&A Bot should be able to understand user questions and provide appropriate answers from the local database, then the citations should be added (must be accomplished) if the response is from the internet, then the web references should be added.
•Train your bot using network security lecture slides, network security textbook, and the Internet.
•By using Wireshark capture data for Step 4's of the LLM workflow shown in Figure 1.
Provide detailed explanations of the trace data.
Also, Maintain a record of Step 1's prompt and its mapping to the trace data in Step 4's.•Task2: Quiz Bot •Your task is to build a quiz bot based on a network security course using the open-source alternatives to ChatGPTthat can be run on your local machine.
Data privacy can be compromised when sending data over the internet, so it is mandatory to keep it on your local system.
•Two types of questions should be offered by the bot: randomly generated questions and specific topic questions and the answers should be pulled from the network security database.
Train your bot using network security quizzes, lecture slides, network securitytextbook, and the Internet.
•The quiz must include multiple-choice questions, true/false questions, and open-ended questions.
•Finally, the bot should be able to provide feedback on the user's answers.
Outline•Review•OSI Security Architecture•Attack model
OSI Security Architecture
OSI Security Architecture•International Telecommunication Union –Telecommunication (ITU-T) recommends X.800•Security Architecture for Open Systems Interconnection (OSI)•Defines a systematic way of defining and providing security requirements•Used by IT managers and vendors in their productsSecurity attacksSecurity mechanismsSecurity servicesa process (or a device incorporating such a process) to detect, prevent, or recover from an attackenhances the security of the data processing systems and the information transfers, such as policies
Other Security Architectures•NIST, Cybersecurity Framework (CSF)•https://www.nist.gov/cyberframework•VIRTUAL WORKSHOP #2| February 15, 2023 (9:00 AM –5:30 PM EST).
Discuss potential significant updates to the CSF •https://www.nist.gov/news-events/events/2023/02/journey-nist-cybersecurity-framework-csf-20-workshop-2•OWASP -Open Web Application Security Project•Web application security•OWASP Application Security Verification Standard (ASVS) -https://owasp.org/www-project-application-security-verification-standard/•OWASP Web Security Testing -https://owasp.org/www-project-web-security-testing-guide/•OWASP foundation
Security attack•Definition: any action that compromises the security of information owned by an organization•Two types of security attacks•Passive attack•Active attack
Feistel Encryption and Decryption
DES encryption •64 bits plaintext•56 bits effectivekey length
DES Weakness•short length key (56 bits) is not secure enough.
Brutal force search takes short time.
Triple DES (3DES) Decrypting with the wrong key will further convolute the output
3DES•Triple DES with three different keys –brute-force complexity 2168•3DES is the FIPS-approved symmetric encryption algorithm•Weakness: slow speed for encryption FIPS – Federal Information Processing Standards.
The United States' Federal Information Processing Standards are publicly announced standards developed by the National Institute of Standards and Technology for use in computer systems by non-military American government agencies and government contractors
AES•clearly a replacement for DES was needed•have theoretical attacks that can break it•have demonstrated exhaustive key search attacks•can use Triple-DES –but slow with small blocks•US NIST issued call for ciphers in 1997•15 candidates accepted in Jun 98 •5 were short-listed in Aug-99 •Rijndael was selected as the AES in Oct-2000•issued as FIPS PUB 197 standard in Nov-2001
Criteria to evaluate AES•General security•Software implementations•Restricted-space environments•Hardware implementations•Attacks on implementations•Encryption versus decryption•Key agility•Other versatility and flexibility•Potential for instruction-level parallelismCryptographic Standards and Guidelines | CSRC (nist.gov)
AES Specification•symmetric block cipher •128-bit data, 128/192/256-bit keys •stronger & faster than Triple-DES •provide full specification & design details •both C & Java implementations•NIST have released all submissions & unclassified analyseshttps://csrc.nist.gov/CSRC/media/Projects/Cryptographic-Standards-and-Guidelines/documents/aes-development/Rijndael-ammended.pdf
The AES Cipher -Rijndael •an iterative rather than feistelcipher•treats data in 4 groups of 4 bytes•operates an entire block in every round•designed to be:•resistant against known-plaintext attacks•speed and code compactness on many CPUs•design simplicity
Rijndael•processes data as 4 groups of 4 bytes (state) = 128 bits•has 10/12/14 rounds in which state undergoes: •byte substitution (1 S-box used on every byte) •shift rows (permute bytes row by row) •mix columns (alter each byte in a column as a function of all of the bytes in the column) •add round key (XOR state with key material) •128-bit keys –10 rounds, 192-bit keys –12 rounds, 256-bit keys –14 rounds
